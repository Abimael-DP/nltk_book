<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <othercredit role="translator">
      <firstname>Tiago</firstname><surname>Tresoldi</surname>
      <contrib>Traduziu o documento para o português brasileiro.</contrib>
    </othercredit>
    <title>Fundamentos de processamento lingüístico: Tokenização de textos e classificação de palavras</title>
    &versiondate; &copyright;
  </articleinfo>

<!------------------------------------------------------------------------>

<section id="intro"><title>Introdução</title>

<para>
Textos são geralmente representados em computadores por meio de arquivos que
contêem uma seqüência potencialmente longa de caracteres. Para a maioria dos
tipos de processamento lingüístico é necessário identificar e categorizar
as palavras de um texto. Esta se revela uma tarefa nada trivial. Neste
capítulo iremos introduzir os <emphasis>tokens</emphasis> como 
blocos constituíntes dos textos e mostraremos como estes últimos podem ser
<emphasis>tokenizados</emphasis> (isto é, como divide-se um texto em tokens).
Em seguida, iremos considerar a categorização dos tokens conforme suas
funções gramaticais e/ou sintáticas, além de realizar uma exploração 
preliminar do Brown Corpus, uma coleção de mais de um milhão de palavras 
em língua inglesa com <emphasis>tags</emphasis> (informações quanto à 
categorização das mesmas). Ao longo do capítulo serão apresentadas algumas 
aplicações interessantes: geração de texto aleatório, classificação automática de
palavras e análise dos verbos modais em textos de diferentes gêneros
(sempre em língua inglesa).
</para>

</section> <!-- Introduction -->

<!------------------------------------------------------------------------>

<section id="token"><title>Tokens: os blocos constituíntes do texto</title>

<para>Como podemos saber que determinada parte de um texto constitui uma
<glossterm>palavra</glossterm> e como podemos representar estas e
as informações a elas associadas em uma máquina? Pode parecer excessivamente
pedante pedir uma definição para "palavra"; não podemos dizer simplesmente
que trata-se de uma seqüência de caracteres com um espaço no início e no fim?
Com o evoluir do estudo, a questão revela-se mais complexa.
Para termos uma noção realista dos problemas, vamos considerar o seguinte texto 
retirado do Wall Street Journal:
<example id="wsj_0034">
<title>Parágrafo 12 do arquivo <filename>wsj_0034</filename></title>
<literallayout><literal>
It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

"I'm very alarmed to see these rich valuations," says Smith Barney's
Mr. Porter.
</literal></literallayout>
</example>
</para>

<para>
Vamos começar pela string <literal>aren't</literal>. De acordo com nossa
definição simplista, ela constitui uma única palavra. Mas vamos
considerar uma situação em que desejemos verificar se todas a todas as palavras
presentes em nosso texto é dada uma definição em um diciónario e este
apresenta definições para <literal lang="en">are</literal> e
<literal lang="en">not</literal>, mas não para 
<literal lang="en">aren't</literal>. Neste caso, seria muito mais
favorável à nossa pesquisa reconhecer em <literal>aren't</literal>
uma contração de duas palavras distintas.

<!--We can make a similar point about
<literal>1992's</literal>. We might want to run a small program over our
text to extract all words which express dates. In this case, we would
get achieve more generality by first stripping oexcept in this case, we
would not expect to find <literal>1992</literal> in a dictionary.--> 

Se tomarmos nossa definição simplista de palavra literalmente (como deveríamos,
caso estejamos pensando em implementar um código), nos depararemos com outros
menores mas sempre reais problemas. Por exemplo, assumindo que nosso arquivo
seja constituído por linhas separadas, como em <xref linkend="wsj_0034"/>,
então todas as palavras que estiverem no início de uma linha não serão
reconhecidas devido à ausência de um espaço antes destas (a menos que
consideremos o caractere de "nova linha" como um espaço). Segundo, de acordo
com nosso critério, símbolos de pontuação irão fazer parte das palavras;
ou seja, uma string como <literal>investors,</literal> também será considerada
uma palavra, já que não há nenhum espaço entre <literal>investors</literal> e
a vírgula que a segue. Conseqüentemente, corremos o risco de não conseguir
reconhecer em <literal>investors,</literal> (com a vírgula final) um token
do mesmo tipo de <literal>investors</literal> (sem a vírgula final). Mais
importante, gostaríamos que que os sinais de pontuação funcionassem como
<quote>first-class citizen</quote> para a tokenização e a para os processamentos
subseqüentes. Por exemplo, podemos desejar implementar uma regra que
especifique que uma palavra seguida por um ponto é provavelmente uma
abreviatura, caso a palavra que a siga inicie-se por letra minúscula.
Porém, para formular este tipo de regra, é necessário que possamos identificar
o ponto como um token isolado.
</para>

<para>
Um desafio um pouco diferente é apresentado por exemplos como os
seguintes (provenientes do corpus MedLine [ref]):
<orderedlist>
<listitem>
<para>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.
</para>
</listitem>
<listitem>
<para>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.
</para>
</listitem>
</orderedlist>
Em casos como estes, nos deparamos com termos que dificilmente seriam encontrados 
em qualquer dicionário ou vocabulário não específico da língua inglesa. Além
disso, não teríamos sucesso ao tentar analisar sintaticamente estas strings
utilizando uma gramática padrão do inglês. Para determinadas finalidades,
seria conveniente agrupar expressões como
<literal>alpha-galactosyl-1,4-beta-galactosyl-specific adhesin</literal>
e <literal>4.53 +/- 0.15%</literal> de forma que fossem percebidas pelo
<emphasis>parser</emphasis> como átomos não analisáveis. Para isto,
deveremos tratá-las como <quote>palavras</quote> únicas para qualquer
finalidade dos processamentos subseqüentes. O resultado geral disto tudo
é que, mesmo que nos atenhamos a uma única língua como o inglês, a questão
de como definir uma palavra depende muito de quais são nossos objetivos.
</para>

<note><para>
Caso nos voltemos para outras línguas que não o inglês, a segmentação das
palavras pode ser ainda mais desafiadora. Por exemplo, na ortografia chinêsa os
caracteres correspondem a morfemas monosilábicos. Muitos morfemas constituem
por si próprios uma palavra, mas várias palavras contêem mais de um morfema;
a maioria é constituída por dois morfemas. Apesar disso, não há nenhuma
representação visual dos limites das palavras em um texto em chinês.
</para></note>

<para>
Vamos dar uma olhada mais detalhada ao <xref linkend="wsj_0034"/>.
Suponhamos que os espaços venham a ser utilizados como delimitadores de
palavras e que a lista de todas as palavras do texto tenha sido colocada
em ordem alfabética; poderíamos esperar por um resultado como o seguinte:
<informalexample>
<programlisting>
120, 1992, And, Barney, But, European, European, Fund, I, It, It, Japanese, Korea, 
Mr, Porter, Smith, South, Spain, a, a, a, ...  
</programlisting>
</informalexample>

<!--
Words according to the Unix <command>wc</command>:
<programlisting><![CDATA[
wc -w wsj_0034_pars12-13
    90 wsj_0034_pars12-13
]]></programlisting>
-->
Se a esta altura pedirmos a um programa que nos informe quantas palavras
há no texto, ele provavelmente fornecerá como resposta 90.
<indexterm><primary>word token</primary></indexterm>
<indexterm><primary>word type</primary></indexterm>
Esta resposta deve-se ao fato de que cada uma das três ocorrências da palavra
<literal>a</literal> está sendo considerada uma palavra isolada. Mas o que
significa dizer que determinado objeto <literal>a</literal> ocorre três
vezes? Estamos na presença de três palavras <literal>a</literal> ou de uma
única palavra repetida três vezes? Podemos responder "ambos os casos",
se traçarmos uma distinção entre um <emphasis>token</emphasis> de palavra e um
<emphasis>tipo</emphasis> de palavra. Um tipo de palavra é algo abstrato;
é aquilo ao qual nos referimos quando dizemos que conhecemos o significado
da palavra <literal>deprecate</literal> ou quando dizemos que as palavras
<literal>barf</literal> e <literal>vomit</literal> são sinônimos. Por outro
lado, um token de palavra é algo que existe no tempo e no espaço. Por exemplo,
podemos nos referir à palavra <literal>grunge</literal> que pronunciei
em Edinburgo no dia 14 de julho de 2003; da mesma forma, podemos dizer que
o segundo token em <xref linkend="wsj_0034"/> é um token do tipo de palavra
<literal>probably</literal> ou que há dois tokens do tipo 
<literal>European</literal> no texto. Em termos mais gerais, podemos dizer
que há 90 tokens de palavras em <xref linkend="wsj_0034"/> mas apenas 76
tipos de palavras.
</para>

<para>
Os termos <glossterm>token</glossterm> e <glossterm>tipo</glossterm>
também podem ser aplicados a outras entidades lingüísticas. Por exemplo, um
<glossterm>token de sentença</glossterm> é uma ocorrência individual de uma
determinada sentença; mas um <glossterm>tipo de sentença</glossterm> é uma
sentença em termos abstratos, sem contexto. Se alguém repete uma sentença,
este alguém pronunciou dois tokens de sentença, mas apenas um tipo de
sentença foi utilizado. Quando a categoria de token ou tipo for óbvia a 
partir do contexto, utilizaremos apenas os termos 
<glossterm>token</glossterm> e <glossterm>tipo</glossterm>.
</para>

<!-- SUBSECTION: Strings -->
<section id="token.representing">
<title>Representando tokens</title>

<para>Quando a linguagem escrita é armazenada em um arquivo de computador,
ela é normalmente representada por meio de uma seqüência ou
<glossterm>string</glossterm> de caracteres. Isto é, em um arquivo de texto
padrão, as palavras são strings, as sentenças são strings e o próprio
texto não passa, na verdade, de uma longa string. Os caracteres de uma string
não precisam ser necessariamente alfanuméricos; estas também podem
incluir caracteres especiais que representem os espaços, as tabulações,
os sinais de nova linha, etc.</para>

<para>Muito do processamento computacional é realizado acima do plano dos
caracteres. Quando compilamos uma linguagem de programação, por exemplo, o
compilador espera que o <emphasis>input</emphasis> (os dados de entrada) seja
uma seqüência de tokens com os quais ele seja capaz de lidar; por exemplo,
as classes dos identificadores, constantes textuais e alfanuméricas.
De forma análoga, um parser espera que seu input seja uma seqüência de tokens
de palavras e não uma seqüência de caracteres individuais. Em sua forma
mais simples, portanto, a <glossterm>tokenização</glossterm> de um texto
envolve a busca pelas posições da string que contêem os chamados
"caracteres em branco" (espaços, tabulações ou sinais de nova linha) ou 
sinais de pontuação específicos, separando a string em tokens nestas 
posições. Por exemplo, suponhamos que estejamos trabalhando com um arquivo 
que contenha as duas linhas seguintes:
</para>
<programlisting>
<![CDATA[
The cat climbed
the tree.
]]>
</programlisting>
<para>Do ponto de vista do parser, este arquivo é meramente uma string
de caracteres:</para>
<programlisting>
'The&blank;cat&blank;climbed\n&blank;the&blank;tree.'
</programlisting>
<para>Notem que utilizamos apóstrofos para delimitar as strings,
"&blank;" para representar espaços em branco e a expressão "\n" para 
representar um sinal de nova linha.</para>

<para>
Como acabamos de dizer, para tokenizar este texto e permitir sua utilização
pelo parser é necessário indicar explicitamente quais substrings são palavras.
Uma forma conveniente de se fazer isto em Python é dividir a string em uma
<emphasis>lista</emphasis> de palavras, onde cada palavra é uma string, como
em <literal>'dog'</literal>.<footnote><para>Dissemos que é 
<quote>conveniente</quote> pois o Python facilita a interação por uma lista, 
processando seus itens um a um.</para> </footnote>

Em Python, uma lista é representada como uma série de objetos (neste caso,
strings) separados por vírgulas, delimitada por conchetes:</para>

<programlisting><![CDATA[
>>> words = ['the', 'cat', 'climbed', 'the', 'tree']
>>> words
['the', 'cat', 'climbed', 'the', 'tree']
]]></programlisting>

<para>
Note que introduzimos uma nova variável, chamada <literal>words</literal>,
à qual é atribuída a lista e que entramos com o nome da variável em uma nova
linha para verificar seu conteúdo. De agora em diante, iremos pular esta última
etapa na listagem de nossos programas de exemplo e simplesmente apresentar
o resultado, como a seguir:
 </para>
<programlisting><![CDATA[
>>> words = ['the', 'cat', 'climbed', 'the', 'tree']
['the', 'cat', 'climbed', 'the', 'tree']
]]></programlisting>

<para>Em resumo, acabamos de mostrar como, em sua forma mais simples, a
tokenização de um texto pode ser realizada convertendo uma única string
representando o texto em uma lista de strings, onde cada elemento
representa uma palavra.</para>

<para>Apesar disto, há alguns problemas com nossa visão simplista em que
um token de palavra é considerado uma mera seqüencia de caracteres. Primeiro,
esta visão torna difícil associar outras informações (como a classe
gramatical da palavra) ao token. Segundo, ela não nos permite distinguir
entre tipos de palavras e tokens de palavras. Mostraremos agora como podemos
solucionar ambos os problemas.</para>

<para>
No NLTK, os tokens de palavras são normalmente representados como membros da classe
<ulink url="&refdoc;/nltk.token.Token-class.html"><literal>Token</literal></ulink>.
Os <literal>token</literal>s podem ser criados a partir de strings de palavras
utilizando-se o <ulink url="&refdoc;/nltk.token.Token-class.html#__init__">construtor
de <literal>token</literal>s</ulink>, que é definido no módulo
<ulink url="&refdoc;/nltk.token-module.html"><literal>nltk.token</literal></ulink>.
Eis um exemplo simples da utilização deste construtor.
</para>

<programlisting><![CDATA[
>>> from nltk.token import * 
>>> my_word_type = 'dog' 
'dog'
>>> first_word_token = Token(TEXT=my_word_type) 
<dog>
>>> second_word_token = Token(TEXT='cat')
<cat>
]]></programlisting>

<para>Para o leitor lingüisticamente orientado e que está familiarizado
com gramáticas baseadas em features (propriedades), pode ser útil
pensar em um <literal>Token</literal> como um tipo de estrututura
atributo/valor. Em outras palavras, a expressão <literal>Token(TEXT='cat')</literal>
nos diz que construímos um objeto <literal>Token</literal> que possui o
atributo <literal>TEXT</literal> e que o valor deste atributo é a string
<literal>'cat'</literal>. Intuitivamente, pensamos no atributo
<literal>TEXT</literal> como aquele que define o tipo do token. Para o
leitor computacionalmente orientado, um <literal>Token</literal> pode
ser pensado como um tipo de diciónario da linguagem Python; veja [XREF].
A vantagem em trabalhar com tokens desta forma é que podemos facilmente
associar uma série de propriedades adicionais a um
<literal>Token</literal> &mdash; a escolha dos atributos será determinada
pelas necessidades particulares de cada pesquisa. Por exemplo, podemos
desejar especificar atributos para uma <emphasis>tag</emphasis> (informação)
que se refira à classe gramatical ou que seja uma referência semântica:
</para>

<programlisting><![CDATA[
>>> my_word_token['TAG'] = 'Noun' 
'Noun'
>>> my_word_token['REFERENT'] = 'entity123' 
'entity123'
]]></programlisting>

<para>
Nada disto, porém, nos permite distinguir duas ocorrências de um mesmo
tipo de palavra. Para isto, utilizamos as <emphasis>localizações</emphasis>,
que serão vistas a seguir.
</para>
</section><!--Representing Tokens-->

<section id="token.locations">
<title> Localicações </title>

    <para> <glossterm>Localizações de índice</glossterm> ('index locations',
    em inglês) especificam posições
    dentro de um texto. A localização da quarta palavra de um texto seria
    especificada da seguinte forma:</para>

<programlisting><![CDATA[
>>> from nltk.token import * 
>>> WordIndexLocation(3) 
[3w]
]]></programlisting>

<para> <glossterm>Localizações de intervalo</glossterm> ('span locations',
      em inglês) especificam partes de textos utilizando-se um 
      <glossterm>índice inicial</glossterm> e um
      <glossterm>índice final</glossterm>.  A localização de uma seqüência
      de texto que se inicie no caractere de índice
      <replaceable>s</replaceable> e que termine no caractere de índice
      <replaceable>e</replaceable> é representada da forma
      <literal>@[</literal><replaceable>s</replaceable><literal>:</literal><replaceable>e</replaceable><literal>c]</literal>.
      Esta localização especifica uma região do texto que inicia em
      <replaceable>s</replaceable> e inclui todos os caracteres até (mas não
      incluíndo) <replaceable>e</replaceable>. As
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html"
      ><literal>CharSpanLocations</literal></ulink> são criadas por meio do
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html#__init__"
      >construtor de <literal>CharSpanLocation</literal>s</ulink>, que é
      definido no módulo <ulink url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink>:</para>

<programlisting><![CDATA[
>>> from nltk.token import * 
>>> my_loc = CharSpanLocation(1, 5) 
[1:5c]
]]></programlisting>

<para>Note que uma localização textual <emphasis>não</emphasis> inclui
o texto de seu índice final. Esta convenção pode parecer pouco intuitiva à
primeira vista, mas ela apresenta uma série de vantagens. Primeiro, está em
conformidade com a notação de intervalos da linguagem Python (por exemplo,
<literal>x[1:3]</literal> especifica os elementos 1 e 2 de
<literal>x</literal>).

<footnote><para>Ao contrário dos intervalos da linguagem Python, 
as localizações textuais <emphasis>não</emphasis> permitem a utilização de
índices negativos.</para></footnote> 

Segundo, ela permite que as localizações textuais especifiquem pontos
<emphasis>entre</emphasis> os tokens, ao invés de apenas intervalos. Por
exemplo, <literal>Location(3,3)</literal> especifica o ponto imediatamente anterior
ao texto de índice 3. Por último, ela simplifica a aritmética dos
índices; por exemplo, o comprimento de <literal>Location(5,10)</literal> é
igual a <literal>10-5</literal> e duas localizações são contíguas se o início 
de uma for igual ao final da outra.</para>

<para>Uma localização textual pode incluir em uma tag informações sobre a
<glossterm>fonte</glossterm>, indicando-nos de onde provém o texto. Um
exemplo típico de fonte seria o nome do arquivo a partir do qual o texto
foi lido.<footnote><para> Os índices das localizações são
baseados em zero e portanto à primeira sentença é atribuído o índice zero 
e não o índice um.</para></footnote>
</para>

<programlisting><![CDATA[
>>> my_loc = CharSpanLocation(1, 5, 'foo.txt') 
[1:5c]
>>> my_loc.srource()
'foo.txt'
]]></programlisting>

<para> Como discutido acima, um token de texto representa uma única
      ocorrência de uma seqüência de texto. No NLTK, um token é definido
      por um tipo, além da localização onde dado tipo ocorre. Um token
      com o texto <replaceable>t</replaceable> e a localização
      <literal>@[<replaceable>l</replaceable>]</literal> pode ser
      representado por
      <literal>&lt;<replaceable>t</replaceable>>@[<replaceable>l</replaceable>]</literal>.
      Esta informação quanto à localização pode ser utilizada para distinguir
      duas ocorrências de um mesmo texto. Os Tokens são construídos com o <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      >construtor de <literal>Token</literal>s</ulink>: </para>

<programlisting><![CDATA[
>>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
<hello>@[0,5c]
>>> token2 = Token(TEXT='world', LOC=CharSpanLocation(6, 11)) 
<world>@[6,11c]
]]></programlisting>

<para> Dois tokens serão iguais somente se eles forem iguais em todos seus
atributos, neste caso <literal>text</literal> e <literal>loc</literal>.
</para>

<programlisting><![CDATA[
>>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
<hello>@[0,5c]
>>> token2 = Token(TEXT='hello', LOC=CharSpanLocation(6, 11)) 
<hello>@[6,11c]
>>> token1 == token2 
False
>>> token1['TEXT'] == token2['TEXT'] 
True
]]></programlisting>

<note>
<para> Quando a localização de um token é desconhecida ou irrelevante, esta
pode ser omitida. Porém, a distinção entre um token de palavra e um tipo de
palavra não é perdida neste caso.
</para>
</note>

<note>
<para> O índice inicial, o índice final e a fonte de localização podem
      ser lidos utilizando as funções-membro <literal>start()</literal>, 
      <literal>end()</literal>, e <literal>source()</literal>:
<programlisting><![CDATA[
>>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5, 'corpus.txt')) 
>>> loc1 = token1['LOC']
[0:5c]
>>> loc1.start()
0
>>> loc1.end()
5
>>> loc1.source()
'corpus.txt'
]]></programlisting>
</para>
</note>

</section> <!-- Locations -->

</section>  <!-- Tokens -->

<!------------------------------------------------------------------------>

<section> <title> Tokenização </title>

<para> Muitas tarefas do processamento de linguagem natural envolvem a
      análise de textos de dimensões variadas, indo de uma única sentença até
      corpora extensos. Há várias formas de se representar textos usando o
      NLTK. A forma mais simples é por meio de uma única <literal>string</literal>.
      Estas strings podem ser lidas diretamente de arquivos:</para>

<programlisting><![CDATA[
>>> text_str = open('corpus.txt').read() 
'Hello world.  This is a test file.\n'
]]></programlisting>

<para> Porém, como notamos em <xref linkend="token.representing"/>, é
normalmente preferível representar um texto como uma lista (no Python,
<literal>list</literal> de <literal>Token</literal>s. Estas listas são normalmente
criadas utilizando-se um <glossterm>tokenizador</glossterm> como o
<ulink url="&refdoc;/nltk.tokenizer.WhitespaceTokenizer-class.html">
<literal>WhitespaceTokenizer</literal></ulink> (que separa as strings em
palavras conforme os espaços em branco)></para>

<programlisting><![CDATA[
>>> from nltk.tokenizer import *
>>> text_token = Token(TEXT='Hello world.  This is a test file.')
<Hello world.  This is a test file.>
>>> WhitespaceTokenizer(SUBTOKENS='WORDS').tokenize(text_token)
>>> print text_token 
<[<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]>
]]></programlisting>

<note>
<para>Por <quote>espaço em branco</quote> devemos entender não apenas
o espaço entre as palavras, mas também as marcas de tabulação e os
finais de linha.</para>
</note>
<para> Observe que a variável <literal>text_token</literal> contém, inicialmente,
um único token para o texto inteiro. Sucessivamente, tokenizamos este único
token nos tokens de palavra que o compõem, e o resultado final é armazenado como
um atributo do token original. Em outras palavras, o token-fonte original e
os tokens de palavra resultantes são armazenados em conjunto, como o código a
seguir demonstra:<footnote><para>O método padrão para se exibir o conteúdo de um
token contendo subtokens é a simples exibição de seus subtokens.</para></footnote>
</para>

<programlisting><![CDATA[
>>> print text_token['TEXT'] 
<Hello world.  This is a test file.>
>>> print text_token['WORDS'] 
[<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]
]]></programlisting>

<para> Se quisermos que todos os tokens contenham informações sobre suas
localizações, é necessário marcar como verdadeira a opção 
<literal>add_locs</literal>:
</para>

<programlisting><![CDATA[
>>> WhitespaceTokenizer(SUBTOKENS='WORDS').tokenize(text_token, add_locs=True) 
>>> print text_token 
<[<Hello>@[0:5c], <world.>@[6:12c], <This>@[14:18c], <is>@[19:21c], <a>@[22:23c],
<test>@[24:28c], <file.>@[29:34c]]>
]]></programlisting>

<note>
<para> A tokenização pode normalizar um texto, convertendo todos as letras das
palavras para minúsculas, expandindo as contrações e, possivelmente, realizando o
<emphasis>stemming</emphasis> das palavras. Um exemplo em que o stemming é
utilizado é apresentado abaixo:
<programlisting><![CDATA[
>>> text_token = Token(TEXT='stemming can be fun and exciting') 
>>> WhitespaceTokenizer(SUBTOKENS='WORDS').tokenize(text_token) 
>>> from nltk.stemmer.porter import * 
>>> stemmer = PorterStemmer() 
>>> for word_token in text_token['WORDS']: 
...     stemmer.stem(word_token)
>>> print text_token 
<[<TEXT='stemming', STEM='stem'>, <TEXT='can', STEM='can'>, <TEXT='be', STEM='be'>,
<TEXT='fun', STEM='fun'>, <TEXT='and', STEM='and'>, <TEXT='exciting', STEM='excit'>]>
]]></programlisting>
</para>
</note>

<para>Dissemos em <xref linkend="intro"/> que a tokenização baseada apenas
em espaços em branco é demasiado simplista para a maioria das tarefas;
entre outros, ela não separa a última palavra de uma frase ou de uma
sentença de caracteres de pontuação, como a vírgula, o ponto, o ponto de
exclamação e o ponto de interrogação. Como seu nome sugere, o
<literal>RegexpTokenizer</literal> emprega uma expressão regular ("regular
expression", em inglês) para determinar como um texto deve ser dividido. 
Esta expressão regular especifica os caracteres que podem ser incluídos em 
uma palavra válida. Para definir um tokenizador que processe sinais de pontuação 
como tokens separados, poderíamos usar:</para>

<programlisting><![CDATA[
>>> regexp = r'\w+|[^\w\s]+'
'\w+|[^\w\s]+'
>>> tokenizer = RegexpTokenizer(regexp, SUBTOKENS='WORDS')
>>> tokenizer.tokenize(text_token)
>>> print text_token
<[<Hello>, <.>, <Isn>, <'>, <t>, <this>, <fun>, <?>]>
]]></programlisting>
<tip>
<para>Lembre-se que <literal>\w+|[^\w\s]+</literal> é uma disjunção de duas
subexpressões, <literal>w+</literal> e <literal>[^\w\s]+</literal>. A primeira
destas aceita um ou mais caracteres <quote>de palavra</quote>; ou seja,
caracteres que não sejam espaços em branco ou sinais de pontuação. O segundo
padrão é uma expressão negada; ela aceita um ou mais caracteres que não sejam
caracteres de palavra (ou seja, não aceita o conjunto <literal>\w</literal>) ou
espaços em branco (ou seja, não aceita o conjunto <literal>\s</literal>).</para>
</tip>
<para> A expressão regular deste exemplo irá aceitar uma seqüência que seja
constituída por um ou mais caracteres de palavra <literal>\w+</literal>.
Também irá aceitar uma seqüência que seja constituída por um ou mais sinais
de pontuação (ou caracteres não-palavra e não-espaço,
<literal>[^\w\s]+</literal>).
</para>

<para> Há várias formas por meio das quais podemos melhorar esta expressão
       regular. Por exemplo, em sua versão atual ela separa a string
       "$22.50" em quatro tokens distintos; podemos porém desejar que ela
       identifique neste tipo de string um único token. A solução para isto
       é modificar a expressão regular, adicionado uma nova subexpressão
       para tratar especificamente de strings neste formato: </para>

<programlisting><![CDATA[
>>> text_token = Token(TEXT='That poster costs $22.40.') 
>>> regexp = r'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
>>> tokenizer = RegexpTokenizer(regexp, SUBTOKENS='WORDS')
>>> tokenizer.tokenize(text_token)
>>> print text_token
<[<That>, <poster>, <costs>, <$22.40>, <.>]>
]]></programlisting>

<para>Certas vezes, é mais conveniente escrever uma expressão regular que
aceite o que é encontrado <emphasis>entre</emphasis> os tokens, como os
espaços em branco ou os sinais de pontuação. O construtor do 
<literal>RegexpTokenizer</literal> aceita um parâmetro opcional
<literal>negative</literal> que inverte o significado de uma expressão
regular. Por exemplo, os dois tokenizadores a seguir são eqüivalentes em
termos de funcionamento:
<literal>RegexpTokenizer(r'[^\s]+')</literal>,
<literal>RegexpTokenizer(r'\s+', negative=1)</literal>.
</para>

<para> 
O módulo <literal>nltk.corpus</literal> possibilita o trabalho com vários
corpora incluídos no NLTK, bem como com tokenizadores específicos. Eis um
exemplo de sua utilização:
<programlisting><![CDATA[
>>> from nltk.corpus import gutenberg 
>>> print gutenberg.read('milton-paradise.txt')
<[<**This>, <is>, <the>, <Project>, <Gutenberg>,
<Etext>, <of>, <Paradise>, <Lost(Raben)**> ... ]>
]]></programlisting>
Note como <literal>gutenberg.read</literal> não apenas carrega o conteúdo
relevante em uma string; ele também utiliza um tokenizador específico,
apropriados para os textos do corpus Gutenberg.
</para>

</section> <!-- Regexp tokenizer -->

<!------------------------------------------------------------------------>

<section id="frequency">
<title> Contando tokens </title>

  <para>Provavelmente, a coisa mais fácil que se possa fazer com os tokens, uma vez 
  que os tenhamos extraído de um texto, é contá-los. Podemos fazer isto
  como mostrado a seguir, quando comparamos o comprimento das traduções
  para o inglês e para o finlandês do livro do Gênesis:
  </para>

<programlisting><![CDATA[
>>> from nltk.corpus import genesis
>>> corpus = genesis.read('english-kjv.txt')
>>> print len(corpus['WORDS'])
38240
>>> corpus = genesis.read('finnish.txt')
>>> print len(corpus['WORDS'])
26595
]]></programlisting>

    <para> Podemos fazer uma contagem mais sofisticada por meio de uma
    <glossterm>distribuição de freqüência</glossterm>. Geralmente,
    uma distribuição de freqüência armazena o número de vezes que
    cada determinado valor ocorre como resultado de uma experiência. Neste 
    caso, podemos
    utilizar uma distribuição de freqüência para armazenar a freqüência
    de cada palavra em um documento. Distribuições de freqüência são
    geralmente inicializadas executando-se repetidamente uma experiência e
    incrementando a contagem de cada valor toda vez que
    este for obtido como resultado. O programa a seguir cria uma distribuição
    de freqüência que armazena o número de vezes que cada palavra ocorre em
    um texto, exibindo ao final a palavra mais freqüente:</para>

<programlisting><![CDATA[
>>> from nltk.probability import FreqDist
>>> fd = FreqDist()
>>> for token in corpus['WORDS']:
...     fd.inc(token['TEXT'])
>>> print fd.max()
'the'
]]></programlisting>

    <para> Uma vez que tenhamos construído uma distribuição de freqüência que
    armazene os resultados de uma experiência, podemos utilizá-la para examinar
    uma série de propriedades interessantes desta experiência. Estas propriedades
    são resumidas em <xref linkend="table.freqdist"/>.</para>

<table id="table.freqdist"> 
  <title>Módulo de distribuição de freqüência</title> 
  <tgroup cols="2">
    <colspec colwidth='2cm'/>
    <colspec colwidth='6cm'/>
    <colspec colwidth='6cm'/>
    <thead> 
      <row> 
        <entry>Nome</entry> 
        <entry>Código de exemplo</entry> 
        <entry>Descrição</entry> 
      </row> 
    </thead> 

    <tbody>
      <row><entry>Contagem</entry><entry>
<programlisting><![CDATA[
>>> fd.count('the')
6
]]></programlisting>
</entry>
<entry><para>O número de vezes em que determinado resultado ocorreu
</para></entry></row>

      <row><entry>Freqüência</entry><entry>
<programlisting><![CDATA[
>>> fd.freq('the')
0.012
]]></programlisting>
</entry>
<entry><para>A freqüência para o resultado dado
</para></entry></row>

      <row><entry>N</entry><entry>
<programlisting><![CDATA[
>>> fd.N()
500
]]></programlisting>
</entry>
<entry><para>O número de resultados diferentes
</para></entry></row>

      <row><entry>Resultados</entry><entry>
<programlisting><![CDATA[
>>> fd.samples()
['happy', 'but', 'the',
'in', 'of', ...]
]]></programlisting>
</entry>
<entry><para>Todos os resultados distintos que foram armazenados
</para></entry></row>

      <row><entry>Max</entry><entry>
<programlisting><![CDATA[
>>> fd.max()
'the'
]]></programlisting>
</entry>
<entry><para>O resultado com maior número de ocorrências
</para></entry></row>
</tbody>
</tgroup>
</table>

    <para> Podemos utilizar uma <literal>FreqDist</literal> para
      examinar a distribuição do comprimento das palavras em um determinado
      corpus. Para isto, construiremos uma distribuição de freqüência cujos
      resultados serão os comprimentos das palavras, plotando o resultado
      ao final. Para iniciar, devemos importar as classes que usaremos e
      carregar um corpus a partir de um arquivo de texto:</para>

<programlisting><![CDATA[
>>> from nltk.probability import FreqDist 
>>> from nltk.corpus import genesis
]]></programlisting>

      <para> A seguir, iremos definir uma função que recebe o nome de um
      texto como seu argumento e exibe a distibuição dos comprimentos de
      palavra para este texto. Para cada palavra, calculamos seu comprimento e
      incrementamos a contagem de palavras deste comprimento.
</para>

<programlisting><![CDATA[
>>> def length_dist(text):
...     fd = FreqDist()                        # Inicializa um distribuição de freqüência vazia
...     corpus = genesis.read(text)            # tokeniza o texto
...     for token in corpus['WORDS']:          # para cada texto
...         fd.inc(len(token['TEXT']))         # encontra outra palavra com este comprimento
...     for i in range(15):                    # para os comprimentos entre 0 e 14
...         print "%2d" % int(100*fd.freq(i)), # exibe a porcentagem de palavras com este comprimento
...     print
]]></programlisting>


<programlisting><![CDATA[
>>> length_dist('english-kjv.txt')
0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
>>> length_dist('finnish.txt')
0  0  9  6 11 16 16 12  9  6  3  2  2  1  0
]]></programlisting>

    <para> Uma <glossterm>condição</glossterm> especifica o contexto no qual
    uma experiência foi realizada. Normalmente nos interessaremos pelos
    efeitos que as condições produzem no resultado de uma experiência. Por
    exemplo, podemos querer examinar de que forma a distribuição do comprimento
    das palavras (o resultado) é influenciado pela letra inicial das
    mesmas (a condição). Distribuições de freqüência condicionais fornecem
    uma ferramenta para explorar este tipo de questão.</para>

    <para> Uma <glossterm>distribuição de freqüência condicional</glossterm> é uma
    coleção de distribuições de freqüência para uma mesma experiência, executada
    sob condições diferentes. As distribuições de freqüência individuais são
    indexadas por sua condição.</para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import genesis
>>> from nltk.draw.plot import Plot 

>>> cfdist = ConditionalFreqDist()

>>> for text in genesis.items():
...     corpus = genesis.read(text)
...     for token in corpus['WORDS']:
...         cfdist[text].inc(len(token['TEXT']))
]]></programlisting>

        <para> Para plotar os resultados, construímos uma lista de pontos, onde
	a coordenada X refere-se ao comprimento das palavras e a coordenada y 
	refere-se à freqüência de cada um destes comprimentos:</para>

<programlisting><![CDATA[
>>> for cond in cfdist.conditions():
...     wordlens = cfdist[cond].samples()
...     wordlens.sort()
...     points = [(i, cfdist[cond].freq(i)) for i in wordlens]
...     Plot(points).mainloop()
]]></programlisting>

    <section id="frequency.predicting"> 
      <title> Uma aplicação prática: prever a palavra sucessiva </title>

      <para> Distribuições de freqüência condicionais são muitas vezes utilizadas
      para prever certos comportamentos. <glossterm>Prever</glossterm>, neste
      sentido, significa estimar um valor plausível como resultado da execução de 
      determinada
      experiência. A decisão de qual resultado fornecer é normalmente uma função
      do contexto no qual a experiência é realizada. Por exemplo, podemos tentar
      prever uma palavra de um texto (o resultado) baseado-nos no texto que
      antecede esta palavra (o contexto). </para>
      
      <para> Para prever os resultados de uma experiência, é necessário que,
      primeiramente, examinemos um <glossterm>corpus de treinamento</glossterm>
      representativo, no qual o contexto e o resultado para cada execução da
      experiência já são conhecidos. Quando apresentados a uma nova execução da
      experiência, podemos escolher, por exemplo, o resultado que ocorreu mais
      freqüentemente naquele determinado contexto. </para>

      <para> Podemos utilizar uma <literal>ConditionalFreqDist</literal> para
      encontrar a ocorrência mais freqüente de cada contexto. Primeiro,
      armazenamos cada resultado do corpus de treinamento, utilizando o contexto
      no qual a experiência é executada como condição. Então, podemos acessar
      a freqüência de distribuição para cada dado contexto usando este último
      como índice e, finalmente, utilizar o método <literal>max()</literal> para 
      encontrar o resultado mais provável. </para>
      
      <para> Abaixo, usaremos uma <literal>ConditionalFreqDist</literal> para
      encontrar a palavra mais provável em um texto. Para iniciar, carregaremos
      um corpus a partir de um arquivo de texto e criaremos uma
      <literal>ConditionalFreqDist</literal> vazia:</para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import genesis
>>> corpus = genesis.read('english-kjv.txt')
>>> cfdist = ConditionalFreqDist()
]]></programlisting>

        <para> A seguir, examinaremos cada token neste corpus, incrementando
	a contagem apropriada para cada um. A variável <literal>prev</literal>
	será utilizada para armazenar a palavra anterior.</para>
	
<programlisting><![CDATA[
>>> prev = None
>>> for token in corpus['WORDS']:
...     word = token['TEXT']
...     cfdist[prev].inc(word)
...     prev = word
]]></programlisting>

        <note> 
          <para> Certas vezes, o contexto de uma experiência pode não estar
	  disponível ou não existir. Por exemplo, no caso do primeiro token
	  não há nenhum texto que o anteceda. Em situações como esta, somos
	  obrigados a decidir qual contexto utilizar. Neste exemplo,
	  utilizamos o <literal>None</literal> (nada) como contexto para o
	  primeiro token. Outra opção teria sido descartar as informações
	  do primeiro token. </para>
        </note>

        <para> Uma vez que tenhamos construído uma distribuição de freqüência
	condicional para o corpus de treinamento, podemos utilizá-la para
	encontrar a palavra mais provável para um dado contexto. Por exemplo,
	tomando por contexto a palavra <literal>living</literal>, é
	possível inspecionar todas as palavras que ocorreram no mesmo.

<programlisting><![CDATA[
>>> word = 'living'
>>> cfdist['living'].samples()
['creature,', 'substance', 'soul.', 'thing', 'thing,', 'creature']
]]></programlisting>

        Poderíamos criar um ciclo simples para gerar texto: determinamos
	um contexto inicial, escolhemos a palavra mais provável para este
	contexto e passamos a usá-la como nosso novo contexto: </para>
	
<programlisting><![CDATA[
>>> word = 'living'
>>> for i in range(20):
...     print word,
...     word = cfdist[word].max()
living creature that he said, I will not be a wife of the land of the land of the land
]]></programlisting>

        <para> Esta abordagem simplista para a geração de texto tende a 
	prender-se em ciclos repetitivos, como demostrado pelo texto gerado acima.
	Uma abordagem mais avançada poderia escolher aleatoriamente cada palavra,
	tendo as palavras de maior ocorrência uma probabilidade maior de escolha. </para>

    </section> <!-- Predicting -->
  </section> <!-- ConditionalFreqDist -->


<section id="pos">
  <title> Classes de palavras e partes do discurso </title>

<para>Nas seções anteriores tratamos todas as palavras de forma muito
semelhante: qualquer coisa era ou não era um token. Porém, para muitas aplicações,
é importante que possamos distinguir entre <emphasis>tipos diferentes</emphasis>
de tokens. Por exemplo, podemos querer marcar explicitamente que determinada
string é um item lexical comum, que outra constitui uma expressão numérica e
que outra ainda é um sinal de pontuação. Além disto, podemos querer distinguir
entre os diferentes tipos de itens lexicais: há uma longa tradição no campo
da lingüística de se classificar as palavras em categorias chamadas
<firstterm>partes do discurso</firstterm>. Estas também podem ser chamadas
de classes de palavras ou de <firstterm>categorias lexicais</firstterm>.
Exemplo familiares são <type>substantivo</type>, <type>verbo</type>,
<type>preposição</type>, <type>adjetivo</type> e <type>advérbio</type>.
Nesta seção apresentaremos os critérios padrão para a categorização de
palavras desta forma, discutindo as principais classes de palavras da
língua inglesa.
  </para>

<section id="pos.categorise">
  <title> Categorizando palavras </title>

  <para>
  Como decidimos a qual categoria uma palavra deve pertencer? Em geral,
  os lingüistas valem-se de três tipos de avaliações para tomar esta decisão:
  uma de termos formais, uma sintática (ou distributiva) e uma conceitualista (ou
  de semântica). Uma avaliação <firstterm>formal</firstterm> é aquela que
  analisa a estrutura interna de uma palavra. Por exemplo, <literal>-ness</literal>
  é um sufixo que é combinado a um adjetivo para produzir um substantivo.
  Exemplos são <literal>happy</literal>
  &gt; <literal>happiness</literal>, <literal>ill</literal> &gt;
  <literal>illness</literal>.
  
  <footnote>
  <para>Utilizamos <markup>&gt;</markup> com o significado de 'deriva em'.</para>
  </footnote>
  
  Assim, ao encontrarmos uma palavra que termine em <literal>-ness</literal>
  podemos com grande probabilidade identificá-la como sendo um substantivo.
  </para>

  <para>
  Uma avaliação <firstterm>sintática</firstterm> considera os contextos
  sintáticos nos quais uma palavra pode ocorrer. Por exemplo, vamos assumir que
  já tenhamos definido a categoria dos substantivos. Em uma avaliação sintática
  para a língua inglesa, podemos dizer que um adjetivo é aquela palavra que ocorre
  imediatamente antes de um substantivo ou que segue as palavras como
  <literal>be</literal> e <literal>very</literal>. De acordo com estes critérios,
  <literal>near</literal> deveria ser categorizada como um adjetivo:
  </para>
    
    <orderedlist>
      <listitem><para>the near window</para></listitem>
      <listitem><para>The end is (very) near.</para></listitem>
    </orderedlist>

  <para>
  Um exemplo de avaliação <firstterm>conceitualista</firstterm> é definir um
  substantivo como o <quote>nome de uma pessoa, lugar ou coisa</quote>. Dentro
  da lingüística moderna, avaliações por definição para classes de palavras
  têm sido vistas com considerável desconfiança, principalmente devido à
  dificuldade de se formalizar tais conceitos. Apesar disto,
  avaliações conceitualistas são a base de muitas de nossas intuições quanto a
  classes de palavras e nos permitem estimar, com uma boa probabilidade de
  acerto, a categorização de palavras em línguas que desconhecemos;
  isto é, se por exemplo soubermos apenas que em holandês 
  <literal>verjaardag</literal> significa o mesmo que a palavra inglesa
  <literal>birthday</literal> ou a portuguesa <literal>aniversário</literal>,
  podemos supor que <literal>verjaardag</literal> seja um substantivo
  também em holandês. Porém, é necessária cautela: mesmo que possamos
  traduzir <literal>zij is van dag jarig</literal> como <literal>it's her
  birthday today</literal> ou <literal>é o seu (dela) aniversário hoje</literal>,
  a palavra <literal>jarig</literal> é na verdade um adjetivo em holandês
  e não há uma eqüivalência exata desta em inglês ou em português.
  </para>

<!--http://www.askoxford.com/pressroom/archive/odelaunch/-->

  <para>
  Todas as línguas adquirem novos itens lexicais. A lista de palavras que
  recentemente foi adicionada ao Oxford Dictionary of English inclui
  <literal>cyberslacker, fatoush, blamestorm, SARS, cantopop, bupkis,
  noughties, muggle</literal> e <literal>robata</literal>. Note que todas
  estas palavras são substantivos; isto se reflete no fato dos substantivos
  serem considerados uma <glossterm>classe aberta</glossterm>. Em contraste,
  as preposições são consideradas uma <glossterm>classe fechada</glossterm>, ou
  seja, há um conjunto limitado de palavras que pertence a tal classe (por
  exemplo, sempre em inglês, <literal>above, along, at, below, beside, between, 
  during, for, from, in, near, on, outside, over, past, through, towards, under, up, 
  with</literal>) e as mudanças ocorrem muito gradualmente ao longo do tempo.

<!--    
    Some word classes consist of a limited set of so-called
    <firstterm>function</firstterm>
    words. Prepositions are one such class, comprising items like
     etc.  These are called
    <glossterm>closed classes</glossterm>, in the sense that although
    languages acquire new lexical items.  Content words such as
    nouns are not limited in this way, and are continually being
    extended with the invention of new words.  These are called
    <glossterm>open classes</glossterm>.
-->
  </para>

</section>

<section id="pos.english">
  <title> Classes de palavras (em inglês) </title>

  <para>
    Esta seção apresenta uma breve visão geral sobre as classe de palavras
    na língua inglesa. Leitores interessados em maiores detalhes são
    aconselhados a consultar uma gramática da língua.
  </para>

  <para>
    Os lingüistas costumam reconhecer quatro categorias principais de
    classes abertas de palavras em inglês: substantivos, verbos,
    adjetivos e advérbios. Substantivos referem-se, geralmente, a pessoas,
    lugares, coisas ou conceitos, como  
    <emphasis>woman, Scotland, book, intelligence</emphasis>. No contexto
    da sentença, os substantivos podem ser encontrados após determinantes e
    adjetivos, além de podem servir de sujeito ou objeto para um verbo:
  </para>

  <table id="table.nouns">
    <title> Padrões sintáticos envolvendo alguns substantivos </title>
    <tgroup cols="3">
      <colspec colwidth='2cm'/>
      <thead>
        <row>
          <entry>Palavra</entry>
          <entry>Após um determinante</entry>
          <entry>Sujeito de um verbo</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>woman</entry>
          <entry><emphasis>the</emphasis> woman who I saw yesterday ...</entry>
          <entry>the woman <emphasis>sat</emphasis> down</entry>
        </row>
        <row>
          <entry>Scotland</entry>
          <entry><emphasis>the</emphasis> Scotland I remember as a child ...</entry>
          <entry>Scotland <emphasis>has</emphasis> five million people</entry>
        </row>
        <row>
          <entry>book</entry>
          <entry><emphasis>the</emphasis> book I bought yesterday ...</entry>
          <entry>this book <emphasis>recounts</emphasis> the colonisation of Australia</entry>
        </row>
        <row>
          <entry>intelligence</entry>
          <entry><emphasis>the</emphasis> intelligence displayed by the child ...</entry>
          <entry>Mary's intelligence <emphasis>impressed</emphasis> her teachers</entry>
        </row>
      </tbody>
    </tgroup>
  </table>

  <para>
    Em inglês, substantivos podem ser morfologicamente complexos. Por exemplo,
    palavras como <literal>books</literal> e <literal>women</literal> estão
    no plural. Como vimos anteriormente, palavras terminadas com o sufixo
    <literal>-ness</literal> são substantivos derivados de adjetivos, como
    <literal>happiness</literal> e <literal>illness</literal>. O sufixo
    <literal>-ment</literal> é encontrado em alguns substantivos derivados
    de verbos, como <literal>government</literal> e 
    <literal>establishment</literal>.
  </para>

  <para>
    Os substantivos são geralmente divididos em <glossterm>substantivos
    comuns</glossterm> e <glossterm>substantivos próprios</glossterm> (ou
    "nomes comuns" e "nomes próprios").
    Substantivos próprios identificam índividuos ou entidades em
    particular, como <literal>Moses</literal> e 
    <literal>Scotland</literal>, enquanto substantivos comuns são todos
    os restantes. Outra distinção importante existe entre 
    <glossterm>substantivos contáveis</glossterm> e <glossterm>substantivos
    incontáveis</glossterm>. Substantivos contáveis são pensados como
    entidades distintas que podem ser contadas, como <literal>pig</literal>
    (por exemplo, <literal>one pig, two pigs, many pigs</literal>). Eles
    não podem ocorrer com a palavra <literal>much</literal> (como em
    *<literal>much pigs</literal>). Substantivos contáveis, por outro
    lado, não são vistos como entidades distintas (por exemplo,
    <literal>sand</literal>). Eles não podem ser pluralizados e não
    podem ocorrer com numerais (por exemplo, *<literal>two sands</literal>,
    *<literal>many sands</literal>). Por outro lado, eles podem ocorrer
    com <literal>much</literal> (como em <literal>much sand</literal>).
  </para>
    
  <para>
    Verbos são palavras que descrevem eventos e ações, como
    <literal>fall</literal> e <literal>eat</literal>. No contexto da
    sentença, verbos expressam a relação envolvendo os referentes de um
    ou mais sintagmas nominais.
  </para>


  <table id="table.verbs">
    <title> Padrões sintáticos envolvendo alguns verbos </title>
    <tgroup cols="3">
      <colspec colwidth='2cm'/>
      <colspec colwidth='5cm'/>
      <thead>
        <row>
          <entry>Palavra</entry>
          <entry>Simples</entry>
          <entry>Com modificadores e adjuntos (em itálico)</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>fall</entry>
<!-- probably more plausible to think of "last week" as a sentence
modifier -->
          <entry>Rome fell</entry>
	      <entry>Last week, dot com stocks
	      <emphasis>suddenly</emphasis> fell <emphasis>like a
	      stone</emphasis></entry>
        </row>
        <row>
          <entry>eat</entry>
          <entry>Mice eat cheese</entry>
          <entry>John ate the pizza <emphasis>with gusto</emphasis></entry>
        </row>
      </tbody>
    </tgroup>
  </table>

  <para>
    Verbos podem ser classificados de acordo com o número de argumentos
    (geralmente sintagmas nominais) que os acompanham. A palavra
    <literal>fall</literal> ("cair") é <glossterm>intransitiva</glossterm>, requerendo
    apenas um argumento (a entidade que cai). A
    palavra <literal>eat</literal> (comer) é <glossterm>transitiva</glossterm>,
    requerendo dois argumentos (o comedor e o comido). Outros verbos
    são mais complexos; por exemplo <literal>put</literal> (colocar)
    requer três argumentos, o agente que está realizando a ação de colocar,
    a entidade que está sendo colocada em algum lugar e a sua localização
    final. O sufixo <literal>-ing</literal> é encontrado em substantivos
    derivados de verbos, como <literal>the falling of the leaves</literal>
    (o que é conhecido como <glossterm>gerúndio</glossterm>).
  </para>

  <para>
    Em inglês, verbos podem ser morfologicamente complexos. Por exemplo, o
    <glossterm>particípio presente</glossterm> de um verbo termina em
    <literal>-ing</literal> e expressa a idéia de estar em execução, de uma
    ação incompleta (como <literal>falling</literal> e 
    <literal>eating</literal>). O <glossterm>particípio passado</glossterm>
    de um verbo geralmente termina em <literal>-ed</literal> e expressa a
    idéia de uma ação concluída (como <literal>fell</literal> e
    <literal>ate</literal>).
  </para>

  <para>
    Duas outras importantes classes são os <glossterm>adjetivos</glossterm>
    e <glossterm>advérbios</glossterm>. Adjetivos descrevem substantivos e
    podem ser utilizados como modificadores (por exemplo, <literal>large</literal>
    em <literal>the large pizza</literal>) ou em predicados (por exemplo,
    <literal>the pizza is large</literal>). Em inglês, adjetivos podem ser
    morfologicamente complexos (como 
    <literal>fall<subscript>V</subscript>+ing</literal> em
    <literal>the falling stocks</literal>).
    Advérbios modificam verbos para especificar o tempo, o modo, o lugar ou
    a direção do evento descrito pelo verbo
    (como <literal>quickly</literal> em <literal>the stocks fell quickly</literal>).
    Advérbios também podem modificar adjetivos (como <literal>really</literal>
    em <literal>Mary's teacher was really nice</literal>).
  </para>

  <para>
    O inglês possui várias categorias de classes fechadas de palavras além
    das preposições e cada dicionário e gramática as classifica de forma diferente.
    <xref linkend="table.closed_class"/> fornece um exemplo de classes
    fechadas de palavras, segundo a classificação utilizada no Brown Corpus.
    
    <footnote>
      <para>Note que as tags referentes à função gramatical podem ser apresentadas
      tanto em letras maiúsculas quanto em letras minúsculas &mdash; não há nenhuma
      significação resultante desta diferença.</para>
     </footnote>
  </para>


<table id="table.closed_class">
  <title> Algumas classes fechadas de palavras em inglês, com as tags do conjunto Brown </title>
  <tgroup cols="3">
    <colspec colwidth='1.5cm'/>
    <colspec colwidth='6cm'/>
    <tbody>
      <row>
        <entry> ap </entry>
        <entry> determiner/pronoun, post-determiner </entry>
        <entry> many other next more last former little several enough
                 most least only very few fewer past same </entry>
      </row>
      <row>
        <entry> at </entry>
        <entry> article </entry>
        <entry> the an no a every th' ever' ye </entry>
      </row>
      <row>
        <entry> cc </entry>
        <entry> conjunction, coordinating </entry>
        <entry> and or but plus & either neither nor yet 'n' and/or minus an' </entry>
      </row>
      <row>
        <entry> cs </entry>
        <entry> conjunction, subordinating </entry>
        <entry> that as after whether before while like because if since for
                than until so unless though providing once lest
                till whereas whereupon supposing albeit then </entry>
      </row>
      <row>
        <entry> in </entry>
        <entry> preposition </entry>
        <entry> of in for by considering to on among at through with under
                into regarding than since despite ... </entry>
      </row>
      <row>
        <entry> md </entry>
        <entry> modal auxiliary </entry>
        <entry> should may might will would must can could shall ought need 
                 wilt </entry>
      </row>
      <row>
        <entry> pn </entry>
        <entry> pronoun, nominal </entry>
        <entry> none something everything one anyone nothing nobody everybody
                everyone anybody anything someone no-one nothin' </entry>
      </row>
      <row>
        <entry> ppl </entry>
        <entry> pronoun, singular, reflexive </entry>
        <entry> itself himself myself yourself herself oneself ownself </entry>
      </row>
      <row>
        <entry> pp$ </entry>
        <entry> determiner, possessive </entry>
        <entry> our its his their my your her out thy mine thine </entry>
      </row>
      <row>
        <entry> pp$$ </entry>
        <entry> pronoun, possessive </entry>
        <entry> ours mine his hers theirs yours </entry>
      </row>
      <row>
        <entry> pps </entry>
        <entry> pronoun, personal, nominative, 3rd person singular </entry>
        <entry> it he she thee </entry>
      </row>
      <row>
        <entry> ppss </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular </entry>
        <entry> they we I you ye thou you'uns </entry>
      </row>
      <row>
        <entry> wdt </entry>
        <entry> WH-determiner </entry>
        <entry> which what whatever whichever</entry>
      </row>
      <row>
        <entry> wps </entry>
        <entry> WH-pronoun, nominative </entry>
        <entry> that who whoever whosoever what whatsoever </entry>
      </row>
    </tbody>
  </tgroup>
</table>

</section>

<section id="tagging">
  <title> Conjuntos de tags das funções gramaticais </title>

  <para>
    A maior parte dos conjuntos de tags das funções gramaticais utiliza
    as mesmas categorias básicas, como substantivo, verbo, adjetivo e
    preposição. Estes conjuntos de tags diferem, porém, tanto na forma
    em que as palavras são divididas em categorias quanto na forma como estas
    últimas são definidas. Por exemplo, <literal>is</literal> pode
    receber a tag de verbo em determinado conjunto mas pode ser considerado uma 
    forma distinta
    de <literal>to be</literal> em outro &mdash; de fato, acabamos de
    verificar esta situação no conjunto de tags do Brown Corpus. Não há
    como livrar-se desta variação entre os conjuntos de tags, já que estas
    etiquetas de informação referentes à função gramatical são usadas de
    diferentes formas para diferentes finalidades. Em outras palavras, não há
    uma "forma correta" de se atribuir tags, apenas há formas mais 
    e menos apropriadas, dependendo dos objetivos de cada um.
<!--
  <note><para> There are several part-of-speech tag sets in widespread
  use, because there are different schemes for classifying words
  (owing to the different weight given to formal, syntactic and
  notional criteria), and because different processing tasks call for
  finer or coarser classification.</para></note>
-->
  </para>

  <para>
    Observe que o processo de tagging descarta algumas distinções (como por 
    exemplo a identidade lexical que é perdida quando todos os pronomes
    pessoais recebem uma tag comum <literal>prp</literal>) e
    introduz novas, removendo ambigüidades (como por exemplo
    <literal>deal</literal> que recebe ou a tag <literal>vb</literal> ou
    a tag <literal>nn</literal>). Esta forma de operação facilita
    a classificação e predição. Observe que quando introduzimos distinções
    mais específicas no conjunto de tags obtemos uma melhor informação
    quanto ao contexto lingüístico, mas é necessário um esforço maior para
    classificar cada token (há um numero maior de tags possíveis entre as quais
    escolher a correta). Da mesma forma, com um menor nível de distinção
    entre as tags será mais simples classificar cada token, mas uma quantidade
    menor de informações referentes ao contexto são repassadas.
  </para>

    <para>
      Neste tutorial, usaremos as seguintes tags:
      <literal>at</literal> (artigo)
      <literal>nn</literal> (substantivo),
      <literal>vb</literal> (verbo),
      <literal>jj</literal> (adjetivo),
      <literal>in</literal> (preposição),
      <literal>cd</literal> (numeral), e
      <literal>end</literal> (pontuação de final de sentença).
      Como dissemos, esta é uma versão radicalmente simplificada do conjunto de
      tags do Brown Corpus, o qual em versão integral apresenta 87 tags de base
      além de várias combinações. Uma lista completa é fornecida no apêndice
      do tutorial do tagging.
    </para>

  </section> <!-- part-of-speech tagsets -->

</section> <!-- Word Classes and Parts of Speech -->


<section id="basics.representation">
  <title> Representando tokens e corpora com tags </title>

  <para>
    Nas seções anteriores foi discutida a natureza e a utilização das
    tags no processamento lingüístico. Nesta seção introduziremos a
    representação computacional de tags. Primeiro iremos considerar os
    tokens com tags individualmente, mostrando como eles são criados
    e como podem ser acessados; finalmente nos interessaremos pelos
    corpora com tags.
  </para>

  <para>
    Lembre-se que um token do NLTK é uma espécie de dicionário da linguagem
    Python e que podemos associar propriedades arbitrárias adicionais a cada
    token. Por convenção, um token com tag é representado adicionando-se
    a propriedade <literal>TAG</literal> ao token em questão. Isto pode ser
    efetuado durante a construção do token, como mostrado a seguir:
  </para>

<programlisting><![CDATA[
>>> tok = Token(TEXT='fly', TAG='nn')
>>> print tok
<fly/nn>
]]></programlisting>

  <para>
    Podemos acessar as propriedades deste token da maneira usual, como
    mostrado abaixo:
  </para>

<programlisting><![CDATA[
>>> print tok['TEXT']
fly
>>> print tok['TAG']
nn
]]></programlisting>

  <para>
    Certas vezes podemos desejar adicionar uma tag para um token já existente
    que não possui nenhuma tag. Isto pode ser feito como mostrado a seguir:
    primeiro criamos um token que consiste apenas de texto e em seguida
    adicionamos a tag:
  </para>

<programlisting><![CDATA[
>>> tok = Token(TEXT='fly')
<fly>
>>> tok['TAG'] = 'nn'
>>> print tok
<fly/NN>
]]></programlisting>

  <para>
    Vários corpora extensos (como o Brown Corpus e a partes do Wall Street
    Journal) passaram por um processo de tagging manual, recebendo tags
    referentes às funções gramaticais. Antes que possamos utilizar estes
    corpora, é necessário que os carreguemos a partir dos arquivos que os
    contêem e que os tokenizemos.
  </para>

  <para> 
    Textos com tags são geralmente armazenados em arquivos como seqüências
    de tokens separados por espaços em branco, onde cada token é representado
    na forma <literal>texto/tag</literal>, como mostrado abaixo em uma
    seqüência extraída do Brown Corpus.  
  </para>

<para><literal>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps
said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb
accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt
interest/nn of/in both/abx governments/nns ''/'' ./.
</literal></para>

  <para>
    É possível utilizar o módulo <literal>nltk.corpus</literal> para ler e
    tokenizar dados lidos de um corpus com tags, como mostrado abaixo:
  </para>

<programlisting><![CDATA[
>>> from nltk.corpus import brown
>>> tok = brown.read('ca01')
>>> print tok['WORDS']
[<The/at>, <Fulton/np-tl>, <County/nn-tl>, <Grand/jj-tl>, <Jury/nn-tl>,
<said/vbd>, <Friday/nr>, <an/at>, <investigation/nn>, <of/in>, <Atlanta's/np$>,
<recent/jj>, <primary/nn>, <election/nn>, <produced/vbd>, <``/``>, <no/at>,
<evidence/nn>, <''/''>, <that/cs>, <any/dti>, <irregularities/nns>, <took/vbd>,
<place/nn>, <./.>, ...]
]]></programlisting>

  <para>
    Observe que a tokenização de um texto com tags resulta em um token único
    que contém a seqüência de tokens com tags, armazenados em sua propriedade
    <literal>WORDS</literal>. Eis outro exemplo onde é criado um único token
    para uma string de texto e invoca-se o método 
    <literal>TaggedTokenReader()</literal> para adicionar os subtokens.
  </para>

<programlisting><![CDATA[
>>> from nltk.token import *
>>> from nltk.tokenreader.tagged import TaggedTokenReader
>>> text_str = """
... John/nn saw/vb the/at book/nn on/in the/at table/nn ./end  He/nn sighed/vb ./end
... """
>>> reader = TaggedTokenReader(SUBTOKENS='WORDS')
>>> text_token = reader.read_token(text_str)
>>> print text_token['WORDS']
[<John/nn>, <saw/vb>, <the/at>, <book/nn>, <on/in>, <the/at>, 
 <table/nn>, <./end>, <He/nn>, <sighed/vb>, <./end>]
]]></programlisting>

  <important><para> Se o <literal>TaggedTokenReader</literal> encontra uma
  palavra sem tag, ele irá atribuir à palavra a tag de default 
  <literal>None</literal>
  </para></important>

  <para>Os subtokens são as palavras individuais e são acessados utilizando-se
  a propriedade <literal>WORDS</literal>. Cada palavra possui uma propriedade
  <literal>TEXT</literal> e uma propriedade <literal>TAG</literal>. Uma lista
  de todas as propriedades definidas para um token pode ser obtida utilizando-se
  a função <literal>properties()</literal>.
  </para>

<programlisting><![CDATA[
>>> print text_token['WORDS'][1]
<saw/vb>
>>> print text_token['WORDS'][1]['TAG']
'vb'
>>> print text_token.properties()
['TEXT', 'WORDS']
>>> print text_token['WORDS'][1].properties()
['TEXT', 'TAG']
]]></programlisting>

</section> <!-- Tagged Tokens and Tagged Corpora -->

<section id="more-applications">

  <title> Mais aplicações </title>

  <para>Agora que podemos acessar um texto com tags, é possível realizar uma
  variedade de diferentes tarefas de processamento. Iremos considerar aqui
  apenas duas: estimar a tag de função gramatical de uma palavra e explorar
  a distribuição de freqüência de verbos modais de acordo com o gênero de
  cada texto.
  </para>

<section id="more-applications.classifying">
  <title> Classificando palavras automaticamente </title>

  <para>
    Um corpus com tags pode ser utilizado para <emphasis>treinar</emphasis>
    um classificador simples que pode ser utilizado para estimar a tag
    de palavras ainda não classificadas. Para cada palavra, iremos contar o
    número de vezes que esta recebe cada tag. Por exemplo, a palavra
    <literal>deal</literal> recebe 89 vezes a tag <literal>nn</literal> e 41
    vezes a tag <literal>vb</literal>. Baseados nisso, se tivéssemos de
    estimar uma tag para a palavra <literal>deal</literal> escolheríamos
    <literal>nn</literal>, com um índice de acerto de mais de dois terços.
    O programa listado a seguir executa esta tarefa de tagging quando
    treinado com uma seção de Brown Corpus (a chamada 
    <emphasis>belles lettres</emphasis>, literatura criativa avaliada de
    acordo com seu conteúdo estético).
  </para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import brown
>>> cfdist = ConditionalFreqDist()

>>> for item in brown.items('belles-lettres'):
...     text_token = brown.read(item)
...     for word_token in text_token['WORDS']:
...         word = word_token['TEXT']
...         tag = word_token['TAG']
...         cfdist[word].inc(tag)

>>> for word in "John saw 3 polar bears".split():
...     print word, cfdist[word].max()
John np
saw vbd
3 cd-tl
polar jj
bears vbz
]]></programlisting>
    
    <para>
      Note que <literal>bears</literal> recebeu incorretamente a tag de
      "forma na terceira pessoa do singular de um verbo", pois esta
      palavra é encontrada mais freqüentemente como verbo que como
      substantivo na literatura estética.
    </para>

    <para>
      Um problema desta abordagem é a criação de um extenso modelo, com
      uma entrada para cada combinação possível de palavra e tag. Para
      certas tarefas, é possível construir modelos razoavelmente bem
      sucedidos que, em comparação, são muito menores. Por exemplo, vamos
      tentar estimar se uma palavra é um verbo, um substantivo ou um
      adjetivo analisando apenas sua letra final. Podemos fazer isto da
      seguinte forma:
    </para>      

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import brown

>>> tokens = []
>>> for item in brown.items('belles-lettres'):
...     for tok in brown.read(item)['WORDS']:
...         if tok['TAG'] in ['nn', 'jj'] and len(tok['TEXT']) > 3:
...             tokens.append(tok)

>>> split = len(tokens)*9/10
>>> train, test = tokens[:split], tokens[split:]

>>> cfdist = ConditionalFreqDist()

>>> def condition(tok):
...     return tok['TEXT'][-1]

>>> for tok in train:
...     cond = condition(tok)
...     cls = tok['TAG']
...     cfdist[cond].inc(cls)

>>> correct = total = 0
>>> for tok in test:
...     cond = condition(tok)
...     cls = tok['TAG']
...     if cls == cfdist[cond].max():
...         correct += 1
...     total += 1

>>> print correct*100/total
71
]]></programlisting>

    <para>
      Este resultado de 71% é marginalmente melhor que o resultado de
      65% que obteríamos ao atribuirmos a tag <literal>nn</literal>
      a todas as palavras. Podemos inspecionar o modelo para ver qual
      tag é atribuída a cada palavra em função de sua letra final.
      Desta forma, podemos aprender que as palavras terminadas em
      <literal>c</literal> e <literal>l</literal> têm mais probabilidade
      de serem adjetivos que substantivos.
    </para>

<programlisting><![CDATA[
>>> print [(c, cfdist[c].max()) for c in cfdist.conditions()]
[..., ('a', 'nn'), ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'),
('m', 'nn'), ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ...]
]]></programlisting>

</section> <!-- classifying words automatically -->

  <section id="more-applications.modals">
  <title> Explorando gêneros textuais </title>

  <para>
    Agora que podemos carregar uma quantidade significativa de texto com tags,
    é possível processá-lo e extrair deste informações de interesse. O código a
    seguir interage nos quinze gêneros do Brown Corpus (acessados por meio do
    a método <literal>brown.groups()</literal>). O material para cada gênero
    está armazenado dentro de um conjunto de arquivos (acessados por meio do método
    <literal>brown.items()</literal>). Estes são tokenizados em série e
    armazenados em <literal>text_token</literal>.
  </para>

<programlisting><![CDATA[
>>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> cfdist = ConditionalFreqDist()
>>> for genre in brown.groups():               # cada gênero
...     for item in brown.items(genre):        # cada arquivo
...         text_token = brown.read(item)      # tokeniza
]]></programlisting>

  <para>
    O próximo passo é construir uma lista com todos os verbos modais
    que foram encontrados. Para isto, extraímos e normalizamos o texto
    de cada token que possuir a tag <literal>md</literal>. Para cada
    uma destas palavras incrementaremos uma contagem. Este exemplo
    utiliza a distribuição condicional de freqüência, na qual a
    condição é o gênero atual e o evento é o verbo modal.
  </para>

<programlisting><![CDATA[
...         found = [token['TEXT'].lower()             # normaliza
...                  for token in text_token['WORDS']  # cada token
...                  if token['TAG'] == 'md']          # que for um modal
...         for modal in found:
...             cfdist[genre].inc(modal)               # incrementa a contagem 
]]></programlisting>

  <para>
    A distribuição condicional de freqüência nada mais é que um mapeamento
    entre cada gênero e a distribuição dos verbos modais em cada gênero. O
    fragmento de código a seguir identifica um pequeno conjunto de verbos
    modais de interesse e processa as estruturas de dados para exibir como
    resultado as contagens requeridas.
  </para>

<programlisting><![CDATA[
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']

>>> print "%40s" % 'Genre',              # gera os cabeçalhos de coluna
>>> for modal in modals:
...     print "%6s" % modal,
>>> print

>>> for genre in cfdist.conditions():    # gera as colunas
...     print "%40s" % genre,
...     for modal in modals:
...         print "%6d" % cfdist[genre].count(modal),
...     print

                                   Genre    can  could    may  might   must   will
                       skill and hobbies    273     59    130     22     83    259
                                   humor     17     33      8      8      9     13
                            popular lore    168    142    165     45     95    163
                          belles-lettres    249    216    213    113    169    222
                        fiction: science     16     49      4     12      8     16
                        press: reportage     94     86     66     36     50    387
miscellaneous: government & house organs    115     37    152     13     99    237
                      fiction: adventure     48    154      6     58     27     48
                        fiction: mystery     44    145     13     57     31     17
                        fiction: romance     79    195     11     51     46     43
                                religion     84     59     79     12     54     64
                                 learned    366    159    325    126    202    330
                          press: reviews     44     40     45     26     18     56
                        press: editorial    122     56     74     37     53    225
                        fiction: general     39    168      8     42     55     50
]]></programlisting>

  <para>
    Há alguns padrões interessantes nesta tabela. Por exemplo, compare as
    colunas para literatura governamental ("miscellaneous: government & house organs") 
    e literatura de aventura ("fiction: adventure"); a
    primeira é dominada pelo uso de <literal>can, may, must e will</literal>
    enquanto a última é caracterizada pelo uso de <literal>could</literal>
    e <literal>might</literal>. Com algum esforço adicional é possível
    adivinhar o gênero de cada novo texto automaticamente, de acordo com sua
    distribuição de verbos modais.
  </para>

  <para>
    Agora que vimos como tokens com tags e corpora com tags são criados e
    acessados, estamos prontos para dar uma olhada na categorização
    automática de palavras.
  </para>

  <important><para>
    No NLTK, a tokenização e o tagging são operações que 
    <emphasis>anotam</emphasis> os dados existentes. Assim, um
    documento é armazenado como um único token de texto e quando tokenizado
    para criar um conjunto de subtokens o texto original continua disponível
    por meio da propriedade <literal>TEXT</literal>. Note que o método
    padrão para exibição de texto tokenizado simplesmente não exibe este
    conteúdo.
  </para></important>


</section> <!-- Exploring text genres -->

</section>  <!-- More applications -->

<section id="reading">
  <title>Leituras adicionais</title>

<para>John Hopkins Center for Language and Speech Processing, 1999
Summer Workshop on Normalization of Non-Standard Words <ulink
url="http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf"><citetitle>Final
Report</citetitle></ulink>.
</para>

  <para>Glossário do SIL de termos lingüísticos (em inglês):
    http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/
  </para>

  <para>
    Language Files: Materials for an
    Introduction to Language and Linguistics (Eighth Edition),
    The Ohio State University Department of Linguistics,
    http://www.ling.ohio-state.edu/publications/files/
  </para>

</section>

<section id="exercises">
  <title>Exercícios</title>

  <orderedlist>

    <listitem>
      <formalpara>
        <title> Acessar e tokenizar um arquivo de texto </title>
        <para> Obter algum texto puro (por exemplo, visitando uma página da web e
	salvando seu conteúdo como texto puro) e armazer no arquivo
	'corpus.txt'.</para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Utilizando os métodos <literal>open()</literal> e
          <literal>read()</literal>, carregar o texto em uma variável de
	  tipo string e exibí-la.
        </para></listitem>
        <listitem><para>
	  Agora, inicialize um novo token com <literal>Token()</literal>,
	  utilizando este texto. Tokenize o texto com o
          <literal>WhitespaceTokenizer</literal> e especifique que
	  o rsultado resultado deverá ser armazenado na propriedade
	  <literal>WORDS</literal>. Exiba o resultado.
        </para></listitem>
        <listitem><para>
	  A seguir, calcule o numero de tokens utilizando a função
          <literal>len()</literal> e exiba o resultado.
        </para></listitem>
        <listitem><para>
	  Finalmente, discuta as falhas deste método de tokenização de um
	  texto. Em particular, identifique qualquer conteúdo que não tenha
	  sido corretamente tokenizado (talvez seja necessário utilizar
	  algum texto mais complexo).
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Tokenizar um texto utilizando expressões regulares </title>
        <para> Obtenha algum texto puro (por exemplo, visitando uma página da
	web e salvando seu conteúdo como texto puro) e armazene-o no
	arquivo 'corpus.txt' para que possas responder às seguintes questões.
	</para>
      </formalpara>
      <orderedlist>
        <listitem><para>
	  Processadores de texto normalmente dividem em sílabas as palavras
	  ao final das linhas (nas chamadas "quebra de linha"). Quando um 
	  documento processado por estes
	  programas é convertido para texto puro, estas partes geralmente não
	  são recombinadas. É fácil reconhecer estes casos procurando na
	  web por palavras quebradas como <literal>depart- ment</literal>.
	  Crie um <literal>RegexpTokenizer</literal> que trate este tipo
	  de palavra quebrada como um token único.
        </para></listitem>
        <listitem><para>
	  Considere o seguinte título de um livro:
          <emphasis>This Is the Beat Generation: New York-San Francisco-Paris</emphasis>
	  (Esta é a Geração Beat: Nova Iorque-São Francisco-Paris). O que seria
	  necessário para poder tokenizar este tipo de string de tal forma que cada
	  nome de cidade fosse armazenado como um único token?
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Trabalhando com texto com tags </title>
        <para> Escreva um programa que carregue o Brown Corpus e,
	dada uma palavra, liste todas as tags possíveis para esta e sua
	contagem de freqüência. Por exemplo, para a palavra
	<literal>strike</literal> o programa deverá gerar:
        <literal>[('nn', 25), ('vb', 21)]</literal>.
	(Sugestão: esta tarefa envolve reverter e ordenar a lista de
	tuples que estará na forma
        <literal>[(21, 'vb'), (25, 'nn')]</literal>.
	Para converter estas listas à forma requerida, use
        <literal>word_freq = [(y,x) for (x,y) in freq_word]</literal>.)
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
	  Use seu programa para exibir as tags e suas freqüências para as
	  seguintes palavras:
          <literal>can, fox, get, lift, like, but, frank, line, interest</literal>.
	  Assegure-se de que você conhece o significado de cada uma das tags
	  mais freqüentes.
        </para></listitem>
        <listitem><para>
	  Escreva um programa para encontrar as 20 palavras que possuem a
	  maior variedade de tags possíveis.
        </para></listitem>
        <listitem><para>
	  Escolha palavras que podem ser tanto substantivos quanto verbos
	  (como <literal>deal</literal>). Tente adivinhar qual é a tag mais
	  provável para cada palavra e confira se você está certo.
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Prevendo a próxima palavra </title>
        <para> O programa para previsão de palavras que vimos neste
	capítulo prende-se rapidamente em um ciclo. Modifique o programa
	de forma que a próxima palavra seja escolhida aleatoriamente
	entre uma lista das <replaceable>n</replaceable> palavras mais
	prováveis no contexto dado. (Sugestão: armazene as
	<replaceable>n</replaceable> palavras mais prováveis em uma
	lista <literal>lwords</literal> e então escolha aleatoriamente
	uma palavra a partir desta lista por meio do método 
	<literal>random.choice()</literal>.)
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
	  Escolha um gênero particular, como uma seção do Brown Corpus, uma
	  tradução do livro do Gênesis ou um dos corpora de grupos de
	  discussão ("newsgroups"). Treine seu sistema neste corpus e faça-o 
	  gerar texto
	  aleatoriamente. Você pode experimentar com diferentes palavras
	  iniciais. O texto resultante é compreensível? Examine os pontos
	  fortes e fracos deste método para geração aleatória de texto.
        </para></listitem>
        <listitem><para>
	  Repita a experiência com diferentes gêneros e com diferentes
	  quantidades de dados para treinamento. O que você pode observar?
        </para></listitem>
        <listitem><para>
	  Agora, treine seu sistema utilizando dois gêneros distintos e
	  experimente com a geração de texto de um gênero híbrido. Como
	  na questão anterior, discuta sobre suas observações.
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Classificação automática de palavras </title>
        <para> 
	  O programa para a classificação de palavras como substantivos ou
	  adjetivos obteve um índice de acerto de 71%. Vamos tentar criar
	  condições melhores para obter um sistema com índice de acerto
	  de 80% ou mais.
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
	  Considere a alternativa de utilizar um sufixo mais longo,
	  como por exemplo os últimos dois ou três caracteres. O que acontece com o
	  desempenho? Quais sufixos são reconhecidos como pertinentes aos
	  adjetivos?
        </para></listitem>
        <listitem><para>
	  Explore outras alternativas, como utilizar um comprimento variável de 
	  prefixos, o comprimento das próprias palavras ou o número de
	  vogais em uma palavra.
        </para></listitem>
        <listitem><para>
	  Por último, combine as múltiplas condições em um tuple e explore
	  quais combinações de condições fornecem os melhores resultados.
        </para></listitem>
      </orderedlist>
    </listitem>

  </orderedlist>
</section> <!-- Exercises -->

<appendix>
  <title> Nomes próprios utilizados freqüentemente </title>

<table id="table.properties"> 
  <title> Nomes próprios utilizados freqüentemente </title> 
  <tgroup cols="3">
    <colspec colwidth='3cm'/>
    <colspec colwidth='6cm'/>
    <thead> 
      <row> 
        <entry>Propriedade</entry> 
        <entry>Módulo</entry> 
      </row> 
    </thead> 
    <tbody> 
 
      <row>
        <entry> CHUNK </entry>
        <entry><literal>parser.chunk</literal></entry>
      </row>

      <row>
        <entry> CLASS </entry>
        <entry><literal>classifier</literal></entry>
      </row>

      <row>
        <entry> CLUSTER </entry>
        <entry><literal>clusterer</literal></entry>
      </row>

      <row>
        <entry> CONTEXT </entry>
        <entry><literal>feature.word, parser.chunk</literal></entry>
      </row>

      <row>
        <entry> FEATURES </entry>
        <entry><literal>clusterer, classifier, feature</literal></entry>
      </row>

      <row>
        <entry> LEAF </entry>
        <entry><literal>parser</literal></entry>
      </row>

      <row>
        <entry> LOC </entry>
        <entry><literal>tokenizer</literal></entry>
      </row>

      <row>
        <entry> PROB </entry>
        <entry><literal>clusterer, classifier, parser</literal></entry>
      </row>

      <row>
        <entry> STEM </entry>
        <entry><literal>stemmer</literal></entry>
      </row>

      <row>
        <entry> TAG </entry>
        <entry><literal>tagger</literal></entry>
      </row>

      <row>
        <entry> TEXT </entry>
        <entry><literal>tokenizer</literal></entry>
      </row>

      <row>
        <entry> TREE </entry>
        <entry><literal>parser</literal></entry>
      </row>

      <row>
        <entry> WORDS, LINES </entry>
        <entry><literal>tokenizer</literal></entry>
      </row>

    </tbody>
  </tgroup>
</table>

</appendix>

&index;

</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
adaptive-fill-mode:nil
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:t
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/Users/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

