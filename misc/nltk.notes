*
* Natural Language Toolkit
* Edward Loper
*
* $Id$
*

> To do soon
  1. comment/finish current code in nltk.py
  2. write proposal

>> Proposal Notes
  - Overview: describe project
  - Design Goals
      - easy to learn -- the whole idea is that they don't have to
        spend so much time learning an unfamiliar infrastructure
        (python).  
      - simplicity -- each class should be reimplementable.  i don't
        want to hide complexity, i want to structure it.
      - independance of pieces -- a project should require
        understanding only part of the toolkit.  that part should be
        independant of other parts
      - consistancy
  - Code Design & Impl
      - class hierarchy with interfaces
      - two main sections: data classes & processing classes
      - strict typing
      - language features: which ones will/won't I use?
  - Documentation Design & Impl
      - web pages
      - epydoc reference pages
      - tutorials
  - Preliminary Class Hierarchy
     - talk about the current nltk.py hierarchy
     - Data classes
        - Set
        - Tokens, token types, text locations
        - syntax trees?
        - samples, events, and frequency distributions
        - taggers
     - Critique of an impl: NthOrderTagger
        - Required classes: Tokenizer, SimpleTokenType,
          TaggedTokenType, CFSample, FreqDist, CFFreqDist(?)
> Design
>> Data Classes
  - Set
  - FreqDist
  - Word (words can be tagged, analyzed, etc.)
  - SyntaxTree
  - FreqDist
  - ProbDist

>> Processing Classes
  - Tokenizer: string \to [Word].  
      - TaggedTokenizer deals with the/DT, etc.
      - ChunkTokenizer?  Doesn't return [Word!!]
        Maybe use a ChunkReader?
  - Parser: string \to [SyntaxTree]
      - ChunkParser: returns 1-layer NP trees
      - ChartParser
  - Tagger: [Word] \to [(tag, Word)]?
    or [Word] \to [TaggedWord]
    or [Word] \to [Word] with some words as TaggedWord.
      - Tagger using FreqDist or ProbDist?
  - LanguageModel: train on data, can generate or 

>> Questions
  - how do I maintain a notion of original source?  e.g., where
    different chunks came from.  Have foo.source attribute?  Like
    word.source?  What would be stored in source?  character index in
    original string?  pointer to that string?  name of the file it
    came from?

>> Pset correlation
Pset 1

Pset 2
  - TaggedTokenizer
  - FreqDistTagger
    - FreqDist
  - FallbackTagger: takes multiple taggers as args
  - TaggedChunkTokenizer
  - RegexChunker
Pset 3

Pset 4


> Ramblings\ldots

>> Design
>>> Goals
  - simple
  - easy to use


>>> Conventions
  - InterfacesI
  - class names start with captial letter

>>> Structure
  1. Data Classes
    - Set
    - Syntax Tree
    - Word
    - FreqDist
    - ProbDist
  2. Processing Classes

>> Random Notes

Design & implement a natural language toolkit in python for steven's
class..  Use epydoc, interfaces.  Interface class is a class all of
whose members raise NotImplementedError.  E.g.:

# class TokenizerI:
#     def tokenize(words):
#         """## epydoc string... """
#         raise NotImplementedError("Interface method not implemented")

Interface hierarchies define interfaces..  Classes are subclassed from
their interfaces or from other classes.  The interfaces are just a way
to formalize what we expect; also allows us to inherit epydoc strings.

Components:
  - tokenizer (impl space tokenizer, maybe 1 other, let them implement
               a better tokenizer?  e.g., one that splits 's off, etc.)
  - parser
  - language modler -- can generate, etc.

Data Structures
  - word.. string?  
  - list of words, set of words, etc?
  - set
  - syntax tree?  a list with an id?
  - frequency chart -- basically like a dict, but default 0 values,
    and you can ask for relative freq, etc..  freq['a']+=1, 
    freq['b']/freq.N..

Utilities
  - list pretty-printing.
  - 

Issues:
  - what programming methods/styles should I use/not use?  What would
    be harder to learn.  e.g., learning curve for list comprehensions,
    exceptions, classes.

Proposal:
  1. structure
  2. design goals
  3. timetable
  4. (preliminary) design overview
  5. bibliography
  6. purpose

>> Tokens, Types, and Sources..

We'd like to remember where tokens come from.
Does it make sense to define a Token as a TokenType+Source?

Token defines:
  - token.type()
  - token.source()
  - testing for equality (==source and ==type)

TokenType defines:
  - testing for equality

Source defines:
  - testing for equality

What gets tags?  Tokens or TokenTypes??
Probably TokenTypes: "bank/NP" is a different type than "bank/VP"

What does a Source consist of?  typically an index into a string?
Maybe a pointer to that string?

Maybe sources just define comparison.
