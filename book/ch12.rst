.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. overview of book

.. TODO: mention sentiment classification as an example of a trivial vs hard problem
   (trivial with semantic orientation of adjectives; hard with interesting examples
   of where reviews easily mislead this approach and motivate deeper analysis, e.g.
   http://www.aclweb.org/anthology/W04-3253

.. preface::

=================================
Afterword: The Language Challenge
=================================

Natural language throws up some interesting computational challenges.
We've explored many of these in the preceding chapters, including
tokenization, tagging, classification, information extraction,
and building syntactic and semantic representations.
You should now be equipped to work with large datasets, to create
robust models of linguistic phenomena, and to extend them into
components for practical language technologies.  We hope that
the Natural Language Toolkit (|NLTK|) has served to open up the exciting
endeavor of practical natural language processing to a broader
audience than before.

In spite of all that has come before, language presents us with
far more than a temporary challenge for computation.  Consider the following
sentences which attest to the riches of language:

.. ex::
   .. ex:: Overhead the day drives level and grey, hiding the sun by a flight of grey spears.  (William Faulkner, *As I Lay Dying*, 1935)
   .. ex:: When using the toaster please ensure that the exhaust fan is turned on. (sign in dormitory kitchen)
   .. ex:: Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activities with Ki values of 45.1-271.6 |mu|\ M (Medline, PMID: 10718780)
   .. ex:: Iraqi Head Seeks Arms (spoof news headline)
   .. ex:: The earnest prayer of a righteous man has great power and wonderful results. (James 5:16b)
   .. ex:: Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll, *Jabberwocky*, 1872)
   .. ex:: There are two ways to do this, AFAIK :smile:  (internet discussion archive)
    
Other evidence for the riches of language is the vast array of disciplines
whose work centers on language.  Some obvious disciplines include
translation, literary criticism, philosophy, anthropology and psychology.
Many less obvious disciplines investigate language use, including
law, hermeneutics, forensics, telephony, pedagogy, archaeology, cryptanalysis and speech
pathology.  Each applies distinct methodologies to gather
observations, develop theories and test hypotheses.  All serve to
deepen our understanding of language and of the intellect that is
manifested in language.
          
In view of the complexity of language and the broad range of interest
in studying it from different angles, it's clear that we have barely
scratched the surface here.  Additionally, within |NLP| itself,
there are many important methods and applications that we haven't
mentioned.

In our closing remarks we will take a broader view of |NLP|,
including its foundations and the further directions you might
want to explore.  Some of the topics are not well-supported by |NLTK|,
and you might like to rectify that problem by contributing new software
and data to the toolkit.

----------------------------------------
Language Processing vs Symbol Processing
----------------------------------------

The very notion that natural language could be treated in a
computational manner grew out of a research program, dating back to
the early 1900s, to reconstruct mathematical reasoning using logic,
most clearly manifested in work by Frege, Russell, Wittgenstein,
Tarski, Lambek and Carnap.  This work led to the notion of language as
a formal system amenable to automatic processing.  Three later
developments laid the foundation for natural language processing.  The
first was `formal language theory`:dt:.  This defined a language as a set
of strings accepted by a class of automata, such as context-free
languages and pushdown automata, and provided the underpinnings for
computational syntax.

The second development was `symbolic logic`:dt:. This provided a
formal method for capturing selected aspects of natural language that
are relevant for expressing logical proofs. A formal calculus in
symbolic logic provides the syntax of a language, together with rules
of inference and, possibly, rules of interpretation in a set-theoretic
model; examples are propositional logic and First Order Logic.  Given
such a calculus, with a well-defined syntax and semantics, it becomes
possible to associate meanings with expressions of natural language by
translating them into expressions of the formal calculus. For example,
if we translate `John saw Mary`:lx: into a formula ``saw(j,m)``, we
(implicitly or explicitly) intepret the English verb `saw`:lx: as a
binary relation, and `John`:lx: and `Mary`:lx: as denoting
individuals.  More general statements like `All birds fly`:lx: require
quantifiers, in this case |forall|, meaning `for all`:lx:: |forall|\
`x (bird(x)`:math: |rarr| `fly(x))`:math:.  This use of logic provided
the technical machinery to perform inferences that are an important
part of language understanding.

A closely related development was the `principle of
compositionality`:dt:, namely that the meaning of a complex expression
is composed from the meaning of its parts and their mode of
combination (chap-semantics_). 
This principle provided a useful correspondence between
syntax and semantics, namely that the meaning of a complex expression
could be computed recursively.  Consider the sentence `It is not true
that`:lx: `p`:math:, where `p`:math: is a proposition.  We can
represent the meaning of this sentence as `not(p)`:math:.  Similarly, we
can represent the meaning of `John saw Mary`:lx: as `saw(j, m)`:math:.  Now we
can compute the interpretation of `It is not true that John saw Mary`:lx:
recursively, using the above information, to get
`not(saw(j,m))`:math:. 

The approaches just outlined share the premise that computing with
natural language crucially relies on rules for manipulating symbolic
representations. For a certain period in the development of |NLP|,
particularly during the 1980s, this premise provided a common starting
point for both linguists and practitioners of |NLP|, leading to a family
of grammar formalisms known as unification-based (or feature-based)
grammar (cf. chap-featgram_), and to |NLP| applications implemented in the Prolog
programming language. Although grammar-based |NLP| is still a
significant area of research, it has become somewhat eclipsed in the
last 15\ |ndash|\ 20 years due to a variety of factors. One
significant influence came from automatic speech recognition. Although
early work in speech processing adopted a model that emulated the
kind of rule-based phonological `phonology`:topic: processing typified
by the *Sound Pattern of English* [ChomskyHalle68]_,
this turned out to be hopelessly inadequate in dealing
with the hard problem of recognizing actual speech in anything like
real time. By contrast, systems which involved learning patterns from
large bodies of speech data were significantly more accurate,
efficient and robust. In addition, the speech community found that
progress in building better systems was hugely assisted by the
construction of shared resources for quantitatively measuring
performance against common test data. Eventually, much of the |NLP|
community embraced a `data intensive`:dt: orientation to language
processing, coupled with a growing use of machine-learning techniques
and evaluation-led methodology.

----------------------------------
Contemporary Philosophical Divides
----------------------------------

The contrasting approaches to |NLP| described in the preceding section
relate back to early metaphysical debates about `rationalism`:dt:
versus `empiricism`:dt: and `realism`:dt: versus `idealism`:dt: that
occurred in the Enlightenment period of Western philosophy.  These
debates took place against a backdrop of orthodox thinking in which
the source of all knowledge was believed to be divine revelation.
During this period of the seventeenth and eighteenth centuries,
philosophers argued that human reason or sensory experience has
priority over revelation.  Descartes and Leibniz, amongst others, took
the rationalist position, asserting that all truth has its origins in
human thought, and in the existence of "innate ideas" implanted in our
minds from birth.  For example, they argued that the principles of
Euclidean geometry were developed using human reason, and were not the
result of supernatural revelation or sensory experience.  In contrast,
Locke and others took the empiricist view, that our primary source of
knowledge is the experience of our faculties, and that human reason
plays a secondary role in reflecting on that experience.  Often-cited
evidence for this position was Galileo's discovery |mdash| based on
careful observation of the motion of the planets |mdash| that the
solar system is heliocentric and not geocentric.  In the context of
linguistics, this debate leads to the following question: to what
extent does human linguistic experience, versus our innate "language
faculty", provide the basis for our knowledge of language?  In |NLP|
this matter surfaces as differences in the priority of corpus data
versus linguistic introspection in the construction of computational models.

A further concern, enshrined in the debate between realism and
idealism, was the metaphysical status of the constructs of a theory.
Kant argued for a distinction between phenomena, the manifestations we
can experience, and "things in themselves" which can never been
known directly.  A linguistic realist would take a theoretical
construct like `noun phrase`:dt: to be a real world entity that exists
independently of human perception and reason, and which actually
*causes* the observed linguistic phenomena.  A linguistic idealist, on
the other hand, would argue that noun phrases, along with more
abstract constructs like semantic representations, are intrinsically
unobservable, and simply play the role of useful fictions.  The way
linguists write about theories often betrays a realist position, while
|NLP| practitioners occupy neutral territory or else lean towards the
idealist position.  Thus, in |NLP|, it is often enough if a theoretical
abstraction leads to a useful result; it does not matter whether this
result sheds any light on human linguistic processing.

These issues are still alive today, and show up in the distinctions
between symbolic vs statistical methods, deep vs shallow processing,
binary vs gradient classifications, and scientific vs engineering
goals.  However, such contrasts are now highly nuanced, and the debate
is no longer as polarized as it once was.  In fact, most of the
discussions |mdash| and most of the advances even |mdash| involve a
"balancing act".  For example, one intermediate position is to assume
that humans are innately endowed with analogical and memory-based
learning methods (weak rationalism), and to use these methods to identify
meaningful patterns in their sensory language experience (empiricism).
We have seen many examples of this methodology throughout this book.
Statistical methods inform symbolic models any time corpus statistics
are used to guide the selection of rules in a rule-based grammar.
Symbolic methods inform statistical models any time a corpus that
was created using rule-based methods is used to train a statistical
language model.  Now the circle is closed, and the dichotomy has virtually
disappeared.

--------------
The Holy Grail
--------------

* NLP-Complete Problems: SLDS, MT
  (cf AI-complete)

* Why they are hard

* The problem of grounding.  Embodied conversational agents.

* Approaches: "grammar engineering" (scaling up a rule-based approach
  with the help of engineering methods such as grammar test suites);
  "grammar inference" (training on manually-checked annotated data). 

* Even simple problems!
  http://itre.cis.upenn.edu/~myl/languagelog/archives/001445.html


------------
NLTK Roadmap
------------

The Natural Language Toolkit is work-in-progress, and is being
continually expanded as people contribute code.  Some areas of
|NLP| and linguistics are not (yet) well supported in |NLTK|,
and contributions in these areas are particularly welcome.
Check |NLTK-URL| for news about developments after the publication
date of the book. 

Phonology and Morphology
------------------------

* definitions
* linguistic challenges
  (suppletion, non-concatenative morphology)

* standard methods: finite state
* need for integration: single lexicon; easier to do morphosyntax (e.g. morphological features that are salient in the syntax)
  (see online materials for this chapter for links)

High-Performance Components
---------------------------

* PCFG induction, SRL training, ...
* trained models (often fast to use)
* NLTK's package system provides a convenient way to distribute these
* parallel processing, MapReduce, ...
* interfaces to other systems for machine learning and data mining

Lexical Semantics
-----------------

* lexicon model with semantic information
* multi-word expressions
* inheritance, etc

Natural Language Generation
---------------------------

Unification-based approach in ``nltk_contrib``.

Other Languages
---------------

* available corpora
* examples in HOWTOs (incl a language-specific HOWTO)
* book translations
  (widely-agreed terminology)
  (resources in the language so that examples aren't all about English)

NLTK-Contrib
------------

* staging area
* anything in Python relevant to NLP is welcome

Teaching Materials
------------------

* teachers who develop materials for this content, such as presentation slides,
  problem sets and solutions, etc, including extended treatments that make up for
  shortcomings of this book's coverage, are invited to share them via the |NLTK| website.

* Of particular value are materials that help |NLP| become a mainstream course in
  the undergraduate programs of computer science and linguistics departments;
  and accessible at the secondary level where there is significant scope for
  including computational content in the language and literature curricula.

--------
Envoi...
--------

Linguists are sometimes asked how many languages they speak,
and have to explain that linguistics actually concerns the
study of abstract structures that are shared by languages.
Similarly, computer scientists are sometimes asked how many
programming languages they know, and have to explain that computer science
actually concerns data structures and algorithms that can be
implemented in any programming language.

This book has covered many topics in the field of Natural Language Processing.
Most of the examples have used Python and English.
However, it would be unfortunate if readers concluded that
|NLP| is about how to write Python programs to manipulate English text.
Our choice of Python and English was expedient, nothing more.
Programming is a useful |mdash| and marketable |mdash| skill,
but it should only be seen as a means to an end:
as a way to understand the data structures and algorithms
for representing and manipulating collections of linguistically annotated text,
and as a way to build new technologies that can work with natural language.  

.. include:: footer.rst
