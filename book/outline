Introduction to Computational Linguistics
Steven Bird, Ewan Klein & Edward Loper

The field of computational linguistics (CL) is experiencing rapid
growth as its theories, methodologies and tools find application in a
rich variety of new language technologies.  Ubiquitous computing
devices demand non-keyboard based interfaces, including new
handwriting, speech and dialogue technologies.  The explosion of text
and multimedia content on the world-wide web demands new information
extraction, translation and summarisation technologies.  Linguists
need to have a working knowledge of the theories, methodologies and
tools of CL so that they can participate in this exciting new
endeavour.  There are important opportunities for linguists to
contribute their insights to the future development of CL and, in the
reverse direction, to apply the results of CL research back in
linguistics.

In the late 1980s and early 1990s there was a promising convergence
between the fields of linguistics and CL.
(This had been a feature of the 1960s, e.g. with the application of the
SPE model in speech synthesis systems.)
Computational linguists often looked to linguistics as a source of
knowledge about language.  Over the last 5-10 years we have seen a
new divergence, as computational linguists have discovered that
linguistic analyses often failed to account for the linguistic
patterns attested in the large corpora used to develop their systems.
Once linguists learn to work with these large datasets, their own
analytical work will benefit (e.g. broader coverage, earlier
refutation of false hypotheses), and will once more find application
in the development of language technologies.

The goal of this book is to equip linguists to work with these large
datasets, create robust models of linguistic phenomena, and deploy
them in prototype language technologies.  In order to do this, the
book will contain an equal balance of theoretical foundations and
practical application.  For example, we believe that readers must
understand parsing algorithms, and they must also be able to implement
a parser.

Integrating theory and practice is a difficult challenge, and none of
the current CL textbooks address it satisfactorily.  One approach is
to focus on theory and leave all implementation work as an exercise
for the reader (e.g. Jurafsky & Martin, Manning & Schutze, McEnery &
Wilson).  Another option is to focus on specific programming tasks for
linguistics (e.g. Hammond).  Neither approach is suitable for a
one-semester course on CL in a linguistics department.  The former
provides the teacher no framework for a practical component, and
assumes that the teacher is already a computational linguist.  The
latter gets bogged down in the mechanics of programming and doesn't
succeed in teaching any significant CL.  The teacher of a one-semester
course faces an acute dilemma: forget the practical component and just
teach the theory or teach programming and try to fit in some CL at the
end once they can manage all the "housekeeping" functions (file I/O,
representing and displaying grammars and trees, etc).  It is clear
that these considerable overheads and shortcomings warrant a fresh
approach to CL pedagogy.

Apart from the practical component, CL courses may also depend on
software for in-class demonstrations.  This context calls for
interactive graphical user interfaces, making it possible to view
program state (e.g. the chart of a chart parser), observe program
execution step-by-step (e.g. execution of a finite-state machine), and
even make minor modifications to programs in response to ``what if''
questions from the class.  Because of these difficulties it is common
to avoid live demonstrations, and keep classes for theoretical
presentations only.  Apart from being dull, this approach leaves
students to solve important practical problems on their own, or to
deal with them less efficiently in office hours.
 
This book confronts these problems by tightly integrating the
theoretical content with a practical component based on NLTK, the
Natural Language Toolkit.  The toolkit was developed in conjunction
with a CL course at the University of Pennsylvania, a leading centre
for CL teaching and research.  (Loper continues to conduct research
and teaching at Penn.)  The practical component will be supported by
significant corpus samples from the Linguistic Data Consortium, the
leading publisher of linguistic data.  (Bird continues to be
affiliated with the LDC.)  Key features of the toolkit are that
students augment and replace existing components, they learn
structured programming by example, and they manipulate sophisticated
models from the outset.  The integrated material will continue to be
tested in courses in 2003 at the universities of Pennsylvania,
Edinburgh and Melbourne.

A carefully argued rationale for the design of the toolkit, selection
of programming language, and suitability for CL teaching, is presented
in a paper "NLTK: The Natural Language Toolkit" by Edward Loper and
Steven Bird, and appeared in the Proceedings of the ACL Workshop on
Effective Tools and Methodologies for Teaching Natural Language
Processing and Computational Linguistics, Philadelphia, July 2002.
It is available at [http://arxiv.org/abs/cs/0205028].
The toolkit is available from nltk.sourceforge.net.  Over the last 18
months there have been more than 4,000 downloads.  We are confident
that the demand for the toolkit will generate demand for the textbook.

The book will consist of an introduction, a sequence of
technology-oriented chapters, and a final chapter showing how the
technologies are combined into sophisticated applications.
Each of the technology-oriented chapters will be organized as
follows:

X.1 introduction: motivation, background, history
    - why is this an interesting problem for CL?

X.2 linguistic overview (for non-linguist readers)
    - how have linguists addressed the problem?
    - what are the shortcomings of the non-computational approach?

X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)

X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions

X.5 implementation
    - how to do it using NLTK

X.6 resources
    - annotated bibliography
    - online resources

X.7 exercises
    - simple problems and worked solutions
    - suggested term projects

Technical background, e.g. in elementary statistics and information
theory, will be provided along the way.  This "just-in-time"
philosophy will ensure that the learning curve is as shallow as it can
be, and that the benefits of learning this content are realized as
quickly as possible.

Chapter topics are selected on the basis of centrality to CL and of
existing support within NLTK.

PROPOSED CHAPTERS AND TARGET DATES

1. Introduction

PART I: Tagging and Parsing

2. Tagging [Ewan, March]
  - regexps, stemming, tokenization, tagging (frequency-based),
    pos/sense/morphological tagging

3. Parsing [Steven, March]
  - simple grammars, simple parsing algorithms (top down, bottom up,
    left corner)

4. Light Parsing [Steven, April]
  - chunking

5. Chart Parsing [Edward]

PART II: Models of Linguistic Information

6. Lexicon
  - wordnet, semantic relationships, hierarchical lexicon

7. Feature-Based Grammars [Ewan][depends on new NLTK code]
  - simple model, adding features to existing simple grammars, unification
  - LFG, HPSG (toy grammars to illustrate the essence of the approach)

8. Semantics [Ewan][depends on new NLTK code]
  - GPSG-style approach using lambda-abstracts
  - HPSG-style approach using unification

PART III: Data-Intensive Approaches

9. Classification [Edward, March]
   - word-sense disambiguation, named-entity detection, coreference
     annotation, document classification

10. Speech Processing
   - speech recognition and synthesis

11. Corpus Analysis [Steven, June]
   - survey of corpus types, methods for working with annotations

PART IV: Applications

12. Language Technology Applications [All]
  - a selection of: machine translation, dialog systems, document summarization,
    information extraction, text retrieval, question answering

Further chapters to be added in a second edition: finite state
transducers, probabilistic context free grammars.
