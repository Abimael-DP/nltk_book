.. -*- mode: rst -*-
.. include:: ../definitions.rst
.. include:: regexp-defns.rst

.. standard global imports

    >>> import nltk, re, pprint
    >>> from nltk import word_tokenize

.. TODO: recipes for flattening a list of lists into a list, and for the reverse grouping a list into a list of lists
.. TODO: discuss duck typing
.. TODO: check ch06-extras for further content
.. TODO: shared values between multiple dictionaries
.. TODO: general technique for computing transitive closures, e.g.
         adjectives connected by synonymy to a particular word, cf.
         http://www.aclweb.org/anthology/W04-3253
.. TODO: explain function vs method
.. TODO: say more about performance tuning of a Python program
.. TODO: add exercises on string formatting
.. TODO: this chapter presumes knowledge of dictionaries, not defined until ch05
.. TODO: the "b" flag for read(), when to use "rb"; also the "rU" flag

.. _app-python:

=========================================
13. Appendix: Enough Python for this Book
=========================================

This appendex introduces the Python programming language, and it is
intended for readers who are new to Python, or new to programming.
Unlike other introductions to Python, this presentation focuses on
linguistically-motivated tasks.

We'll address the following questions:

1. How can you get started with Python, and use the interpreter and editor?

2. How do the fundamental building blocks of Python work, such as loops, functions and assignment?

3. What are some of the common pitfalls with Python programming and how can you avoid them?

This chapter contains many examples and exercises.
If you are new to programming, we encourage you to work through them carefully
and consult other introductions to programming if necessary.

The Python website also provides extensive documentation,
including introductory tutorials and links to other resources.
Please see ``http://docs.python.org/``.
When using the online Python documentation, be aware that
your installed version might be different from the version
of the documentation you are reading.  You can easily
check what version you have, with ``import sys; sys.version``,
and then consult version-specific documentation on the Python website.


---------------
Getting Started
---------------

One of the friendly things about Python is that it allows you
to type directly into the interactive `interpreter`:dt: |mdash|
the program that will be running your Python programs.
You can access the Python interpreter using a simple graphical interface
called the Interactive DeveLopment Environment (|IDLE|).
On a Mac you can find this under *Applications*\ |rarr|\ *MacPython*,
and on Windows under *All Programs*\ |rarr|\ *Python*.
Under Unix you can run Python from the shell by typing ``idle``
(if this is not installed, try typing ``python``).
The interpreter will print a blurb about your Python version;
simply check that you are running Python 3.2 or later
(here it is for 3.4.2):

.. doctest-ignore::
    Python 3.4.2 (default, Nov 12 2014, 18:23:59) 
    [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.54)] on darwin
    Type "help", "copyright", "credits" or "license" for more information.
    >>>

.. note::
   If you are unable to run the Python interpreter, you probably don't
   have Python installed correctly.  Please visit |PYTHON-URL| for
   detailed instructions. NLTK 3.0 works for Python 2.6, 2.7 and 3.2 onwards.
   If you are using Python 2.6 or 2.7, note that the ``/`` operator rounds
   fractional results downwards (so ``1/3`` will give you ``0``).
   In order to get the expected behavior of division
   you need to type: ``from __future__ import division``.
   Users of Python 2.6 and 2.7 will also need to type the following
   statement in order to use the print function in our examples:
   ``from __future__ import print_function``.


The ``>>>`` prompt indicates that the Python interpreter is now waiting
for input.  When copying examples from this book, don't type
the "``>>>``" yourself.  Now, let's begin by using Python as a calculator:

    >>> 1 + 5 * 2 - 3
    8
    >>>

Once the interpreter has finished calculating the answer and displaying it, the
prompt reappears. This means the Python interpreter is waiting for another instruction.

.. note:: |TRY|
   Enter a few more expressions of your own. You can use asterisk (``*``)
   for multiplication and slash (``/``) for division, and parentheses for
   bracketing expressions.

.. XXX The following example currently wraps over a page boundary, which
   makes it difficult to read, esp since you can't see where the "^" is
   pointing.

The preceding examples demonstrate how you can work interactively with the
Python interpreter, experimenting with various expressions in the language
to see what they do.
Now let's try a nonsensical expression to see how the interpreter handles it:

    >>> 1 +
      File "<stdin>", line 1 
        1 +
          ^
    SyntaxError: invalid syntax
    >>>

This produced a `syntax error`:dt:.  In Python, it doesn't make sense
to end an instruction with a plus sign. The Python interpreter
indicates the line where the problem occurred (line 1 of ``<stdin>``,
which stands for "standard input").

Now that we can use the Python interpreter, we're ready to start
learning Python, and applying it to language data.


.. _sec-texts-as-lists-of-words:

-----------------------
Texts as Lists of Words
-----------------------

What is a text?  At one level, it is a sequence of symbols on a page such
as this one, or a sequence of characters in a text file.  At another level, it is a sequence of chapters, made up
of a sequence of sections, where each section is a sequence of paragraphs,
and so on.  However, for our purposes, it is sufficient to think of a text as nothing
more than a sequence of words and punctuation. For this, you need to
understand the "list" data type.

Lists
-----

Here's how we will represent a text in Python:

    >>> sent = ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']
    >>>

After the prompt we've given a name we made up, ``sent``, short for
"sentence," followed
by the equals sign, and then some quoted words, separated with
commas, and surrounded with brackets.  This bracketed material
is known as a `list`:dt: in Python:.
We can inspect it by typing the name inspect-var_. We can ask for its
length using the built-in ``len`` function len-sent_. 

    >>> sent # [_inspect-var]
    ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']
    >>> len(sent) # [_len-sent]
    9
    >>>

The expression ``len(sent)`` means that we are 'calling' a function ``len``
with an argument ``sent``. When we don't know, or don't care, about the
particular argument, we'll write ``len()`` to refer to the
function. We'll look more at functions, and how you can define your
own, in sec-vocabularies-as-sets-of-words_.

.. note:: |TRY|
   Make up a few sentences of your own, by typing a name, equals
   sign, and a list of words, like this:
   ``ex1 = ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']``.

Once you're comfortable defining little texts like this, you can try
out another built-in function:

    >>> sorted(sent)
    ['?', 'What', 'airspeed', 'an', 'is', 'of', 'swallow', 'the', 'unladen']
    >>>

So ``sorted()`` will take a list as argument and give us back the sorted
version of the list. You might wonder why ``'?'`` occurs first in the sequence.
In fact it it is standard for computers to sort punctuation
and uppercase letters before lowercase letters. This is due
to the computer's underlying numerical representation of text, which we can see
by asking Python to tell us the ordinal value of a character:

    >>> ord('?') 
    63 
    >>> ord('W')
    87
    >>> ord('w')
    119
    >>>

.. note:: |TRY|
   Try sorting the sentences that you made up.

A pleasant surprise is that we can use Python's addition operator on lists.
Adding two lists creates a new list
with everything from the first list, followed
by everything from the second list:

    >>> ['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail']
    ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']
    >>>

This special use of the addition operation is called `concatenation`:dt:;
it combines the lists together into a single list.  We can concatenate
sentences to build up a text.

We don't have to type out the lists each time; we can name them,
then refer back to them using those names:

    >>> frag1 = ['Monty', 'Python']
    >>> frag2 = ['and', 'the', 'Holy', 'Grail']
    >>> frag1 + frag2
    ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']
    >>>

We can count the number of times a word occurs in a list.
To do this, we specify the name of the list, followed by a period,
then the method name ``count()``, followed by the word in parentheses:
    
    >>> frag1.count('Python')
    1
    >>> frag2.count('Python')
    0
    >>>

Methods in Python work pretty much like functions in that they are
applied to arguments; we'll say more about them, and the notation
that's used, later in this chapter.
 
We can also `append`:dt: an item to a list.
When we use ``append()``, no result is printed, but the list is
modified and we can inspect it in the usual way.
    
    >>> sent.append("!!!")
    >>> sent
    ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?', '!!!']
    >>> 

.. note:: |TRY|
   Before continuing, experiment with your own example sentences and make sure you can
   use Python's general-purpose functions ``len()`` and ``sorted()``, and the
   special list methods ``count()`` and ``append()``.
   

Indexing Lists
--------------

As we have seen, a text in Python is a list of words, represented
using a combination of brackets and quotes. We can use Python to
perform various operations on a text, such as getting its length,
or counting the number of times a certain word appears.

Each item of a list has a position, or `index`:dt:, starting from
zero. Here are the indexes for our example sentence::

   | What | is | the | airspeed | of | an | unladen | swallow | ? |
    0      1    2     3          4    5    6         7         8   9

We look up the items of a list by index as follows:

    >>> sent[3]
    'airspeed'
    >>>

We can do the converse; given a word, find the index of its first occurrence:

    >>> sent.index('airspeed')
    3
    >>>

Notice that our indexes start from zero: ``sent`` element zero, i.e., ``sent[0]``,
is the first word ``What``, and ``sent[7]`` is ``swallow``. 
The reason is simple: the moment Python accesses the content of a list from
the computer's memory, it is already at the first element;
we have to tell it how many elements forward to go to access a given item.
Thus, zero steps forward gives us the first element.

.. note::
   This practice of counting from zero is initially confusing,
   but typical of modern programming languages.
   You'll quickly get the hang of it if
   you've mastered the system of counting centuries where 19XY is a year
   in the 20th century, or if you live in a country where the floors of
   a building are numbered from 1, and so walking up `n-1`:math: flights of
   stairs takes you to level `n`:math:. 

Now, if we accidentally use an index that is too large, we get an error:

    >>> sent[10]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: list index out of range
    >>>

This time it is not a syntax error, because the program fragment is syntactically correct.
Instead, it is a `runtime error`:dt:, and it produces a ``Traceback`` message that
shows the context of the error, followed by the name of the error, i.e.,
``IndexError``, and a brief explanation.

Slicing Lists
-------------

We can access sublists as well, a technique is known as `slicing`:dt:.

    >>> sent[2:8]
    ['the', 'airspeed', 'of', 'an', 'unladen', 'swallow']
    >>>

Note that the slice goes up to, but does not include, the 8th element.
Our visualization of lists, with the numbers appearing to the left of
their cells, is a helpful way to remember this behavior of slicing::

   | What | is | the | airspeed | of | an | unladen | swallow | ? |
    0      1    2     3          4    5    6         7         8   9

We can also use slicing to isolate a single item in a list, e.g.,
``sent[3:4]``. But by contrast with ``sent[3]``, which returns an
*element* of the list ``sent``, slicing always returns a sublist, even
though in this case the sublist only contains a single item:

    >>> sent[3:4]
    ['airspeed']

Slicing is useful when we want to extract manageable chunks of language from large texts.
Here we have loaded the entire text of *Monty Python and the Holy
Grail*, and have sliced out 25 items:

.. doctest-ignore::
    >>> holy_grail[1600:1625]
    ['We', "'", 're', 'an', 'anarcho', '-', 'syndicalist', 'commune', '.', 'We',
    'take', 'it', 'in', 'turns', 'to', 'act', 'as', 'a', 'sort', 'of', 'executive',
    'officer', 'for', 'the', 'week']
    >>>

By convention, ``m:n`` means elements `m`:mathit:\ |dots|\ `n-1`:mathit:.
As the next example shows,
we can omit the first number if the slice begins at the start of the
list slice2_, and we can omit the second number if the slice goes to the end slice3_:

    >>> sent[:4] # [_slice2]
    ['What', 'is', 'the', 'airspeed']
    >>> sent[4:] # [_slice3]
    ['of', 'an', 'unladen', 'swallow', '?', '!!!']
    >>>

We can modify an element of a list by assigning to one of its index values.
In the next example, we put ``sent[0]`` on the left of the equals sign list-assignment_.  We can also
replace an entire slice with new material slice-assignment_.  A consequence of this
last change is that the list only has four elements, and trying to access a later value
generates an error list-error_.

    >>> sent[0] = 'First' # [_list-assignment]
    >>> sent[9] = 'Last'
    >>> len(sent)
    10
    >>> sent[1:9] = ['Second', 'Third'] # [_slice-assignment]
    >>> sent
    ['First', 'Second', 'Third', 'Last']
    >>> len(sent)
    4
    >>> sent[4] # [_list-error]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: list index out of range
    >>>    

.. note:: |TRY|
   Take a few minutes to define a sentence of your own and modify individual words and
   groups of words (slices) using the same methods used earlier.  Check your understanding
   by trying the exercises on lists at the end of this chapter.

Variables
---------

Several times now, we have seen lines like the following:

    >>> sent = ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']
    >>>

Such lines have the form: *variable = expression*.  Python will evaluate
the expression on the right, and save its result to the variable on
the left.  This process is
called `assignment`:dt:.  It does not generate any output;
you have to type the variable on a line of its
own to inspect its contents.  The equals sign is slightly misleading,
since information is moving from the right side to the left.
It might help to think of it as a left-arrow.
The name of the variable can be anything you like, e.g., ``my_sent``, ``sentence``, ``xyzzy``.
It must start with a letter, and can include numbers and underscores.
Here are some examples of variables and assignments:
  
    >>> my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode',
    ... 'forth', 'from', 'Camelot', '.']
    >>> noun_phrase = my_sent[1:4]
    >>> noun_phrase
    ['bold', 'Sir', 'Robin']
    >>> wOrDs = sorted(noun_phrase)
    >>> wOrDs
    ['Robin', 'Sir', 'bold']
    >>>

Remember that capitalized words appear before lowercase words in sorted lists.

.. note::
   In the previous example we split the definition
   of ``my_sent`` over two lines.  Python expressions can be split across
   multiple lines, so long as this happens within any kind of brackets.
   Python uses the "``...``" prompt to indicate that more input is
   expected.  It doesn't matter how much indentation is used in these
   continuation lines, but some indentation usually makes them easier to read.

It is good to choose meaningful variable names to remind you |mdash| and to help anyone
else who reads your Python code |mdash| what your code is meant to do.
Python does not try to make sense of the names; it blindly follows your instructions,
and does not object if you do something confusing, such as ``one = 'two'`` or ``two = 3``.
The only restriction is that
a variable name cannot be any of Python's reserved words, such as
``def``, ``if``, ``not``,
and ``import``.  If you use a reserved word, Python will produce a syntax error:

    >>> not = 'Camelot'           # doctest: +SKIP
    File "<stdin>", line 1
        not = 'Camelot'
            ^
    SyntaxError: invalid syntax
    >>>

.. caution::
   Take care with your choice of names (or `identifiers`:dt:) for Python
   variables.  First, you should start the name with a letter, optionally
   followed by digits (``0`` to ``9``) or letters. Thus, ``abc23`` is fine, but
   ``23abc`` will cause a syntax error. 
   Names are case-sensitive, which means that ``myVar`` and ``myvar``
   are distinct variables.  Variable names cannot contain whitespace,
   but you can separate words using an underscore, e.g.,
   ``my_var``. Be careful not to insert a hyphen instead of an
   underscore: ``my-var`` is wrong, since Python interprets the
   "``-``" as a minus sign. 


   .. _sec-vocabularies-as-sets-of-words:
   
-----------------------------
Vocabularies as Sets of Words
-----------------------------

.. reimport

   >>> from nltk.book import *

The most obvious fact about texts that emerges from the examples in sec-overview-of-nltk_
is that
they differ in the vocabulary they use.  In this section we will see how to use the
computer to count the words in a text in a variety of useful ways.
As explained sec-overview-of-nltk_, you will first need to install
NLTK's data, and then load the example texts from
NLTK's book module, using the command ``from nltk.book import *``.

Let's begin by finding out the length of a text from start to finish,
in terms of the words and punctuation symbols that appear.
Here we will do it for ``text3``, which contains the text of the Book
of Genesis:

    >>> len(text3)
    44764
    >>>

So Genesis has 44,764 words and punctuation symbols, or "tokens."
A `token`:dt: is the technical name for a sequence of characters
|mdash| such as ``hairy``, ``his``, or ``:)`` |mdash| that we want to treat as a
group. When we count the number of tokens in a text, say, the phrase
`to be or not to be`:lx:, we are counting occurrences of these
sequences. Thus, in our example phrase there are two occurrences of `to`:lx:,
two of `be`:lx:, and one each of `or`:lx: and `not`:lx:. But there are
only four distinct vocabulary items in this phrase.
How many distinct words does the book of Genesis contain?
To work this out in Python, we have to pose the question slightly
differently.  The vocabulary of a text is just the *set* of tokens
that it uses, since in a set, all duplicates are collapsed
together. In Python we can obtain the vocabulary items of ``text3`` with the
command: ``set(text3)``.  When you do this, many screens of words will
fly past.  Now try the following: 

    >>> sorted(set(text3)) # [_sorted-set]
    ['!', "'", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)',
    'A', 'Abel', 'Abelmizraim', 'Abidah', 'Abide', 'Abimael', 'Abimelech',
    'Abr', 'Abrah', 'Abraham', 'Abram', 'Accad', 'Achbor', 'Adah', ...]
    >>> len(set(text3)) # [_len-set]
    2789
    >>>

By applying Python's built-in ``sorted`` function to the expression ``set(text3)``
sorted-set_,  we obtain a sorted list of vocabulary items, beginning
with various punctuation symbols and continuing with words starting with `A`:lx:.
We discover the size of the vocabulary indirectly, by asking
for the number of items in this set, and again we can use ``len`` to
obtain this number len-set_.  Although it has 44,764 tokens, this book
has only 2,789 distinct words, or "word types."
A `word type`:dt: is the form or spelling of the word independently of its
specific occurrences in a text |mdash| that is, the
word considered as a unique item of vocabulary.  Our count of 2,789 items
will include punctuation symbols, so we will generally call these
unique items `types`:dt: instead of word types. 

Now, let's calculate a measure of the lexical
richness of the text.  The next example shows us that the number of
distinct words is just 6% of the total number of words, or equivalently
that each word is used 16 times on average
(remember if you're using Python 2, to start with ``from __future__ import division``).

    >>> len(set(text3)) / len(text3)
    0.06230453042623537
    >>>

Next, let's focus on particular words.  We can count how often a word occurs
in a text, and compute what percentage of the text is taken up by a specific word:

    >>> text3.count("smote")
    5
    >>> 100 * text4.count('a') / len(text4)
    1.4643016433938312
    >>>

.. note:: |TRY|
   How many times does the word `lol`:lx: appear in ``text5``?
   How much is this as a percentage of the total number of words
   in this text? 

You may want to repeat such calculations on several texts,
but it is tedious to keep retyping the formula.  Instead,
you can come up with your own name for a task, like
"lexical_diversity" or "percentage", and associate it with a block of code.
Now you only have to type a short
name instead of one or more complete lines of Python code, and
you can re-use it as often as you like. The block of code that does a
task for us is called a `function`:dt:. We have already looked at some of
Python's built in functions, and now it's time to define a couple of our own. 
To introduce a function definition we use the keyword ``def``, followed by a
short name for the function. 
Here's a couple of examples,
``lexical_diversity()`` and   ``percentage()``:

    >>> def lexical_diversity(text): # [_fun-parameter1]
    ...     return len(set(text)) / len(text) # [_locvar]
    ...
    >>> def percentage(count, total): # [_fun-parameter2]
    ...     return 100 * count / total
    ...

.. caution::
   The Python interpreter changes the prompt from
   ``>>>`` to ``...`` after encountering the colon at the
   end of the first line.  The ``...`` prompt indicates
   that Python expects an `indented code block`:dt: to appear next.
   It is up to you to do the indentation, by typing four
   spaces or hitting the tab key.  To finish the indented block just
   enter a blank line.

In the definition of ``lexical_diversity()`` fun-parameter1_, we
specify a `parameter`:dt: named ``text`` . This parameter is
a "placeholder" for the actual text whose lexical diversity we want to
compute, and reoccurs in the block of code that will run when the
function is used locvar_. Similarly, ``percentage()`` is defined to
take two parameters, named ``count`` and ``total`` fun-parameter2_.

Once Python knows that ``lexical_diversity()`` and ``percentage()``
are the names for specific blocks
of code, we can go ahead and use these functions:

    >>> lexical_diversity(text3)
    0.06230453042623537
    >>> lexical_diversity(text5)
    0.13477005109975562
    >>> percentage(4, 5)
    80.0
    >>> percentage(text4.count('a'), len(text4))
    1.4643016433938312
    >>> 

To recap, we use or `call`:dt: a function such as ``lexical_diversity()`` by typing its name, followed
by an open parenthesis, the name of the text, and then a close
parenthesis. These parentheses will show up often; their role is to separate
the name of a task |mdash| such as ``lexical_diversity()`` |mdash| from the data
that the task is to be performed on |mdash| such as ``text3``.
The data value that we place in the parentheses when we call a
function is an `argument`:dt: to the function.

You have already encountered several functions in this chapter, such
as ``len()``, ``set()``, and ``sorted()``. By convention, we will
always add an empty pair of parentheses after a function name, as in
``len()``, just to make clear that what we are talking about is a
function rather than some other kind of Python expression.
Functions are an important concept in programming, and we only
mention them at this point to give you a sense of the
power and creativity of programming.  Don't worry if you find it a bit
confusing right now. 

Later we'll see how to use functions when tabulating data, as in tab-brown-types_.
Each row of the table will involve the same computation but
with different data, and we'll do this repetitive work using a function.

.. table:: tab-brown-types

   ==================  ======  =====  =================
   Genre               Tokens  Types  Lexical diversity
   ==================  ======  =====  =================
   skill and hobbies   82345   11935  0.145
   humor               21695   5017   0.231
   fiction: science    14470   3233   0.223
   press: reportage    100554  14394  0.143
   fiction: romance    70022   8452   0.121
   religion            39399   6373   0.162
   ==================  ======  =====  =================

   Lexical Diversity of Various Genres in the *Brown Corpus*

.. _sec-making-decisions:

----------------
Making Decisions
----------------

.. reimport

   >>> from nltk.book import *

So far, our little programs have had some interesting qualities:
the ability to work with language, and
the potential to save effort through automation.
A key feature of programming is the ability of machines to
make decisions on our behalf, executing instructions when
certain conditions are met, or repeatedly looping through
text data until some condition is satisfied.  This feature
is known as `control`:dt:, and is the focus of this section.

Conditionals
------------

Python supports a wide range of operators, such as ``<`` and ``>=``, for
testing the relationship between values. The full set of these `relational
operators`:dt: is shown in tab-inequalities_.

.. table:: tab-inequalities

   ======== ==================================================
   Operator Relationship
   ======== ==================================================
   ``<``    less than
   ``<=``   less than or equal to
   ``==``   equal to (note this is two "``=``" signs, not one)
   ``!=``   not equal to
   ``>``    greater than
   ``>=``   greater than or equal to
   ======== ==================================================

   Numerical Comparison Operators

Each of these operators allow us to express a
Python "test" that yields either true or false. Let's verify that
these operators do indeed behave in this way:

    >>> len('old') > 4
    True
    >>> len('Pierre') == 4
    False


Here are some examples of using the operators to express a criterion, or
*condition*, for selecting different words from a sentence of news
text |mdash| only the operator is changed from one
line to the next.  They use the first sentence from the
*Wall Street Journal* corpus, possibly the most well-known sentence
in NLP, naming Pierre Vinken of Elsevier Publishers,
who was coincidentally an early builder of corpora.

    >>> sent7
    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',
    'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']
    >>> [w for w in sent7 if len(w) < 4]
    [',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']
    >>> [w for w in sent7 if len(w) <= 4]
    [',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']
    >>> [w for w in sent7 if len(w) == 4]
    ['will', 'join', 'Nov.']
    >>> [w for w in sent7 if len(w) != 4]
    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board',
    'as', 'a', 'nonexecutive', 'director', '29', '.']
    >>> 

There is a common pattern to all of these examples: 
``[w for w in text if`` *condition*``]``.
This syntactic form
is a Python idiom which defines a new list by performing the
same operation on every element of an existing list.  In the preceding examples, it goes through
each word in ``sent7``, assigning each one in turn to the variable ``w`` and
testing whether it meets the relevant condition. So the first of the
preceding examples could be paraphrased as: "the list which consist
of all words ``w`` in ``sent7``, as long as the length of ``w`` is
less than 4".

.. note::
   The notation just described is called a "list comprehension."  This is our first example
   of a Python idiom, a fixed notation that we use habitually without bothering to
   analyze each time.  Mastering such idioms is an important part of becoming a
   fluent Python programmer.

In the cases shown in the previous code example, the condition is always a numerical comparison.
However, we can also test various properties of words,
using the methods listed in tab-word-tests_.

.. table:: tab-word-tests

   ====================   ===========================================================================
   Function               Meaning
   ====================   ===========================================================================
   ``s.startswith(t)``    test if ``s`` starts with ``t``
   ``s.endswith(t)``      test if ``s`` ends with ``t``
   ``t in s``             test if ``t`` is a substring of ``s``
   ``s.islower()``        test if ``s`` contains cased characters and all are lowercase
   ``s.isupper()``        test if ``s`` contains cased characters and all are uppercase
   ``s.isalpha()``        test if ``s`` is non-empty and all characters in ``s`` are alphabetic
   ``s.isalnum()``        test if ``s`` is non-empty and all characters in ``s`` are alphanumeric
   ``s.isdigit()``        test if ``s`` is non-empty and all characters in ``s`` are digits
   ``s.istitle()``        test if ``s`` contains cased characters and is titlecased
                          (i.e. all words in ``s`` have initial capitals)
   ====================   ===========================================================================

   Some Word Comparison Operators

Here are some examples of these operators being used to
select words from our texts:
words ending with `-ableness`:lx:;
words containing `gnt`:lx:;
words having an initial capital;
and words consisting entirely of digits.

    >>> sorted(w for w in set(text1) if w.endswith('ableness'))
    ['comfortableness', 'honourableness', 'immutableness', 'indispensableness', ...]
    >>> sorted(term for term in set(text4) if 'gnt' in term)
    ['Sovereignty', 'sovereignties', 'sovereignty']
    >>> sorted(item for item in set(text6) if item.istitle())
    ['A', 'Aaaaaaaaah', 'Aaaaaaaah', 'Aaaaaah', 'Aaaah', 'Aaaaugh', 'Aaagh', ...] 
    >>> sorted(item for item in set(sent7) if item.isdigit())
    ['29', '61']
    >>>

We can also create more complex conditions.  If `c`:math: is a
condition, then ``not`` `c`:math: is also a condition.
If we have two conditions `c`:math:\ `1`:subscript: and `c`:math:\ `2`:subscript:,
then we can combine them to form a new condition using conjunction and disjunction:
`c`:math:\ `1`:subscript: ``and`` `c`:math:\ `2`:subscript:,
`c`:math:\ `1`:subscript: ``or`` `c`:math:\ `2`:subscript:.

.. note:: |TRY|
   Run the following examples and try to explain what is going on in each one.
   Next, try to make up some conditions of your own.

.. doctest-ignore::
      >>> sorted(w for w in set(text7) if '-' in w and 'index' in w)
      >>> sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10)
      >>> sorted(w for w in set(sent7) if not w.islower())
      >>> sorted(t for t in set(text2) if 'cie' in t or 'cei' in t)

Operating on Every Element
--------------------------

In sec-computing-with-language-simple-statistics_, we saw some examples of
counting items other than words.  Let's take a closer look at the notation we used:

    >>> [len(w) for w in text1]
    [1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]
    >>> [w.upper() for w in text1]
    ['[', 'MOBY', 'DICK', 'BY', 'HERMAN', 'MELVILLE', '1851', ']', 'ETYMOLOGY', '.', ...]
    >>>

These expressions have the form ``[f(w) for ...]`` or ``[w.f() for ...]``, where
``f`` is a function that operates on a word to compute its length, or to
convert it to uppercase.
For now, you don't need to understand the difference between the notations ``f(w)`` and
``w.f()``. 

Let's return to the question of vocabulary size, and apply the same idiom here:

    >>> len(text1)
    260819
    >>> len(set(text1))
    19317
    >>> len(set(word.lower() for word in text1))
    17231
    >>>

Now that we are not double-counting words like `This`:lx: and `this`:lx:, which differ only
in capitalization, we've wiped 2,000 off the vocabulary count!  We can go a step further
and eliminate numbers and punctuation from the vocabulary count by filtering out any
non-alphabetic items:

    >>> len(set(word.lower() for word in text1 if word.isalpha()))
    16948
    >>> 

This example is slightly complicated: it lowercases all the purely alphabetic items.
Perhaps it would have been simpler just to count the lowercase-only items, but this
gives the wrong answer (why?).

Don't worry if you don't feel confident with list comprehensions yet,
since you'll see many more examples along with explanations in the following chapters.

Nested Code Blocks
------------------

Most programming languages permit us to execute a block of code when a
`conditional expression`:dt:, or ``if`` statement, is satisfied.  We
already saw examples of conditional tests in code like ``[w for w in
sent7 if len(w) < 4]``. In the following program, we have created a
variable called ``word`` containing the string value ``'cat'``. The
``if`` statement checks whether the test ``len(word) < 5`` is true.
It is, so the body of the ``if`` statement is invoked and the
``print`` statement is executed, displaying a message to the user.
Remember to indent the ``print`` statement by typing four spaces.

    >>> word = 'cat'
    >>> if len(word) < 5:
    ...     print('word length is less than 5')
    ...   # [_blank-line]
    word length is less than 5
    >>>

When we use the Python interpreter we have to add an extra blank line blank-line_
in order for it to detect that the nested block is complete.

If we change the conditional test to ``len(word) >= 5``,
to check that the length of ``word`` is greater than or equal to ``5``,
then the test will no longer be true.
This time, the body of the ``if`` statement will not be executed,
and no message is shown to the user:

    >>> if len(word) >= 5:
    ...   print('word length is greater than or equal to 5')
    ... 
    >>>

An ``if`` statement is known as a `control structure`:dt:
because it controls whether the code in the indented block will be run.
Another control structure is the ``for`` loop.
Try the following, and remember to include the colon and the four spaces:

    >>> for word in ['Call', 'me', 'Ishmael', '.']:
    ...     print(word)
    ...
    Call
    me
    Ishmael
    .
    >>>

This is called a loop because Python executes the code in
circular fashion.  It starts by performing the
assignment ``word = 'Call'``,
effectively using the ``word`` variable to name the first
item of the list.  Then, it displays the value of ``word``
to the user.  Next, it goes back to the ``for`` statement,
and performs the assignment ``word = 'me'``, before displaying this new value
to the user, and so on.  It continues in this fashion until
every item of the list has been processed.

Looping with Conditions
-----------------------

Now we can combine the ``if`` and ``for`` statements.
We will loop over every item of the list, and print
the item only if it ends with the letter *l*.  We'll pick another
name for the variable to demonstrate that Python doesn't
try to make sense of variable names.

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>> for xyzzy in sent1:
    ...     if xyzzy.endswith('l'):
    ...         print(xyzzy)
    ...
    Call
    Ishmael
    >>>

You will notice that ``if`` and ``for`` statements
have a colon at the end of the line,
before the indentation begins. In fact, all Python
control structures end with a colon.  The colon
indicates that the current statement relates to the
indented block that follows.

We can also specify an action to be taken if
the condition of the ``if`` statement is not met.
Here we see the ``elif`` (else if) statement, and
the ``else`` statement.  Notice that these also have
colons before the indented code.

    >>> for token in sent1:
    ...     if token.islower():
    ...         print(token, 'is a lowercase word')
    ...     elif token.istitle():
    ...         print(token, 'is a titlecase word')
    ...     else:
    ...         print(token, 'is punctuation')
    ...
    Call is a titlecase word
    me is a lowercase word
    Ishmael is a titlecase word
    . is punctuation
    >>>

As you can see, even with this small amount of Python knowledge,
you can start to build multiline Python programs.
It's important to develop such programs in pieces,
testing that each piece does what you expect before
combining them into a program.  This is why the Python
interactive interpreter is so invaluable, and why you should get
comfortable using it.

Finally, let's combine the idioms we've been exploring.
First, we create a list of `cie`:lx: and `cei`:lx: words,
then we loop over each item and print it.  Notice the
extra information given in the print statement: `end=' '`.
This tells Python to print a space (not the default newline) after each word.

    >>> tricky = sorted(w for w in set(text2) if 'cie' in w or 'cei' in w)
    >>> for word in tricky:
    ...     print(word, end=' ')
    ancient ceiling conceit conceited conceive conscience
    conscientious conscientiously deceitful deceive ...
    >>>


.. _sec-sequences:

---------
Sequences
---------

So far, the only data type we have worked with is the list data type.
Lists are one of several kinds of `sequence`:dt: data type.
There are two other important kinds of sequence that you need to know
about: strings and tuples.

Strings
-------

Some of the methods we used to access the elements of a list also work with individual words,
or `strings`:dt:.  For example, we can assign a string to a variable assign-string_,
index a string index-string_, and slice a string slice-string_:

    >>> name = 'Monty' # [_assign-string]
    >>> name[0] # [_index-string]
    'M'
    >>> name[:4] # [_slice-string]
    'Mont'
    >>>

We can also perform multiplication and addition with strings:

    >>> name * 2
    'MontyMonty'
    >>> name + '!'
    'Monty!'
    >>>

We can join the words of a list to make a single string, or split a string into a list, as follows:

    >>> ' '.join(['Monty', 'Python'])
    'Monty Python'
    >>> 'Monty Python'.split()
    ['Monty', 'Python']
    >>>

We will come back to the topic of strings in chap-words_.
For the time being, we have two important building blocks, 
lists and strings.

.. XXX next sentence is confusing, since you don't in fact use parentheses
.. Although the point about ("foo") not being a tuple is a good one, I would be
.. inclined to advise students to use parentheses as well as comma. Cf your
.. example of the precedence problem in the debugging section later on:

.. For example, ``"%s.%s.%02d" % "ph.d.", "n", 1`` produces a run-time error
.. ``TypeError: not enough arguments for format string``.  This is because the
.. percent operator has higher precedence than
.. the comma operator.  The fix is to add parentheses in order to
.. force the required scope.  

.. Just checked the Python Tutorial:
.. As you see, on output tuples are always enclosed in parentheses, so that nested
.. tuples are interpreted correctly; they may be input with or without surrounding
.. parentheses, although often parentheses are necessary anyway (if the tuple is part
.. of a larger expression).

.. SB: done, I think

Tuples
------

Another kind of sequence is called a `tuple`:dt:.
Tuples are formed with the comma operator create-tuple_, and typically enclosed
in parentheses. Like lists and strings, tuples can be indexed index-tuple_
and sliced slice-tuple_, and have a length length-tuple_.

   >>> t = ('walk', 'fem', 3) # [_create-tuple]
   >>> t
   ('walk', 'fem', 3)
   >>> t[0] # [_index-tuple]
   'walk'
   >>> t[1:] # [_slice-tuple]
   ('fem', 3)
   >>> len(t) # [_length-tuple]
   3

.. caution::
   Tuples are constructed using the comma operator.  Parentheses are a more
   general feature of Python syntax, designed for grouping.
   A tuple containing the single element ``'snark'`` is defined by adding a
   trailing comma, like this: "``'snark',``".  The empty tuple is a special
   case, and is defined using empty parentheses ``()``. 

.. XXX how about making the following contrast:
.. >>> type(('snark'))
.. <type 'str'>
.. >>> type(('snark',))
.. <type 'tuple'>

.. XXX this would be a good place to explain tuple assignment / sequence unpacking
.. (unless you did in a revision of ch03 -- it is mentioned only in an exercise to
.. this chapter AF

Let's compare strings, lists and tuples directly, and do the indexing, slice, and length
operation on each type:

    >>> raw = 'I turned off the spectroroute'
    >>> text = ['I', 'turned', 'off', 'the', 'spectroroute']
    >>> pair = (6, 'turned')
    >>> raw[2], text[3], pair[1]
    ('t', 'the', 'turned')
    >>> raw[-3:], text[-3:], pair[-3:]
    ('ute', ['off', 'the', 'spectroroute'], (6, 'turned'))
    >>> len(raw), len(text), len(pair)
    (29, 5, 2)

Notice in this code sample that we computed multiple values on a
single line, separated by commas.  These comma-separated expressions
are actually just tuples |mdash| Python allows us to omit the
parentheses around tuples if there is no ambiguity. When we print a
tuple, the parentheses are always displayed.


Operating on Sequence Types
---------------------------

We can iterate over the items in a sequence ``s`` in a variety of useful ways,
as shown in tab-python-sequence_.

.. table:: tab-python-sequence

   ======================================  ===============================================
   Python Expression                       Comment                                        
   ======================================  ===============================================
   ``for item in s``                       iterate over the items of ``s``
   ``for item in sorted(s)``               iterate over the items of ``s`` in order
   ``for item in set(s)``                  iterate over unique elements of ``s``
   ``for item in reversed(s)``             iterate over elements of ``s`` in reverse
   ``for item in set(s).difference(t)``    iterate over elements of ``s`` not in ``t``
   ======================================  ===============================================

   Various ways to iterate over sequences


The sequence functions illustrated in tab-python-sequence_ can be combined
in various ways; for example, to get unique elements of ``s`` sorted
in reverse, use ``reversed(sorted(set(s)))``.
We can randomize the contents of a list ``s`` before iterating over
them, using ``random.shuffle(s)``.

We can convert between these sequence types.  For example,
``tuple(s)`` converts any kind of sequence into a tuple, and
``list(s)`` converts any kind of sequence into a list.
We can convert a list of strings to a single string using the
``join()`` function, e.g. ``':'.join(words)``.

Some other objects, such as a ``FreqDist``, can be converted into a
sequence (using ``list()`` or ``sorted()``) and support iteration, e.g. 

    >>> raw = 'Red lorry, yellow lorry, red lorry, yellow lorry.'
    >>> text = word_tokenize(raw)
    >>> fdist = nltk.FreqDist(text) 
    >>> sorted(fdist)
    [',', '.', 'Red', 'lorry', 'red', 'yellow']
    >>> for key in fdist:
    ...     print(key + ':', fdist[key], end='; ')
    ...
    lorry: 4; red: 1; .: 1; ,: 3; Red: 1; yellow: 2

In the next example, we use tuples to re-arrange the
contents of our list.  (We can omit the parentheses
because the comma has higher precedence than assignment.)

    >>> words = ['I', 'turned', 'off', 'the', 'spectroroute']
    >>> words[2], words[3], words[4] = words[3], words[4], words[2]
    >>> words    
    ['I', 'turned', 'the', 'spectroroute', 'off']

This is an idiomatic and readable way to move items inside a list.
It is equivalent to the following traditional way of doing such
tasks that does not use tuples (notice that this method needs a
temporary variable ``tmp``).

    >>> tmp = words[2]
    >>> words[2] = words[3]
    >>> words[3] = words[4]
    >>> words[4] = tmp

As we have seen, Python has sequence functions such as ``sorted()`` and ``reversed()``
that rearrange the items of a sequence.  There are also functions that
modify the `structure`:em: of a sequence and which can be handy for
language processing.  Thus, ``zip()`` takes
the items of two or more sequences and "zips" them together into a single list of tuples.
Given a sequence ``s``, ``enumerate(s)`` returns pairs consisting of
an index and the item at that index.

    >>> words = ['I', 'turned', 'off', 'the', 'spectroroute']
    >>> tags = ['noun', 'verb', 'prep', 'det', 'noun']
    >>> zip(words, tags)
    <zip object at ...>
    >>> list(zip(words, tags))
    [('I', 'noun'), ('turned', 'verb'), ('off', 'prep'),
    ('the', 'det'), ('spectroroute', 'noun')]
    >>> list(enumerate(words))
    [(0, 'I'), (1, 'turned'), (2, 'off'), (3, 'the'), (4, 'spectroroute')]

.. note::
   It is a widespread feature of Python 3 and NLTK 3 to only perform
   computation when required (a feature known as "lazy evaluation").
   If you ever see a result like ``<zip object at 0x10d005448>`` when
   you expect to see a sequence, you can force the object to be
   evaluated just by putting it in a context that expects a sequence,
   like ``list(``\ `x`:mathit:\ ``)``, or ``for item in`` `x`:mathit:.

For some |NLP| tasks it is necessary to cut up a sequence into two or more parts.
For instance, we might want to "train" a system on 90% of the data and test it
on the remaining 10%.  To do this we decide the location where we want to
cut the data cut-location_, then cut the sequence at that location cut-sequence_.  

    >>> text = nltk.corpus.nps_chat.words()
    >>> cut = int(0.9 * len(text)) # [_cut-location]
    >>> training_data, test_data = text[:cut], text[cut:] # [_cut-sequence]
    >>> text == training_data + test_data # [_cut-preserve]
    True
    >>> len(training_data) / len(test_data) # [_cut-ratio]
    9.0

We can verify that none of the original data is lost during this process, nor is it duplicated
cut-preserve_.  We can also verify that the ratio of the sizes of the two pieces is what
we intended cut-ratio_.

Combining Different Sequence Types
----------------------------------

Let's combine our knowledge of these three sequence types, together with list
comprehensions, to perform the task of sorting the words in a string by
their length.

    >>> words = 'I turned off the spectroroute'.split() # [_string-object]
    >>> wordlens = [(len(word), word) for word in words] # [_tuple-comprehension]
    >>> wordlens.sort() # [_sort-method]
    >>> ' '.join(w for (_, w) in wordlens) # [_discard-length]
    'I off the turned spectroroute'


.. XXX cf earlier remark about explaining what "in-place" means

Each of the above lines of code contains a significant feature.
A simple string is actually an object with methods defined on it such as ``split()`` string-object_.
We use a list comprehension to build a list of tuples tuple-comprehension_,
where each tuple consists of a number (the word length) and the
word, e.g. ``(3, 'the')``.  We use the ``sort()`` method sort-method_ 
to sort the list in-place.  Finally, we discard the length
information and join the words back into a single string discard-length_.
(The underscore discard-length_ is just a regular Python variable,
but we can use underscore by convention to indicate that we will
not use its value.)

We began by talking about the commonalities in these sequence types,
but the above code illustrates important differences in their
roles.  First, strings appear at the beginning and the end: this is
typical in the context where our program is reading in some text and
producing output for us to read.  Lists and tuples are used in the
middle, but for different purposes.  A list is typically a sequence of
objects all having the `same type`:em:, of `arbitrary length`:em:.  We often
use lists to hold sequences of words.  In contrast,
a tuple is typically a collection of objects of `different types`:em:, of
`fixed length`:em:.  We often use a tuple to hold a `record`:dt:,
a collection of different `fields`:dt: relating to some entity.
This distinction between the use of lists and tuples takes some
getting used to,
so here is another example:

    >>> lexicon = [
    ...     ('the', 'det', ['Di:', 'D@']),
    ...     ('off', 'prep', ['Qf', 'O:f'])
    ... ]

Here, a lexicon is represented as a list because it is a
collection of objects of a single type |mdash| lexical entries |mdash|
of no predetermined length.  An individual entry is represented as a
tuple because it is a collection of objects with different
interpretations, such as the orthographic form, the part of speech,
and the pronunciations (represented in the SAMPA computer-readable
phonetic alphabet ``http://www.phon.ucl.ac.uk/home/sampa/``).
Note that these pronunciations are stored using a list. (Why?)

.. note::
   A good way to decide when to use tuples vs lists is to ask whether
   the interpretation of an item depends on its position.  For example,
   a tagged token combines two strings having different interpretation,
   and we choose to interpret the first item as the token and the
   second item as the tag.  Thus we use tuples like this: ``('grail', 'noun')``;
   a tuple of the form ``('noun', 'grail')`` would be nonsensical since
   it would be a word ``noun`` tagged ``grail``.
   In contrast, the elements of a text are all tokens, and position is
   not significant.  Thus we use lists like this: ``['venetian', 'blind']``;
   a list of the form ``['blind', 'venetian']`` would be equally valid.
   The linguistic meaning of the words might be different, but the
   interpretation of list items as tokens is unchanged. 

The distinction between lists and tuples has been described in terms of
usage.  However, there is a more fundamental difference: in Python,
lists are `mutable`:dt:, while tuples are `immutable`:dt:.  In other
words, lists can be modified, while tuples cannot.  Here are some of
the operations on lists that do in-place modification of the list.

    >>> lexicon.sort()
    >>> lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3`nd'])
    >>> del lexicon[0]

.. note:: |TRY|
   Convert ``lexicon`` to a tuple, using ``lexicon = tuple(lexicon)``,
   then try each of the above operations, to confirm that none of
   them is permitted on tuples.

Generator Expressions
---------------------

We've been making heavy use of list comprehensions, for compact and readable
processing of texts.  Here's an example where we tokenize and normalize a text:

    >>> text = '''"When I use a word," Humpty Dumpty said in rather a scornful tone,
    ... "it means just what I choose it to mean - neither more nor less."'''
    >>> [w.lower() for w in word_tokenize(text)]
    ['``', 'when', 'i', 'use', 'a', 'word', ',', "''", 'humpty', 'dumpty', 'said', ...]

Suppose we now want to process these words further.  We can do this by inserting the above
expression inside a call to some other function max-comprehension_,
but Python allows us to omit the brackets max-generator_.

    >>> max([w.lower() for w in word_tokenize(text)]) # [_max-comprehension]
    'word'
    >>> max(w.lower() for w in word_tokenize(text)) # [_max-generator]
    'word'

The second line uses a `generator expression`:dt:.  This is more than a notational convenience:
in many language processing situations, generator expressions will be more efficient.
In max-comprehension_, storage for the list object must be allocated
before the value of max() is computed.  If the text is
very large, this could be slow.  In max-generator_, the data is streamed to the calling
function.  Since the calling function simply has to find the maximum value |mdash| the
word which comes latest in lexicographic sort order |mdash| it can process the stream
of data without having to store anything more than the maximum value seen so far. 


.. _sec-reusing-code:

------------
Reusing Code
------------

By this time you've probably typed and retyped a lot of code in the Python
interactive interpreter.  If you mess up when retyping a complex example you have
to enter it again.  Using the arrow keys to access and modify previous commands is helpful but only goes so
far.  In this section we see two important ways to reuse code: text editors and Python functions.


Creating Programs with a Text Editor
------------------------------------

The Python interactive interpreter performs your instructions as soon as you type
them.  Often, it is better to compose a multi-line program using a text editor,
then ask Python to run the whole program at once.  Using |IDLE|, you can do
this by going to the ``File`` menu and opening a new window.  Try this now, and
enter the following one-line program:

::

     print('Monty Python')

Save this program in a file called ``monty.py``, then
go to the ``Run`` menu, and select the command ``Run Module``.
(We'll learn what modules are shortly.)
The result in the main |IDLE| window should look like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    Monty Python
    >>>

You can also type ``from monty import *`` and it will do the same thing.

From now on, you have a choice of using the interactive interpreter or a
text editor to create your programs.  It is often convenient to test your ideas
using the interpreter, revising a line of code until it does what you expect.
Once you're ready, you can paste the code
(minus any ``>>>`` or ``...`` prompts) into the text editor,
continue to expand it, and finally save the program
in a file so that you don't have to type it in again later.
Give the file a short but descriptive name, using all lowercase letters and separating
words with underscore, and using the ``.py`` filename extension, e.g., ``monty_python.py``.

.. note:: |IMPORTANT|
   Our inline code examples include the ``>>>`` and ``...`` prompts
   as if we are interacting directly with the interpreter.  As they get more complicated,
   you should instead type them into the editor, without the prompts, and run them
   from the editor as shown above.  When we provide longer programs in this book,
   we will leave out the prompts to remind you to type them into a file rather
   than using the interpreter.  You can see this already in code-random-text_ above.
   Note that it still includes a couple of lines with the Python prompt;
   this is the interactive part of the task where you inspect some data and invoke a function.
   Remember that all code samples like code-random-text_ are downloadable
   from |NLTK-URL|.

Functions
---------

Suppose that you work on analyzing text that involves different forms
of the same word, and that part of your program needs to work out
the plural form of a given singular noun.  Suppose it needs to do this
work in two places, once when it is processing some texts, and again
when it is processing user input.

Rather than repeating the same code several times over, it is more
efficient and reliable to localize this work inside a `function`:dt:.
A function is just a named block of code that performs some well-defined
task, as we saw in sec-vocabularies-as-sets-of-words_.
A function is usually defined to take some inputs, using special variables known as `parameters`:dt:,
and it may produce a result, also known as a `return value`:dt:.
We define a function using the keyword ``def`` followed by the
function name and any input parameters, followed by the body of the
function.  Here's the function we saw in sec-vocabularies-as-sets-of-words_:

    >>> def lexical_diversity(text):
    ...     return len(text) / len(set(text))

We use the keyword ``return`` to indicate the value that is
produced as output by the function.  In the above example,
all the work of the function is done in the ``return`` statement.
Here's an equivalent definition which does the same work
using multiple lines of code.  We'll change the parameter name
from ``text`` to ``my_text_data`` to remind you that this is an arbitrary choice:

    >>> def lexical_diversity(my_text_data):
    ...     word_count = len(my_text_data)
    ...     vocab_size = len(set(my_text_data))
    ...     diversity_score = vocab_size / word_count
    ...     return diversity_score

Notice that we've created some new variables inside the body of the function.
These are `local variables`:dt: and are not accessible outside the function.
So now we have defined a function with the name ``lexical_diversity``. But just
defining it won't produce any output!
Functions do nothing until they are "called" (or "invoked"):

    >>> from nltk.corpus import genesis
    >>> kjv = genesis.words('english-kjv.txt')
    >>> lexical_diversity(kjv)
    0.06230453042623537

Let's return to our earlier scenario, and actually define a simple
function to work out English plurals.  The function ``plural()`` in code-plural_
takes a singular noun and generates a plural form, though it is not always
correct.  (We'll discuss functions at greater length in sec-functions_.)

.. pylisting:: code-plural
   :caption: A Python Function: this function tries to work out the
             plural form of any English noun; the keyword ``def`` (define)
             is followed by the function name, then a parameter inside
             parentheses, and a colon; the body of the function is the
             indented block of code; it tries to recognize patterns
             within the word and process the word accordingly; e.g., if the
             word ends with `y`:lx:, delete the `y`:lx: and add `ies`:lx:.

   def plural(word):
       if word.endswith('y'):
           return word[:-1] + 'ies'
       elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
           return word + 'es'
       elif word.endswith('an'):
           return word[:-2] + 'en'
       else:
           return word + 's'

   >>> plural('fairy')
   'fairies'
   >>> plural('woman')
   'women'

The ``endswith()`` function is always associated with a string object
(e.g., ``word`` in code-plural_).  To call such functions, we give
the name of the object, a period, and then the name of the function.
These functions are usually known as `methods`:dt:.   

Modules
-------

Over time you will find that you create a variety of useful little text processing functions,
and you end up copying them from old programs to new ones.  Which file contains the
latest version of the function you want to use?
It makes life a lot easier if you can collect your work into a single place, and
access previously defined functions without making copies.

To do this, save your function(s) in a file called (say) ``text_proc.py``.
Now, you can access your work simply by importing it from the file:

.. doctest-ignore::
    >>> from text_proc import plural
    >>> plural('wish')
    wishes
    >>> plural('fan')
    fen

Our plural function obviously has an error, since the plural of
`fan`:lx: is `fans`:lx:.
Instead of typing in a new version of the function, we can
simply edit the existing one.  Thus, at every
stage, there is only one version of our plural function, and no confusion about
which one is being used.

A collection of variable and function definitions in a file is called a Python
`module`:dt:.  A collection of related modules is called a `package`:dt:.
|NLTK|\ 's code for processing the Brown Corpus is an example of a module,
and its collection of code for processing all the different corpora is
an example of a package.  |NLTK| itself is a set of packages, sometimes
called a `library`:dt:.

.. caution:: If you are creating a file to contain some of your Python
   code, do *not* name your file ``nltk.py``: it may get imported in
   place of the "real" NLTK package.  When it imports modules, Python
   first looks in the current directory (folder).


------------------
Questions of Style
------------------

Programming is as much an art as a science.  The undisputed "bible" of programming,
a 2,500 page multi-volume work by Donald Knuth, is called 
`The Art of Computer Programming`:em:.  Many books have been written on
`Literate Programming`:em:, recognizing that humans, not just computers,
must read and understand programs.  Here we pick up on some issues of
programming style that have important ramifications for the readability
of your code, including code layout, procedural vs declarative style,
and the use of loop variables.

Python Coding Style
-------------------

When writing programs you make many subtle choices about names,
spacing, comments, and so on.  When you look at code written by
other people, needless differences in style make it harder
to interpret the code.  Therefore, the designers of the Python
language have published a style guide for Python code, available
at ``http://www.python.org/dev/peps/pep-0008/``.
The underlying value presented in the style guide is `consistency`:em:,
for the purpose of maximizing the readability of code.
We briefly review some of its key recommendations here, and refer
readers to the full guide for detailed discussion with examples.

.. XXX use pylisting for following example?

Code layout should use four spaces per indentation level.  You should make sure that
when you write Python code in a file, you 
avoid tabs for indentation, since these can be misinterpreted by
different text editors and the indentation can be messed up.
Lines should be less than 80 characters long; if necessary you can
break a line inside parentheses, brackets, or braces, because
Python is able to detect that the line continues over to the next line.
If you need to break a line outside parentheses, brackets, or braces,
you can often add extra parentheses, and you can always add a backslash at
the end of the line that is broken:

.. doctest-ignore::
    >>> if (len(syllables) > 4 and len(syllables[2]) == 3 and
    ...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]):
    ...     process(syllables)
    >>> if len(syllables) > 4 and len(syllables[2]) == 3 and \
    ...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]:
    ...     process(syllables)

.. note::
   Typing spaces instead of tabs soon becomes a chore.  Many programming
   editors have built-in support for Python, and can automatically indent
   code and highlight any syntax errors (including indentation errors).
   For a list of Python-aware editors, please see
   ``http://wiki.python.org/moin/PythonEditors``.

Procedural vs Declarative Style
-------------------------------

.. XXX ?? Is this really referring to coding style, or was it originally in a
.. different context? Make a more clear contrast between coding style and programming
.. style?

.. XXX following snippet is first time they've seen +=

.. XXX this section seems to be more oriented towards "Python for C-Programmers"
.. rather than to novices.

We have just seen how the same task can be performed in different
ways, with implications for efficiency.  Another factor influencing
program development is *programming style*.  Consider the following
program to compute the average length of words in the Brown Corpus:

    >>> tokens = nltk.corpus.brown.words(categories='news')
    >>> count = 0
    >>> total = 0
    >>> for token in tokens:
    ...     count += 1
    ...     total += len(token)
    >>> total / count
    4.401545438271973

.. XXX above is first use of +=, so needs explaining

In this program we use the variable ``count`` to keep track of the
number of tokens seen, and ``total`` to store the combined length of
all words.  This is a low-level style, not far removed from machine
code, the primitive operations performed by the computer's CPU.
The two variables are just like a CPU's registers, accumulating values
at many intermediate stages, values that are meaningless until the end. 
We say that this program is written in a *procedural* style, dictating
the machine operations step by step.  Now consider the following
program that computes the same thing:

    >>> total = sum(len(t) for t in tokens)
    >>> print(total / len(tokens))
    4.401...

.. Monkey patching so that the next example doesn't take forever
    >>> tokens = "the cat sat on the mat".split()

The first line uses a generator expression to sum the token lengths,
while the second line computes the average as before.
Each line of code performs a complete, meaningful task, which
can be understood in terms of high-level properties like:
"``total`` is the sum of the lengths of the tokens".
Implementation details are left to the Python interpreter.
The second program uses a built-in function, and constitutes
programming at a more abstract level; the resulting code is
more declarative.  Let's look at an extreme example:

    >>> word_list = []
    >>> i = 0
    >>> while i < len(tokens):
    ...     j = 0
    ...     while j < len(word_list) and word_list[j] <= tokens[i]:
    ...         j += 1
    ...     if j == 0 or tokens[i] != word_list[j-1]:
    ...         word_list.insert(j, tokens[i])
    ...     i += 1
    ...

The equivalent declarative version uses familiar built-in functions,
and its purpose is instantly recognizable:

    >>> word_list = sorted(set(tokens))

.. XXX you are taking for granted that the reader already knows what a loop variable is.

Another case where a loop variable seems to be necessary is for printing
a counter with each line of output.  Instead, we can use ``enumerate()``, which
processes a sequence ``s`` and produces a tuple of the form ``(i, s[i])`` for each
item in ``s``, starting with ``(0, s[0])``.  Here we enumerate the key-value pairs of the
frequency distribution, resulting in nested tuples ``(rank, (word, count))``.
We print ``rank+1`` so that the counting appears to start from ``1``,
as required when producing a list of ranked items.

    >>> fd = nltk.FreqDist(nltk.corpus.brown.words())
    >>> cumulative = 0.0
    >>> most_common_words = [word for (word, count) in fd.most_common()]
    >>> for rank, word in enumerate(most_common_words):
    ...     cumulative += fd.freq(word)
    ...     print("%3d %6.2f%% %s" % (rank + 1, cumulative * 100, word))
    ...     if cumulative > 0.25:
    ...         break
    ...
      1   5.40% the
      2  10.42% ,
      3  14.67% .
      4  17.78% of
      5  20.19% and
      6  22.40% to
      7  24.29% a
      8  25.97% in

It's sometimes tempting to use loop variables to store a maximum or minimum value
seen so far.  Let's use this method to find the longest word in a text.

    >>> text = nltk.corpus.gutenberg.words('milton-paradise.txt')
    >>> longest = ''
    >>> for word in text:
    ...     if len(word) > len(longest):
    ...         longest = word
    >>> longest
    'unextinguishable'

However, a more transparent solution uses two list comprehensions,
both having forms that should be familiar by now:

    >>> maxlen = max(len(word) for word in text)
    >>> [word for word in text if len(word) == maxlen]
    ['unextinguishable', 'transubstantiate', 'inextinguishable', 'incomprehensible']

Note that our first solution found the first word having the longest length, while the
second solution found *all* of the longest words (which is usually what we would want).
Although there's a theoretical efficiency difference between the two solutions,
the main overhead is reading the data into main memory; once it's there, a second pass
through the data is effectively instantaneous.  We also need to balance our concerns about
program efficiency with programmer efficiency.  A fast but cryptic solution
will be harder to understand and maintain.

Some Legitimate Uses for Counters
---------------------------------

.. XXX it just struck me that we don't seem to explain range() at any point, tho'
.. it's used a couple of times in ch03 (added exercise to ch01; also mentioned in ch02).

There are cases where we still want to use loop variables in a list comprehension.
For example, we need to use a loop variable to extract successive overlapping n-grams
from a list:
    
    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> n = 3
    >>> [sent[i:i+n] for i in range(len(sent)-n+1)]
    [['The', 'dog', 'gave'],
     ['dog', 'gave', 'John'],
     ['gave', 'John', 'the'],
     ['John', 'the', 'newspaper']]

It is quite tricky to get the range of the loop variable right.
Since this is a common operation in |NLP|\ , |NLTK|
supports it with functions ``bigrams(text)`` and ``trigrams(text)``, and
a general purpose ``ngrams(text, n)``.

Here's an example of how we can use loop variables in
building multidimensional structures.
For example, to build an array with *m* rows and *n* columns,
where each cell is a set, we could use a nested list comprehension:

    >>> m, n = 3, 7
    >>> array = [[set() for i in range(n)] for j in range(m)]
    >>> array[2][5].add('Alice')
    >>> pprint.pprint(array)
    [[set(), set(), set(), set(), set(), set(), set()],
     [set(), set(), set(), set(), set(), set(), set()],
     [set(), set(), set(), set(), set(), {'Alice'}, set()]]

Observe that the loop variables ``i`` and ``j`` are not used
anywhere in the resulting object, they are just needed for a syntactically
correct ``for`` statement.  As another example of this usage, observe
that the expression ``['very' for i in range(3)]`` produces a list
containing three instances of ``'very'``, with no integers in sight.

Note that it would be incorrect to do this work using multiplication,
for reasons concerning object copying that were discussed earlier in this section.

    >>> array = [[set()] * n] * m
    >>> array[2][5].add(7)
    >>> pprint.pprint(array)
    [[{7}, {7}, {7}, {7}, {7}, {7}, {7}],
     [{7}, {7}, {7}, {7}, {7}, {7}, {7}],
     [{7}, {7}, {7}, {7}, {7}, {7}, {7}]]


.. XXX this seems to more of an opening statement that a closing one -- since it
.. assumes the reader already knows some other languages

Iteration is an important programming device.
It is tempting to adopt idioms from other languages.
However, Python offers some elegant and highly readable alternatives,
as we have seen.


.. _sec-functions:

---------------------------------------------------
Functions: The Foundation of Structured Programming
---------------------------------------------------

.. XXX excessive duplication of information that was already presented in ch02;
.. e.g. motivation for functions, terminology of 'parameter' and 'body', return
.. statement etc.  (some collapsing)

Functions provide an effective way to package and re-use program code,
as already explained in sec-reusing-code_.
For example, suppose we find that we often want to read text from an HTML file.
This involves several steps: opening the file, reading it in, normalizing
whitespace, and stripping HTML markup.  We can collect these steps into a
function, and give it a name such as ``get_text()``, as shown in code-get-text_.

.. pylisting:: code-get-text
   :caption: Read text from a file

   import re
   def get_text(file):
       """Read text from a file, normalizing whitespace and stripping HTML markup."""
       text = open(file).read()
       text = re.sub(r'<.*?>', ' ', text)
       text = re.sub('\s+', ' ', text)
       return text

Now, any time we want to get cleaned-up text from an HTML file, we can just call
``get_text()`` with the name of the file as its only argument.  It will return
a string, and we can assign this to a variable, e.g.:
``contents = get_text("test.html")``.  Each time we want to use this series of
steps we only have to call the function.

Using functions has the benefit of saving space in our program.  More
importantly, our choice of name for the function helps make the program *readable*.
In the case of the above example, whenever our program needs to read cleaned-up
text from a file we don't have to clutter the program with four lines of code, we
simply need to call ``get_text()``.  This naming helps to provide some "semantic
interpretation" |mdash| it helps a reader of our program to see what the program "means".

Notice that the above function definition contains a string.  The first string inside
a function definition is called a `docstring`:dt:.  Not only does it document the
purpose of the function to someone reading the code, it is accessible to a programmer
who has loaded the code from a file::

|   >>> help(get_text)
|   Help on function get_text in module __main__:
|
|   get(text)
|       Read text from a file, normalizing whitespace and stripping HTML markup.

We have seen that functions help to make our work reusable and readable.  They
also help make it *reliable*.  When we re-use code that has already been developed
and tested, we can be more confident that it handles a variety of cases correctly.
We also remove the risk that we forget some important step, or introduce a bug.
The program that calls our function also has increased reliability.  The author
of that program is dealing with a shorter program, and its components behave
transparently.

To summarize, as its name suggests, a function captures functionality.
It is a segment of code that can be given a meaningful name and which performs
a well-defined task.  Functions allow us to abstract away from the details,
to see a bigger picture, and to program more effectively.

The rest of this section takes a closer look at functions, exploring the
mechanics and discussing ways to make your programs easier to read.

Function Inputs and Outputs
---------------------------

We pass information to functions using a function's parameters,
the parenthesized list of variables and constants following
the function's name in the function definition.  Here's a complete example:

    >>> def repeat(msg, num):  # [_fun-def]
    ...     return ' '.join([msg] * num)
    >>> monty = 'Monty Python'  
    >>> repeat(monty, 3) # [_fun-call]
    'Monty Python Monty Python Monty Python'

We first define the function to take two parameters, ``msg`` and ``num``
fun-def_. Then we call the function and pass it two arguments, ``monty`` and ``3``
fun-call_; these arguments fill the "placeholders" provided by the parameters and
provide values for the occurrences of ``msg`` and ``num`` in the function body.

It is not necessary to have any parameters, as we see in the following example:

    >>> def monty():
    ...     return "Monty Python"
    >>> monty()
    'Monty Python'

A function usually communicates its results back to the calling program via the ``return`` statement,
as we have just seen.  To the calling program, it looks as if the function call had been replaced
with the function's result, e.g.:

    >>> repeat(monty(), 3)
    'Monty Python Monty Python Monty Python'
    >>> repeat('Monty Python', 3)
    'Monty Python Monty Python Monty Python'

A Python function is not required to have a return statement.
Some functions do their work as a side effect, printing a result,
modifying a file, or updating the contents of a parameter to the function
(such functions are called "procedures" in some other programming languages).

.. XXX these examples would be easier for a novice to grasp if they knew more about
.. how functions are used in context. For example, they need to first know that you might
.. want to write ``foo = my_sort2(l)`` or have a function that only produces
.. side-effects before they see the significance of these.

Consider the following three sort functions.
The third one is dangerous because a programmer could
use it without realizing that it had modified its input.
In general, functions should modify the contents of a parameter
(``my_sort1()``), or return a value (``my_sort2()``),
not both (``my_sort3()``).

    >>> def my_sort1(mylist):      # good: modifies its argument, no return value
    ...     mylist.sort()
    >>> def my_sort2(mylist):      # good: doesn't touch its argument, returns value
    ...     return sorted(mylist)
    >>> def my_sort3(mylist):      # bad: modifies its argument and also returns it
    ...     mylist.sort()
    ...     return mylist

Parameter Passing
-----------------

Back in sec-back-to-the-basics_ you saw that assignment works on values,
but that the value of a structured object is a `reference`:em: to that object.  The same
is true for functions.  Python interprets function parameters as values (this is
known as `call-by-value`:dt:).  In the following code, ``set_up()`` has two parameters,
both of which are modified inside the function.  We begin by assigning an empty string
to ``w`` and an empty list to ``p``.  After calling the function, ``w`` is unchanged,
while ``p`` is changed:

    >>> def set_up(word, properties):
    ...     word = 'lolcat'
    ...     properties.append('noun')
    ...     properties = 5
    ...
    >>> w = ''
    >>> p = []
    >>> set_up(w, p)
    >>> w
    ''
    >>> p
    ['noun']

Notice that ``w`` was not changed by the function.
When we called ``set_up(w, p)``, the value of ``w`` (an empty string) was assigned to
a new variable ``word``.  Inside the function, the value of ``word`` was modified.
However, that change did not propagate to ``w``.  This parameter passing is
identical to the following sequence of assignments:

    >>> w = ''
    >>> word = w
    >>> word = 'lolcat'
    >>> w
    ''

Let's look at what happened with the list ``p``.
When we called ``set_up(w, p)``, the value of ``p`` (a reference to an empty
list) was assigned to a new local variable ``properties``,
so both variables now reference the same memory location.
The function modifies ``properties``, and this change is also
reflected in the value of ``p`` as we saw.  The function also
assigned a new value to properties (the number ``5``); this
did not modify the contents at that memory location, but
created a new local variable.
This behavior is just as if we had done the following sequence of assignments:

    >>> p = []
    >>> properties = p
    >>> properties.append('noun')
    >>> properties = 5
    >>> p
    ['noun']

Thus, to understand Python's call-by-value parameter passing,
it is enough to understand how assignment works.  Remember that you
can use the ``id()`` function and ``is`` operator to check your
understanding of object identity after each statement.  

Variable Scope
--------------

Function definitions create a new, local `scope`:dt: for variables.
When you assign to a new variable inside the body of a function,
the name is only defined within that function.  The name is not
visible outside the function, or in other functions.  This behavior
means you can choose variable names without being concerned about
collisions with names used in your other function definitions.
 
When you refer to an existing name from within the body
of a function, the Python interpreter first tries to resolve
the name with respect to the names that are local to the function.
If nothing is found, the interpreter checks if it is a global
name within the module.  Finally, if that does not succeed, the
interpreter checks if the name is a Python built-in.  This is
the so-called `LGB rule`:dt: of name resolution: local,
then global, then built-in.

.. caution::
   A function can enable access to a global variable using the
   ``global`` declaration.  However, this practice should be
   avoided as much as possible.  Defining global variables
   inside a function introduces dependencies on context
   and limits the portability (or reusability) of the function.
   In general you should use parameters for function inputs
   and return values for function outputs.

Checking Parameter Types
------------------------

Python does not allow us to declare the type of a variable when we write a program,
and this permits us to define functions that are flexible
about the type of their arguments.  For example, a tagger might expect
a sequence of words, but it wouldn't care whether this sequence is expressed
as a list or a tuple (or an iterator, another sequence type that is
outside the scope of the current discussion).

However, often we want to write programs for later use by others, and want
to program in a defensive style, providing useful warnings when functions
have not been invoked correctly.  The author of the following ``tag()``
function assumed that its argument would always be a string.

    >>> def tag(word):
    ...     if word in ['a', 'the', 'all']:
    ...         return 'det'
    ...     else:
    ...         return 'noun'
    ...
    >>> tag('the')
    'det'
    >>> tag('knight')
    'noun'
    >>> tag(["'Tis", 'but', 'a', 'scratch']) # [_list-arg]
    'noun'

The function returns sensible values for the arguments ``'the'`` and ``'knight'``,
but look what happens when it is passed a list list-arg_ |mdash| it fails to
complain, even though the result which it returns is clearly incorrect.
The author of this function could take some extra steps to
ensure that the ``word`` parameter of the ``tag()`` function is a string.
A naive approach would be to check the type of the argument using
``if not type(word) is str``, and if ``word`` is not a string, to simply
return Python's special empty value, ``None``. This is a slight improvement, because
the function is checking the type of the argument, and trying to return a "special", diagnostic
value for the wrong input.
However, it is also dangerous because the calling program
may not detect that ``None`` is intended as a "special" value, and this diagnostic
return value may then be
propagated to other parts of the program with unpredictable consequences.
This approach also fails if the word is a Unicode string, which has
type ``unicode``, not ``str``.
Here's a better solution, using an ``assert`` statement together with Python's ``basestring``
type that generalizes over both ``unicode`` and ``str``.

    >>> def tag(word):
    ...     assert isinstance(word, basestring), "argument to tag() must be a string"
    ...     if word in ['a', 'the', 'all']:
    ...         return 'det'
    ...     else:
    ...         return 'noun'

If the ``assert`` statement fails, it will produce an error that cannot be ignored,
since it halts program execution. 
Additionally, the error message is easy to interpret.  Adding assertions to
a program helps you find logical errors, and is a kind of `defensive programming`:dt:.
A more fundamental approach is to document the parameters to each function
using docstrings as described later in this section.

.. XXX should we mention try / except here?

Functional Decomposition
------------------------

Well-structured programs usually make extensive use of functions.
When a block of program code grows longer than 10-20 lines, it is a
great help to readability if the code is broken up into one or more
functions, each one having a clear purpose.  This is analogous to
the way a good essay is divided into paragraphs, each expressing one main idea.

.. XXX not clear here whether you are really talking about actions (in which case
.. :lx: role is maybe inappropriate / misleading) or about lexical semantics.

.. XXX the following code snippet could well occur earlier, e.g in section where you
.. talk about function inputs and outputs, so as to motivate different kinds of
.. return values, and then perhaps repeated here.

Functions provide an important kind of abstraction.
They allow us to group multiple actions into a single, complex action,
and associate a name with it.
(Compare this with the way we combine the actions of
`go`:lx: and `bring back`:lx: into a single more complex action `fetch`:lx:.)
When we use functions, the main program can be written at a higher level
of abstraction, making its structure transparent, e.g.

.. doctest-ignore::
    >>> data = load_corpus()
    >>> results = analyze(data)
    >>> present(results)

Appropriate use of functions makes programs more readable and maintainable.
Additionally, it becomes possible to reimplement a function
|mdash| replacing the function's body with more efficient code |mdash|
without having to be concerned with the rest of the program.

Consider the ``freq_words`` function in code-freq-words1_.
It updates the contents of a frequency distribution that is
passed in as a parameter, and it also prints a list of the
`n`:math: most frequent words.

.. pylisting:: code-freq-words1
   :caption: Poorly Designed Function to Compute Frequent Words

   from urllib import request
   from bs4 import BeautifulSoup

   def freq_words(url, freqdist, n):
       html = request.urlopen(url).read().decode('utf8')
       raw = BeautifulSoup(html).get_text()
       for word in word_tokenize(raw):
           freqdist[word.lower()] += 1
       result = []
       for word, count in freqdist.most_common(n):
           result = result + [word]
       print(result)

   >>> constitution = "http://www.archives.gov/exhibits/charters/constitution_transcript.html"
   >>> fd = nltk.FreqDist()
   >>> freq_words(constitution, fd, 30)
   ['the', ',', 'of', 'and', 'shall', '.', 'be', 'to', ';', 'in', 'states',
   'or', 'united', 'a', 'state', 'by', 'for', 'any', '=', 'which', 'president',
   'all', 'on', 'may', 'such', 'as', 'have', ')', '(', 'congress']

This function has a number of problems.
The function has two side-effects: it modifies the contents of its second
parameter, and it prints a selection of the results it has computed.
The function would be easier to understand and to reuse elsewhere if we
initialize the ``FreqDist()`` object inside the function (in the same place
it is populated), and if we moved the selection and display of results to the
calling program. Given that its task is to identify frequent words, it
should probably just return a list, not the whole frequency distribution.
In code-freq-words2_ we `refactor`:dt: this function,
and simplify its interface by dropping the ``freqdist`` parameter.

.. pylisting:: code-freq-words2
   :caption: Well-Designed Function to Compute Frequent Words

   from urllib import request
   from bs4 import BeautifulSoup

   def freq_words(url, n):
       html = request.urlopen(url).read().decode('utf8')
       text = BeautifulSoup(html).get_text()
       freqdist = nltk.FreqDist(word.lower() for word in word_tokenize(text))
       return [word for (word, _) in fd.most_common(n)]

   >>> freq_words(constitution, 30)
   ['the', ',', 'of', 'and', 'shall', '.', 'be', 'to', ';', 'in', 'states',
   'or', 'united', 'a', 'state', 'by', 'for', 'any', '=', 'which', 'president',
   'all', 'on', 'may', 'such', 'as', 'have', ')', '(', 'congress']

The readability and usability of the ``freq_words`` function is improved.

.. note::
   We have used ``_`` as a variable name. This is no different to any other
   variable except it signals to the reader that we don't have a use
   for the information it holds.

Documenting Functions
---------------------

If we have done a good job at decomposing our program into functions, then it should
be easy to describe the purpose of each function in plain language, and provide
this in the docstring at the top of the function definition.  This statement
should not explain how the functionality is implemented; in fact it should be possible
to re-implement the function using a different method without changing this
statement.

For the simplest functions, a one-line docstring is usually adequate (see code-get-text_).
You should provide a triple-quoted string containing a complete sentence on a single line.
For non-trivial functions, you should still provide a one sentence summary on the first line,
since many docstring processing tools index this string.  This should be followed by
a blank line, then a more detailed description of the functionality
(see ``http://www.python.org/dev/peps/pep-0257/`` for more information in docstring
conventions).

.. XXX it would be really nice to have a screen dump of the HTML output. 

Docstrings can include a `doctest block`:dt:, illustrating the use of
the function and the expected output.  These can be tested automatically
using Python's ``docutils`` module.
Docstrings should document the type of each parameter to the function, and the return
type.  At a minimum, that can be done in plain text.  However, note that |NLTK| uses
the Sphinx markup language to document parameters.  This format
can be automatically converted into richly structured
API documentation (see |NLTK-URL|), and includes special handling of certain
"fields" such as ``param`` which allow the inputs and outputs of functions to be
clearly documented.  code-sphinx_ illustrates
a complete docstring.

.. pylisting:: code-sphinx
   :caption: Illustration of a complete docstring, consisting of a one-line summary,
             a more detailed explanation, a doctest example, and Sphinx markup
             specifying the parameters, types, return type, and exceptions.

   def accuracy(reference, test):
       """
       Calculate the fraction of test items that equal the corresponding reference items.

       Given a list of reference values and a corresponding list of test values,
       return the fraction of corresponding values that are equal.
       In particular, return the fraction of indexes
       {0<i<=len(test)} such that C{test[i] == reference[i]}.

           >>> accuracy(['ADJ', 'N', 'V', 'N'], ['N', 'N', 'V', 'ADJ'])
           0.5

       :param reference: An ordered list of reference values
       :type reference: list
       :param test: A list of values to compare against the corresponding
           reference values
       :type test: list
       :return: the accuracy score
       :rtype: float
       :raises ValueError: If reference and length do not have the same length
       """

       if len(reference) != len(test):
           raise ValueError("Lists must have the same length.")
       num_correct = 0
       for x, y in zip(reference, test):
           if x == y:
               num_correct += 1
       return float(num_correct) / len(reference)


.. _sec-doing-more-with-functions:

-------------------------
Doing More with Functions
-------------------------

This section discusses more advanced features, which you may prefer to skip on the
first time through this chapter.

Functions as Arguments
----------------------

So far the arguments we have passed into functions have been simple objects like
strings, or structured objects like lists.  Python also lets us pass a function as
an argument to another function.  Now we can abstract out the operation, and apply
a `different operation`:em: on the `same data`:em:.  As the following examples show,
we can pass the built-in function ``len()`` or a user-defined function ``last_letter()``
as arguments to another function:

    >>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the',
    ...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.']
    >>> def extract_property(prop):
    ...     return [prop(word) for word in sent]
    ...
    >>> extract_property(len)
    [4, 4, 2, 3, 5, 1, 3, 3, 6, 4, 4, 4, 2, 10, 1]
    >>> def last_letter(word):
    ...     return word[-1]
    >>> extract_property(last_letter)
    ['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']    

The objects ``len`` and ``last_letter`` can be
passed around like lists and dictionaries.  Notice that parentheses
are only used after a function name if we are invoking the function;
when we are simply treating the function as an object these are omitted.

Python provides us with one more way to define functions as arguments
to other functions, so-called `lambda expressions`:dt:.  Supposing there
was no need to use the above ``last_letter()`` function in multiple places,
and thus no need to give it a name.  We can equivalently write the following:

    >>> extract_property(lambda w: w[-1])
    ['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']

Our next example illustrates passing a function to the ``sorted()`` function.
When we call the latter with a single argument (the list to be sorted),
it compares the elements directly.
However, we can specify an operation to be performed on the item
before the comparison takes place, and we can specify whether sorting
should be based on increasing or decreasing values.

    >>> sorted(sent)
    [',', '.', 'Take', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds',
    'take', 'the', 'the', 'themselves', 'will']
    >>> sorted(sent, key=str.lower)
    [',', '.', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds', 'Take',
    'take', 'the', 'the', 'themselves', 'will']
    >>> sorted(sent, key=len, reverse=True)
    ['themselves', 'sounds', 'sense', 'Take', 'care', 'will', 'take', 'care',
    'the', 'and', 'the', 'of', 'of', ',', '.']

Accumulative Functions
----------------------

These functions start by initializing some storage, and iterate over
input to build it up, before returning some final object (a large structure
or aggregated result).  A standard way to do this is to initialize an
empty list, accumulate the material, then return the list, as shown
in function ``search1()`` in code-search-examples_.

.. pylisting:: code-search-examples
   :caption: Accumulating Output into a List
   
   def search1(substring, words):
       result = []
       for word in words:
           if substring in word:
               result.append(word)
       return result

   def search2(substring, words):
       for word in words:
           if substring in word:
               yield word

   >>> for item in search1('zz', nltk.corpus.brown.words()):
   ...     print(item, end=" ")
   Grizzlies' fizzled Rizzuto huzzahs dazzler jazz Pezza ...
   >>> for item in search2('zz', nltk.corpus.brown.words()):
   ...     print(item, end=" ")
   Grizzlies' fizzled Rizzuto huzzahs dazzler jazz Pezza ...
   
The function ``search2()`` is a generator.
The first time this function is called, it gets as far as the ``yield``
statement and pauses.  The calling program gets the first word and does
any necessary processing.  Once the calling program is ready for another
word, execution of the function is continued from where it stopped, until
the next time it encounters a ``yield`` statement.  This approach is
typically more efficient, as the function only generates the data as it is
required by the calling program, and does not need to allocate additional
memory to store the output (cf. our discussion of generator expressions above).

Here's a more sophisticated example of a generator which produces
all permutations of a list of words.  In order to force the ``permutations()``
function to generate all its output, we wrap it with a call to ``list()`` listperm_.

    >>> def permutations(seq):
    ...     if len(seq) <= 1:
    ...         yield seq
    ...     else:
    ...         for perm in permutations(seq[1:]):
    ...             for i in range(len(perm)+1):
    ...                 yield perm[:i] + seq[0:1] + perm[i:]
    ...
    >>> list(permutations(['police', 'fish', 'buffalo'])) # [_listperm]
    [['police', 'fish', 'buffalo'], ['fish', 'police', 'buffalo'],
     ['fish', 'buffalo', 'police'], ['police', 'buffalo', 'fish'],
     ['buffalo', 'police', 'fish'], ['buffalo', 'fish', 'police']]

.. note::
   The ``permutations`` function uses a technique called recursion,
   discussed below in sec-algorithm-design_.
   The ability to generate permutations of a set of words is
   useful for creating data to test a grammar (chap-parse_).

Higher-Order Functions
----------------------

Python provides some higher-order functions that are standard
features of functional programming languages such as Haskell.
We illustrate them here, alongside the equivalent expression
using list comprehensions.

Let's start by defining a function ``is_content_word()``
which checks whether a word is from the open class of content words.
We use this function as the first parameter of ``filter()``,
which applies the function to each item in the sequence contained
in its second parameter, and only retains the items for which
the function returns ``True``.

    >>> def is_content_word(word):
    ...     return word.lower() not in ['a', 'of', 'the', 'and', 'will', ',', '.']
    >>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the', 
    ...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.'] 
    >>> list(filter(is_content_word, sent))
    ['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']
    >>> [w for w in sent if is_content_word(w)]
    ['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']
    
Another higher-order function is ``map()``, which applies a function
to every item in a sequence.  It is a general version of the
``extract_property()`` function we saw in sec-doing-more-with-functions_.  
Here is a simple way to find the average length of a sentence in the news
section of the Brown Corpus, followed by an equivalent version with list comprehension
calculation:

    >>> lengths = list(map(len, nltk.corpus.brown.sents(categories='news')))
    >>> sum(lengths) / len(lengths)
    21.75081116158339
    >>> lengths = [len(sent) for sent in nltk.corpus.brown.sents(categories='news')]
    >>> sum(lengths) / len(lengths)
    21.75081116158339

In the above examples we specified a user-defined function ``is_content_word()``
and a built-in function ``len()``.  We can also provide a lambda expression.
Here's a pair of equivalent examples which count the number of vowels in each word.

    >>> list(map(lambda w: len([filter(lambda c: c.lower() in "aeiou", w)]), sent))
    [2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]
    >>> [len([c for c in w if c.lower() in "aeiou"]) for w in sent]
    [2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]
    
The solutions based on list comprehensions are usually more readable than the
solutions based on higher-order functions, and we have favored the former
approach throughout this book.

Named Arguments
---------------

When there are a lot of parameters it is easy to get confused about the
correct order.  Instead we can refer to parameters by name, and even assign
them a default value just in case one was not provided by the calling
program.  Now the parameters can be specified in any order, and can be omitted.

    >>> def repeat(msg='<empty>', num=1):
    ...     return msg * num
    >>> repeat(num=3)
    '<empty><empty><empty>'
    >>> repeat(msg='Alice')
    'Alice'
    >>> repeat(num=5, msg='Alice')
    'AliceAliceAliceAliceAlice'

.. XXX this is going to be confusing for the novice. I suggest omitting the kwargs**
.. parameter for simplicity, and referring the reader to the Python Tutorial.

These are called `keyword arguments`:dt:.
If we mix these two kinds of parameters, then we must ensure that the unnamed parameters precede the named ones.
It has to be this way, since unnamed parameters are defined by position.  We can define a function that takes
an arbitrary number of unnamed and named parameters, and access them via an in-place list of arguments ``*args`` and
an "in-place dictionary" of keyword arguments ``**kwargs``.
(Dictionaries will be presented in sec-dictionaries_.)

    >>> def generic(*args, **kwargs):
    ...     print(args)
    ...     print(kwargs)
    ...
    >>> generic(1, "African swallow", monty="python")
    (1, 'African swallow')
    {'monty': 'python'}

When ``*args`` appears as a function parameter, it actually corresponds to all the unnamed parameters of
the function.  Here's another illustration of this aspect of Python syntax, for the ``zip()`` function which
operates on a variable number of arguments.  We'll use the variable name ``*song`` to demonstrate that
there's nothing special about the name ``*args``.

    >>> song = [['four', 'calling', 'birds'],
    ...         ['three', 'French', 'hens'],
    ...         ['two', 'turtle', 'doves']]
    >>> list(zip(song[0], song[1], song[2]))
    [('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]
    >>> list(zip(*song))
    [('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]

It should be clear from the above example that typing ``*song`` is just a convenient
shorthand, and equivalent to typing out ``song[0], song[1], song[2]``.

Here's another example of the use of keyword arguments in a function
definition, along with three equivalent ways to call the function:

    >>> def freq_words(file, min=1, num=10):
    ...     text = open(file).read()
    ...     tokens = word_tokenize(text)
    ...     freqdist = nltk.FreqDist(t for t in tokens if len(t) >= min)
    ...     return freqdist.most_common(num)
    >>> fw = freq_words('ch01.rst', 4, 10)
    >>> fw = freq_words('ch01.rst', min=4, num=10)
    >>> fw = freq_words('ch01.rst', num=10, min=4)

A side-effect of having named arguments is that they permit optionality.  Thus we
can leave out any arguments where we are happy with the default value:
``freq_words('ch01.rst', min=4)``, ``freq_words('ch01.rst', 4)``.
Another common use of optional arguments is to permit a flag.
Here's a revised version of the same function that reports its
progress if a ``verbose`` flag is set:

    >>> def freq_words(file, min=1, num=10, verbose=False):
    ...     freqdist = FreqDist()
    ...     if verbose: print("Opening", file)
    ...     text = open(file).read()
    ...     if verbose: print("Read in %d characters" % len(file))
    ...     for word in word_tokenize(text):
    ...         if len(word) >= min:
    ...             freqdist[word] += 1
    ...             if verbose and freqdist.N() % 100 == 0: print(".", sep="")
    ...     if verbose: print
    ...     return freqdist.most_common(num)

.. caution::
   Take care not to use a mutable object as the default value of
   a parameter.  A series of calls to the function will use the
   same object, sometimes with bizarre results as we will see in
   the discussion of debugging below.

.. caution::
   If your program will work with a lot of files, it is a good idea to
   close any open files once they are no longer required. Python will
   close open files automatically if you use the ``with`` statement:

   >>> with open("lexicon.txt") as f:
   ...     data = f.read()
   ...     # process the data

.. _sec-program-development:

-------------------
Program Development
-------------------

Programming is a skill that is acquired over several years of
experience with a variety of programming languages and tasks.  Key
high-level abilities are *algorithm design* and its manifestation in
*structured programming*.  Key low-level abilities include familiarity
with the syntactic constructs of the language, and knowledge of a
variety of diagnostic methods for trouble-shooting a program which
does not exhibit the expected behavior.

This section describes the internal structure of a program module and
how to organize a multi-module program.  Then it describes various
kinds of error that arise during program development, what you can
do to fix them and, better still, to avoid them in the first place.

Structure of a Python Module
----------------------------

The purpose of a program module is to bring logically-related definitions and functions
together in order to facilitate re-use and abstraction.  Python modules are nothing
more than individual ``.py`` files.  For example, if you were working
with a particular corpus format, the functions to read and write the format could be
kept together.  Constants used by both formats, such as field separators,
or a ``EXTN = ".inf"`` filename extension, could be shared.  If the format was updated,
you would know that only one file needed to be changed.  Similarly, a module could
contain code for creating and manipulating a particular data structure such as
syntax trees, or code for performing a particular processing task such as
plotting corpus statistics.

When you start writing Python modules, it helps to have some
examples to emulate.  You can locate the code for any |NLTK| module on your
system using the ``__file__`` variable, e.g.:

    >>> nltk.metrics.distance.__file__
    '/usr/lib/python2.5/site-packages/nltk/metrics/distance.pyc'

This returns the location of the compiled ``.pyc`` file for the module, and 
you'll probably see a different location on your machine. The file that you will need
to open is the corresponding ``.py`` source file, and this will be in the same
directory as the ``.pyc`` file.
Alternatively, you can view the latest version of this module on the web
at ``http://code.google.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance.py``.

Like every other |NLTK| module, ``distance.py`` begins with a group of comment
lines giving a one-line title of the module and identifying the authors.
(Since the code is distributed, it also includes the URL where the
code is available, a copyright statement, and license information.)
Next is the module-level docstring, a triple-quoted multiline string
containing information about the module that will be printed when
someone types ``help(nltk.metrics.distance)``.

.. XXX how about putting this in a pylisting?  (didn't work)

::

    # Natural Language Toolkit: Distance Metrics
    #
    # Copyright (C) 2001-2015 NLTK Project
    # Author: Edward Loper <edloper@gmail.com>
    #         Steven Bird <stevenbird1@gmail.com>
    #         Tom Lippincott <tom@cs.columbia.edu>
    # URL: <http://nltk.org/>
    # For license information, see LICENSE.TXT
    #

    """
    Distance Metrics.

    Compute the distance between two items (usually strings).
    As metrics, they must satisfy the following three requirements:

    1. d(a, a) = 0
    2. d(a, b) >= 0
    3. d(a, c) <= d(a, b) + d(b, c)
    """


After this comes all the import statements required for the module,
then any global variables,
followed by a series of function definitions that make up most
of the module.  Other modules define "classes," the main building block
of object-oriented programming, which falls outside the scope of this book.
(Most |NLTK| modules also include a ``demo()`` function which can be used
to see examples of the module in use.) 

.. note::
   Some module variables and functions are only used within the module.
   These should have names beginning with an underscore, e.g. ``_helper()``,
   since this will hide the name.  If another module imports this one,
   using the idiom: ``from module import *``, these names will not be imported.
   You can optionally list the externally accessible names of a module using
   a special built-in variable like this: ``__all__ = ['edit_distance', 'jaccard_distance']``.

Multi-Module Programs
---------------------

Some programs bring together a diverse range of tasks, such as loading data from
a corpus, performing some analysis tasks on the data, then visualizing it.
We may already have stable modules that take care of loading data and producing visualizations.
Our work might involve coding up the analysis task, and just invoking functions
from the existing modules.  This scenario is depicted in fig-multi-module_.

.. _fig-multi-module:
.. figure:: ../images/multi-module.png
   :scale: 20:50:30

   Structure of a Multi-Module Program: The main program ``my_program.py`` imports functions
   from two other modules; unique analysis tasks are localized to the main program, while
   common loading and visualization tasks are kept apart to facilitate re-use and abstraction.

By dividing our work into several modules and using ``import`` statements to
access functions defined elsewhere, we can keep the individual modules simple
and easy to maintain.  This approach will also result in a growing collection
of modules, and make it possible for us to build sophisticated systems involving
a hierarchy of modules.  Designing such systems well is a
complex software engineering task, and beyond the scope of this book.  

Sources of Error
----------------

Mastery of programming depends on having a variety of problem-solving skills to
draw upon when the program doesn't work as expected.  Something as trivial as
a mis-placed symbol might cause the program to behave very differently.
We call these "bugs" because they are tiny in comparison to the damage
they can cause.  They creep into our code unnoticed, and it's only much later
when we're running the program on some new data that their presence is detected.
Sometimes, fixing one bug only reveals another, and we get the distinct impression
that the bug is on the move.  The only reassurance we have is that bugs are
spontaneous and not the fault of the programmer.
 
Flippancy aside, debugging code is hard because there are so many ways for
it to be faulty.  Our understanding of the input data, the algorithm, or
even the programming language, may be at fault.  Let's look at examples
of each of these.
  
First, the input data may contain some unexpected characters.
For example, WordNet synset names have the form ``tree.n.01``, with three
components separated using periods.  The |NLTK| WordNet module initially
decomposed these names using ``split('.')``.  However, this method broke when
someone tried to look up the word `PhD`:lx:, which has the synset
name ``ph.d..n.01``, containing four periods instead of the expected two.
The solution was to use ``rsplit('.', 2)`` to do at most two splits, using
the rightmost instances of the period, and leaving the ``ph.d.`` string intact.
Although several people had tested
the module before it was released, it was some weeks before someone detected
the problem (see ``http://code.google.com/p/nltk/issues/detail?id=297``).

Second, a supplied function might not behave as expected.
For example, while testing |NLTK|\ 's interface to WordNet, one of the
authors noticed that no synsets had any antonyms defined, even though
the underlying database provided a large quantity of antonym information.
What looked like a bug in the WordNet interface turned out to
be a misunderstanding about WordNet itself: antonyms are defined for
lemmas, not for synsets.  The only "bug" was a misunderstanding
of the interface (see ``http://code.google.com/p/nltk/issues/detail?id=98``).

.. XXX much easier to get the point of the next example if it is on a single line, so
.. a doctest block would work better

Third, our understanding of Python's semantics may be at fault.
It is easy to make the wrong assumption about the relative
scope of two operators.
For example, ``"%s.%s.%02d" % "ph.d.", "n", 1`` produces a run-time
error ``TypeError: not enough arguments for format string``.
This is because the percent operator has higher precedence than
the comma operator.  The fix is to add parentheses in order to
force the required scope.  As another example, suppose we are
defining a function to collect all tokens of a text having a
given length.  The function has parameters for the text and
the word length, and an extra parameter that allows the initial
value of the result to be given as a parameter:

    >>> def find_words(text, wordlength, result=[]):
    ...     for word in text:
    ...         if len(word) == wordlength:
    ...             result.append(word)
    ...     return result
    >>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) # [_find-words-1]
    ['omg', 'teh', 'teh', 'mat']
    >>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 2, ['ur']) # [_find-words-2]
    ['ur', 'on']
    >>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) # [_find-words-3]
    ['omg', 'teh', 'teh', 'mat', 'omg', 'teh', 'teh', 'mat']

The first time we call ``find_words()`` find-words-1_, we get all three-letter
words as expected.  The second time we specify an initial value for the result,
a one-element list ``['ur']``, and as expected, the result has this word along with the
other two-letter word in our text.  Now, the next time we call ``find_words()`` find-words-3_
we use the same parameters as in find-words-1_, but we get a different result!
Each time we call ``find_words()`` with no third parameter, the result will
simply extend the result of the previous call, rather than start with the
empty result list as specified in the function definition.  The program's
behavior is not as expected because we incorrectly assumed that the default
value was created at the time the function was invoked.  However, it is
created just once, at the time the Python interpreter loads the function.
This one list object is used whenever no explicit value is provided to the function.

Debugging Techniques
--------------------

Since most code errors result from the programmer making incorrect assumptions,
the first thing to do when you detect a bug is to `check your assumptions`:em:.
Localize the problem by adding ``print`` statements to the program, showing the
value of important variables, and showing how far the program has progressed.

If the program produced an "exception" |mdash| a run-time error |mdash|
the interpreter will print a `stack trace`:dt:,
pinpointing the location of program execution at the time of the error.
If the program depends on input data, try to reduce this to the smallest
size while still producing the error.

Once you have localized the problem to a particular function, or to a line
of code, you need to work out what is going wrong.  It is often helpful to
recreate the situation using the interactive command line.  Define some
variables then copy-paste the offending line of code into the session
and see what happens.  Check your understanding of the code by reading
some documentation, and examining other code samples that purport to do
the same thing that you are trying to do.  Try explaining your code to
someone else, in case they can see where things are going wrong.

Python provides a `debugger`:dt: which allows you to monitor the execution
of your program, specify line numbers where execution will stop (i.e. `breakpoints`:dt:),
and step through sections of code and inspect the value of variables.
You can invoke the debugger on your code as follows:

.. doctest-ignore::
    >>> import pdb
    >>> import mymodule
    >>> pdb.run('mymodule.myfunction()')

It will present you with a prompt ``(Pdb)`` where you can type instructions
to the debugger.  Type ``help`` to see the full list of commands.
Typing ``step`` (or just ``s``) will execute the current line and
stop.  If the current line calls a function, it will enter the function
and stop at the first line.  Typing ``next`` (or just ``n``) is similar,
but it stops execution at the next line in the current function.  The
``break`` (or ``b``) command can be used to create or list breakpoints.  Type
``continue`` (or ``c``) to continue execution as far as the next breakpoint.
Type the name of any variable to inspect its value.

We can use the Python debugger to locate the problem in our ``find_words()``
function.  Remember that the problem arose the second time the function was
called.  We'll start by calling the function without using the debugger first-run_,
using the smallest possible input.  The second time, we'll call it with the
debugger second-run_.
.. doctest-ignore::
 
    >>> import pdb
    >>> find_words(['cat'], 3) # [_first-run]
    ['cat']
    >>> pdb.run("find_words(['dog'], 3)") # [_second-run]
    > <string>(1)<module>()
    (Pdb) step
    --Call--
    > <stdin>(1)find_words()
    (Pdb) args
    text = ['dog']
    wordlength = 3
    result = ['cat']

Here we typed just two commands into the debugger: ``step`` took us inside
the function, and ``args`` showed the values of its arguments (or parameters).
We see immediately that ``result`` has an initial value of ``['cat']``, and not
the empty list as expected.  The debugger has helped us to localize the problem,
prompting us to check our understanding of Python functions.

Defensive Programming
---------------------

In order to avoid some of the pain of debugging, it helps to adopt
some defensive programming habits.  Instead of writing a 20-line
program then testing it, build the program bottom-up out of
small pieces that are known to work.  Each time you combine these
pieces to make a larger unit, test it carefully to see that it works
as expected.  Consider adding ``assert`` statements to your code,
specifying properties of a variable, e.g. ``assert(isinstance(text, list))``.
If the value of the ``text`` variable later becomes a string when your
code is used in some larger context, this will raise an ``AssertionError``
and you will get immediate notification of the problem.

Once you think you've found the bug, view your solution as a hypothesis.
Try to predict the effect of your bugfix before re-running the program.
If the bug isn't fixed, don't fall into the trap of blindly changing
the code in the hope that it will magically start working again.
Instead, for each change, try to articulate a hypothesis about what
is wrong and why the change will fix the problem.  Then undo the change
if the problem was not resolved.

As you develop your program, extend its functionality, and fix any bugs,
it helps to maintain a suite of test cases.
This is called `regression testing`:dt:, since it is meant to detect
situations where the code "regresses" |mdash| where a change to the
code has an unintended side-effect of breaking something that
used to work.  Python provides a simple regression testing framework
in the form of the ``doctest`` module.  This module searches a file
of code or documentation for blocks of text that look like
an interactive Python session, of the form you have already seen
many times in this book.  It executes the Python commands it finds,
and tests that their output matches the output supplied in the original
file.  Whenever there is a mismatch, it reports the expected and actual
values.  For details please consult the ``doctest`` documentation at
``http://docs.python.org/library/doctest.html``.  Apart from its
value for regression testing, the ``doctest`` module is useful for
ensuring that your software documentation stays in sync with your
code.

Perhaps the most important defensive programming strategy is to
set out your code clearly, choose meaningful variable and function
names, and simplify the code wherever possible by decomposing it into
functions and modules with well-documented interfaces.


.. _sec-dictionaries:

------------
Dictionaries
------------

A dictionary, also known as a `mapping`:dt:, `associative array`:dt:
or `hash array`:dt:, is a data type for storing correspondences
between keys and values. For instance, we could define a dictionary
that maps from words to their definitions (like a real-world
dictionary). But we could also map from a word to its part of speech,
or a word and its frequency, and so forth.

Lists vs Dictionaries
---------------------

A text, as we have seen, is treated in Python as a list of words.
An important property of lists is that we can "look up" a particular
item by giving its index, e.g. ``text1[100]``.  Notice how we specify
a number, and get back a word.  We can think of a list as a simple
kind of table, as shown in fig-maps01_.

.. _fig-maps01:
.. figure:: ../images/maps01.png
   :scale: 20:90:20

   List Look-up: we access the contents of a Python list with the help of an integer index.

Contrast this situation with frequency distributions (sec-computing-with-language-simple-statistics_),
where we specify a word, and get back a number, e.g. ``fdist['monstrous']``, which
tells us the number of times a given word has occurred in a text.  Look-up using words is
familiar to anyone who has used a dictionary.  Some more examples are shown in
fig-maps02_.

.. _fig-maps02:
.. figure:: ../images/maps02.png
   :scale: 22

   Dictionary Look-up: we access the entry of a dictionary using a key
   such as someone's name, a web domain, or an English word;
   other names for dictionary are map, hashmap, hash, and associative array.

In the case of a phonebook, we look up an entry using a `name`:em:,
and get back a number.  When we type a domain name in a web browser,
the computer looks this up to get back an IP address.  A word
frequency table allows us to look up a word and find its frequency in
a text collection.  In all these cases, we are mapping from names to
numbers, rather than the other way around as with a list.
In general, we would like to be able to map between
arbitrary types of information.  tab-linguistic-objects_ lists a variety
of linguistic objects, along with what they map.

.. table:: tab-linguistic-objects

    ======================  ============  ====================================================
    Linguistic Object       Maps From     Maps To
    ======================  ============  ====================================================
    Document Index          Word          List of pages (where word is found)
    Thesaurus               Word sense    List of synonyms
    Dictionary              Headword      Entry (part-of-speech, sense definitions, etymology)
    Comparative Wordlist    Gloss term    Cognates (list of words, one per language)
    Morph Analyzer          Surface form  Morphological analysis (list of component morphemes)
    ======================  ============  ====================================================

    Linguistic Objects as Mappings from Keys to Values

Most often, we are mapping from a "word" to some structured object.
For example, a document index maps from a word (which we can represent
as a string), to a list of pages (represented as a list of integers).
In this section, we will see how to represent such mappings in Python.

Dictionaries in Python
----------------------

Python provides a `dictionary`:dt: data type that can be used for
mapping between arbitrary types.  It is like a conventional dictionary,
in that it gives you an efficient way to look things up.  However,
as we see from tab-linguistic-objects_, it has a much wider range of uses.

To illustrate, we define ``pos`` to be an empty dictionary and then add four
entries to it, specifying the part-of-speech of some words.  We add
entries to a dictionary using the familiar square bracket notation:

    >>> pos = {}
    >>> pos
    {}
    >>> pos['colorless'] = 'ADJ' # [_pos-colorless]
    >>> pos
    {'colorless': 'ADJ'}
    >>> pos['ideas'] = 'N'
    >>> pos['sleep'] = 'V'
    >>> pos['furiously'] = 'ADV'
    >>> pos # [_pos-inspect]
    {'furiously': 'ADV', 'ideas': 'N', 'colorless': 'ADJ', 'sleep': 'V'}

So, for example, pos-colorless_ says that
the part-of-speech of `colorless`:lx: is adjective, or more
specifically, that the `key`:dt: ``'colorless'``
is assigned the `value`:dt: ``'ADJ'``  in dictionary ``pos``.
When we inspect the value of ``pos`` pos-inspect_ we see
a set of key-value pairs.  Once we have populated the dictionary 
in this way, we can employ the keys to retrieve values:

    >>> pos['ideas']
    'N'
    >>> pos['colorless']
    'ADJ'

Of course, we might accidentally use a key that hasn't been assigned a value.

    >>> pos['green']
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    KeyError: 'green'

This raises an important question.  Unlike lists and strings, where we
can use ``len()`` to work out which integers will be legal indexes,
how do we work out the legal keys for a dictionary?  If the dictionary
is not too big, we can simply inspect its contents by evaluating the
variable ``pos``.  As we saw above (line pos-inspect_), this gives
us the key-value pairs.  Notice that they are not in the same order they
were originally entered; this is because dictionaries are not sequences
but mappings (cf. fig-maps02_), and the keys are not inherently
ordered.  

Alternatively, to just find the keys, we can convert the
dictionary to a list dict-to-list_ |mdash| or use
the dictionary in a context where a list is expected,
as the parameter of ``sorted()`` dict-sorted_,
or in a ``for`` loop dict-for-loop_.

    >>> list(pos) # [_dict-to-list]
    ['ideas', 'furiously', 'colorless', 'sleep']
    >>> sorted(pos) # [_dict-sorted]
    ['colorless', 'furiously', 'ideas', 'sleep']
    >>> [w for w in pos if w.endswith('s')] # [_dict-for-loop]
    ['colorless', 'ideas']

.. note::
   When you type ``list(pos)`` you might see a different order
   to the one shown above.  If you want to see the keys in order, just sort them.

As well as iterating over all keys
in the dictionary with a ``for`` loop, we can use the ``for`` loop
as we did for printing lists:

    >>> for word in sorted(pos):
    ...     print(word + ":", pos[word])
    ... 
    colorless: ADJ
    furiously: ADV
    sleep: V
    ideas: N

Finally, the dictionary methods ``keys()``, ``values()`` and
``items()`` allow us to access the keys, values, and key-value pairs as separate lists.
We can even sort tuples sort-tuples_, which orders them according to their first element
(and if the first elements are the same, it uses their second elements).

    >>> list(pos.keys())
    ['colorless', 'furiously', 'sleep', 'ideas']
    >>> list(pos.values())
    ['ADJ', 'ADV', 'V', 'N']
    >>> list(pos.items())
    [('colorless', 'ADJ'), ('furiously', 'ADV'), ('sleep', 'V'), ('ideas', 'N')]
    >>> for key, val in sorted(pos.items()): # [_sort-tuples]
    ...     print(key + ":", val)
    ...
    colorless: ADJ
    furiously: ADV
    ideas: N
    sleep: V

We want to be sure that when we look something up in a dictionary, we
only get one value for each key. Now 
suppose we try to use a dictionary to store the fact that the
word `sleep`:lx: can be used as both a verb and a noun:

    >>> pos['sleep'] = 'V'
    >>> pos['sleep']
    'V'
    >>> pos['sleep'] = 'N'
    >>> pos['sleep']
    'N'

Initially, ``pos['sleep']`` is given the value ``'V'``. But this is
immediately overwritten with the new value ``'N'``.
In other words, there can only be one entry in the dictionary for ``'sleep'``.
However, there is a way of storing multiple values in
that entry: we use a list value,
e.g. ``pos['sleep'] = ['N', 'V']``.  In fact, this is what we
saw in sec-lexical-resources_ for the CMU Pronouncing Dictionary,
which stores multiple pronunciations for a single word.

Defining Dictionaries
---------------------

We can use the same key-value pair format to create a dictionary.  There's
a couple of ways to do this, and we will normally use the first:

    >>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}
    >>> pos = dict(colorless='ADJ', ideas='N', sleep='V', furiously='ADV')

Note that dictionary keys must be immutable types, such as strings and tuples.
If we try to define a dictionary using a mutable key, we get a ``TypeError``:

    >>> pos = {['ideas', 'blogs', 'adventures']: 'N'}
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: list objects are unhashable

Default Dictionaries
--------------------

If we try to access a key that is not in a dictionary, we get an error.
However, its often useful if a dictionary can automatically create
an entry for this new key and give it a default value, such as zero or
the empty list.  For this reason, a special kind of dictionary
called a ``defaultdict`` is available.
In order to use it, we have to supply a parameter which can be used to
create the default value, e.g. ``int``, ``float``, ``str``, ``list``, ``dict``,
``tuple``.

    >>> from collections import defaultdict
    >>> frequency = defaultdict(int)
    >>> frequency['colorless'] = 4
    >>> frequency['ideas']
    0
    >>> pos = defaultdict(list)
    >>> pos['sleep'] = ['NOUN', 'VERB']
    >>> pos['ideas']
    []

.. note::
   These default values are actually functions that convert other
   objects to the specified type (e.g. ``int("2")``, ``list("2")``).
   When they are called with no parameter |mdash| ``int()``, ``list()``
   |mdash| they return ``0`` and ``[]`` respectively.

The above examples specified the default value of a dictionary entry to
be the default value of a particular data type.  However, we can specify
any default value we like, simply by providing the name of a function
that can be called with no arguments to create the required value.
Let's return to our part-of-speech example, and create a dictionary
whose default value for any entry is ``'N'`` default-noun_.
When we access a non-existent entry non-existent_,
it is automatically added to the dictionary automatically-added_.

    >>> pos = defaultdict(lambda: 'NOUN') # [_default-noun]
    >>> pos['colorless'] = 'ADJ'
    >>> pos['blog'] # [_non-existent]
    'NOUN'
    >>> list(pos.items())
    [('blog', 'NOUN'), ('colorless', 'ADJ')] # [_automatically-added]

.. note::
   The above example used a `lambda expression`:em:, introduced in
   sec-functions_.  This lambda expression specifies no
   parameters, so we call it using parentheses with no arguments.
   Thus, the definitions of ``f`` and ``g`` below are equivalent:

      >>> f = lambda: 'NOUN'
      >>> f()
      'NOUN'
      >>> def g():
      ...     return 'NOUN'
      >>> g()
      'NOUN'

Let's see how default dictionaries could be used in a more substantial
language processing task.
Many language processing tasks |mdash| including tagging |mdash| struggle to correctly process
the hapaxes of a text.  They can perform better with a fixed vocabulary and a
guarantee that no new words will appear.  We can preprocess a text to replace
low-frequency words with a special "out of vocabulary" token ``UNK``, with
the help of a default dictionary.  (Can you work out how to do this without
reading on?)

We need to create a default dictionary that maps each word to its replacement.
The most frequent `n`:math: words will be mapped to themselves.
Everything else will be mapped to ``UNK``.

    >>> alice = nltk.corpus.gutenberg.words('carroll-alice.txt')
    >>> vocab = nltk.FreqDist(alice)
    >>> v1000 = [word for (word, _) in vocab.most_common(1000)]
    >>> mapping = defaultdict(lambda: 'UNK')
    >>> for v in v1000:
    ...     mapping[v] = v
    ... 
    >>> alice2 = [mapping[v] for v in alice]
    >>> alice2[:100]
    ['UNK', 'Alice', "'", 's', 'UNK', 'in', 'UNK', 'by', 'UNK', 'UNK', 'UNK',
    'UNK', 'CHAPTER', 'I', '.', 'UNK', 'the', 'Rabbit', '-', 'UNK', 'Alice',
    'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by',
    'her', 'sister', 'on', 'the', 'UNK', ',', 'and', 'of', 'having', 'nothing',
    'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'UNK', 'into', 'the',
    'book', 'her', 'sister', 'was', 'UNK', ',', 'but', 'it', 'had', 'no',
    'pictures', 'or', 'UNK', 'in', 'it', ',', "'", 'and', 'what', 'is', 'the',
    'use', 'of', 'a', 'book', ",'", 'thought', 'Alice', "'", 'without',
    'pictures', 'or', 'conversation', "?'" ...]
    >>> len(set(alice2))
    1001

.. note: |TRY|
   Repeat the above example for different vocabulary sizes and different texts.
   How small a vocabulary can you tolerate while still getting something useful
   from the text?

Incrementally Updating a Dictionary
----------------------------------- 

We can employ dictionaries to count occurrences, emulating the method
for tallying words shown in fig-tally_.
We begin by initializing an empty ``defaultdict``, then process each
part-of-speech tag in the text.  If the tag hasn't been seen before,
it will have a zero count by default.  Each time we encounter a tag,
we increment its count using the ``+=`` operator.

.. pylisting:: code-dictionary
   :caption: Incrementally Updating a Dictionary, and Sorting by Value

    >>> from collections import defaultdict
    >>> counts = defaultdict(int)
    >>> from nltk.corpus import brown
    >>> for (word, tag) in brown.tagged_words(categories='news', tagset='universal'):
    ...     counts[tag] += 1
    ...
    >>> counts['NOUN']
    30640
    >>> sorted(counts)
    ['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']
    
    >>> from operator import itemgetter
    >>> sorted(counts.items(), key=itemgetter(1), reverse=True)
    [('NOUN', 30640), ('VERB', 14399), ('ADP', 12355), ('.', 11928), ...]
    >>> [t for t, c in sorted(counts.items(), key=itemgetter(1), reverse=True)]
    ['NOUN', 'VERB', 'ADP', '.', 'DET', 'ADJ', 'ADV', 'CONJ', 'PRON', 'PRT', 'NUM', 'X']

The listing in code-dictionary_ illustrates an important idiom for
sorting a dictionary by its values, to show words in decreasing
order of frequency.  The first parameter of ``sorted()`` is the items
to sort, a list of tuples consisting of a POS tag and a frequency.
The second parameter specifies the sort key using a function ``itemgetter()``.
In general, ``itemgetter(n)`` returns a function that can be called on
some other sequence object to obtain the `n`:math:\ th element, e.g.:

    >>> pair = ('NP', 8336)
    >>> pair[1]
    8336
    >>> itemgetter(1)(pair)
    8336

The last parameter of ``sorted()`` specifies that the items should be returned
in reverse order, i.e. decreasing values of frequency.

There's a second useful programming idiom at the beginning of
code-dictionary_, where we initialize a ``defaultdict`` and then use a
``for`` loop to update its values. Here's a schematic version:

|    ``>>> my_dictionary = defaultdict(``\ *function to create default value*\ ``)``
|    ``>>> for`` *item* ``in`` *sequence*\ ``:``
|    ``...      my_dictionary[``\ *item_key*\ ``]`` *is updated with information about item*

Here's another instance of this pattern, where we index words according to their last two letters:

    >>> last_letters = defaultdict(list)
    >>> words = nltk.corpus.words.words('en')
    >>> for word in words:
    ...     key = word[-2:]
    ...     last_letters[key].append(word)
    ...
    >>> last_letters['ly']
    ['abactinally', 'abandonedly', 'abasedly', 'abashedly', 'abashlessly', 'abbreviately',
    'abdominally', 'abhorrently', 'abidingly', 'abiogenetically', 'abiologically', ...]
    >>> last_letters['zy']
    ['blazy', 'bleezy', 'blowzy', 'boozy', 'breezy', 'bronzy', 'buzzy', 'Chazy', ...]

The following example uses the same pattern to create an anagram dictionary.
(You might experiment with the third line to get an idea of why this program works.)

    >>> anagrams = defaultdict(list)
    >>> for word in words:
    ...     key = ''.join(sorted(word))
    ...     anagrams[key].append(word)
    ...
    >>> anagrams['aeilnrt']
    ['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']

Since accumulating words like this is such a common task,
|NLTK| provides a more convenient way of creating a ``defaultdict(list)``,
in the form of ``nltk.Index()``.

    >>> anagrams = nltk.Index((''.join(sorted(w)), w) for w in words)
    >>> anagrams['aeilnrt']
    ['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']

.. note::
   ``nltk.Index`` is a ``defaultdict(list)`` with extra support for
   initialization.  Similarly,
   ``nltk.FreqDist`` is essentially a ``defaultdict(int)`` with extra
   support for initialization (along with sorting and plotting methods).

Complex Keys and Values
-----------------------

We can use default dictionaries with complex keys and values.
Let's study the range of possible tags for a word, given the
word itself, and the tag of the previous word.  We will see
how this information can be used by a POS tagger.

    >>> pos = defaultdict(lambda: defaultdict(int))
    >>> brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')
    >>> for ((w1, t1), (w2, t2)) in nltk.bigrams(brown_news_tagged): # [_processing-pairs]
    ...     pos[(t1, w2)][t2] += 1 # [_tag-word-update]
    ...
    >>> pos[('DET', 'right')] # [_compound-key]
    defaultdict(<class 'int'>, {'ADJ': 11, 'NOUN': 5})

This example uses a dictionary whose default value for an entry
is a dictionary (whose default value is ``int()``, i.e. zero).
Notice how we iterated over the bigrams of the tagged
corpus, processing a pair of word-tag pairs for each iteration processing-pairs_.
Each time through the loop we updated our ``pos`` dictionary's
entry for ``(t1, w2)``, a tag and its *following* word tag-word-update_.
When we look up an item in ``pos`` we must specify a compound key compound-key_,
and we get back a dictionary object.
A POS tagger could use such information to decide that the
word `right`:lx:, when preceded by a determiner, should be tagged as ``ADJ``. 

Inverting a Dictionary
----------------------

Dictionaries support efficient lookup, so long as you want to get the value for
any key.  If ``d`` is a dictionary and ``k`` is a key, we type ``d[k]`` and
immediately obtain the value.  Finding a key given a value is slower and more
cumbersome:

    >>> counts = defaultdict(int)
    >>> for word in nltk.corpus.gutenberg.words('milton-paradise.txt'):
    ...     counts[word] += 1
    ...
    >>> [key for (key, value) in counts.items() if value == 32]
    ['brought', 'Him', 'virtue', 'Against', 'There', 'thine', 'King', 'mortal',
    'every', 'been']

If we expect to do this kind of "reverse lookup" often, it helps to construct
a dictionary that maps values to keys.  In the case that no two keys have
the same value, this is an easy thing to do.  We just get all the key-value
pairs in the dictionary, and create a new dictionary of value-key
pairs. The next example also illustrates another way of initializing a
dictionary ``pos`` with key-value pairs.

    >>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}
    >>> pos2 = dict((value, key) for (key, value) in pos.items())
    >>> pos2['N']
    'ideas'

Let's first make our part-of-speech dictionary a bit more realistic
and add some more words to ``pos`` using the dictionary ``update()`` method, to
create the situation where multiple keys have the same value. Then the
technique just shown for reverse lookup will no longer work (why
not?).  Instead, we have to use ``append()`` to accumulate the words
for each part-of-speech, as follows:

    >>> pos.update({'cats': 'N', 'scratch': 'V', 'peacefully': 'ADV', 'old': 'ADJ'})
    >>> pos2 = defaultdict(list)
    >>> for key, value in pos.items():
    ...     pos2[value].append(key)
    ... 
    >>> pos2['ADV']
    ['peacefully', 'furiously']

Now we have inverted the ``pos`` dictionary, and can look up any part-of-speech and find
all words having that part-of-speech.  We can do the same thing even
more simply using |NLTK|\ 's support for indexing as follows:

    >>> pos2 = nltk.Index((value, key) for (key, value) in pos.items())
    >>> pos2['ADV']
    ['peacefully', 'furiously']

A summary of Python's dictionary methods is given in tab-dict_.

.. table:: tab-dict

   ==================================  ==========================================================
   Example                             Description
   ==================================  ==========================================================
   ``d = {}``                          create an empty dictionary and assign it to ``d``
   ``d[key] = value``                  assign a value to a given dictionary key
   ``d.keys()``                        the list of keys of the dictionary
   ``list(d)``                         the list of keys of the dictionary
   ``sorted(d)``                       the keys of the dictionary, sorted
   ``key in d``                        test whether a particular key is in the dictionary
   ``for key in d``                    iterate over the keys of the dictionary
   ``d.values()``                      the list of values in the dictionary
   ``dict([(k1,v1), (k2,v2), ...])``   create a dictionary from a list of key-value pairs
   ``d1.update(d2)``                   add all items from ``d2`` to ``d1``
   ``defaultdict(int)``                a dictionary whose default value is zero
   ==================================  ==========================================================

   Python's Dictionary Methods: A summary of commonly-used methods and idioms
   involving dictionaries.

   
---------
Exercises
---------

#. |easy| Try using the Python interpreter as a calculator, and
   typing expressions like ``12 / (4 + 1)``.
   
#. |easy| Given an alphabet of 26 letters, there are 26 to the power
   10, or ``26 ** 10``, ten-letter strings we can form.  That works out
   to ``141167095653376L`` (the ``L`` at the end just indicates that
   this is Python's long-number format).  How many hundred-letter
   strings are possible?

#. |easy| The Python multiplication operation can be applied to lists.
   What happens when you type ``['Monty', 'Python'] * 20``,
   or ``3 * sent1``?

#. |easy| Review sec-vocabularies-as-sets-of-words_ on
   vocabularies as sets of words.  How many words are there in ``text2``?
   How many distinct words are there?

#. |easy| Compare the lexical diversity scores for humor
   and romance fiction in tab-brown-types_.  Which genre is
   more lexically diverse?

#. |easy| Produce a dispersion plot of the four main protagonists in
   *Sense and Sensibility*: Elinor, Marianne, Edward, and Willoughby.
   What can you observe about the different roles played by the males
   and females in this novel?  Can you identify the couples?

#. |easy| Find the collocations in ``text5``.

#. |easy| Consider the following Python expression: ``len(set(text4))``.
   State the purpose of this expression.  Describe the two steps
   involved in performing this computation.

#. |easy| Review sec-texts-as-lists-of-words_
   on lists and strings.
   
   a) Define a string and assign it to a variable, e.g.,
      ``my_string = 'My String'`` (but put something more interesting in the string).
      Print the contents of this variable in two ways, first
      by simply typing the variable name and pressing enter, then
      by using the ``print`` statement.
   
   b) Try adding the string to itself using ``my_string + my_string``, or multiplying
      it by a number, e.g., ``my_string * 3``.  Notice that the strings
      are joined together without any spaces.  How could you fix this?

#. |easy| Define a variable ``my_sent`` to be a list of words, using
   the syntax ``my_sent = ["My", "sent"]`` (but with your own words,
   or a favorite saying).
   
   a) Use ``' '.join(my_sent)`` to convert this into a string.
   b) Use ``split()`` to split the string back into the list form
      you had to start with.

#. |easy| Define several variables containing lists of words, e.g., ``phrase1``,
   ``phrase2``, and so on.  Join them together in various combinations (using the plus operator)
   to form whole sentences.  What is the relationship between
   ``len(phrase1 + phrase2)`` and ``len(phrase1) + len(phrase2)``?

#. |easy| Consider the following two expressions, which have the same
   value.  Which one will typically be more relevant in |NLP|?  Why?

   a) ``"Monty Python"[6:12]``
   b) ``["Monty", "Python"][1]``

#. |easy| We have seen how to represent a sentence as a list of words, where
   each word is a sequence of characters.  What does ``sent1[2][2]`` do?
   Why?  Experiment with other index values.

#. |easy| The first sentence of ``text3`` is provided to you in the
   variable ``sent3``.  The index of `the`:lx: in ``sent3`` is 1, because ``sent3[1]``
   gives us ``'the'``.  What are the indexes of the two other occurrences
   of this word in ``sent3``?

#. |easy| Review the discussion of conditionals in sec-making-decisions_.
   Find all words in the Chat Corpus (``text5``)
   starting with the letter `b`:lx:.  Show them in alphabetical order.

#. |easy| Type the expression ``list(range(10))`` at the interpreter prompt.
   Now try ``list(range(10, 20))``, ``list(range(10, 20, 2))``, and ``list(range(20, 10, -2))``.
   We will see a variety of uses for this built-in function in later chapters.

#. |easy| Find out more about sequence objects using Python's help facility.
   In the interpreter, type ``help(str)``, ``help(list)``, and ``help(tuple)``.
   This will give you a full list of the functions supported by each type.
   Some functions have special names flanked with underscore; as the
   help documentation shows, each such function corresponds to something
   more familiar.  For example ``x.__getitem__(y)`` is just a long-winded
   way of saying ``x[y]``.

#. |easy| Identify three operations that can be performed on both tuples
   and lists.  Identify three list operations that cannot be performed on
   tuples.  Name a context where using a list instead of a tuple generates
   a Python error.

#. |easy| Find out how to create a tuple consisting of a single item.
   There are at least two ways to do this.

#. |easy| Create a list ``words = ['is', 'NLP', 'fun', '?']``.  Use
   a series of assignment statements (e.g. ``words[1] = words[2]``)
   and a temporary variable ``tmp`` to transform this list into the
   list ``['NLP', 'is', 'fun', '!']``.  Now do the same transformation
   using tuple assignment.

#. |easy| Does the method for creating a sliding window of n-grams
   behave correctly for the two limiting cases: `n`:math: = 1, and `n`:math: = ``len(sent)``?

#. |easy| We pointed out that when empty strings and empty lists occur
   in the condition part of an ``if`` clause, they evaluate to
   ``False``. In this case, they are said to be occurring in a
   Boolean context.
   Experiment with different kind of non-Boolean expressions in Boolean
   contexts, and see whether they evaluate as ``True`` or ``False``.

#. |easy| Use the inequality operators to compare strings, e.g.
   ``'Monty' < 'Python'``.  What happens when you do ``'Z' < 'a'``?
   Try pairs of strings which have a common prefix, e.g. ``'Monty' < 'Montague'``.
   Read up on "lexicographical sort" in order to understand what is
   going on here.  Try comparing structured objects, e.g.
   ``('Monty', 1) < ('Monty', 2)``.  Does this behave as expected?

#. |easy| Write code that removes whitespace at the beginning and end of a
   string, and normalizes whitespace between words to be a single
   space character.

   #) do this task using ``split()`` and ``join()``

   #) do this task using regular expression substitutions

#. |soso| Use ``text9.index()`` to find the index of the word `sunset`:lx:.
   You'll need to insert this word as an argument between the parentheses.
   By a process of trial and error, find the slice for the complete sentence that
   contains this word.

#. |soso| Using list addition, and the ``set`` and ``sorted`` operations, compute the
   vocabulary of the sentences ``sent1`` ... ``sent8``.

#. |soso| What is the difference between the following two lines?
   Which one will give a larger value?  Will this be the case for other texts?
   
   .. doctest-ignore::
       >>> sorted(set(w.lower() for w in text1))
       >>> sorted(w.lower() for w in set(text1))

#. |soso| What is the difference between the following two tests:
   ``w.isupper()`` and ``not w.islower()``?

#. |soso| Write the slice expression that extracts the last two words of ``text2``.

#. |soso| Find all the four-letter words in the Chat Corpus (``text5``).
   With the help of a frequency distribution (``FreqDist``), show these
   words in decreasing order of frequency.

#. |soso| Review the discussion of looping with conditions in sec-making-decisions_.
   Use a combination of ``for`` and ``if`` statements to loop over the words of
   the movie script for *Monty Python and the Holy Grail* (``text6``)
   and ``print`` all the uppercase words, one per line.

#. |soso| Write expressions for finding all words in ``text6`` that
   meet the conditions listed below.  The result should be in the form of
   a list of words: ``['word1', 'word2', ...]``.
   
   a) Ending in `ize`:lx:
   b) Containing the letter `z`:lx:
   c) Containing the sequence of letters `pt`:lx:
   d) Having all lowercase letters except for an initial capital (i.e., ``titlecase``)

#. |soso| Define ``sent`` to be the list of words
   ``['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']``.
   Now write code to perform the following tasks:

   a) Print all words beginning with `sh`:lx:
   b) Print all words longer than four characters

#. |soso| What does the following Python code do?  ``sum(len(w) for w in text1)``
   Can you use it to work out the average word length of a text?

#. |soso| Define a function called ``vocab_size(text)`` that has a single
   parameter for the text, and which returns the vocabulary size of the text.

#. |soso| Define a function ``percent(word, text)`` that calculates
   how often a given word occurs in a text, and expresses the result
   as a percentage.

#. |soso| We have been using sets to store vocabularies.  Try the following
   Python expression: ``set(sent3) < set(text1)``.  Experiment with this using
   different arguments to ``set()``.  What does it do?
   Can you think of a practical application for this?

#. |soso| Write code to sort words by length. Consult the Python
   documentation concerning the ``key`` parameter.

#. |soso| Create a list of words and store it in a variable ``sent1``.
   Now assign ``sent2 = sent1``.  Modify one of the items in ``sent1``
   and verify that ``sent2`` has changed.

   a) Now try the same exercise but instead assign ``sent2 = sent1[:]``.
      Modify ``sent1`` again and see what happens to ``sent2``.  Explain.
   b) Now define ``text1`` to be a list of lists of strings (e.g. to
      represent a text consisting of multiple sentences.  Now assign
      ``text2 = text1[:]``, assign a new value to one of the words,
      e.g. ``text1[1][1] = 'Monty'``.  Check what this did to ``text2``.
      Explain.
   c) Load Python's ``deepcopy()`` function (i.e. ``from copy import deepcopy``),
      consult its documentation, and test that it makes a fresh copy of any
      object.

#. |soso| Initialize an *n*\ -by-*m* list of lists of empty strings using list
   multiplication, e.g. ``word_table = [[''] * n] * m``.  What happens
   when you set one of its values, e.g. ``word_table[1][2] = "hello"``?
   Explain why this happens.  Now write an expression using ``range()``
   to construct a list of lists, and show that it does not have this problem.

#. |soso| Write code to initialize a two-dimensional array of sets called
   ``word_vowels`` and process a list of words, adding each
   word to ``word_vowels[l][v]`` where ``l`` is the length of the word and ``v`` is
   the number of vowels it contains.

#. |soso| Write a function ``novel10(text)`` that prints any word that
   appeared in the last 10% of a text that had not been encountered earlier.

#. |soso| Write a program that takes a sentence expressed as a single string,
   splits it and counts up the words.  Get it to print out each word and the
   word's frequency, one per line, in alphabetical order.

#. |soso| Write a function ``shorten(text, n)`` to process a text, omitting the `n`:math:
   most frequently occurring words of the text.  How readable is it?

#. |soso| Write code to print out an index for a lexicon, allowing someone
   to look up words according to their meanings (or pronunciations; whatever
   properties are contained in lexical entries).

#. |soso| Write a list comprehension that sorts a list of WordNet synsets for
   proximity to a given synset.  For example, given the synsets
   ``minke_whale.n.01``, ``orca.n.01``, ``novel.n.01``, and ``tortoise.n.01``,
   sort them according to their ``shortest_path_distance()`` from ``right_whale.n.01``.

#. |soso| Write a function that takes a list of words (containing duplicates) and
   returns a list of words (with no duplicates) sorted by decreasing frequency.
   E.g. if the input list contained 10 instances of the word ``table`` and 9 instances
   of the word ``chair``, then ``table`` would appear before ``chair`` in the output
   list.

#. |soso| Write a function that takes a text and a vocabulary as its arguments
   and returns the set of words that appear in the text but not in the
   vocabulary.  Both arguments can be represented as lists of strings.
   Can you do this in a single line, using ``set.difference()``?

#. |soso| Import the ``itemgetter()`` function from the ``operator`` module in Python's
   standard library (i.e. ``from operator import itemgetter``).  Create a list
   ``words`` containing several words.  Now try calling:
   ``sorted(words, key=itemgetter(1))``, and ``sorted(words, key=itemgetter(-1))``.
   Explain what ``itemgetter()`` is doing.

#. |soso| Write a recursive function ``lookup(trie, key)`` that looks up a key in a trie,
   and returns the value it finds.  Extend the function to return a word when it is uniquely
   determined by its prefix (e.g. ``vanguard`` is the only word that starts with ``vang-``,
   so ``lookup(trie, 'vang')`` should return the same thing as ``lookup(trie, 'vanguard')``).

#. |soso| Read up on "keyword linkage" (chapter 5 of [Scott2006]_).  Extract keywords from
   |NLTK|\ 's Shakespeare Corpus and using the NetworkX package, plot keyword linkage networks. 

#. |soso| Read about string edit distance and the Levenshtein Algorithm.
   Try the implementation provided in ``nltk.edit_distance()``.
   In what way is this using dynamic programming?  Does it use the bottom-up or
   top-down approach?
   [See also ``http://norvig.com/spell-correct.html``]

#. |soso| The Catalan numbers arise in many applications of combinatorial mathematics,
   including the counting of parse trees (sec-grammar-development_).  The series
   can be defined as follows: C\ :subscript:`0` = 1, and
   C\ :subscript:`n+1` = |Sigma|\ :subscript:`0..n` (C\ :subscript:`i`\ C\ :subscript:`n-i`).

   a) Write a recursive function to compute `n`:math:\ th Catalan number C\ :subscript:`n`.

   b) Now write another function that does this computation using dynamic programming.

   c) Use the ``timeit`` module to compare the performance of these functions as `n`:math:
      increases.

#. |hard| 
   Reproduce some of the results of [Zhao07]_ concerning authorship identification.

#. |hard| Study gender-specific lexical choice, and see if you can
   reproduce some of the results of ``http://www.clintoneast.com/articles/words.php``

#. |hard| Write a recursive function that pretty prints a trie in alphabetically
   sorted order, e.g.:: 

    chair: 'flesh'
    ---t: 'cat'
    --ic: 'stylish'
    ---en: 'dog'

#. |hard| With the help of the trie data structure, write a recursive
   function that processes text, locating the uniqueness point in
   each word, and discarding the remainder of each word.  How much compression does this
   give?  How readable is the resulting text?

#. |hard| Obtain some raw text, in the form of a single, long string.
   Use Python's ``textwrap`` module to break it up into multiple lines.
   Now write code to add extra spaces between words, in order to justify
   the output.  Each line must have the same width, and spaces must be
   approximately evenly distributed across each line.  No line can
   begin or end with a space.

#. |hard| Develop a simple extractive summarization tool, that prints the
   sentences of a document which contain the highest total word
   frequency.  Use ``FreqDist()`` to count word frequencies, and use
   ``sum`` to sum the frequencies of the words in each sentence.
   Rank the sentences according to their score.  Finally, print the *n*
   highest-scoring sentences in document order.  Carefully review the
   design of your program, especially your approach to this double
   sorting.  Make sure the program is written as clearly as possible.
   
#. |hard|
   Read the following article on semantic orientation of adjectives.
   Use the NetworkX package to visualize
   a network of adjectives with edges to indicate same vs different
   semantic orientation.  ``http://www.aclweb.org/anthology/P97-1023``
         
#. |hard|
   Design an algorithm to find the "statistically improbable
   phrases" of a document collection.
   ``http://www.amazon.com/gp/search-inside/sipshelp.html``

#. |hard| Write a program to implement a brute-force algorithm for
   discovering word squares, a kind of `n`:math: |times| `n`:math: crossword
   in which the entry in the `n`:math:\ th row is the same as the entry
   in the `n`:math:\ th column.  For discussion, see
   ``http://itre.cis.upenn.edu/~myl/languagelog/archives/002679.html``

.. #. |hard| Extend the program in Example compound-keys_ in the following ways:

   a) Define two sets ``verbs`` and ``preps``, and add each verb and preposition
      as they are encountered.  (Note that you can add an item to a set without
      bothering to check whether it is already present.)

   b) Create nested loops to display the results, iterating over verbs and
      prepositions in sorted order.  Generate one line of output per verb,
      listing prepositions and attachment ratios as follows:
      ``raised: about 0:3, at 1:0, by 9:0, for 3:6, from 5:0, in 5:5...``

   c) We used a tuple to represent a compound key consisting of two strings.
      However, we could have simply concatenated the strings, e.g.
      ``key = verb + ":" + prep``, resulting in a simple string key.
      Why is it better to use tuples for compound keys?


.. include:: footer.rst
