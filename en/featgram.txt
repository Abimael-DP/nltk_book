.. -*- mode: rst -*-
.. include:: ../definitions.txt

.. _chap-featgram:

========================
9. Feature Based Grammar
========================

--------------
 Introduction
--------------

The framework of context-free grammars that we presented in Chapter
chap-parse_ describes syntactic constituents with the help of a
limited set of category labels. These atomic labels are adequate for
talking about the gross structure of sentences. But when we start to
make finer grammatical distinctions it becomes awkward to enrich the
set of categories in a systematic manner. In this chapter we will
address this challenge by decomposing categories using features
(somewhat similar to the key-value pairs of Python dictionaries). 

We will start off by looking at the phenomenon of syntactic agreement;
we will show how agreement constraints can be expressed elegantly using
features, and illustrate how their use in a simple grammar. Feature
structures are a general data structure for representing information
of any kind; we will briefly look at them from a more formal point of
view, and explain how they are made available in |NLTK|. In the
final part of the chapter, we demonstrate that the additional
expressiveness of features opens out a wide spectrum of possibilities
for describing sophisticated aspects of linguistic structure.

--------------------------------
Decomposing Lingustic Categories
--------------------------------

Syntactic Agreement
-------------------


Consider the following contrasts:

.. _thisdog:
.. ex::
   .. ex::
      this dog
   .. ex::
      \*these dog

.. _thesedogs:
.. ex::
   .. ex::
      these dogs
   .. ex::
      \*this dog

|nopar| In English, nouns are usually morphologically marked as being singular
or plural. The form of the demonstrative also varies in a
similar way; there is a singular form `this`:lx: and a plural form `these`:lx:.
Examples thisdog_ and thesedogs_ show that there are constraints on
the realization of demonstratives and nouns within a noun phrase:
either both are singular or both are plural. A similar kind
of constraint is observed with subjects and predicates:

.. _subjpredsg:
.. ex::
   .. ex::
      the dog runs
   .. ex::
      \*the dog run

.. _subjpredpl:
.. ex::
   .. ex::
      the dogs run
   .. ex::
      \*the dogs runs


|nopar| Here again, we can see that morphological properties of the verb co-vary
with morphological properties of the subject noun phrase; this co-variance is
usually termed `agreement`:dt: The element which determines the
agreement, here the subject noun phrase, is called the agreement
`controller`:dt:, while the element whose form is determined by
agreement, here the verb, is called the `target`:dt:.
If we look further at verb agreement in English, we will see that
present tense verbs typically have two inflected forms: one for third person
singular, and another for every other combination of person and
number:

.. _agreement-paradigm:

+------------+-------------+----------+
|            |**singular** |**plural**|
+------------+-------------+----------+
|**1st per** |*I run*      |*we run*  |
|            |             |          |
+------------+-------------+----------+
|**2nd per** |*you run*    |*you run* |
|            |             |          |
+------------+-------------+----------+
|**3rd per** |*he/she/it   |*they run*|
|            |runs*        |          |
+------------+-------------+----------+

We can make the role of morphological properties a bit more explicit
as illustrated in runs_ and run_. These representations indicate that
the verb agrees with its subject in person and number. (We use '3' as
an abbreviation for 3rd person, 'SG' for singular and 'PL' for plural.)

.. _runs:
.. ex::
   .. gloss::
      the | dog       |run-s  
          | dog.3.SG  |run-3.SG  

.. _run:
.. ex::
   .. gloss::
      the | dog-s     |run
          | dog.3.PL  |run-3.PL 

Despite the undoubted interest of agreement as a topic in its own
right, we have introduced it here for another reason: we want to look
at what happens when we try encode agreement constraints in a
context-free grammar.  Suppose we take as our starting point the very
simple CFG in agcfg0_.

.. _agcfg0:
.. ex::
   .. parsed-literal::

     `S`:gc: |rarr| `NP VP`:gc:
     `NP`:gc: |rarr| `Det N`:gc: 
     `VP`:gc: |rarr| `V`:gc: 

     `Det`:gc: |rarr| 'this'
     `N`:gc: |rarr| 'dog'
     `V`:gc: |rarr| 'runs'

|nopar| agcfg0_ allows us to generate the sentence `this dog runs`:lx:;
however, what we really want to do is also generate `these dogs
run`:lx: while blocking unwanted strings such as `*this dogs run`:lx:
and `*these dog runs`:lx:. The most straightforward approach is to
add new non-terminals and productions to the grammar which reflect our
number distinctions and agreement constraints (we ignore person for the time being):

.. _agcfg1:
.. ex::
   .. parsed-literal::

     `S_SG`:gc: |rarr| `NP_SG VP_SG`:gc:
     `S_PL`:gc: |rarr| `NP_PL VP_PL`:gc:
     `NP_SG`:gc: |rarr| `Det_SG N_SG`:gc: 
     `NP_PL`:gc: |rarr| `Det_PL N_PL`:gc: 
     `VP_SG`:gc: |rarr| `V_SG`:gc: 
     `VP_PL`:gc: |rarr| `V_PL`:gc: 

     `Det_SG`:gc: |rarr| 'this'
     `Det_PL`:gc: |rarr| 'these'
     `N_SG`:gc: |rarr| 'dog'
     `N_PL`:gc: |rarr| 'dogs'
     `V_SG`:gc: |rarr| 'runs'
     `V_PL`:gc: |rarr| 'run'

|nopar| It should be clear that this grammar will do the required
task, but only at the cost of duplicating our previous set of
rules. Rule multiplication is of course more severe if we add in
person agreement constraints.


Using Attributes and Constraints
--------------------------------

We spoke informally of linguistic categories having *properties*; for
example, that a verb has the property of being plural. Let's try to
make this more explicit:

.. _num0:
.. ex::
   .. parsed-literal::

     `N`:gc:\ [`num`:feat: = `pl`:fval:\ ]

|nopar| In num0_, we have introduced some new notation which says that the
category `N`:gc: has a `feature`:dt: called `num`:feat: (short for
'number') and that the value of this feature is `pl`:fval: (short for
'plural'). We can add similar annotations to other categories, and use
them in lexical entries:

.. _agcfg2:
.. ex::
   .. parsed-literal::

     `Det`:gc:\ [`num`:feat: = `sg`:fval:\ ] |rarr| 'this'
     `Det`:gc:\ [`num`:feat: = `pl`:fval:\ ]  |rarr| 'these'
     `N`:gc:\ [`num`:feat: = `sg`:fval:\ ] |rarr| 'dog'
     `N`:gc:\ [`num`:feat: = `pl`:fval:\ ] |rarr| 'dogs'
     `V`:gc:\ [`num`:feat: = `sg`:fval:\ ] |rarr| 'runs'
     `V`:gc:\ [`num`:feat: = `pl`:fval:\ ] |rarr| 'run'

|nopar| Does this help at all? So far, it looks just like a slightly more
verbose alternative to what was specified in agcfg1_. Things become
more interesting when we allow *variables* over feature values, and use
these to state constraints. This is illustrated in agcfg3_.

.. _agcfg3:
.. ex::
   .. _srule:
   .. ex::
      .. parsed-literal::

        `S`:gc: |rarr| `NP`:gc:\ [`num`:feat: = `?n`:math:\ ] `VP`:gc:\ [`num`:feat: = `?n`:math:\ ]

   .. _nprule:
   .. ex::
      .. parsed-literal::

       `NP`:gc:\ [`num`:feat: = `?n`:math:\ ] |rarr| `Det`:gc:\ [`num`:feat: = `?n`:math:\ ] `N`:gc:\ [`num`:feat: = `?n`:math:\ ]

   .. _vprule:
   .. ex::
      .. parsed-literal::

       `VP`:gc:\ [`num`:feat: = `?n`:math:\ ] |rarr| `V`:gc:\ [`num`:feat: = `?n`:math:\ ]

|nopar| We are using '`?n`:math:' as a variable over values of `num`:feat:; it can
be instantiated either to `sg`:fval: or `pl`:fval:. Its scope is
limited to individual rules. That is, within srule_, for example,
`?n`:math: must be instantiated to the same constant value; we can
read the rule as saying that whatever value `NP`:gc: takes for the feature
`num`:feat:, `VP`:gc: must take the same value. 

In order to understand how these feature constraints work, it's
helpful to think about how one would go about building a tree. Lexical
rules will admit the following local trees (trees of
depth one):

.. ex::
   .. _this:
   .. ex:: 
      .. tree:: (Det[NUM\ SG] this) 
   .. _these:
   .. ex:: 
      .. tree:: (Det[NUM\ PL] these) 

.. ex::
   .. _dog:
   .. ex:: 
      .. tree:: (N[NUM\ SG] dog) 
   .. _dogs:
   .. ex:: 
      .. tree:: (N[NUM\ PL] dogs) 

|nopar| Now nprule_ says that whatever the `num`:feat: values of `N`:gc: and
`Det`:gc: are, they have to be the same. Consequently,  nprule_ will
permit this_ and dog_ to be combined into an `NP`:gc:  as shown in
good1_ and it will also allow these_ and dogs_ to be combined, as in
good2_. By contrast,  bad1_ and bad2_ are prohibited because the roots
of their
constituent local trees differ in their values for the `num`:feat: feature.

.. ex::
   .. _good1:
   .. ex::
      .. tree:: (NP[NUM\ PL] (Det[NUM\ SG] this)(N[NUM\ SG] dog))

   .. _good2:
   .. ex::
      .. tree:: (NP[NUM\ PL] (Det[NUM\ PL] these)(N[NUM\ PL] dogs))

.. ex::
   .. _bad1:
   .. ex::
      .. tree:: (NP[NUM\ ...] (Det[NUM\ SG] this)(N[NUM\ PL] dogs))

   .. _bad2:
   .. ex::
      .. tree:: (NP[NUM\ ...] (Det[NUM\ PL] these)(N[NUM\ SG] dog))

Rule vprule_ can be thought of as saying that the `num`:feat: value of the
head verb has to be the same as the `num`:feat: value of the `VP`:gc:
mother. Combined with srule_, we derive the consequence that if the
`num`:feat: value of the subject head noun is `pl`:fval:, then so is
the `num`:feat: value of the `VP`:gc:\ 's head verb.

.. ex::
   .. tree:: (S (NP[NUM\ PL] (Det[NUM\ PL] these)(N[NUM\ PL] dogs))(VP[NUM\ PL] (V[NUM\ PL] run)))

Grammar feat0cfg_ illustrates most of the ideas we have introduced so
far in this chapter, plus a couple of new ones.

.. _feat0cfg:
.. ex::
.. include:: ../../examples/parse/feat0.cfg
   :literal:

|nopar| First, you will notice that a feature annotation on a syntactic
category can contain more than one specification; for example,
`V`:gc:\ [`tense`:feat: = `pres`:fval:, `num`:feat: = `pl`:fval:\
]. In general, there is no upper bound on the number of features we
specify as part of our syntactic categories.

Second, we have used feature variables in lexical entries as well
as grammatical rules. For example, `the`:lx: has been assigned the
category `Det`:gc:\ [`num`:feat: = `?n`:math:]. Why is this?  Well,
you know that the definite article `the`:lx: can combine with both
singular and plural nouns. One way of describing this would be to add
two lexical entries to the grammar, one each for the singular and
plural versions of `the`:lx:. However, a more elegant solution is to
leave the `num`:feat: value `underspecified`:dt: and letting it agree
in number with whatever noun it combines with.

A final point to note about feat0cfg_ is the statment ``%start
S``. This a 'directive' which tells the parser to take `S`:gc: as the
start symbol for the grammar.

In general, when we are trying to develop even a very small grammar,
it is convenient to put the rules in a file where they can be edited,
tested and revised. Assuming we have saved feat0cfg_ as a file named
'feat0.cfg', the function ``GrammarFile.read_file()`` allows us to
read the grammar into NLTK, ready for use in parsing.

.. doctest-ignore::
    >>> from nltk_lite.parse import GrammarFile
    >>> from pprint import pprint
    >>> from nltk_lite import tokenize
    >>> g = GrammarFile.read_file('feat0.cfg')

|nopar| We can inspect the rules and the lexicon.

.. doctest-ignore::
    >>> print g.earley_grammar()
    Grammar with 7 productions (start state = Start[])
	Start -> S
	S -> NP[num=?n] VP[num=?n]
	NP[num=?n] -> N[num=?n]
	NP[num=?n] -> PropN[num=?n]
	NP[num=?n] -> Det[num=?n] N[num=?n]
        NP[num=pl] -> N[num=pl]
	VP[num=?n, tense=?t] -> IV[num=?n, tense=?t]
	VP[num=?n, tense=?t] -> TV[num=?n, tense=?t] NP
    >>> pprint(g.earley_lexicon())
     {'Jody': [PropN[num=sg]],
      'Kim': [PropN[num=sg]],
      'all': [Det[num=pl]],
      'car': [N[num=sg]],
      'cars': [N[num=pl]],
      'child': [N[num=sg]],
      'children': [N[num=pl]],
      'disappear': [IV[num=pl, tense=pres]],
      'disappeared': [IV[num=?n, tense=past]],
      'disappears': [IV[num=sg, tense=pres]],
      'dog': [N[num=sg]],
      'dogs': [N[num=pl]],
      'every': [Det[num=sg]],
      'girl': [N[num=sg]],
      'girls': [N[num=pl]],
      'like': [TV[num=pl, tense=pres]],
      'liked': [TV[num=?n, tense=past]],
      'likes': [TV[num=sg, tense=pres]],
      'saw': [TV[num=?n, tense=past]],
      'see': [TV[num=pl, tense=pres]],
      'sees': [TV[num=sg, tense=pres]],
      'some': [Det[num=?n]],
      'the': [Det[num=?n]],
      'these': [Det[num=pl]],
      'this': [Det[num=sg]],
      'walk': [IV[num=pl, tense=pres]],
      'walked': [IV[num=?n, tense=past]],
      'walks': [IV[num=sg, tense=pres]]}

Now we can tokenize a sentence and use the ``get_parse_list()`` function to
invoke the Earley chart parser.

.. doctest-ignore::
     >>> from nltk_lite import tokenize
     >>> sent = 'Kim likes children'
     >>> tokens = list(tokenize.whitespace(sent))
     >>> tokens 
     ['Kim', 'likes', 'children']
     >>> cp = g.earley_parser()
     >>> trees = cp.get_parse_list(tokens)
	       |.K.l.c.|
     Predictor |> . . .| Start -> * S 
     Predictor |> . . .| S -> * NP[num=?n] VP[num=?n] 
     Predictor |> . . .| NP[num=?n] -> * N[num=?n] 
     Predictor |> . . .| NP[num=?n] -> * PropN[num=?n] 
     Predictor |> . . .| NP[num=?n] -> * Det[num=?n] N[num=?n] 
     Predictor |> . . .| NP[num=pl] -> * N[num=pl] 
     Scanner   |[-] . .| PropN[num=sg] -> 'Kim' * 
     Completer |[-] . .| NP[num=sg] -> PropN[num=sg] * 
     Completer |[-> . .| S -> NP[num=sg] * VP[num=sg] 
     Predictor |. > . .| VP[num=?n, tense=?t] -> * IV[num=?n, tense=?t] 
     Predictor |. > . .| VP[num=?n, tense=?t] -> * TV[num=?n, tense=?t] NP 
     Scanner   |. [-] .| TV[num=sg, tense=pres] -> 'likes' * 
     Completer |. [-> .| VP[num=sg, tense=pres] -> TV[num=sg, tense=pres] * NP 
     Predictor |. . > .| NP[num=?n] -> * N[num=?n] 
     Predictor |. . > .| NP[num=?n] -> * PropN[num=?n] 
     Predictor |. . > .| NP[num=?n] -> * Det[num=?n] N[num=?n] 
     Predictor |. . > .| NP[num=pl] -> * N[num=pl] 
     Scanner   |. . [-]| N[num=pl] -> 'children' * 
     Completer |. . [-]| NP[num=pl] -> N[num=pl] * 
     Completer |. [---]| VP[num=sg, tense=pres] -> TV[num=sg, tense=pres] NP * 
     Completer |[=====]| S -> NP[num=sg] VP[num=sg] * 
     Completer |[=====]| Start -> S * 
     Completer |[=====]| [INIT] -> Start * 

|nopar| Finally, we can inspect the resulting parse trees (in this case, a
single one).

.. doctest-ignore::
     >>> for tree in trees: print tree
     ... 
     ([INIT]:
       (Start:
	 (S:
	   (NP[num=sg]: (PropN[num=sg]: 'Kim'))
	   (VP[num=sg, tense=pres]:
	     (TV[num=sg, tense=pres]: 'likes')
	     (NP[num=pl]: (N[num=pl]: 'children'))))))


Terminology
-----------

So far, we have only seen feature values like `sg`:fval: and
`pl`:fval:. These simple values are usually called `atomic`:dt:
|mdash| that is, they can't be decomposed into subparts. A special
case of atomic values are `boolean`:dt: values, that is, values which
just specify whether a property is true or false of a category. For
example, we might want to distinguish `auxiliary`:dt: verbs such as
`can`:lx:, `may`:lx:, `will`:lx: and `do`:lx: with the boolean feature
`aux`:feat:. Then our lexicon for verbs could include entries such as
the following:

.. ex::
      .. parsed-literal::

        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `+`:math:\ ] |rarr| 'can'
        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `+`:math:\ ] |rarr| 'may'

        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `-`:math:\ ] |rarr| 'walks'
        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `-`:math:\ ] |rarr| 'likes'


A frequently used abbreviation for boolean features allows the value
to be prepended to the feature:

.. ex::
      .. parsed-literal::

        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `+aux`:feat:\ ] |rarr| 'can'
        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `-aux`:feat:\ ] |rarr| 'walks'


We have spoken informally of attaching 'feature annotations' to
syntactic categories. A more general
approach is to treat the whole category |mdash| that is, the
non-terminal symbol plus the annotation |mdash| as a bundle of
features. Consider, for example, the object we have written as ncat0_.

.. _ncat0:
.. ex::
      .. parsed-literal::

        `N`:gc:\ [`num`:feat: = `sg`:fval:\ ] 

|nopar| The syntactic category `N`:gc:, as we have seen before, provides part
of speech information. This information can itself be captured as a
feature value pair, using  `pos`:feat: to represent 'part of speech':

.. _ncat1:
.. ex::
      .. parsed-literal::

        [`pos`:feat: = `N`:fval:, `num`:feat: = `sg`:fval:\ ] 

|nopar| In fact, we  regard ncat1_ as our 'official' representation of a
feature-based linguistic category, and
ncat0_ as a convenient abbreviation.
A bundle of feature-value pairs is called a `feature structure`:dt:
or an `attribute value matrix`:dt: (AVM). A feature structure which
contains a specification for the feature `pos`:feat: is a `linguistic
category`:dt:. 

In addition to atomic-valued features, we allow features whose values
are themselves feature structures. For example, we might want to group
together agreement features (e.g., person, number and gender) as a
distinguished part of a category, as shown in agr0_.

.. _agr0:
.. ex::
      .. avm:: 
        [pos = N           ]
        [                  ]
        [agr = [per = 3   ]]
        [      [num = pl  ]]
        [      [gend = fem]]


|nopar| In this case, we say that the feature `agr`:feat: has a `complex`:dt: value.

There is no particular significance to the *order* of features in a
feature structure. So agr0_ is equivalent to agr0_.

.. _agr1:
.. ex::
      .. avm::
        [agr = [num = pl  ]]
        [      [per = 3   ]]
        [      [gend = fem]]
        [                  ]
        [pos = N           ]



Exercises
---------

#. |easy| Augment agcfg1_ so that it will generate strings like `I am
   happy`:lx: and `she is happy`:lx: but not `*you is happy`:lx: or
   `*they am happy`:lx:.

#. |soso| Augment agcfg1_ so that it will correctly describe the following
   Spanish noun phrases:

    .. ex::
       .. gloss::
	  un                                  | cuadro  | hermos-o
	  INDEF.SG.MASC                       | picture | beautiful-SG.MASC
	  'a beautiful picture'               

       .. gloss::
	  un-os                               | cuadro-s   | hermos-os           
	  INDEF-PL.MASC                       | picture-PL | beautiful-PL.MASC   
	  'beautiful pictures'                  

       .. gloss::
	  un-a                                | cortina | hermos-a
	  INDEF.SG.FEM                        | curtain | beautiful-SG.FEM
	  'a beautiful curtain'     

       .. gloss::
	  un-as                               | cortina-s | hermos-as
	  INDEF.PL.FEM                        | curtain   | beautiful-PL.FEM
	  'beautiful curtains'     


#. |soso| Redo the previous two exercises, but using feat0cfg_ as your
   starting point.


---------------------------------
Computing with Feature Structures
---------------------------------

In this section, we will show how feature structures can be
constructed and manipulated in |NLTK|. We will also discuss the
fundamental operation of unification, which allows us to combine the
information contained in two different feature structures.

Feature Structures in NLTK
--------------------------

Feature structures in NLTK are declared with the
``FeatureStructure()`` constructor. Atomic feature values can be strings or
integers.

    >>> from nltk_lite.featurestructure import *
    >>> fs1 = FeatureStructure(tense='past', num='sg') 
    >>> print fs1
    [ num   = 'sg'   ]
    [ tense = 'past' ]

|nopar| We can think of a feature structure as being like a Python dictionary,
and access its values by indexing in the usual way.

    >>> fs1 = FeatureStructure(per=3, num='pl', gend='fem')
    >>> print fs1['gend']
    fem

|nopar| However, we cannot use this syntax to *assign* values to features:

    >>> fs1[case] = 'acc'
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    NameError: name 'case' is not defined

|nopar| We can also define feature structures which have complex values, as
discussed earlier.

    >>> fs2 = FeatureStructure(pos='N', agr=fs1)
    >>> print fs2
    [       [ gend = 'fem' ] ]
    [ agr = [ num  = 'pl'  ] ]
    [       [ per  = 3     ] ]
    [                        ]
    [ pos = 'N'              ]
    >>> print fs2['agr']
    [ gend = 'fem' ]
    [ num  = 'pl'  ]
    [ per  = 3     ]
    >>> print fs2['agr']['per']
    3

An alternative method of specifying feature structures in NLTK is to
use the ``parse`` method of ``FeatureStructure``. This gives us the
facility to use square bracket notation for embedding one feature
structure within another.

    >>> FeatureStructure.parse("[pos='N', agr=[per=3, num='pl', gend='fem']]")
    [agr=[gend='fem', num='pl', per=3], pos='N']


Feature Structures as Graphs
----------------------------

Feature structures are not inherently tied to linguistic objects; they are
general purpose structures for representing knowledge. For example, we
could encode information about a person in a feature structure:

    >>> person01 = FeatureStructure(name='Lee', telno='01 27 86 42 96', age=33)

.. _person01:
.. ex::
      .. avm:: 
        [name = lee              ]
        [telno = 01 27 86 42 96  ]
        [age = 33                ]


It is sometimes helpful to view feature structures as graphs; more
specifically, `directed acyclic graphs`:dt: (DAGs). dag01_ is equivalent to
the AVM person01_.

.. _dag01:
.. ex::
   .. image:: ../images/dag01.png
      :scale: 40

|nopar| The feature names appear as labels on the directed arcs, and feature
values appear as labels on the nodes which are pointed to by the arcs.

Just as before, feature values can be complex:

.. _dag02:
.. ex::
   .. image:: ../images/dag02.png
      :scale: 40

|nopar| When we look at such graphs, it is natural to think in terms of
paths through the graph. A `feature path`:dt: is a sequence of arcs
that can be followed from the root node. We will represent paths in NLTK as
tuples. Thus, ``('address', 'street')`` is a feature path whose value
in dag02_
is the string 'rue Pascal'.

Now let's consider a situation where Lee has a spouse named 'Kim', and
Kim's address is the same as Lee's.
We might represent this as dag04_.

.. _dag04:
.. ex::
   .. image:: ../images/dag04.png
      :scale: 40

|nopar| However, rather than repeating the address
information in the feature structure, we can 'share' the same
sub-graph between different arcs:

.. _dag03:
.. ex::
   .. image:: ../images/dag03.png
      :scale: 40


|nopar| In other words, the value of the path ``('address')`` in dag03_ is
identical to the value of the path ``('spouse', 'address')``.  DAGs
such as dag03_ are said to involve `structure sharing`:dt: or
`reentrancy`:dt:. When two paths have the same value, they are said to
be `equivalent`:dt:.

There are a number of notations for representing reentrancy in
matrix-style representations of feature structures. In NLTK, we adopt
the following convention: the first occurrence of a shared feature structure 
is prefixed with an integer in parentheses, such as ``(1)``, and any
subsequent reference to that structure uses the notation
``->(1)``, as shown below.

    >>> fs=FeatureStructure.parse("""[name='Lee', address=(1)[number=74, street='rue Pascal'], 
    ...                               spouse=[name='Kim', address->(1)]]""")
    >>> print fs
    [ address = (1) [ number = 74           ] ]
    [               [ street = 'rue Pascal' ] ]
    [                                         ]
    [ name    = 'Lee'                         ]
    [                                         ]
    [ spouse  = [ address -> (1)  ]           ]
    [           [ name    = 'Kim' ]           ]


|nopar| This is similar to more conventional displays of AVMs, as shown in
reentrant01_.

.. _reentrant01:
.. ex::
      .. avm:: 

	 [ address = (1) [ number = 74           ] ]
	 [               [ street = 'rue Pascal' ] ]
	 [                                         ]
	 [ name    = 'Lee'                         ]
	 [                                         ]
	 [ spouse  = [ address -> (1)  ]           ]
	 [           [ name    = 'Kim' ]           ]

|nopar| The bracketed integer is sometimes called a `tag`:dt: or a
`coindex`:dt:. The choice of integer is not significant.
There can be any number of tags within a single feature structure.

    >>> fs1 = FeatureStructure.parse("[A='a', B=(1)[C='c'], D->(1), E->(1)]")

.. _reentrant02:
.. ex::
      .. avm::
 
	 [ A = 'a'             ]
	 [                     ]
	 [ B = (1) [ C = 'c' ] ]
	 [                     ]
	 [ D -> (1)            ]
	 [ E -> (1)            ]


|nopar| We can also share empty structures:

    >>> fs2 = FeatureStructure.parse("[A=(1)[], B=(2)[], C->(1), D->(2)]")

.. _reentrant03:
.. ex::
      .. avm:: 

	 [ A = (1) [ ] ]
	 [ B = (2) [ ] ]
	 [ C -> (1)    ]
	 [ D -> (2)    ]



Subsumption and Unification
---------------------------

It is standard to think of feature structures as providing `partial
information`:dt: about some object, in the sense that we can order
feature structures according to how general they are. For example,
fs01_ is more general (less specific) than fs02_, which in turn is more general than fs03_.

.. ex::
   .. _fs01:
   .. ex::
      .. avm::

         [number = 74]

   .. _fs02:
   .. ex::
      .. avm::

         [number = 74          ]
         [street = 'rue Pascal']

   .. _fs03:
   .. ex::
      .. avm::

         [number = 74          ]
         [street = 'rue Pascal']
         [city = 'Paris'       ]

|nopar| This ordering is called `subsumption`:dt:; a more general feature
structure `subsumes`:dt: a less general one. If `FS`:math:\
:subscript:`0` subsumes `FS`:math:\ :subscript:`1` (formally, we write
`FS`:math:\ :subscript:`0` |SquareSubsetEqual| `FS`:math:\
:subscript:`1`), then `FS`:math:\ :subscript:`1` must have all the
paths and path equivalences of `FS`:math:\ :subscript:`0`, and may
have additional paths and equivalences as well. Thus, dag04_ subsumes
dag03_, since the latter has additional path equivalences.. It should
be obvious that subsumption only provides a partial ordering on
feature structures, since some feature structures are
incommensurable. For example, fs04_ neither subsumes nor is subsumed
by fs01_.


.. _fs04:
.. ex::
   .. avm::

         [telno = 01 27 86 42 96]

So we have seen that some feature structures are more specific than
others. How do we go about specialising a given feature structure?
For example, we might decide that addresses should
consist of not just a street number and a street name, but also a
city. That is, we might want to *merge*  graph dag042_ with dag041_ to
yield dag043_.

.. ex::
     .. _dag041:
     .. ex::
	.. image:: ../images/dag04-1.png
	   :scale: 40

     .. _dag042:
     .. ex::
	.. image:: ../images/dag04-2.png
	   :scale: 40

     .. _dag043:
     .. ex::
	.. image:: ../images/dag04-3.png
	   :scale: 40

|nopar| Merging information from two feature structures is called
`unification`:dt: and in NLTK is supported by the ``unify()`` method
defined in the ``FeatureStructure`` class.

    >>> fs1 = FeatureStructure(number=74, street='rue Pascal')
    >>> fs2 = FeatureStructure(city='Paris')
    >>> print fs1.unify(fs2)
    [ city   = 'Paris'      ]
    [ number = 74           ]
    [ street = 'rue Pascal' ]
    >>> 

Unification is formally defined as a binary operation: `FS`:math:\
:subscript:`0` |SquareIntersection| `FS`:math:\
:subscript:`1`. Unification is symmetric, so 

.. ex::
    `FS`:math:\ :subscript:`0` |SquareIntersection| `FS`:math:\
    :subscript:`1` = `FS`:math:\ :subscript:`1` |SquareIntersection|
    `FS`:math:\ :subscript:`0`.

|nopar| The same is true in NLTK:

    >>> print fs2.unify(fs1)
    [ city   = 'Paris'      ]
    [ number = 74           ]
    [ street = 'rue Pascal' ]
    >>>

.. but >>> fs1.unify(fs2) is fs2.unify(fs1)
       False
   only works with repr()

If we unify two feature structures which stand in the subsumption
relationship, then the result of unification is the most specific of
the two:

.. ex::
    If `FS`:math:\ :subscript:`0` |SquareSubsetEqual| `FS`:math:\
    :subscript:`1`,  then `FS`:math:\ :subscript:`0`
    |SquareIntersection| `FS`:math:\ :subscript:`1` = `FS`:math:\
    :subscript:`1` 

|nopar| For example, the result of unifying fs02_ with fs03_ is fs03_.

Unification between `FS`:math:\ :subscript:`0` and `FS`:math:\
:subscript:`1` will fail if the two feature structures share a path |pi|,
but the value of |pi| in `FS`:math:\ :subscript:`0` is a distinct
atom from the value of |pi| in `FS`:math:\ :subscript:`1`. In NLTK,
this is implemented by setting the result of unification to be
``None``.

    >>> fs0 = FeatureStructure(A='a')
    >>> fs1 = FeatureStructure(A='b')
    >>> fs2 = fs0.unify(fs1)
    >>> print fs2
    None

Now, if we look at how unification interacts with structure-sharing,
things become really interesting. First, let's define the |NLTK| version
of dag04_.

    >>> fs0=FeatureStructure.parse("""[name='Lee', 
    ...                                address=[number=74, 
    ...                                         street='rue Pascal'], 
    ...                                spouse= [name='Kim',
    ...                                         address=[number=74, 
    ...                                                  street='rue Pascal']]]""")

.. _unification01:
.. ex::
      .. avm:: 

	 [ address = [ number = 74           ]               ]
	 [           [ street = 'rue Pascal' ]               ]
	 [                                                   ]
	 [ name    = 'Lee'                                   ]
	 [                                                   ]
	 [           [ address = [ number = 74           ] ] ]
	 [ spouse  = [           [ street = 'rue Pascal' ] ] ]
	 [           [                                     ] ]
	 [           [ name    = 'Kim'                     ] ]

|nopar| What happens when we augment Kim's address with a specification
for `city`:feat:? (Notice that ``fs1`` includes the whole path from the root of
the feature structure down to `city`:feat:.)

    >>> fs1=FeatureStructure.parse("[spouse = [address = [city = 'Paris']]]")

unification02_ shows the result of unifying ``fs0`` with ``fs1``:

.. _unification02:
.. ex::
      .. avm:: 

	 [ address = [ number = 74           ]               ]
	 [           [ street = 'rue Pascal' ]               ]
	 [                                                   ]
	 [ name    = 'Lee'                                   ]
	 [                                                   ]
	 [           [           [ city   = 'Paris'      ] ] ]
	 [           [ address = [ number = 74           ] ] ]
	 [ spouse  = [           [ street = 'rue Pascal' ] ] ]
	 [           [                                     ] ]
	 [           [ name    = 'Kim'                     ] ]

|nopar| By contrast, the result is very different if ``fs1`` is unified with
the structure-sharing version ``fs2`` (also shown as dag03_):

    >>> fs2=FeatureStructure.parse("""[name='Lee', address=(1)[number=74, street='rue Pascal'],
    ...                                spouse=[name='Kim', address->(1)]]""")

.. _unification03:
.. ex::
      .. avm:: 

	 [               [ city   = 'Paris'      ] ]
	 [ address = (1) [ number = 74           ] ]
	 [               [ street = 'rue Pascal' ] ]
	 [                                         ]
	 [ name    = 'Lee'                         ]
	 [                                         ]
	 [ spouse  = [ address -> (1)  ]           ]
	 [           [ name    = 'Kim' ]           ]

|nopar| Rather than just updating what was in effect Kim's 'copy' of Lee's address,
we have now updated `both`:em: their addresses at the same time. More
generally, if a unification involves specialising the value of some
path |pi|, then that unification simultaneously specialises the value
of `any path that is equivalent to`:em: |pi|.

As we have already seen, structure sharing can also be stated in |NLTK|
using variables such as ``?x``. 

    >>> fs1=FeatureStructure.parse("[address1=[number=74, street='rue Pascal']]")
    >>> fs2=FeatureStructure.parse("[address1=?x, address2=?x]")
    >>> print fs2
    [ address1 = ?x ]
    [ address2 = ?x ]
    >>> print fs2.unify(fs1)
    [ address1 = (1) [ number = 74           ] ]
    [                [ street = 'rue Pascal' ] ]
    [                                          ]
    [ address2 -> (1)                          ]



Exercises
---------

#. |soso| List two feature structures which subsume [A=?x, B=?x].

#. |soso| Ignoring structure sharing, give an informal algorithm for unifying
   two feature structures. 


---------------------------------
Extending a Feature-Based Grammar
---------------------------------


Subcategorization
-----------------

In Chapter chap-parse_, we proposed to augment our
category labels in order to represent different subcategories of
verb. More specifically,
we introduced labels such as `iv`:gc: and `tv`:gc: for intransitive
and transitive verbs respectively.  This allowed us to write rules
like the following:

.. _subcatcfg0:
.. ex::
   .. parsed-literal::

      `vp`:gc:  |rarr| `iv`:gc:  
      `vp`:gc:  |rarr| `tv np`:gc: 

|nopar| Although it is tempting to think of `iv`:gc: and `tv`:gc: as two
kinds of `v`:gc:, this is unjustified: from a formal point of view,
`iv`:gc: has no closer relationship with `tv`:gc: than it does,
say, with `np`:gc:. As it stands, `iv`:gc: and `tv`:gc: are
unanalyzable nonterminal symbols from a CFG. One unwelcome consequence
is that we do not seem able to say anything about the class of verbs
in general. For example, we cannot say something like "All lexical
items of category `v`:gc: can be marked for tense", since `bark`:lx:,
say, is an item of category `iv`:gc:, not `v`:gc:.

Using features gives us some useful room for manoeuvre but there is no
obvious consensus on how to model subcategorization information. One
approach which has the merit of simplicity is due to Generalized
Phrase Structure Grammar (GPSG). GPSG stipulates that lexical
categories may bear a `subcat`:feat: whose values are integers. This
is illustrated in a modified portion of feat0cfg_, shown in subcatgpsg_.

.. _subcatgpsg:
.. ex::
   ::

     VP[tense=?t, num=?n] -> V[subcat=0, tense=?t, num=?n]
     VP[tense=?t, num=?n] -> V[subcat=1, tense=?t, num=?n] NP
     VP[tense=?t, num=?n] -> V[subcat=2, tense=?t, num=?n] Sbar

     V[subcat=0, tense=pres, num=sg] -> 'disappears' | 'walks'
     V[subcat=1, tense=pres, num=sg] -> 'sees' | 'likes'
     V[subcat=2, tense=pres, num=sg] -> 'says' | 'claims'

     V[subcat=0, tense=pres, num=pl] -> 'disappear' | 'walk'
     V[subcat=1, tense=pres, num=pl] -> 'see' | 'like'
     V[subcat=2, tense=pres, num=pl] -> 'say' | 'claim'

     V[subcat=0, tense=past, num=?n] -> 'disappeared' | 'walked'
     V[subcat=1, tense=past, num=?n] -> 'saw' | 'liked'
     V[subcat=2, tense=past, num=?n] -> 'said' | 'claimed'

|nopar| When we see a lexical category like `v`:gc:\ [`subcat`:feat: 
`1`:fval:\ ], we can interpret the `subcat`:feat: specification as a
pointer to the rule in which `v`:gc:\ [`subcat`:feat:  `1`:fval:\ ]
is introduced as the head daughter in a `vp`:gc: expansion rule. By
convention, there is a one-to-one correspondence between
`subcat`:feat: values and rules which introduce lexical heads. It's
worth noting that the choice of integer which acts as a value for
`subcat`:feat: is completely arbitrary |mdash| we could equally well
have chosen 3999, 113 and 57 as our two values in subcatgpsg_.  On this
approach, `subcat`:feat: can *only* appear on lexical categories; it
makes no sense, for example, to specify a `subcat`:feat: value on
`vp`:gc:.

In our third class of verbs above, we have specified a category
`s-bar`:gc:. This is a label for subordinate clauses such as the
complement of `claim`:lx: in the example `You claim that you like
children`:lx:. We require two further rules to analyse such sentences:

.. _sbar:
.. ex::
   ::

     S-BAR -> Comp S
     Comp -> 'that'

|nopar| The resulting structure is the following.

.. _sbartree:
.. ex::
      .. tree::  (S (NP you)(VP (V[-AUX,\ SUBCAT\ 2] claim)(S-BAR (Comp that) (S (NP you)(VP (V[-AUX,\ SUBCAT\ 1] like)(NP children))))))

An alternative treatment of subcategorization, due originally to a framework
known as categorial grammar, is represented in feature-based frameworks such as PATR
and Head-driven Phrase Structure Grammar. Rather than using
`subcat`:feat: values as a way of indexing rules, the `subcat`:feat:
value directly encodes the valency of a head (the list of
arguments that it can combine with). For example, a verb like
`put`:lx: which takes  `np`:gc: and `pp`:gc: complements (`put the
book on the table`:lx:) 
might be represented as subcathpsg0_:

.. _subcathpsg0:
.. ex::  `v`:gc:\ [`subcat`:feat: |langle|\ `np`:gc:, `np`:gc:, `pp`:gc:\ |rangle| ] 

|nopar| This says that the verb can combine with three  arguments. The
leftmost element in the list is the subject `np`:gc:, while everything
else |mdash| an `np`:gc: followed by a `pp`:gc:  in this case |mdash| comprises the
subcategorized-for complements. When a verb like `put`:lx: is combined
with appropriate complements, the requirements which are specified in
the  `subcat`:feat: are discharged, and only a subject `np`:gc: is
needed. This category, which corresponds to what is traditionally
thought of as `vp`:gc:, might be represented as follows.

.. _subcathpsg1:
.. ex::  `v`:gc:\ [`subcat`:feat: |langle|\ `np`:gc:\ |rangle| ] 

Finally, a sentence is a kind of verbal category which has *no*
requirements for further arguments, and hence has a `subcat`:feat:
whose value is the empty list. The tree subcathpsg2_ shows how these
category assigments combine in a parse of `Kim put the book on the table`:lx:.

.. _subcathpsg2:
.. ex::
      .. tree:: (V[SUBCAT\ \<\>] (NP Kim)(V[SUBCAT\ \<NP\>](V[SUBCAT\ \<NP,\ NP,\ VP\>] put)<NP the\ book><PP on\ the\ table>))


Auxiliary verbs and Inversion
-----------------------------

Inverted clauses |mdash| where the order of subject and verb is
switched |mdash| occur in English interrogatives and also after
'negative' adverbs:

.. _inv1:
.. ex::
   .. _inv1a:
   .. ex::

      Do you like children?

   .. _inv1b:
   .. ex::

      Can Jody walk?

.. _inv2:
.. ex::
   .. _inv2a:
   .. ex::

      Rarely do you see Kim.

   .. _inv2b:
   .. ex::

      Never have I seen this dog.

|nopar| However, we cannot place just any verb in pre-subject position:

.. _inv3:
.. ex::
   .. _inv3a:
   .. ex::

      \*Like you children?

   .. _inv3b:
   .. ex::

      \*Walks Jody?

.. _inv4:
.. ex::
   .. _inv4a:
   .. ex::

      \*Rarely see you Kim.

   .. _inv4b:
   .. ex::

      \*Never saw I this dog.

Verbs which can be positioned inititally in inverted clauses belong to
the class known as `auxiliaries`:dt:, and as well as  `do`:lx:,
`can`:lx: and `have`:lx:  include `be`:lx:, `will`:lx:  and
`shall`:lx:. One way of capturing such structures is with the
following rule:

.. _sinv:
.. ex::
   ::

     S[+inv] -> V[+AUX] NP VP

|nopar| That is, a clause marked as [`+inv`:feat:] consists of an auxiliary
verb followed by a `vp`:gc:. (In a more detailed grammar, we would
need to place some constraints on the form of the `vp`:gc:, depending
on the choice of auxiliary.) invtree_ illustrates the structure of an
inverted clause.

.. _invtree:
.. ex::
      .. tree:: (S[+INV](V[+AUX,\ SUBCAT\ 3] do)(NP you)(VP(V[-AUX,\ SUBCAT\ 1] like)(NP children)))



Unbounded Dependency Constructions
----------------------------------

Consider the following contrasts: 

.. _gap1:
.. ex::
   .. _gap1a:
   .. ex::

      You like Jody.

   .. _gap1b:
   .. ex::

      \*You like.

.. _gap2:
.. ex::
   .. _gap2a:
   .. ex::

      You put the card into the slot.

   .. _gap2b:
   .. ex::

      \*You put into the slot.

   .. _gap2c:
   .. ex::

      \*You put the card.

   .. _gap2d:
   .. ex::

      \*You put.

The verb `like`:lx: requires an `np`:gc: complement, while
`put`:lx: requires both a following `np`:gc: and `pp`:gc:. Examples
gap1_ and gap2_ show that these complements are *obligatory*:
omitting them leads to ungrammaticality. Yet there are contexts in
which obligatory complements can be omitted, as gap3_ and gap4_
illustrate.

.. _gap3:
.. ex::
   .. _gap3a:
   .. ex::

      Kim knows who you like.

   .. _gap3b:
   .. ex::

      This music, you really like.

.. _gap4:
.. ex::
   .. _gap4a:
   .. ex::

      Which card do you put into the slot?

   .. _gap4b:
   .. ex::

      Which slot do you put the card into?

|nopar| That is, an obligatory complement can be omitted if there is an
appropriate `filler`:dt: in the sentence, such as the question word
`who`:lx: in gap3a_, the preposed topic `this music`:lx: in gap3b_, or
the `wh`:lx: phrases `which card/slot`:lx: in gap4_. It is common to
say that sentences like gap3_ |ndash| gap4_ contain `gaps`:dt: where
the obligatory complements have been omitted, and these gaps are
sometimes made explicit using an underscore:

.. _gap5:
.. ex::
   .. _gap5a:
   .. ex::

      Which card do you put __ into the slot?

   .. _gap5b:
   .. ex::

      Which slot do you put the card into __?

|nopar| So, a gap can occur if it is `licensed`:dt: by a filler. Conversely,
fillers can only occur if there is an appropriate gap elsewhere  in
the sentence, as shown by the following examples.

.. _gap6:
.. ex::
   .. _gap6a:
   .. ex::

      \*Kim knows who you like Jody.

   .. _gap6b:
   .. ex::

      \*This music, you really like hip-hop.

.. _gap7:
.. ex::
   .. _gap7a:
   .. ex::

      \*Which card do you put this into the slot?

   .. _gap7b:
   .. ex::

      \*Which slot do you put the card into this one?

The mutual co-occurence between filler and gap leads to gap3_ |ndash|
gap4_ is sometimes termed a 'dependency'. One issue of considerable
importance in theoretical linguistics has been the nature of the
material that can intervene between a filler and the gap that it
licenses; in particular, can we simply list a finite set of strings
that separate the two? The answer is No: there is no upper bound on
the distance between filler and gap. This fact can be easily
illustrated with constructions involving sentential complements, as
shown in gap8_. 

.. _gap8:
.. ex::
   .. _gap8a:
   .. ex::

      Who do you like __?

   .. _gap8b:
   .. ex::

      Who do you claim that you like __?

   .. _gap8c:
   .. ex::

      Who do you claim that Jody says that you like __?

|nopar| Since we can have indefinitely deep recursion of sentential
complements, the gap can be embedded indefinitely far inside the whole
sentence. This constellation of properties leads to the notion of an
`unbounded dependency construction`:dt:; that is, a filler-gap
dependency where there is no upper bound on the distance between
filler and gap.

A variety of mechanisms have been suggested for handling unbounded
dependencies in formal grammars; we shall adopt an approach due to
Generalized Phrase Structure Grammar that involves something called
`slash categories`:dt:. A slash category is something of the form
`y/xp`:gc:; we interpret this as a phrase of category `y`:gc: which
is somewhere missing a sub-constituent of category `xp`:gc:. For example,
`s/np`:gc: is an `s`:gc: which is missing an `np`:gc:. The use of
slash categories is illustrated in gaptree1_. 
 
.. _gaptree1:
.. ex::
      .. tree:: (S(NP[+WH] who)(S[+INV]\/NP (V[+AUX,\ subcat\ 3] do)(NP[-WH] you)(VP/NP(V[-AUX,\ SUBCAT\ 1] like)(NP/NP e))))

|nopar| The top part of the tree introduces the filler `who`:lx: (treated as
an expression of category `np`:gc:\ [`+wh`:feat:]) together with a
corresponding gap-containing constituent `s/np`:gc:. The gap information is
then 'percolated' down the tree via the `vp/np`:gc: category, until it
reaches the category `np/np`:gc:. At this point, the dependency 
is discharged by realizing the gap information as the empty string `e`
immediately dominated by `np/np`:gc:.

Do we need to think of slash categories as a completely new kind of
object in our grammars?  Fortunately, no, we don't |mdash| in fact, we
can accommodate them within our existing feature-based framework. We
do this by treating slash as a feature, and the category to its right
as a value. In other words, our 'official' notation for `s/np`:gc:
will be `s`:gc:\ [`slash`:feat: = `NP`:fval:\ ]. Once we have taken this
step, it is straightforward to write a small grammar in NLTK for
analyzing unbounded dependency constructions. feat1cfg_ illustrates
the main principles of slash categories, and also includes rules for
inverted clauses. To simplify presentation, we have omitted any
specification of tense on the verbs.

.. _feat1cfg:
.. ex::
.. include:: ../../examples/parse/feat1.cfg
   :literal:

feat1cfg_ contains one gap-introduction rule, namely

.. ex::
   .. parsed-literal::

      `s[-inv]`:gc:  |rarr| `np`:gc: `s/np`:gc: 

In order to percolate the slash feature correctly, we need to add
slashes with variable values to both sides of the arrow in rules which
expand `s`:gc:, `vp`:gc: and `np`:gc:. For example,

.. ex::
   .. parsed-literal::

      `vp/?x`:gc:  |rarr| `v`:gc: `s-bar/?x`:gc: 

|nopar| says that a slash value can be specified on the `vp`:gc: mother of a
constituent if the same value is also specified on the `s-bar`:gc:
daughter. Finally, empty_ allows the slash information on `np`:gc: to
be discharged as the empty string.

..  _empty:
.. ex::
   .. parsed-literal::

      `np/np`:gc:  |rarr|

Using feat1cfg_, we can parse the string `who do you claim that you
like`:lx:  into the tree shown in gapparse_.

.. _gapparse:
.. ex::
    .. tree:: (S[-INV](NP[+WH] who)(S[SLASH\ NP,+INV](V[+AUX,SUBCAT\ 3] do)(NP[-WH] you)(VP[SLASH\ NP](V[-AUX,SUBCAT\ 2] claim)(S-BAR[SLASH\ NP](Comp that)(S[SLASH\ NP,-INV](NP[-WH] you)(VP[SLASH\ NP](V[-AUX,SUBCAT\ 1] like)(NP[SLASH\ NP] )))))))


-------
Summary
-------

To be written


-----------------
 Further Reading
-----------------

The earliest use of features in theoretical linguistics was designed
to capture phonological properties of phonemes. For example, a sound
like /b/ might be decomposed into the structure [`+labial`:feat:, `+voice`:feat:]. 
Within syntax, a radical expansion of the use of features was
pioneered by Generalized Phrase Structure Grammar (GPSG; [Gazdar1985GPS]_).
Within computational linguistics, [Kay1984UG]_ independently proposed that
functional aspects of language could be captured by unification
of attribute-value, and a similar approach was elaborated by
[Shieber1983FIP]_ within the PATR-II formalism. Early work in
Lexical-functional grammar (LFG; [Kaplan1982LFG]_) introduced the notion of
an `f-structure`:dt: which was primarily intended to represent the
grammatical relations and predicate-argument structure associated with
a constituent structure parse.
[Shieber1986IUB]_ provides an excellent introduction to this phase of
research into feature-based grammar.

One conceptual difficulty with algebraic approaches to feature
structures arose when researchers attempted to model negation. An
alternative perspective, pioneered by [Kasper1986LSF]_ and
[Johnson1988AVL]_, argues that grammars involve `descriptions`:em: of
feature structures rather than the structures themselves. These
descriptions are combined using logical operations such as
conjunction, and negation is just the usual logical operation over
feature descriptions. This description-oriented perspective was
integral to LFG from the outset (cf. [Kaplan1989FAL]_, and was also adopted by latter
versions of Head-Driven Phrase Structure Grammar (HPSG; [Sag1999ST]_).

Feature structures, as presented in this chapter, are unable to
capture important constraints on linguistic information. For example,
there is no way of saying that the only
permissible values for `num`:feat: are `sg`:fval: and `pl`:fval:, and
that [`num`:feat: `np`:gc:] is anomalous. Similarly, we cannot say
that the complex value of `agr`:feat: `must`:em: contain specifications for
the features `per`:feat: `num`:feat: and `gend`:feat:, but
`cannot`:em: contain a specification such as [`subcat`:feat: 3].
Typed feature structures were developed to remedy this deficiency. See
[Emele1990TUG]_ for an early review of these developments.

.. lfg
.. rounds
.. typed feature structures


.. include:: footer.txt
