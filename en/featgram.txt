.. -*- mode: rst -*-
.. include:: ../definitions.txt

========================
9. Feature Based Grammar
========================

--------------
 Introduction
--------------


-----------------
Agreement in CFGs
-----------------

The Problem
-----------

Consider the following contrasts:

.. _thisdog:
.. ex::
   .. ex::
      this dog
   .. ex::
      \*these dog

.. _thesedogs:
.. ex::
   .. ex::
      these dogs
   .. ex::
      \*this dog

In English, nouns are usually morphologically marked as being singular
or plural. The form of the demonstrative also varies in a
similar way; there is a singular form `this`:lx: and a plural form `these`:lx:.
Examples thisdog_ and thesedogs_ show that there are constraints on
the realization of demonstratives and nouns within a noun phrase:
either both are singular or both are plural. A similar kind
of constraint is observed with subjects and predicates:

.. _subjpredsg:
.. ex::
   .. ex::
      the dog runs
   .. ex::
      \*the dog run

.. _subjpredpl:
.. ex::
   .. ex::
      the dogs run
   .. ex::
      \*the dogs runs


Here again, we can see that morphological properties of the verb co-vary
with morphological properties of the subject noun phrase; this co-variance is
usually termed `agreement`:dt: The element which determines the
agreement, here the subject noun phrase, is called the agreement
`controller`:dt:, while the element whose form is determined by
agreement, here the verb, is called the `target`:dt:.
If we look further at verb agreement in English, we will see that
present tense verbs typically have two inflected forms: one for third person
singular, and another for every other combination of person and
number:

.. ex::
 +--------+-------------+--------+
 |        |singular     |plural  |
 +--------+-------------+--------+
 |1st per |I run        |we run  |
 +--------+-------------+--------+
 |2nd per |you run      |you run |
 +--------+-------------+--------+
 |3rd per |he/she/it    |they run|
 |        |runs         |        |
 +--------+-------------+--------+

We can make the role of morphological properties a bit more explicit as
illustrated in runs_ and run_. These representations indicate that the verb agrees with its
subject in person and number.

.. _runs:
.. ex::

 +----------+----------+----------+
 |the       |dog       |run-s     |
 +----------+----------+----------+
 |          |dog.3.SG  |run-3.SG  |
 +----------+----------+----------+


.. _run:
.. ex::

 +------------+------------+------------+
 |the         |dog-s       |run         |
 +------------+------------+------------+
 |            |dog-3.PL    |run.3.PL    |
 +------------+------------+------------+


Despite the undoubted interest of agreement as a topic in its own
right, we have introduced it here for another reason: we want to look
at what happens when  we try encode agreement constraints in a context-free grammar. 
Suppose we take as our starting point the very simple CFG in agcfg0_.

.. _agcfg0:
.. ex::
   .. parsed-literal::

     `S`:gc: |rarr| `NP VP`:gc:
     `NP`:gc: |rarr| `Det N`:gc: 
     `VP`:gc: |rarr| `V`:gc: 

     `Det`:gc: |rarr| 'this'
     `N`:gc: |rarr| 'dog'
     `V`:gc: |rarr| 'runs'

agcfg0_ allows us to generate the sentence `this dog runs`:lx:;
however, what we really want to do is also generate `these dogs
run`:lx: while blocking unwanted strings such as `*this dogs run`:lx:
and `*these dog runs`:lx:. The most straightforward approach is to
add new non-terminals and productions to the grammar which reflect our
number distinctions and agreement constraints (we ignore person for the time being):

.. _agcfg1:
.. ex::
   .. parsed-literal::

     `S_SG`:gc: |rarr| `NP_SG VP_SG`:gc:
     `S_PL`:gc: |rarr| `NP_PL VP_PL`:gc:
     `NP_SG`:gc: |rarr| `Det_SG N_SG`:gc: 
     `NP_PL`:gc: |rarr| `Det_PL N_PL`:gc: 
     `VP_SG`:gc: |rarr| `V_SG`:gc: 
     `VP_PL`:gc: |rarr| `V_PL`:gc: 

     `Det_SG`:gc: |rarr| 'this'
     `Det_PL`:gc: |rarr| 'these'
     `N_SG`:gc: |rarr| 'dog'
     `N_PL`:gc: |rarr| 'dogs'
     `V_SG`:gc: |rarr| 'runs'
     `V_PL`:gc: |rarr| 'run'

It should be clear that this grammar will do the required
task, but only at the cost of duplicating our previous set of
rules. Rule multiplication is of course more severe if we add in
person agreement constraints.

Exercises
---------

#. Augment agcfg1_ so that it will generate strings like `I am
   happy`:lx: and `she is happy`:lx: but not `*you is happy`:lx: or
   `*they am happy`:lx:.

#. Augment agcfg1_ so that it will correctly describe the following
   Spanish noun phrases:

   .. ex::
      .. ex::

	 +---------------------+--------------------+--------------------+
	 |un                   |cuadro              |hermos-o            |
	 +---------------------+--------------------+--------------------+
	 |INDEF.SG.MASC        |picture             |beautiful-SG.MASC   |
	 +---------------------+--------------------+--------------------+
	 |'a beautiful picture'                     |                    |
	 +------------------------------------------+--------------------+

      .. ex::

	 +---------------------+--------------------+--------------------+
	 |un-os                |cuadro-s            |hermos-os           |
	 +---------------------+--------------------+--------------------+
	 |INDEF-PL.MASC        |picture-PL          |beautiful-PL.MASC   |
	 +---------------------+--------------------+--------------------+
	 |'beautiful pictures'                      |                    |
	 +------------------------------------------+--------------------+

      .. ex::

	 +---------------------+--------------------+--------------------+
	 |un-a                 |cortina             |hermos-a            |
	 +---------------------+--------------------+--------------------+
	 |INDEF-SG.FEM         |curtain             |beautiful-SG.FEM    |
	 +---------------------+--------------------+--------------------+
	 |'a beautiful curtain'                     |                    |
	 +------------------------------------------+--------------------+

      .. ex::

	 +---------------------+--------------------+--------------------+
	 |un-as                |cortina-s           |hermos-as           |
	 +---------------------+--------------------+--------------------+
	 |INDEF-PL.FEM         |curtain-PL          |beautiful-SG.FEM    |
	 +---------------------+--------------------+--------------------+
	 |'beautiful curtains'                      |                    |
	 +------------------------------------------+--------------------+

.. In grammatical terms, we might say that both nouns and
   demonstratives have a property of `number`:gc: which can take the values singular or
   plural.


Using Attributes and Constraints
--------------------------------

We spoke informally of linguistic categories having *properties*; for
example, that a verb has the property of being plural. Let's try to
make this more explicit:

.. _num0:
.. ex::
   .. parsed-literal::

     `N`:gc:\ [`num`:feat: = `pl`:fval:\ ]

In num0_, we have introduced some  new notation which says that the category `N`:gc: has a 
`feature`:dt: called `num`:feat: (short for 'number') and that the
value of this feature is `pl`:fval: (short for 'plural'). We can add
similar annotations to other categories, and use them in lexical
entries:

.. _agcfg2:
.. ex::
   .. parsed-literal::

     `Det`:gc:\ [`num`:feat: = `sg`:fval:\ ] |rarr| 'this'
     `Det`:gc:\ [`num`:feat: = `pl`:fval:\ ]  |rarr| 'these'
     `N`:gc:\ [`num`:feat: = `sg`:fval:\ ] |rarr| 'dog'
     `N`:gc:\ [`num`:feat: = `pl`:fval:\ ] |rarr| 'dogs'
     `V`:gc:\ [`num`:feat: = `sg`:fval:\ ] |rarr| 'runs'
     `V`:gc:\ [`num`:feat: = `pl`:fval:\ ] |rarr| 'run'

Does this help at all? So far, it looks just like a slightly more
verbose alternative to what was specified in agcfg1_. Things become
more interesting when we allow *variables* over feature values, and use
these to state constraints. This is illustrated in agcfg3_.

.. _agcfg3:
.. ex::
   .. _srule:
   .. ex::
      .. parsed-literal::

        `S`:gc: |rarr| `NP`:gc:\ [`num`:feat: = `?n`:math:\ ] `VP`:gc:\ [`num`:feat: = `?n`:math:\ ]

   .. _nprule:
   .. ex::
      .. parsed-literal::

       `NP`:gc:\ [`num`:feat: = `?n`:math:\ ] |rarr| `Det`:gc:\ [`num`:feat: = `?n`:math:\ ] `N`:gc:\ [`num`:feat: = `?n`:math:\ ]

   .. _vprule:
   .. ex::
      .. parsed-literal::

       `VP`:gc:\ [`num`:feat: = `?n`:math:\ ] |rarr| `V`:gc:\ [`num`:feat: = `?n`:math:\ ]

We are using '`?n`:math:' as a variable over values of `num`:feat:; it can
be instantiated either to `sg`:fval: or `pl`:fval:. Its scope is
limited to individual rules. That is, within srule_, for example,
`?n`:math: must be instantiated to the same constant value; we can
read the rule as saying that whatever value `NP`:gc: takes for the feature
`num`:feat:, `VP`:gc: must take the same value. 

In order to understand how these feature constraints work, it's
helpful to think about how one would go about building a tree. Lexical
rules will admit the following local trees (trees of
depth one):

.. ex::
   .. _this:
   .. ex:: 
      .. tree:: (Det[num=sg] this) 
   .. _these:
   .. ex:: 
      .. tree:: (Det[num=pl] these) 

.. ex::
   .. _dog:
   .. ex:: 
      .. tree:: (N[num=sg] dog) 
   .. _dogs:
   .. ex:: 
      .. tree:: (N[num=pl] dogs) 

Now nprule_ says that whatever the `num`:feat: values of `N`:gc: and
`Det`:gc: are, they have to be the same. Consequently,  nprule_ will
permit this_ and dog_ to be combined into an `NP`:gc:  as shown in
good1_ and it will also allow these_ and dogs_ to be combined, as in
good2_. By contrast,  bad1_ and bad2_ are prohibited because the roots
of their
constituent local trees differ in their values for the `num`:feat: feature.

.. ex::
   .. _good1:
   .. ex::
      .. tree:: (NP[num=pl] (Det[num=sg] this)(N[num=sg] dog))

   .. _good2:
   .. ex::
      .. tree:: (NP[num=pl] (Det[num=pl] these)(N[num=pl] dogs))

.. ex::
   .. _bad1:
   .. ex::
      .. tree:: (NP[num=...] (Det[num=sg] this)(N[num=pl] dogs))

   .. _bad2:
   .. ex::
      .. tree:: (NP[num=...] (Det[num=pl] these)(N[num=sg] dog))

Rule vprule_ can be thought of as saying that `num`:feat: value of the
head verb has to be the same as the `num`:feat: value of the `VP`:gc:
mother. Combined with srule_, we derive the consequence that if the
`num`:feat: value of the subject head noun is `pl`:fval:, then so is
the `num`:feat: value of the `VP`:gc:\ 's head verb.

.. ex::
   .. tree:: (S (NP[num=pl] (Det[num=pl] these)(N[num=pl] dogs))(VP[num=pl] (V[num=pl] run)))

Grammar feat0cfg_ illustrates most of the ideas we have introduced so
far in this chapter, plus a couple of new ones.

.. _feat0cfg:
.. ex::
   ::

     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     % Grammar Rules
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Start -> S

     S -> NP[num=?n] VP[num=?n]

     % NP expansion rules
     NP[num=?n] -> N[num=?n] 
     NP[num=?n] -> PropN[num=?n] 
     NP[num=?n] -> Det[num=?n] N[num=?n]
     NP[num=pl] -> N[num=pl] 

     % VP expansion rules
     VP[tense=?t, num=?n] -> IV[tense=?t, num=?n]
     VP[tense=?t, num=?n] -> TV[tense=?t, num=?n] NP

     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     % Lexical Rules
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Det[num=sg] -> 'this' | 'every'
     Det[num=pl] -> 'these' | 'all'
     Det[num=?n] -> 'the' | 'some'

     PropN[num=sg]-> 'Kim' | 'Jody'

     N[num=sg] -> 'dog' | 'girl' | 'car' | 'child'
     N[num=pl] -> 'dogs' | 'girls' | 'cars' | 'children' 

     IV[tense=pres,  num=sg] -> 'disappears' | 'walks'
     TV[tense=pres, num=sg] -> 'sees' | 'likes'

     IV[tense=pres,  num=pl] -> 'disappear' | 'walk'
     TV[tense=pres, num=pl] -> 'see' | 'like'

     IV[tense=past, num=?n] -> 'disappeared' | 'walked'
     TV[tense=past, num=?n] -> 'saw' | 'liked'

First, you will notice that a feature annotation on a syntactic
category can contain more than one specification; for example,
`V`:gc:\ [`tense`:feat: = `pres`:fval:, `num`:feat: = `pl`:fval:\
]. In general, there is no upper bound on the number of features we
specify as part of our syntactic categories.

Second, we have used feature variables in lexical entries as well
as grammatical rules. For example, `the`:lx: has been assigned the
category `Det`:gc:\ [`num`:feat: = `?n`:math:]. Why is this?  Well,
you know that the definite article `the`:lx: can combine with both
singular and plural nouns. One way of describing this would be to add
two lexical entries to the grammar, one each for the singular and
plural versions of `the`:lx:. However, a more elegant solution is to
leave the `num`:feat: value `underspecified`:dt: and letting it agree
in number with whatever noun it combines with.

A final point to note about feat0cfg_ is that we have used ``%`` as an
escape symbol in order to add comments to the grammar.

In general, when we are trying to develop even a very small grammar,
it is convenient to put the rules in a file where they can be edited,
tested and revised. Assuming we have saved feat0cfg_ as a file named
'feat0.cfg', the function ``GrammarFile.read_file()`` allows us to
read the grammar into NLTK, ready for use in parsing.

.. doctest-ignore::
    >>> from nltk_lite.contrib.grammarfile import GrammarFile
    >>> from pprint import pprint
    >>> from nltk_lite import tokenize
    >>> g = GrammarFile.read_file('feat0.cfg')
    >>>

We can inspect the rules and the lexicon.

.. doctest-ignore::
    >>> print g.earley_grammar()
    Grammar with 7 productions (start state = Start[])
	Start -> S
	S -> NP[num=?n] VP[num=?n]
	NP[num=?n] -> N[num=?n]
	NP[num=?n] -> PropN[num=?n]
	NP[num=?n] -> Det[num=?n] N[num=?n]
        NP[num=pl] -> N[num=pl]
	VP[num=?n, tense=?t] -> IV[num=?n, tense=?t]
	VP[num=?n, tense=?t] -> TV[num=?n, tense=?t] NP
    >>> pprint(g.earley_lexicon())
     {'Jody': [PropN[num=sg]],
      'Kim': [PropN[num=sg]],
      'all': [Det[num=pl]],
      'car': [N[num=sg]],
      'cars': [N[num=pl]],
      'child': [N[num=sg]],
      'children': [N[num=pl]],
      'disappear': [IV[num=pl, tense=pres]],
      'disappeared': [IV[num=?n, tense=past]],
      'disappears': [IV[num=sg, tense=pres]],
      'dog': [N[num=sg]],
      'dogs': [N[num=pl]],
      'every': [Det[num=sg]],
      'girl': [N[num=sg]],
      'girls': [N[num=pl]],
      'like': [TV[num=pl, tense=pres]],
      'liked': [TV[num=?n, tense=past]],
      'likes': [TV[num=sg, tense=pres]],
      'saw': [TV[num=?n, tense=past]],
      'see': [TV[num=pl, tense=pres]],
      'sees': [TV[num=sg, tense=pres]],
      'some': [Det[num=?n]],
      'the': [Det[num=?n]],
      'these': [Det[num=pl]],
      'this': [Det[num=sg]],
      'walk': [IV[num=pl, tense=pres]],
      'walked': [IV[num=?n, tense=past]],
      'walks': [IV[num=sg, tense=pres]]}
     >>> 

Now we can tokenize a sentence and use the ``parse_n()`` function to
invoke the Earley chart parser.

.. doctest-ignore::
     >>> from nltk_lite import tokenize
     >>> sent = 'Kim likes children'
     >>> tokens = list(tokenize.whitespace(sent))
     >>> tokens 
     ['Kim', 'likes', 'children']
     >>> cp = g.earley_parser()
     >>> trees = cp.parse_n(tokens)
	       |.K.l.c.|
     Predictor |> . . .| Start -> * S 
     Predictor |> . . .| S -> * NP[num=?n] VP[num=?n] 
     Predictor |> . . .| NP[num=?n] -> * N[num=?n] 
     Predictor |> . . .| NP[num=?n] -> * PropN[num=?n] 
     Predictor |> . . .| NP[num=?n] -> * Det[num=?n] N[num=?n] 
     Predictor |> . . .| NP[num=pl] -> * N[num=pl] 
     Scanner   |[-] . .| PropN[num=sg] -> 'Kim' * 
     Completer |[-] . .| NP[num=sg] -> PropN[num=sg] * 
     Completer |[-> . .| S -> NP[num=sg] * VP[num=sg] 
     Predictor |. > . .| VP[num=?n, tense=?t] -> * IV[num=?n, tense=?t] 
     Predictor |. > . .| VP[num=?n, tense=?t] -> * TV[num=?n, tense=?t] NP 
     Scanner   |. [-] .| TV[num=sg, tense=pres] -> 'likes' * 
     Completer |. [-> .| VP[num=sg, tense=pres] -> TV[num=sg, tense=pres] * NP 
     Predictor |. . > .| NP[num=?n] -> * N[num=?n] 
     Predictor |. . > .| NP[num=?n] -> * PropN[num=?n] 
     Predictor |. . > .| NP[num=?n] -> * Det[num=?n] N[num=?n] 
     Predictor |. . > .| NP[num=pl] -> * N[num=pl] 
     Scanner   |. . [-]| N[num=pl] -> 'children' * 
     Completer |. . [-]| NP[num=pl] -> N[num=pl] * 
     Completer |. [---]| VP[num=sg, tense=pres] -> TV[num=sg, tense=pres] NP * 
     Completer |[=====]| S -> NP[num=sg] VP[num=sg] * 
     Completer |[=====]| Start -> S * 
     Completer |[=====]| [INIT] -> Start * 
     >>> 

Finally, we can inspect the resulting parse trees (in this case, a
single one).

.. doctest-ignore::
     >>> for tree in trees: print tree
     ... 
     ([INIT]:
       (Start:
	 (S:
	   (NP[num=sg]: (PropN[num=sg]: 'Kim'))
	   (VP[num=sg, tense=pres]:
	     (TV[num=sg, tense=pres]: 'likes')
	     (NP[num=pl]: (N[num=pl]: 'children'))))))
     >>>


Exercises
---------

#. Redo the previous two exercises, but using feat0cfg_ as your
   starting point.

.. Can we do cross-references?




Terminology
-----------


So far, we have only seen feature values like `sg`:fval: and
`pl`:fval. These simple values are usually called `atomic`:dt:
|mdash| that is, they can't be decomposed into subparts. A special
case of atomic values are `boolean`:dt: values, that is, values which
just specify whether a property is true or false of a category. For
example, we might want to distinguish `auxiliary`:dt: verbs such as
`can`:lx:, `may`:lx:, `will`:lx: and `do`:lx: with the boolean feature
`aux`:feat:. Thus, our lexicon for verbs might include entries such as
the following:

.. ex::
      .. parsed-literal::

        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `+`:math:\ ] |rarr| 'can'
        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `+`:math:\ ] |rarr| 'may'

        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `-`:math:\ ] |rarr| 'walks'
        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `aux`:feat: = `-`:math:\ ] |rarr| 'likes'


A frequently used abbreviation for boolean features allows the value
to be prepended to the feature:

.. ex::
      .. parsed-literal::

        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `+aux`:feat:\ ] |rarr| 'can'
        `V`:gc:\ [`tense`:feat: = `pres`:fval:, `-aux`:feat:\ ] |rarr| 'walks'


We have spoken informally of attaching 'feature annotations' to
syntactic categories. A more general
approach is to treat the whole category |mdash| that is, the
non-terminal symbol plus the annotation |mdash| as a bundle of
features. Consider, for example, the object we have written as ncat0_.

.. _ncat0:
.. ex::
      .. parsed-literal::

        `N`:gc:\ [`num`:feat: = `sg`:fval:\ ] 

The syntactic category `N`:gc:, as we have seen before, provides part
of speech information. This information can itself be captured as a
feature specification, as shown in ncat1_.

.. _ncat1:
.. ex::
      .. parsed-literal::

        [`pos`:feat: = `N`:fval:, `num`:feat: = `sg`:fval:\ ] 

In fact, we  regard ncat1_ as our 'official' representation of a
feature-based linguistic category, and
ncat0_ as a convenient abbreviation.
A bundle of feature-value pairs is called a `feature structure`:dt:
or an `attribute value matrix`:dt: (AVM). A feature structure which
contains a specification for the feature `pos`:feat: is a `linguistic
category`:dt:. 

In addition to atomic-valued features, we allow features whose values
are themselves feature structures. For example, we might want to group
together agreement features (e.g., person, number and gender) as a
distinguished part of a category, as shown in agr0_.

.. _agr0:
.. ex::
      .. parsed-literal::

        [pos: N
         agr: [per: 3
               num: pl
               gend: fem]]


In this case, we say that the feature `agr`:feat: has a `complex`:dt: value.

There is no particular significance to the *order* of features in a
feature structure. So agr0_ is equivalent to agr0_.

.. _agr1:
.. ex::
      .. parsed-literal::

        [agr: [num: pl
               per: 3
               gend: fem]
         pos: N]



Feature Structures in NLTK and Unification
------------------------------------------

Feature structures in NLTK are declared with the
``FeatureStructure()`` constructor. Atomic feature values can be strings or
integers.

     >>> from nltk_lite.parse.featurestructure import *
     >>> fs1 = FeatureStructure(tense='past', num='sg') 
     >>> print fs1
     [ num   = 'sg'   ]
     [ tense = 'past' ]
     >>> 

We can think of a feature structure as being like a Python dictionary,
and access its values by indexing in the usual way.

     >>> fs1 = FeatureStructure(per=3, num='pl', gend='fem')
     >>> print fs1['gend']
     fem
     >>> 

However, we cannot use this syntax to *assign* values to features:

     >>> fs1[case] = 'acc'
     Traceback (most recent call last):
       File "<stdin>", line 1, in ?
     NameError: name 'case' is not defined
     >>> 

We can also define feature structures which have complex values, as
discussed earlier.

     >>> fs2 = FeatureStructure(pos='N', agr=fs1)
     >>> print fs2
     [       [ gend = 'fem' ] ]
     [ agr = [ num  = 'pl'  ] ]
     [       [ per  = 3     ] ]
     [                        ]
     [ pos = 'N'              ]
     >>> print fs2['agr']
     [ gend = 'fem' ]
     [ num  = 'pl'  ]
     [ per  = 3     ]
     >>> print fs2['agr']['per']
     3
     >>> 

An alternative method of specifying feature structures in NLTK is to
use the ``parse`` method of ``FeatureStructure``. This gives us the
facility to use square bracket notation for embedding one feature
structure within another.

     >>> FeatureStructure.parse("[pos='N', agr=[per=3, num='pl', gend='fem']]")
     [agr=[gend='fem', num='pl', per=3], pos='N']
     >>>


Feature structures are not inherently tied to linguistic objects; they are
general purpose structures for representing knowledge. For example, we
could encode information about a person in a feature structure:

     >>> person01 = FeatureStructure(name='Lee', telno='01 27 86 42 96', age=33)
     >>> >>> print person01
     [ age   = 33         ]
     [ name  = 'Lee'      ]
     [ telno = '01 27 86 42 96' ]
     >>> 

It is sometimes helpful to picture feature structures as graphs; more
specifically, `directed acyclic graphs`:dt: (DAGs). dag01_ is equivalent to
the feature structure ``person01`` just shown.

.. _dag01:
.. ex::
   .. image:: ../images/dag01.png
      :scale: 40

The feature names appear as labels on the directed arcs, and feature
values appear as labels on the nodes which are pointed to by the arcs.

Just as before, feature values can be complex:

.. _dag02:
.. ex::
   .. image:: ../images/dag02.png
      :scale: 40

When we look at such graphs, it is natural to think in terms of
paths through the graph. A `feature path`:dt: is a sequence of arcs
that can be followed from the root node. We will represent paths in NLTK as
tuples. Thus, ``('address', 'street')`` is a feature path whose value
in dag02_
is the string 'rue Pascal'.

Now let's consider a situation where Lee has a spouse named 'Kim', and
Kim's address is the same as Lee's.
We might represent this as dag04_.

.. _dag04:
.. ex::
   .. image:: ../images/dag04.png
      :scale: 40

However, rather than repeating the address
information in the feature structure, we can 'share' the same
sub-graph between different arcs:

.. _dag03:
.. ex::
   .. image:: ../images/dag03.png
      :scale: 40



In other words, the value of the path ``('address')`` in dag03_ is identical to
the value of the path ``('spouse', 'address')``.
DAGs such as dag03_ are said to involve `structure sharing`:dt: or
`reentrancy`:dt:. When two paths have the same value, they are said to
be `equivalent`:dt:.

There are a number of notations for representing reentrancy in
matrix-style representations of feature structures. In NLTK, we adopt
the following convention: the first occurrence of a shared feature structure 
is prefixed with an integer in parentheses, such as ``(1)``, and any
subsequent reference to that structure uses the notation
``'->(1)'``, as shown below.

     >>> fs=FeatureStructure.parse("[name='Lee', address=(1)[number=74, street='rue Pascal'], spouse=[name='Kim', address->(1)]]")
     >>> print fs
     [ address = (1) [ number = 74           ] ]
     [               [ street = 'rue Pascal' ] ]
     [                                         ]
     [ name    = 'Lee'                         ]
     [                                         ]
     [ spouse  = [ address -> (1)  ]           ]
     [           [ name    = 'Kim' ]           ]
     >>> 

The bracketed integer is sometimes called a `tag`:dt: or a
`coindex`:dt:. The choice of integer is not significant.
There can be any number of tags within a single feature structure.

     >>> fs = FeatureStructure.parse("[A='a', B=(1)[C='c'], D->(1), E->(1)]")
     >>> print fs
     [ A = 'a'             ]
     [                     ]
     [ B = (1) [ C = 'c' ] ]
     [                     ]
     [ D -> (1)            ]
     [ E -> (1)            ]
     >>> fs1 = FeatureStructure.parse("[A=(1)[], B=(2)[], C->(1), D->(2)]")
     >>> print fs
     [ A = (1) [] ]
     [            ]
     [ B = (2) [] ]
     [            ]
     [ C -> (1)   ]
     [ D -> (2)   ]
     >>> 

It is standard to think of feature structures as providing `partial
information`:dt: about some object, in the sense that we can order
feature structures according to how general they are. For example,
fs01_ is more general (less specific) than fs02_, which in turn is more general than fs03_.

.. ex::
   .. _fs01:
   .. ex::
      .. parsed-literal::

         [number: 74]

   .. _fs02:
   .. ex::
      .. parsed-literal::

         [number: 74
          street: 'rue Pascal']

   .. _fs03:
   .. ex::
      .. parsed-literal::

         [number: 74
          street: 'rue Pascal'
          city: 'Paris']

This ordering is called `subsumption`:dt:; a more general feature
structure `subsumes`:dt: a less general one. If `FS`:math:\
:subscript:`0` subsumes `FS`:math:\ :subscript:`1` (formally, we write
`FS`:math:\ :subscript:`0` |SquareSubsetEqual| `FS`:math:\
:subscript:`1`), then `FS`:math:\ :subscript:`1` must have all the
paths and path equivalences of `FS`:math:\ :subscript:`0`, and may
have additional paths and equivalences as well. Thus, dag04_ subsumes
dag03_, since the latter has additional path equivalences.. It should
be obvious that subsumption only provides a partial ordering on
feature structures, since some feature structures are
incommensurable. For example, fs04_ neither subsumes nor is subsumed
by fs01_.


.. _fs04:
.. ex::
   .. parsed-literal::

         [telno = '01 27 86 42 96']

So we have seen that some feature structures are more specific than
others. How do we go about specialising a given feature structure?
For example, we might decide that addresses should
consist of not just a street number and a street name, but also a
city. That is, we might want to *merge*  graph dag042_ with dag041_ to
yield dag043_.

.. ex::
     .. _dag041:
     .. ex::
	.. image:: ../images/dag04-1.png
	   :scale: 40

     .. _dag042:
     .. ex::
	.. image:: ../images/dag04-2.png
	   :scale: 40

     .. _dag043:
     .. ex::
	.. image:: ../images/dag04-3.png
	   :scale: 40



Merging information from two feature structures is called
`unification`:dt: and in NLTK is supported by the ``unify()`` method
defined in the ``FeatureStructure`` class.

     >>> fs1 = FeatureStructure(number=74, street='rue Pascal')
     >>> fs2 = FeatureStructure(city='Paris')
     >>> print fs1.unify(fs2)
     [ city   = 'Paris'      ]
     [ number = 74           ]
     [ street = 'rue Pascal' ]
     >>> 

Unification is formally defined as a binary operation: `FS`:math:\
:subscript:`0` |SquareIntersection| `FS`:math:\
:subscript:`1`. Unification is symmetric, so 

.. ex::
    `FS`:math:\ :subscript:`0` |SquareIntersection| `FS`:math:\
    :subscript:`1` = `FS`:math:\ :subscript:`1` |SquareIntersection|
    `FS`:math:\ :subscript:`0`.

The same is true in NLTK:

     >>> print fs2.unify(fs1)
     [ city   = 'Paris'      ]
     [ number = 74           ]
     [ street = 'rue Pascal' ]
     >>>

.. but >>> fs1.unify(fs2) is fs2.unify(fs1)
       False
   only works with repr()

If we unify two feature structures which stand in the subsumption
relationship, then the result of unification is the most specific of
the two:

.. ex::
    If `FS`:math:\ :subscript:`0` |SquareSubsetEqual| `FS`:math:\
    :subscript:`1`,  then `FS`:math:\ :subscript:`0`
    |SquareIntersection| `FS`:math:\ :subscript:`1` = `FS`:math:\
    :subscript:`1` 

For example, the result of unifying fs02_ with fs03_ is fs03_.

Unification between `FS`:math:\ :subscript:`0` and `FS`:math:\
:subscript:`1` will fail if the two feature structures share a path |pi|,
but the value of |pi| in `FS`:math:\ :subscript:`0` is a distinct
atom from the value of |pi| in `FS`:math:\ :subscript:`1`. In NLTK,
this is implemented by setting the result of unification to be
``None``.

     >>> fs0 = FeatureStructure(A='a')
     >>> fs1 = FeatureStructure(A='b')
     >>> fs2 = fs0.unify(fs1)
     >>> print fs2
     None
     >>>  

Now, if we look at how unification interacts with structure-sharing,
things become really interesting. First, let's define the NLTK version
of dag04_.

     >>> fs0=FeatureStructure.parse("[name='Lee', address=[number=74, street='rue Pascal'], spouse=[name='Kim', address=[number=74, street='rue Pascal']]]")
     >>> print fs0
     [ address = [ number = 74           ]               ]
     [           [ street = 'rue Pascal' ]               ]
     [                                                   ]
     [ name    = 'Lee'                                   ]
     [                                                   ]
     [           [ address = [ number = 74           ] ] ]
     [ spouse  = [           [ street = 'rue Pascal' ] ] ]
     [           [                                     ] ]
     [           [ name    = 'Kim'                     ] ]
     >>> 

what happens when we augment Kim's address with a specification
for `city`:feat:? (Notice that ``fs1`` includes the whole path from the root of
the feature structure down to `city`:feat:.)

     >>> fs1=FeatureStructure.parse("[spouse = [address = [city = 'Paris']]]")
     >>> print fs0.unify(fs1)
     [ address = [ number = 74           ]               ]
     [           [ street = 'rue Pascal' ]               ]
     [                                                   ]
     [ name    = 'Lee'                                   ]
     [                                                   ]
     [           [           [ city   = 'Paris'      ] ] ]
     [           [ address = [ number = 74           ] ] ]
     [ spouse  = [           [ street = 'rue Pascal' ] ] ]
     [           [                                     ] ]
     [           [ name    = 'Kim'                     ] ]
     >>> 

By contrast, the result is very different if ``fs1`` is unified with
the structure-sharing version, dag03_.

     >>> fs2=FeatureStructure.parse("[name='Lee', address=(1)[number=74, street='rue Pascal'], spouse=[name='Kim', address->(1)]]")
     >>> print fs2.unify(fs1)
     [               [ city   = 'Paris'      ] ]
     [ address = (1) [ number = 74           ] ]
     [               [ street = 'rue Pascal' ] ]
     [                                         ]
     [ name    = 'Lee'                         ]
     [                                         ]
     [ spouse  = [ address -> (1)  ]           ]
     [           [ name    = 'Kim' ]           ]
     >>>

Rather than just updating what was in effect Kim's 'copy' of Lee's address,
we have now updated *both* their addresses at the same time. More
generally, if a unification involves specialising the value of some
path |pi|, then that unification simultaneously specialises the value
of *any path that is equivalent to* |pi|.

As we have already seen, structure sharing can also be stated in NLTK
using variables such as ``?x``. 

     >>> fs1=FeatureStructure.parse("[address1=[number=74, street='rue Pascal']]")
     >>> fs2=FeatureStructure.parse("[address1=?x, address2=?x]")
     >>> print fs2
     [ address1 = ?x ]
     [ address2 = ?x ]
     >>> print fs2.unify(fs1)
     [ address1 = (1) [ number = 74           ] ]
     [                [ street = 'rue Pascal' ] ]
     [                                          ]
     [ address2 -> (1)                          ]
     >>> 

Exercises
---------

#. List two feature structures which subsume [A=?x, B=?x].

#. Ignoring structure sharing, give an informal algorithm for unifying
   two feature structures. 


---------------------------------
Extending a Feature-Based Grammar
---------------------------------


Subcatorization
---------------

In the chapter `Parsing <parse.html>`__, we proposed to augment our
category labels in order to represent different subcategories of
verb. More specifically,
we introduced labels such as `Vitr`:gc: and `Vtr`:gc: for intransitive
and transitive verbs respectively.  This allowed us to write rules
like the following:

.. _subcatcfg0:
.. ex::
   .. parsed-literal::

      `VP`:gc:  |rarr| `IV`:gc:  
      `VP`:gc:  |rarr| `TV NP`:gc: 

Although it is tempting to think of `IV`:gc: and `TV`:gc: as two
kinds of `V`:gc:, this is unjustified: from a formal point of view,
`IV`:gc: has no closer relationship with `TV`:gc: than it does,
say, with `NP`:gc:. As it stands, `IV`:gc: and `TV`:gc: are
unanalyzable nonterminal symbols from a CFG. One unwelcome consequence
is that we do not seem able to say anything about the class of verbs
in general. For example, we cannot say something like "All lexical
items of category `V`:gc: can be marked for tense", since `bark`:lx:,
say, is an item of category `IV`:gc:, not `V`:gc:.

Using features gives us some useful room for manoeuvre but there is no
obvious consensus on how to model subcategorization information. One
approach which has the merit of simplicity is due to Generalized
Phrase Structure Grammar (GPSG). GPSG stipulates that lexical
categories may bear a `subcat`:feat: whose values are integers. This
is illustrated in a modified portion of feat0cfg_, shown in subcatgpsg_.

.. _subcatgpsg:
.. ex::
   ::

     VP[tense=?t, num=?n] -> V[subcat=0, tense=?t, num=?n]
     VP[tense=?t, num=?n] -> V[subcat=1, tense=?t, num=?n] NP

     V[subcat=0, tense=pres,  num=sg] -> 'disappears' | 'walks'
     V[subcat=1, tense=pres, num=sg] -> 'sees' | 'likes'

     V[subcat=0, tense=pres,  num=pl] -> 'disappear' | 'walk'
     V[subcat=1, tense=pres, num=pl] -> 'see' | 'like'

     V[subcat=0, tense=past, num=?n] -> 'disappeared' | 'walked'
     V[subcat=1, tense=past, num=?n] -> 'saw' | 'liked'

When we see a lexical category like `V`:gc:\ [`subcat`:feat: =
`1`:fval:\ ], we can interpret the `subcat`:feat: specification as a
pointer to the rule in which `V`:gc:\ [`subcat`:feat: = `1`:fval:\ ]
is introduced as the head daughter in a `VP`:gc: expansion rule. By
convention, there is a one-to-one correspondence between
`subcat`:feat: values and rules which introduce lexical heads. It's
worth noting that the choice of integer which acts as a value for
`subcat`:feat: is completely arbitrary |mdash| we could equally well
have chosen 3999 and 57 as our two values in subcatgpsg_.  On this
approach, `subcat`:feat: can *only* appear on lexical categories; it
makes no sense, for example, to specify a `subcat`:feat: value on
`VP`:gc:.

An alternative treatment of subcategorization, due originally to a framework
known as categorial grammar, is represented in feature-based frameworks such as PATR
and Head-driven Phrase Structure Grammar. Rather than using
`subcat`:feat: values as a way of indexing rules, the `subcat`:feat:
value directly encodes the valency of a head (the list of
arguments that it can combine with). For example, a verb like
`put`:lx: which takes  `NP`:gc: and `PP`:gc: complements (`put the
book on the table`:lx) 
might be represented as subcathpsg0_:

.. _subcathpsg0:
.. ex::  `V`:gc:\ [`subcat`:feat: = `<NP, NP, PP>`:fval:\ ] 

This says that the verb can combine with three  arguments. The
leftmost element in the list is the subject `NP`:gc:, while everything
else |mdash| an `NP`:gc: followed by a `PP`:gc:  in this case |mdash| comprises the
subcategorized-for complements. When a verb like `put`:lx: is combined
with appropriate complements, the requirements which are specified in
the  `subcat`:feat: are discharged, and only a subject `NP`:gc: is
needed. This category, which corresponds to what is traditionally
thought of as `VP`:gc:, might be represented as follows.

.. _subcathpsg1:
.. ex::  `V`:gc:\ [`subcat`:feat: = `<NP>`:fval:\ ] 

Finally, a sentence is a kind of verbal category which has *no*
requirements for further arguments, and hence has a `subcat`:feat:
whose value is the empty list. The tree subcathpsg2_ shows how these
category assigments combine in a parse of `Kim put the book on the table`:lx:.

.. _subcathpsg2:
.. ex::
      .. tree:: (V[subcat=\<\>] (NP Kim)(V[subcat=\<NP\>](V[subcat=\<NP,\ NP,\ VP\>] put)<NP the\ book><PP on\ the\ table>))



Unbounded Dependency Constructions
----------------------------------

Consider the following contrasts: 

.. _gap1:
.. ex::
   .. _gap1a:
   .. ex::

      We liked the music.

   .. _ga1b:
   .. ex::

      \*We liked.

.. _gap2:
.. ex::
   .. _gap2a:
   .. ex::

      We put the card into the slot.

   .. _gap2b:
   .. ex::

      \*We put into the slot.

   .. _gap2c:
   .. ex::

      \*We put the card.

   .. _gap2d:
   .. ex::

      \*We put.

The verb `like`:lx: requires an `NP`:gc: complement, while
`put`:lx: requires both a following `NP`:gc: and `PP`:gc:. Examples
gap1_ and gap2_ show that these complements are *obligatory*:
omitting them leads to ungrammaticality. Yet there are contexts in
which obligatory complements can be omitted, as gap3_ and gap4_
illustrate.

.. _gap3:
.. ex::
   .. _gap3a:
   .. ex::

      She knows which music we like.

   .. _gap3b:
   .. ex::

      This music, we really like.

.. _gap4:
.. ex::
   .. _gap4a:
   .. ex::

      Which card did you put into the slot?

   .. _gap4b:
   .. ex::

      Which slot did you put the card into?



--------------------------------
 Adding Compositional Semantics
--------------------------------

Overview
--------

One of the goals of linguistic theory is to provide a systematic
correspondence between form and meaning. One widely adopted approach
to representing meaning |mdash| or at least, some aspects of meaning
|mdash| involves translating expressions of natural language in to
first order logic. From a computational point of view, a strong
argument in favour of first order logic is that it strikes a
reasonable balance between expressiveness and logical
tractability. On the one hand, it is flexible enough to represent many
aspects of the logical structure of natural language. On the other
hand, automated theorem proving for first order logic has received
much attention, and although inference in first order logic is not
decidable, in practice many reasoning problems are efficiently
solvable using modern theorem provers.

Standard textbooks on first order logic often contain exercises in
which the reader is required to translate between English and logic,
as illustrated in km_ and wq_.\ [#]_

.. [#] These examples come, respectively, from D. Kalish and
       R. Montague (1964) *Logic: Techniques of Formal Reasoning*,
       Harcourt, Brace and World, p94, and W. v. Quine (1952) *Methods
       of Logic*, Routledge and Kegan Paul, p121.


.. _km:
.. ex::

   .. ex:: If all whales are mammals, then Moby Dick is not a fish.

   .. ex:: |forall|\ `x`:math:\ (whale(`x`:math:) |rarr| mammal(`x`:math:)) |rarr| |neg|\ fish(MD)

.. _wq:
.. ex::

   .. ex:: There is a painting that all critics admire.

   .. ex:: |exists|\ `y`:math:\ (painting(`y`:math:) |wedge| |forall|\ `x`:math:\ (critic(`x`:math:) |rarr|  admire(`x`:math:, `y`:math:)))


Although there are numerous subtle and difficult issues about how this
translation should be carried out in particular cases, we will put
these to one side. The main focus of our discussion will be on a
different problem: how can we systematically construct a semantic
representation for a sentence which proceeds in step with the
process of parsing that sentence?


Unfortunately, it is not within the scope of this chapter to introduce the syntax and
semantics of first order logic, so if you don't already have some
familiarity with it, we suggest you consult an appropriate source. 

The |lambda|-calculus
---------------------

In a functional programming language, computation can be carried out
by reducing an expression `E`:math: according to specified rewrite
rules. This reduction is carried out on subparts of `E`:math:, and
terminates when no further subexpressions can be reduced. The
resulting expression `E*`:math: is called the `Normal Form`:dt: of
`E`:math:. Here is an
example of reduction involving a simple Python expression (where
'|DoubleRightArrow|' means 'reduces to'):

.. ex::	
 +--------------------+---------------------------------------------------------+
 |                    | ``len(max(['cat', 'zebra', 'rabbit'] + ['gopher']))``   |
 +--------------------+---------------------------------------------------------+
 | |DoubleRightArrow| | ``len(max(['cat', 'zebra', 'rabbit', 'gopher']))``      |
 +--------------------+---------------------------------------------------------+
 | |DoubleRightArrow| | ``len('zebra')``                                        |
 +--------------------+---------------------------------------------------------+
 | |DoubleRightArrow| |  ``5``                                                  |
 +--------------------+---------------------------------------------------------+

The final expression, ``5``, is considered to be the output of the program.
This fundamental notion of computation is modeled in an abstract way by
the |lambda|-calculus. 

The first basic concept in the |lambda|-calculus is `application`:dt:,
represented by an expression of the form `(F A)`:math:, where
`F`:math: is considered to be a function, and `A`:math: is considered
to be an argument (or input) for `F`:math:.  For example, `(walk
x)`:math: is an application. Application expressions can be applied to
other expressions. For example, in a functional framework, binary
addition might represented as `((+ x) y)`:math: rather than `(x +
y)`:math:. Note that `+`:math: is being treated as a function which is
applied to its first argument `x`:math: to yield a function `(+
x)`:math: that is then applied to the second argument `y`:math:.

The second basic concept in the |lambda|-calculus is 
`abstraction`:dt:. If `M[x]`:math: is an expression containing the free
variable `x`:math:, then |lambda|\ `x.M[x]`:math: denotes the function
`x`:math: |mapsto| `M[x]`:math:. Abstraction and application are
combined in the expression (|lambda|\ `x.((+ x) 3) 4)`:math:, which
denotes the function `x`:math: |mapsto| `x + 3`:math: applied to 4,
giving 4 + 3, which is 7. In general, we have (|lambda|\ `x.M[x] N) =
M[N]`:math:, where `M[N]`:math: is the result of replacing all
occurences of `x`:math: in `M`:math: by `N`:math:. This axiom of the
lambda calculus is known as |beta|-conversion. |beta|-conversion is
the primary form of reduction in the |lambda|-calculus.

The module ``logic`` can parse expressions of the
|lambda|-calculus. The |lambda| symbol is represented as ``'\'``, which
needs to be escaped by a second ``'\'`` in parsable expressions.

    >>> from nltk_lite.contrib.logic import *
    >>> Parser().parse('(walk x)')
    ApplicationExpression('walk', 'x')
    >>> Parser().parse('\\x.(walk x)')
    LambdaExpression('x', '(walk x)')
    >>>

An ``ApplicationExpression`` has subparts consisting of the function
and the argument; a ``LambdaExpression`` has subparts consisting of
the variable (e.g., ``x``) that is bound by the |lambda| and the body
of the expression (e.g., ``walk``).

The |lambda|-calculus is a calculus of functions; by itself, it says
nothing about logical structure. Although it is possible to define
logical operators within the |lambda|-calculus, we shall adopt a
hybrid approach which supplements the |lambda|-calculus with logical
and non-logical constants as primitives.
In order to show how this is done, we turn next to the language of
propositional logic.




Propositional  Logic
********************

The language `L`:subscript:`prop` of propositional logic represents
certain aspects of natural language, but at a high level of
abstraction. The only structure that is made explicit involves
`logical connectives`:dt:\; these correspond to 'logically
interesting' expressions such as `and`:lx: and `not`:lx:. The basic
expressions of the language are `propositional variables`:dt:, usually
written `p`:math:, `q`:math:, `r`:math:, etc. Let `A`:math: be a
finite set of such variables. The set |Omega| of logical connectives
contains the unary operator |neg|, and binary operators |wedge|,
|vee|, |rarr| and |iff|. 

The set of formulas of `L`:subscript:`prop` is described inductively.

   1. Every element of `A`:math: is a formula of `L`:subscript:`prop`.

   2. If |phi| is a formula, then so is |neg| |phi|.

   3. If |phi| and |psi| are formulas, then so are
      (|phi| |wedge| |psi|),
      (|phi| |vee| |psi|),
      (|phi| |rarr| |psi|) and
      (|phi| |iff| |psi|).

   4. Nothing else is a formula of `L`:subscript:`prop`.

The Boolean connectives of propositional logic are supported by
``logic``, and are parsed as function expressions. However, infix
notation is also allowed. The connectives themselves belong to the
``Operator`` class of expressions.

   >>> Parser().parse('(and p q)')
   ApplicationExpression('(and p)', 'q')
   >>> Parser().parse('(p and q)')
   ApplicationExpression('(and p)', 'q')
   >>> Parser().parse('and')
   Operator('and')
   >>> 

Since a negated proposition is syntactically an application, the unary
operator ``not`` and its argument *must* be surrounded by parentheses.

   >>> Parser().parse('(not (p and q))')
   ApplicationExpression('not', '(and p q)')
   >>> 

First Order Logic
*****************

In First Order Logic (FOL), propositions are further analysed into
predicates and arguments, which takes us a step closer to the
structure of natural languages. The standard construction rules for
FOL recognize `terms`:dt: such as individual variables and individual
constants, and `predicates`:dt: which take differing numbers of
arguments. For example, `John walks`:lx: might be formalized as
`walk(john)`:math: and `John loves Mary`:lx: as
`love(john, mary)`:math:.

However, the NLTK ``logic`` module is based on an implementation of the untyped
lambda calculus, and by default all identifiers are considered to be
variables. This means, for example, that the identifier ``walk`` can
be bound by |lambda|: 

   >>> Parser().parse('walk')
   VariableExpression('walk')
   >>> Parser().parse('\\walk.(walk x)')
   LambdaExpression('walk', '(walk x)')
   >>>

In addition, because the language is untyped, there is no formal
distinction between predicate expressions and individual expressions;
anything can be applied to anything. Indeed, functions can be applied
to themselves:

   >>> Parser().parse('(walk walk)')
   ApplicationExpression('walk', 'walk')
   >>> 

In applications to natural language semantics, on the other hand, it
is more usual to forbid self-application (e.g., applications such as
`(walk walk)`:math:) by assigning types to expressions of the
language, and also to allow constants as basic expressions of the
language. We shall use a mixture of convention and supplementary
stipulations in the ``logic`` module to bring our practice closer to
this more standard framework for natural language semantics.  In
particular, we shall use expressions like `x`:math:, `y`:math:,
`z`:math:, or `x`:subscript:`0`, `x`:subscript:`1`, ... to indicate
individual variables.  As an extension over the 'pure'
|lambda|-calculus syntax of ``logic``, we can assign such strings to the
class ``IndVariableExpression``.

   >>> Parser().parse('x')
   IndVariableExpression('x')
   >>> Parser().parse('x01')
   IndVariableExpression('x01')
   >>> 

English-like expressions such as `dog`:math:,
`walk`:math: and `john`:math: will be non-logical constants
(non-logical in contrast to logical constants such as `not`:math: and
`and`:math:). In order to force ``logic.Parser()`` to recognize
non-logical constants, we can initialize the parser with a list of
identifiers.

   >>> lp = Parser(constants=['dog', 'walk', 'love'])
   >>> lp.parse('walk')
   ConstantExpression('walk')
   >>> 

To sum up our discussion so far, while the |lambda|-calculus with only
recognizes one kind of basic expression other than |lambda|, namely
the class of variables (the class ``VariableExpression`` in
``logic``), our extended language adds to ``logic`` three further
classes of basic expression: ``IndVariableExpression``,
``ConstantExpression`` and ``Operator`` (Boolean connectives plus the
equality relation ``=``).  

However, predication will be formalized using function application, as
discussed earlier. In standard FOL, predication is treated in a
'relational' style. For example, the proposition that John loves Mary
is formalized as `love(j, m)`:math:. Semantically, `love`:math: is
modeled as a relation, i.e., a set of pairs, and the proposition is
true in a situation where the pair |langle|\ `j, m`:math:\ |rangle|
belongs to this set. By contrast, in the functional style of the
|lambda| calculus, `John loves Mary`:lx: is formalized as `((love m)
j)`:math:. Instead of being modeled as a relation,, `love`:math:
denotes a function. Before going into detail about this function,
let's first look at a simpler case, namely the different styles of
interpreting a unary constant such as `walk`:math:.

In the relational approach, `walk`:math: denotes some set `S`:math: of
individuals. The formula `walk(j)`:math: is true in a situation if the
individual denoted by `j`:math: belongs to `W`:math:. Now,
corresponding to every set is something called the `characteristic
function`:dt: of that set. If `f`:math: is the characteristic function
of some set `S`:math:, then for any individual `a`:math:, `f(a) =
True`:math: if and only if `a`:math: belongs to `S`:math:. To be more
specific, suppose in some situation our domain of discourse is the set
containing the individuals `j`:math: (John), `m`:math: (Mary) and
`f`:math: (Fido); and the set of individuals
that walk is `W = {j, f}`:math:. So in this situation, the formulas
`walk(j)`:math: and `walk(f)`:math: are both true, while
`walk(m)`:math: is false. 
Now of course we can use the characteristic function `f`:subscript:`W`
as the interpretation of `walk`:math: in the functional style.
The diagram cf01_
gives a graphical representation of the mapping  `f`:subscript:`W`.

.. _cf01:
.. ex::
   .. image:: ../images/models_walk_cf.png
      :scale: 25

Binary relations can be converted into functions in a very similar
fashion. Suppose for example that on the relational style of
interpretation, `love`:math: denotes the following set of pairs:

.. ex:: {|langle|\ `j, m`:math:\ |rangle|, |langle|\ `m, f`:math:\ |rangle|,  
         |langle|\ `f, j`:math:\ |rangle|}

That is, John loves Mary, Mary loves Fido, and Fido loves John. One
option on the functional style would be to treat `love`:math: as the
expected characteristic function of this set, i.e., a
function from pairs to truth values. This mapping is illustrated in cf02_.

.. _cf02:
.. ex::
   .. image:: ../images/models_love_cf01.png
      :scale: 20

However, a more uniform approach can be achieved through a technique
known as 'currying', in which `n`:math:\ -ary functions are converted
into functions of one argument.



.. +-----------------+---------------+----------------+
   |   English       | Relational    |  Functional    |
   +=================+===============+================+
   |     John walks  |``walk(j)``    |``(walk j)``    |
   +-----------------+---------------+----------------+
   |John loves Mary  |``love(j, m)`` |``((love m) j)``|
   +-----------------+---------------+----------------+

 

|Lambda|-abstraction can be carried out on Boolean expressions. For
example, the following can be thought of as the property of being an
`x`:math: who walks and talks:

  >>> Parser().parse('\\x.((walk x) and (talk x))')
  LambdaExpression('x', '(and (walk x) (talk x))')
  >>>

|beta|-conversion can be invoked with the ``simplify`` method of
``ApplicationExpression``\ s.

  >>> WT = Parser().parse('(\\x.((walk x) and (talk x)) john)')
  >>> WT
  ApplicationExpression('\x.(and (walk x) (talk x))', 'john')
  >>> WT.simplify()
  ApplicationExpression('(and (walk john))', '(talk john)')
  >>> 



----------------
Formal Semantics
----------------

A model is a pair `<D,V>`:math:, where `D`:math: is a domain of discourse and
`V`:math: is a valuation function for the non-logical constants of a
first-order language. We assume that the language is based on the
lambda calculus, in the style of Montague grammar.


We also assume that non-logical constants are either individual
constants or functors. In particular, rather than interpreting a
one-place predicate `P`:math: as a set `S`:math:, we interpret it as
the corresponding characteristic function `f`:math:, where
`f(a)`:math: = `True`:math: iff `a`:math: is in `S`:math:. For
example, instead of interpreting `dog`:lx: as the set of individuals
{`d1`:math:, `d2`:math:, `d3`:math:}, we interpret it as the function which maps
`d1`:math:, `d2`:math: and `d3`:math: to `True`:math: and every other entity to
`False`:math:.

Thus, as a first approximation, non-logical constants are interpreted
by the valuation `V`:math: as follows (note that `e`:math: is the type of
entities and `t`:math: is the type of truth values):

  - if |alpha| is an individual constant, then `V`:math:\ (|alpha|)
    is an element of the domain `D`:math:.
  - If |gamma| is a functor of type 
    (`e`:math: x ... x `e`:math:) |rarr| `t`:math:, then
    `V`:math:(|gamma|) is a function `f`:math: from  
    `D`:math: x ... x `D`:math: to {True, False}.

However, since we are basing our language on the lambda calculus , a
binary relation such as the interpretation of `like`:lx: will not in
fact be associated with the type (`e`:math: x `e`:math:) |rarr|
`t`:math:, but rather the type (`e`:math: |rarr| (`e`:math: |rarr|
`t`:math:)); i.e., a function from entities to a function from
entities to truth values. In other words, functors are assigned
'Curried' functions as their values. It should also be noted that
expressions of the language are not explicitly typed.  We leave it to
the grammar writer to assign 'sensible' values to expressions rather
than enforcing any type-to-denotation consistency.

Characteristic Functions
------------------------

Within the ``models`` module, Curried characteristic functions are implemented as
a subclass of dictionaries, using the ``CharFun`` constructor.

   >>> cf = CharFun({'d1' : CharFun({'d2': True}), 'd2' : CharFun({'d1': True})})

Values of a ``CharFun`` are accessed by indexing in the usual way:

   >>> cf['d1']
   {'d2': True}
   >>> cf['d1']['d2']
   True

``CharFun``\ s are 'sparse' data structures in the sense that they omit
key-value pairs of the form ``(e: False)``. In fact, they
behave just like ordinary dictionaries on keys which are
out of their domain, rather than yielding the value ``False``:

   >>> cf['not in domain']
   Traceback (most recent call last):
   ...
   KeyError: 'not in domain'

The assignment of ``False`` values is delegated to a wrapper method
``app`` of the ``Model`` class. ``app`` embodies the Closed World
assumption; i.e., where ``m`` is an instance of ``Model``:

   >>> m.app(cf,'not in domain')
   False


In practise, it is often be more convenient to specify
interpretations as `n`:math:-ary relations (i.e., sets of `n`:math:-tuples) rather
than as `n`:math:-ary functions. ``CharFun`` provides a ``parse`` method which
will convert such relations into Curried characteristic functions:

   >>> s = set([('d1', 'd2'), ('d3', 'd4')])
   >>> cf = CharFun()
   >>> cf.parse(s)
   >>> cf
   {'d2': {'d1': True}, 'd4': {'d3': True}}


``cf.parse`` will raise an exception if the set is not in fact a
relation (i.e., if it contains tuples of different lengths):

  >>> wrong = set([('d1', 'd2'), ('d2', 'd1', 'd3')])
  >>> cf.parse(wrong)
  Traceback (most recent call last):
  ...
  ValueError: Set contains sequences of different lengths

However, unary relations can be parsed to characteristic functions.

  >>> unary = set(['d1', 'd2'])
  >>> cf.parse(unary)
  >>> cf
  {'d2': True, 'd1': True}

The function ``flatten`` returns a set of the entities used as keys in
a ``CharFun`` instance. The same information can be accessed via the
``domain`` attribute of ``CharFun``.

   >>> cf = CharFun({'d1' : {'d2': True}, 'd2' : {'d1': True}})
   >>> flatten(cf)
   set(['d2', 'd1'])
   >>> cf.domain
   set(['d2', 'd1'])



Valuations
----------

A `Valuation`:dt: is a mapping from non-logical constants to appropriate semantic
values in the model. Valuations are created using the ``Valuation`` constructor.

   >>> val = Valuation({'Fido' : 'd1', 'dog' : {'d1' : True, 'd2' : True}})
   >>> val
   {'Fido': 'd1', 'dog': {'d2': True, 'd1': True}}

As with ``CharFun``, an instance of ``Valuation`` will parse valuations using
relations rather than characteristic functions as interpretations.

   >>> setval = [('adam', 'b1'), ('betty', 'g1'),\
   ('girl', set(['g2', 'g1'])), ('boy', set(['b1', 'b2'])),\
   ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))]
   >>> val = Valuation()
   >>> val.parse(setval)
   >>> print val
   {'adam': 'b1',
   'betty': 'g1',
   'boy': {'b1': True, 'b2': True},
   'girl': {'g2': True, 'g1': True},
   'love': {'b1': {'g2': True, 'g1': True},
            'g1': {'b1': True},
            'g2': {'b2': True}}}

Valuations have a ``domain`` attribute, like ``CharFun``, and also a ``symbols``
attribute.

   >>> val.domain
   set(['g1', 'g2', 'b2', 'b1'])
   >>> val.symbols
   ['boy', 'girl', 'love', 'adam', 'betty']   


Assignments
-----------

A variable `Assignment`:dt: is a mapping from individual variables to
entities in the domain. Individual variables are indicated with the
letters ``'x'``, ``'y'``, ``'w'`` and ``'z'``, optionally followed by an integer
(e.g., ``'x0'``, ``'y332'``).  Assignments are created using the ``Assignment``
constructor, which also takes the model's domain of discourse as a parameter.

   >>> dom = set(['u1', 'u2', 'u3', 'u4'])
   >>> g = Assignment(dom, {'x': 'u1', 'y': 'u2'})
   >>> g
   {'y': 'u2', 'x': 'u1'}

In addition, there is also a ``print`` format for assignments which uses a notation
closer to that in logic textbooks:
   
   >>> print g
   g[u2/y][u1/x]

Initialization of an ``Assignment`` instance checks that the variable
really is an individual variable and also that the value belongs to
the domain of discourse:

    >>> Assignment(dom, {'xxx': 'u1', 'y': 'u2'})
    Traceback (most recent call last):
    ...
    AssertionError: Wrong format for an Individual Variable: 'xxx'
    >>> Assignment(dom, {'x': 'u5', 'y': 'u2'})
    Traceback (most recent call last):
    ...
    AssertionError: 'u5' is not in the domain: set(['u4', 'u1', 'u3', 'u2'])

It is possible to update an assignment using the ``add`` method, with
similar restrictions:

    >>> dom = set(['u1', 'u2', 'u3', 'u4'])
    >>> g = models.Assignment(dom, {})
    >>> g.add('u1', 'x')
    {'x': 'u1'}
    >>> g.add('u1', 'xyz')
    Traceback (most recent call last):
    ...
    AssertionError: Wrong format for an Individual Variable: 'xyz'
    >>> g.add('u2', 'x').add('u3', 'y').add('u4', 'x0')
    {'y': 'u3', 'x': 'u2', 'x0': 'u4'}
    >>> g.add('u5', 'x')
    Traceback (most recent call last):
    ...
    AssertionError: u5 is not in the domain set(['u4', 'u1', 'u3', 'u2'])

Variables (and their values) can be selectively removed from an
assignment with the ``purge`` method:

    >>> g
    {'y': 'u3', 'x': 'u2', 'x0': 'u4'}
    >>> g.purge('x')
    >>> g
    {'y': 'u3', 'x0': 'u4'}

With no arguments,  ``purge`` is equivalent to ``clear`` on a dictionary:

    >>> g.purge()
    >>> g
    {}


The ``Model`` constructor takes two parameters, a ``set`` and a ``Valuation``.

   >>> m = Model(val.domain, val)

The top-level method of a ``Model`` instance is ``evaluate``, which
assigns a semantic value to expressions of the ``logic`` module, under an assignment ``g``:

    >>> m.evaluate('all x. ((boy x) implies (not (girl x)))', g)
    True

evaluate
--------

The function ``evaluate`` calls a recursive function ``satisfy``, which in turn calls
a function ``i`` to interpret non-logical constants and individual
variables. ``i`` first tries to call the model's ``Valuation`` and if
that fails, calls the variable assignment ``g``. Any atomic expression
which cannot be assigned a value by ``i`` raises an ``Undefined``
exception; this is caught by ``evaluate``, which returns the string
'Undefined'.

    >>> m.evaluate('(walk adam)', g, trace=2)
       ... checking whether 'walk' is an individual variable
    Expression 'walk' can't be evaluated by i and g.
    'Undefined'


Boolean operators such as `not`:math:, `and`:math: and `implies`:math: are
implemented as dictionaries. For example:

    >>> m.AND
    {False: {False: False, True: False}, True: {False: False, True: True}}

A formula such as '(p and q)' is interpreted by indexing
the value of 'and' with the values of the two propositional arguments,
in the following manner:

   >>> m.AND[m.evaluate('p', g)][m.evaluate('q', g)]


satisfy
-------

The ``satisfy`` function assigns semantic values to arbitrary expressions
according to their syntactic structure, as determined by ``decompose``.




Compositionality
----------------

Sample grammar

coordination

quantification and scope


Feature-based Semantics
-----------------------



-----------------
 Further Reading
-----------------

Gerald Gazdar, Ewan Klein, Geoffrey Pullum and Ivan Sag (1985)
*Generalized Phrase Structure Grammar*, Basil Blackwell.

Ivan A. Sag and Thomas Wasow (1999) *Syntactic Theory: A Formal
Introduction*, CSLI Publications.

Patrick Blackburn and Johan Bos
*Representation and Inference for Natural Language: A First Course in
Computational Semantics*, CSLI Publications


---------
Exercises
---------

1. 


.. include:: footer.txt
