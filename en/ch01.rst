.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: add exercises for Unicode section?
.. TODO: add bullet points on regular expressions to summary
.. TODO: update cspy reference to more recent book
.. TODO: add pointers to regexp toolkits (e.g. Kodos)
.. TODO: adopt simpler hacker example with only single character transpositions;
   move hacker example to later section (later chapter?)

.. _chap-introduction:

=================================================
1. Introduction to Language Processing and Python
=================================================

It is easy to get our hands on large quantities of text.
What can we do with it, assuming we can write some simple programs?
The goal of this chapter is to answer the following questions:

#. what can we achieve by combining simple programming techniques with large quantities of text?
#. how can we automatically extract representative words and phrases from a large text?
#. is the Python programming language suitable for such work?

This chapter is divided into sections that skip between two quite
different styles: "computing with language" where we carry out a range of
linguistically-motivated tasks without necessarily understanding how they
work, and "closer looks at Python" where we systematically review key
programming concepts.  We hope this style of introduction gives you an
authentic taste of what will come later, while covering a range of
elementary concepts in linguistics and computer science.
The chapter contains many examples and exercises;
there is no better way to learn to |NLP| than to dive in and try
these yourself.

.. _sect-computing-with-language-texts-and-words:

----------------------------------------
Computing with Language: Texts and Words
----------------------------------------

Here we will treat the text as data for the programs we write,
programs that manipulate and analyze it in a variety of interesting ways.
The first step is to get started with the Python interpreter.

Getting Started
---------------

..  The emphasised text in this paragraph used to be literal, which is
    probably not applicable.  I'm not sure if emphasis is the best
    markup either.  See note 10 in toolscheck-response.txt in the
    O'Reilly repository.

    The arrows in this section have been written literally, not using
    the macros defined in definitions.rst, since using macros here
    wasn't working for me.

    pbone@csse.unimelb.edu.au

One of the friendly things about Python is that it allows you
to type directly into the interactive `interpreter`:dt: |mdash|
the program that will be running your Python programs.
You can run the Python interpreter using a simple graphical interface
called the Interactive DeveLopment Environment (|IDLE|).
On a Mac you can find this under *Applications→MacPython*,
and on Windows under *All Programs→Python*.
Under Unix you can run Python from the shell by typing ``python``.
The interpreter will print a blurb about your Python version;
simply check that you are running Python 2.4 or greater (here it is 2.5):

.. doctest-ignore::
    Python 2.5 (r25:51918, Sep 19 2006, 08:49:13) 
    [GCC 4.0.1 (Apple Computer, Inc. build 5341)] on darwin
    Type "help", "copyright", "credits" or "license" for more information.
    >>>

.. note::
   If you are unable to run the Python interpreter, you probably don't
   have Python installed correctly.  Please visit |NLTK-URL| for
   detailed instructions.

The ``>>>`` prompt indicates that the Python interpreter is now waiting
for input.  When copying examples from this book be sure not to type
in the ``>>>`` prompt yourself.  Now, let's begin by using Python as a calculator:

    >>> 1 + 5 * 2 - 3
    8
    >>>

Once the interpreter has finished calculating the answer and displaying it, the
prompt reappears. This means the Python interpreter is waiting for another instruction.

Try a few more expressions of your own. You can use asterisk (``*``)
for multiplication and slash (``/``) for division, and parentheses for
bracketing expressions. One strange thing you might come across is
that division doesn't always behave as you might expect; it does integer
division or floating point division depending on how you specify the inputs:

    >>> 3/3
    1
    >>> 1/3
    0
    >>> 1.0/3.0
    0.33333333333333331
    >>>

These examples demonstrate how you can work interactively with the
interpreter, allowing you to experiment and explore.
Now let's try a nonsensical expression to see how the interpreter handles it:

    >>> 1 +
    Traceback (most recent call last):
      File "<stdin>", line 1
        1 +
          ^
    SyntaxError: invalid syntax
    >>>

.. The real output doesn't contain the "Traceback ..." line, but
   doctest doesn't recognize it as an exception without it.

Here we have produced a `syntax error`:dt:.  It doesn't make sense
to end an instruction with a plus sign. The Python interpreter indicates
the line where the problem occurred.

Searching Text
--------------

Now that we can use the Python interpreter, let's see how we can harness its
power to process text.  The first step is to type a line of magic at the
Python prompt, telling the interpreter to load some texts for us to explore:
``from nltk.book import *`` (i.e. import all names from NLTK's book module).
After printing a welcome message, it loads
the text of several books, including *Moby Dick*.  Type the following,
taking care to get spelling and punctuation exactly right:

    >>> from nltk.book import *
    >>> text1
    <Text: Moby Dick by Herman Melville 1851>
    >>> text2
    <Text: Sense and Sensibility by Jane Austen 1811>
    >>>

We can examine the contents of a text in a variety
of ways.  A concordance view shows us a given word in its context.  Here we
look up the word `monstrous`:lx:.  Try seaching for other words; you can use the up-arrow
key to access the previous command and modify the word being searched.

    >>> text1.concordance("monstrous")
    mong the former , one was of a most monstrous size . ... This came towards us , o
    ION OF THE PSALMS . " Touching that monstrous bulk of the whale or ork we have re
    all over with a heathenish array of monstrous clubs and spears . Some were thickl
    ed as you gazed , and wondered what monstrous cannibal and savage could ever have
     that has survived the flood ; most monstrous and most mountainous ! That Himmale
     they might scout at Moby Dick as a monstrous fable , or still worse and more det
    ath of Radney .'" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere lo
    ling Scenes . In connexion with the monstrous pictures of whales , I am strongly 
    >>>

You can now try concordance searches on some of the other texts we have included.
For example, search *Sense and Sensibility* for the word
`affection`:lx:, using ``text2.concordance("affection")``.  Search the book of Genesis
to find out how long some people lived, using:
``text3.concordance("lived")``.  You could look at ``text4``, the
*US Presidential Inaugural Addresses* to see examples of English dating
back to 1789, and search for words like `nation`:lx:, `terror`:lx:, `god`:lx:
to see how these words have been used differently over time.
We've also included ``text5``, the *NPS Chat Corpus*: search this for
unconventional words like `im`:lx:, `ur`:lx:, `lol`:lx:.
(Note that this corpus is uncensored!)

Once you've spent some time examining these texts, we hope you have a new
sense of the richness and diversity of language.  In the next chapter
you will learn how to access a broader range of text, including text in
languages other than English.

If we can find words in a text, we can also take note of their position within
the text.  We produce a dispersion plot, where each bar represents an instance
of a word and each row represents the entire text.  In Figure fig-inaugural_ we
see some striking patterns of word usage over the last 220 years.  You can
produce this plot as shown below (so long as you have Numpy and Pylab installed).
You might like to try different words, and different texts.  Can you predict the
dispersion of a word before you view it?  As before, take
care to get the quotes, commas, brackets and parentheses exactly right.

    >>> text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])
    >>>

.. _fig-inaugural:
.. figure:: ../images/inaugural.png
   :scale: 100

   Lexical Dispersion Plot for Words in Presidential Inaugural Addresses

A concordance permits us to see words in context, e.g. we saw that
`monstrous`:lx: appeared in the context `the monstrous pictures`:lx:.
What other words appear in the same contexts that `monstrous`:lx:
appears in?  We can find out as follows:

    >>> text1.similar("monstrous")
    subtly impalpable curious abundant perilous trustworthy untoward
    singular imperial few maddens loving mystifying christian exasperate
    puzzled fearless uncommon domineering candid
    >>> text2.similar("monstrous")
    great very so good vast a exceedingly heartily amazingly as sweet
    remarkably extremely
    >>>

Observe that we get different results for different books.  Melville and
Austen use this word quite differently.

Now, just for fun, let's try generating some random text in the various
styles we have just seen.  To do this, we type the name of the text
followed by the "generate" function:

    >>> text3.generate()
    In the beginning of his brother is a hairy man , whose top may reach
    unto heaven ; and ye shall sow the land of Egypt there was no bread in
    all that he was taken out of the month , upon the earth . So shall thy
    wages be ? And they made their father ; and Isaac was old , and kissed
    him : and Laban with his cattle in the midst of the hands of Esau thy
    first born , and Phichol the chief butler unto his son Isaac , she
    >>>

Note that first time you run this, it is slow because it gathers statistics
about word sequences.  Each time you run it, you will get different output text.
Now try generating random text in the style of an inaugural address or an
internet chat room.  Although the text is random, it re-uses common words and
phrases from the source text and gives us a sense of its style and content.

.. note:: When text is printed, punctuation has been split off
   from the previous word.  Although this is not correct formatting
   for English text, we do this to make it clear that punctuation does
   not belong to the word.  This is called "tokenization", and you will learn
   about it in Chapter chap-words_.

Counting Vocabulary
-------------------

The most obvious fact about texts that emerges from the previous section is that
they differ in the vocabulary they use.  In this section we will see how to use the
computer to count the words in a text, in a variety of useful ways.
As before you will jump right in and experiment with
the Python interpreter, even though you may not have studied Python systematically
yet.

Let's begin by finding out the length of a text from start to finish,
in terms of the words and punctuation symbols that appear.  We'll use
the text of *Moby Dick* again:

    >>> len(text1)
    260819
    >>>

That's a quarter of a million words long!  But how many distinct words does this text
contain?  To work this out in Python we have to pose the question slightly
differently.  The vocabulary of a text is just the *set* of words that it uses,
and in Python we can list the vocabulary of ``text3`` with the command: ``set(text3)``.
This will produce many screens of words.  Now try the following:

    >>> sorted(set(text3))
    ['!', "'", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)',
    'A', 'Abel', 'Abelmizraim', 'Abidah', 'Abide', 'Abimael', 'Abimelech',
    'Abr', 'Abrah', 'Abraham', 'Abram', 'Accad', 'Achbor', 'Adah', ...]
    >>> len(set(text3))
    2789
    >>> len(text3) / len(set(text3))
    16
    >>>

Here we can see a sorted list of vocabulary items, beginning with various
punctuation symbols and continuing with words starting with `A`:lx:.  Words
starting with `a`:lx: will appear much later, after the last "Z" word, `Zoroaster`:lx:.
We discover the size of the vocabulary indirectly, by asking
for the length of the set.  Finally, we can calculate a measure of the lexical
richness of the text and learn that each word is used 16 times on average.

Next, let's focus in on particular words.  We can count how often a word occurs
in a text, and compute what percentage
of the text is taken up by a specific word:

    >>> text3.count("smote")
    5
    >>> 100.0 * text4.count('a') / len(text4)
    1.4587672822333748
    >>>

You might like to repeat such calculations on several texts,
but it is tedious to keep retyping it for different texts.  Instead,
we can come up with our own name for this task, e.g. "score", and
define a `function`:dt: that can be re-used as often as we like:

    >>> def score(text):
    ...     return len(text) / len(set(text))
    ...
    >>> score(text3)
    16
    >>> score(text4)
    4
    >>>

.. note:: The Python interpreter changes the prompt from
   ``>>>`` to ``...`` after encountering the colon at the
   end of the first line.  The ``...`` prompt indicates
   that Python expects an indented code block to appear next.
   It is up to you to do the indentation, by typing four
   spaces.  To finish the indented block just enter a blank line.

Notice that we used the ``score`` function by typing its name, followed
by an open parenthesis, the name of the text, then a close parenthesis.
This is just what we did for the ``len`` and ``set`` functions earlier.
These parentheses will show up often: their role is to separate
the name of a task |mdash| such as ``score`` |mdash| from the data
that the task is to be performed on |mdash| such as ``text3``.
Functions are an advanced concept in programming and we only
mention them at the outset to give newcomers a sense of the
power and flexibility of programming.  Later we'll see how to use
such functions when tabulating data, as shown below:
same task many times over using functions, we can easily build
up tables like Table brown-types_.

.. table:: brown-types

   ==================  ===========  ==========  =====
   Genre               Token Count  Type Count  Score
   ==================  ===========  ==========  =====
   skill and hobbies   82345        11935       6.9
   humor               21695        5017        4.3
   fiction: science    14470        3233        4.5
   press: reportage    100554       14394       7.0
   fiction: romance    70022        8452        8.3
   religion            39399        6373        6.2
   ==================  ===========  ==========  =====

   Lexical Diversity of Various Genres in the Brown Corpus


Exercises
---------

#. |easy| How many words are there in ``text2``?  How many
   distinct words are there?

#. |easy| Compare the lexical diversity scores for humor
   and romance fiction in Table brown-types_.  Which genre is
   more lexically diverse?

#. |talk| Compare the lexical dispersion plot with Google Trends, which
   shows the frequency with which a term has been referenced in news reports
   or been used in search terms over time.

#. |easy| Produce a dispersion plot of the four main protagonists in
   *Sense and Sensibility*: Elinor, Marianne, Edward, Willoughby.
   What can you observe about the different roles played by the males
   and females in this novel?  Can you identify the couples?

#. |easy| According to Strunk and White's *Elements of Style*,
   the word `however`:lx:, used at the start of a sentence,
   means "in whatever way" or "to whatever extent", and not
   "nevertheless".  They give this example of correct usage:
   `However you advise him, he will probably do as he thinks best.`:lx:
   (http://www.bartleby.com/141/strunk3.html)
   Use the concordance tool to study actual usage of this word
   in the various texts we have been considering.

#. |soso| Consider the following Python expression: ``len(set(text4))``.
   State the purpose of this expression.  Describe the two steps
   involved in performing this computation.

#. |soso| How many times does the word `lol`:lx: appear in ``text5``?
   How much is this as a percentage of the total number of words
   in this text? 

#. |soso| Pick a pair of texts and study the differences between them,
   in terms of vocabulary, vocabulary richness, genre, etc.  Can you
   find pairs of words which have quite different meanings across the
   two texts, such as `monstrous`:lx: in *Moby Dick* and in *Sense and Sensibility*?

#. |soso| Compare the frequency of use of the modal verbs `will`:lx: and
   `could`:lx: in ``text2`` (romance fiction) and ``text7`` (news).
   Which modal verb is more common in which genre? 

We will leave our quest for characteristic words of a text,
and explore a rather different approach that uses the
*ratio* of word frequencies.


------------------------------------------------
A Closer Look at Python: Texts as Lists of Words
------------------------------------------------

You've seen some important building blocks of the Python programming language.
Here we take a break from language processing to take a closer look at Python.

.. _sec-lists:

Lists
-----

What is a text?  At one level, it is a sequence of symbols on a page, such
as this one.  At another level, it is a sequence of chapters, made up
of a sequence of sections, where each section is a sequence of paragraphs,
and so on.  However, for our purposes, we will think of a text as nothing
more than a sequence of words and punctuation.  Here's how we represent
text in Python, in this case the opening sentence of *Moby Dick*:

    >>> sent1 = ["Call", "me", "Ishmael", "."]
    >>>

After the prompt we've given a name we made up, ``sent1``, followed
by the equals sign, and then some quoted words, separated with
commas, and surrounded with brackets.  This bracketed material
is known as a `list`:dt: in Python: it is how we store a text.
Each individual word must be quoted, using double or single quotes,
like ``"this"`` or like ``'this'``.
(When using single quotes, use the *close* quote character at the start and the end.)
Here, we've given this list the name ``sent1``.  We can inspect
it by typing the name, and we can ask for its length:

    >>> sent1
    ['Call', 'me', 'Ishmael', '.']
    >>> len(sent1)
    4
    >>> score(sent1)
    1
    >>>

We can even apply our own "score" function to it.
Some more lists have been defined for you,
one for the opening sentence of each of our texts,
``sent2`` |dots| ``sent8``.  We inspect two of them
here; you can see the rest for yourself using the Python interpreter.

    >>> sent2
    ["The", "family", "of", "Dashwood", "had", "long",
    "been", "settled", "in", "Sussex", "."]
    >>> sent3
    ["In", "the", "beginning", "God", "created", "the",
    "heaven", "and", "the", "earth", "."]

You can type these in or else make up a few sentences of your own.
Now let's repeat some of the other Python operations we saw above in
Section sect-computing-with-language-texts-and-words_:

    >>> sorted(sent3)
    ['.', 'God', 'In', 'and', 'beginning', 'created', 'earth',
    'heaven', 'the', 'the', 'the']
    >>> len(set(sent3))
    9
    >>> sent3.count("the")
    3
    >>>

We can also do arithmetic operations with lists in Python.
Multiplying a list by a number, e.g. ``sent1 * 2``,
creates a longer list containing multiple
copies of the items in the original list.  Adding two
lists, e.g. ``sent4 + sent1``, creates a new list
containing everything from the first list, followed
by everything from the second list:

    >>> sent1 * 2
    ['Call', 'me', 'Ishmael', '.', 'Call', 'me', 'Ishmael', '.']
    >>> sent4 + sent1
    ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the',
    'House', 'of', 'Representatives', ':', 'Call', 'me', 'Ishmael', '.']
    >>>

This special use of the addition operation is called `concatenation`:dt:;
it links the lists together into a single list.  We can concatenate
sentences to build up a text.

.. _sec-indexing-lists:

Indexing Lists
--------------

As we have seen, a text in Python is just a list of words, represented
using a particular combination of brackets and quotes.  Just as with an
ordinary page of text, we can count up the total number of words
(``len(text1)``), and count the occurrences of a particular word
(``text1.count("heaven")``).  And just as we can pick out the
first, tenth, or even 14,278th word in a printed text, we can identify
the elements of a list by their number, or `index`:dt:, by following
the name of the text with the index inside brackets.  We can
also find the index of the first occurrence of any word:

    >>> text4[173]
    'awaken'
    >>> text4.index("awaken")
    173
    >>>

Indexes turn out to be a common way to access the words of a text,
or |mdash| more generally |mdash| the elements of a list.
Python permits us to access sublists as well, extracting
manageable pieces of language from large texts, a technique
known as `slicing`:dt:.

    >>> text5[1040:1060]
    ['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is',
    'so', 'good', 'because', 'you', 'can', 'actually', 'play',
    'a', 'full', 'game', 'without', 'buying', 'it']

Indexes have some subtleties, and we'll explore these with
the help of an artificial sentence:

    >>> sent = ["word1", "word2", "word3", "word4", "word5",
                "word6", "word7", "word8", "word9", "word10",
                "word11", "word12", "word13", "word14", "word15",
                "word16", "word17", "word18", "word19", "word20"]
    >>> sent[0]
    'word1'
    >>> sent[19]
    'word20'
    >>>

Notice that our indexes start from zero:
``sent`` element zero, written ``sent[0]``,
is the first word, ``'word1'``, while
``sent`` element 19 is ``'word20'``. 
This is initially confusing,
but typical of modern programming languages.
(If you've mastered the system of counting
centuries where 19XY is a year in the 20th century,
or if you live in a country where walking up
1 flight of stairs puts you on level 2
of a building, you'll quickly get the hang of this.)
The moment Python accesses the content of a list from
the computer's memory, it is already at the first element;
we have to tell it how many elements forward to go.
If we tell it to go too far, we get an error:

    >>> sent[20]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: list index out of range
    >>>

This time it is not a syntax error; the program fragment is syntactically correct.
Instead, the error occurred while the program was running.
The ``Traceback`` message indicates which line the error occurred on
(line 1 of "standard input").  It is followed by the name of the error,
``IndexError``, and a brief explanation.

Let's take a closer look at slicing, using our artificial sentence again:

    >>> sent[17:20]
    ['word18', 'word19', 'word20']
    >>> sent[17]
    'word18'
    >>> sent[18]
    'word19'
    >>> sent[19]
    'word20'
    >>>

Thus, the slice ``17:20`` includes ``sent`` elements 17, 18, and 19.
By convention, ``m:n`` means elements `m`:mathit:\ |dots|\ `n-1`:mathit:.
We can omit the first number if the slice begins at the start of the
list, and we can omit the second number if the slice goes to the end:

    >>> sent[:3]
    ['word1', 'word2', 'word3']
    >>> text2[141525:]
    ['among', 'the', 'merits', 'and', 'the', 'happiness', 'of', 'Elinor',
    'and', 'Marianne', ',', 'let', 'it', 'not', 'be', 'ranked', 'as', 'the',
    'least', 'considerable', ',', 'that', 'though', 'sisters', ',', 'and',
    'living', 'almost', 'within', 'sight', 'of', 'each', 'other', ',',
    'they', 'could', 'live', 'without', 'disagreement', 'between', 'themselves',
    ',', 'or', 'producing', 'coolness', 'between', 'their', 'husbands', '.',
    'THE', 'END']
    >>>

We can modify an element of a list by assigning to one of its index values,
e.g. putting ``sent[0]`` on the left of the equals sign.  We can also
replace an entire slice with new material:

    >>> sent[0] = "First Word"
    >>> sent[19] = "Last Word"
    >>> sent[1:19] = ["Second Word", "Third Word"]
    >>> sent
    ['First Word', 'Second Word', 'Third Word', 'Last Word']
    >>>


Variables
---------

From the start of Section sect-computing-with-language-texts-and-words_, you have had
access texts called ``text1``, ``text2``, and so on.  It saved a lot
of typing to be able to refer to a 250,000-word book with a short name
like this!  In general, we can make up names for anything we care
to calculate.  We did this ourselves in the previous sections, e.g.
defining a `variable`:dt: ``sent1`` as follows:

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>>

Such lines have the form: *variable = expression*.  Python will evaluate
the expression, and save its result to the variable.  This process does
not generate any output; you have to type the variable on a line of its
own to inspect its contents.  The equals sign is slightly misleading,
since information is copied from the right side to the left.
The variable can be anything you like, e.g. ``my_sent``, ``sentence``, ``xyzzy``.
It must start with a letter, and can include numbers and underscores.
It cannot be any of Python's reserved words, such as ``if``, ``not``,
and ``import``.  Here are some examples:

    >>> mySent = ["The", "family", "of", "Dashwood", "had", "long",
    ...          "been", "settled", "in", "Sussex", "."]
    >>> noun_phrase = mySent[:4]
    >>> noun_phrase
    ['The', 'family', 'of', 'Dashwood']
    >>> wOrDs = sorted(noun_phrase)
    >>> wOrDs
    ['Dashwood', 'The', 'family', 'of']
    >>> not = wOrDs[0]
    File "<stdin>", line 1
        not = wOrDs[0]
            ^
    SyntaxError: invalid syntax
    >>>

It is good to choose meaningful variable names to help you |mdash| and anyone
who reads your Python code |mdash| to understand what your code is meant to do.
Python does not try to make sense of the names; it blindly follows your instructions,
and does not object if you do something confusing, such as ``one = "two"`` or ``two = 3``.

We can use variables to hold intermediate steps of a computation.  This may make
the Python code easier to follow.  Thus ``len(set(text1))`` could also be written:

    >>> vocab = set(text1)
    >>> vocab_size = len(vocab)
    >>> vocab_size
    19317
    >>>

You need to be a little bit careful in your choice of names (or `identifiers`:dt:) for Python
variables. Some of the things you might try will cause an
error. First, you should start the name with a letter, optionally
followed by digits (``0`` to ``9``) or letters. Thus, ``abc23`` is fine, but
``23abc`` will cause a syntax error. You can use underscores (both
within and at the start of the variable name), but not a hyphen, since
this gets interpreted as an arithmetic operator. A second problem is
shown in the following snippet.

Exercises
---------

#. |easy| Create a variable ``phrase`` containing a list of words.
   Experiment with the operations described above, including addition,
   multiplication, indexing, slicing, and sorting. 

#. |easy| The index of `the`:lx: in ``sent3`` is 1, because ``sent3[1]``
   gives us ``'the'``.  What are the indexes of the two other occurrences
   of this word in ``sent3``?

#. |easy| Our artificial sentence had 20 elements.  What does the interpreter
   do when you enter ``sent[20]``?  Why?

#. |easy| We can count backwards from the end of a list using negative indexes.
   The last element of a list always has index ``-1``.
   See what happens when you enter ``text2[-1]``.

#. |soso| Use ``text6.index(??)`` to find the index of the word `sunset`:lx:.
   By a process of trial and error, find the slice for the complete sentence that
   contains this word.

#. |soso| Use the addition, set, and sorted operations to compute the
   vocabulary of the sentences defined above (``sent1`` ...).

#. |soso| Write the slice expression to produces the last two
   words of ``text2``.

.. _computing-with-language-simple-statistics:

------------------------------------------
Computing with Language: Simple Statistics
------------------------------------------

Let's return to our exploration of the ways we can bring our computational
resources to bear on large quantities of text.  We began this discussion in
Section sect-computing-with-language-texts-and-words_, and we saw how to search for words
in context, how to compile the vocabulary of a text, how to generate random
text in the same style, and so on.

In this section we pick up the question of what makes a text distinct,
and use automatic methods to find characteristic words and collocations
of a text.  As in Section sect-computing-with-language-texts-and-words_, you will try
new features of the Python language by copying them into the interpreter,
and you'll learn about these features systematically in the following section.

Before continuing with this section, check your understanding of the
previous section by predicting the output of the following code, and using the
interpreter to check if you got it right.  If you found it difficult
to do this task, it would be a good idea to review the previous section
before continuing further.

.. doctest-ignore::
    >>> saying = ["After", "all", "is", "said", "and", "done", ",",
    ...           "more", "is", "said", "than", "done", "."]
    >>> words = set(saying)
    >>> words = sorted(words)
    >>> words[-2:]

.. _sec-frequency-distributions:

Frequency Distributions
-----------------------

How could we automatically identify the words of a text that are most
informative about the topic and genre of the text?  Let's begin by
finding the most frequent words of the text.  Imagine how you might
go about finding the 50 most frequent words of a book.  One method
would be to keep a tally for each vocabulary item, like that shown in Figure tally_.
We would need thousands of counters and it would be a laborious process,
so laborious that we would rather assign the task to a machine.

.. _tally:
.. figure:: ../images/tally.png
   :scale: 40

   Counting Words Appearing in a Text

The table in Figure tally_ is known as a `frequency distribution`:dt:,
and it tells us the frequency of each vocabulary item in the text.  It is a "distribution"
since it tells us how the the total number of words in the text |mdash| 260,819
in the case of *Moby Dick* |mdash| are distributed across the vocabulary items.
Since we often need frequency distributions in language processing, NLTK
provides built-in support for them.  Let's use a ``FreqDist`` to find the
50 most frequent words of *Moby Dick*.  Be sure to try this for yourself,
taking care to use the correct parentheses and uppercase letters.
(This code assumes that you have already done
``from nltk.book import *`` during your Python session.)

    >>> fdist1 = FreqDist(text1)
    >>> fdist1
    <FreqDist with 260819 samples>
    >>> vocabulary1 = fdist1.sorted()
    >>> vocabulary1[:50]
    [',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that', "'", '-',
    'his', 'it', 'I', 's', 'is', 'he', 'with', 'was', 'as', '"', 'all', 'for',
    'this', '!', 'at', 'by', 'but', 'not', '--', 'him', 'from', 'be', 'on',
    'so', 'whale', 'one', 'you', 'had', 'have', 'there', 'But', 'or', 'were',
    'now', 'which', '?', 'me', 'like']
    >>> fdist1["whale"]
    906
    >>>

Do any words in the above list help us grasp the topic or genre of this text?
Only one word, `whale`:lx:, is slightly informative!  It occurs over 900 times.
This list tells us almost nothing about the text; they just represent the
plumbing of English text.
What proportion of English text is taken up with such words?
We can generate a cumulative frequency plot for these words,
using ``fdist1.plot()``, to produce the graph shown in Figure fdist-moby_.
From this, it looks like these 50 words account for almost half the
words of the book!

.. _fdist-moby:
.. figure:: ../images/fdist-moby.png
   :scale: 25

   Cumulative Frequency Plot for 50 Most Frequent Words in *Moby Dick*

If the frequent words don't help us, how about the words that occur once
only, the so-called `hapaxes`:dt:.  See them using ``fdist1.hapaxes()``.
This list contains `lexicographer`:lx:, `cetological`:lx:,
`contraband`:lx:, `expostulations`:lx:, and about 9,000 others!
It seems that there's too many rare words, and without seeing the
context we probably can't guess what half of them mean in any case.

Next let's look at the *long* words of a text; perhaps these will be
more characteristic and informative.  For this we adapt some notation
from set theory.  We would like to find the words from the vocabulary
of the text that are more than than 15 characters long.  We can
express this in mathematical notation as follows:    

.. _ex-set-comprehension:

.. ex:: {`w`:math: | `w`:math: |element| `V`:math: & `P(w)`:math:\ },
   where `P(w)`:math: is true if and only if `w`:math: is more than 15 characters long.

.. older material: {(x,y) | x\ :superscript:`2` + y\ :superscript:`2` = 1}.

In other words, we want to find all `w`:math: such that `w`:math:
is in the vocabulary and `w`:math: is longer than 15 characters.
We can translate this expression into Python as follows:

    >>> v = set(text1)
    >>> sorted(w for w in v if len(w) > 15)
    ['apprehensiveness', 'comprehensiveness', 'indiscriminately',
    'superstitiousness', 'circumnavigating', 'simultaneousness',
    'physiognomically', 'circumnavigation', 'hermaphroditical',
    'subterraneousness', 'uninterpenetratingly', 'irresistibleness',
    'responsibilities', 'uncompromisedness', 'uncomfortableness',
    'supernaturalness', 'characteristically', 'cannibalistically',
    'circumnavigations', 'indispensableness', 'preternaturalness',
    'CIRCUMNAVIGATION', 'undiscriminating', 'Physiognomically']

The expression ``w for w in v`` could have equally been written
``word for word in vocab``, and means "give me all words, where each
word is an element of the vocabulary set".  For each such word,
we check that its length is greater than 15; all other words will
be ignored.  We will discuss this more carefully later.  For now
you should simply try out the above statements in the Python interpreter,
and try changing the text, and changing the length condition.

Let's return to our task of finding words that characterize a text.
Notice that the long words in ``text4`` reflect its national focus:
`constitutionally`:lx:, `transcontinental`:lx:, while
those in ``text5`` reflect its informal content:
`boooooooooooglyyyyyy`:lx: and `yuuuuuuuuuuuummmmmmmmmmmm`:lx:.
Have we succeeded in automatically extracting words that typify
a text?  Well, these very long words are often hapaxes (i.e. unique)
and perhaps it would be better to find *frequently occurring*
long words.  This seems promising since it eliminates
frequent short words (e.g. `the`:lx:) and infrequent long words
like (`antiphilosophists`:lx:).
Here are all words from the chat corpus
that are longer than 5 characters, that occur more than 5 times:

    >>> fdist5 = FreqDist(text5)
    >>> sorted(w for w in set(text5) if len(w) > 5 and text5.count(w) > 5)
    ['#14-19teens', '<empty>', 'ACTION', 'anybody', 'anyone', 'around',
    'cute.-ass', 'everybody', 'everyone', 'female', 'listening', 'minutes',
    'people', 'played', 'player', 'really', 'seconds', 'should', 'something',
    'watching']

Notice how we have used two conditions: ``len(w) > 5`` ensures that the
words are longer than 5 letters, and ``text5.count(w) > 5`` ensures that
these words occur more than five times.  At last we have managed to
automatically identify the frequently-occuring content-bearing
words of the text.

Collocations
------------

Frequency distributions are very powerful.  Here we briefly explore
a more advanced application that uses word pairs, also known as `bigrams`:dt:.
We can convert a list of words to a list of bigrams as follows:

    >>> bigrams(["more", "is", "said", "than", "done"])
    [('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]
    >>>

Here we see that the pair of words `than-done`:lx: is a bigram, and we write
it in Python as ``('than', 'done')``.  Now, collocations are essentially
just frequent bigrams, except that we want to pay more attention to the
cases that involve rare words.  In particular, we want to find
bigrams that occur more often than we would expect based on
the frequency of individual words.  The ``collocations()`` function
does this for us (we will see how it works later).

    >>> text4.collocations()
    United States; fellow citizens; has been; have been; those who;
    Declaration Independence; Old World; Indian tribes;
    District Columbia; four years; Chief Magistrate; and the;
    the world; years ago; Santo Domingo; Vice President;
    the people; for the; specie payments; Western Hemisphere


Statistics Over Secondary Data
------------------------------

[statistics over word lengths (multiple plots on one graph), eliminating words, normalizing words, freqdists over letters]

..
   IDLE session:
   indexing, max()
   set.difference()?

..
   zipf curve of letters

..
   Forward pointers:
   dictionaries



.. FreqDists in detail, [] notation

Exercises
---------

#. |soso| **The demise of teen language:**
   Read the BBC News article: *UK's Vicky Pollards 'left behind'* ``http://news.bbc.co.uk/1/hi/education/6173441.stm``.
   The article gives the following statistic about teen language:
   "the top 20 words used, including yeah, no, but and like, account for around a third of all words."
   How many word types account for a third
   of all word tokens, for a variety of text sources.  What do you conclude about this statistic?
   Read more about this on *LanguageLog*, at ``http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html``.


---------------------------------------------------
Back to Python: Making Decisions and Taking Control
---------------------------------------------------

So far, our simple programs have been able to manipulate sequences of
words, and perform some operation on each one.  We applied this to lists
consisting of a few words, but the approach works the same for lists of
arbitrary size, containing thousands of items.  Thus, such programs
have some interesting qualities: (i) the ability to work with
language, and (ii) the potential to save human effort through
automation.  Another useful feature of programs is their ability to
`make decisions`:em: on our behalf; this is our focus in this section.

Conditionals
------------

Python supports a wide range of operators like ``<`` and ``>=`` for
testing the relationship between values. The full set of these `relational
operators`:dt: are shown in Table inequalities_.

.. table:: inequalities

   ======== ==============
   Operator Relationship
   ======== ==============
   ``<``    less than
   ``<=``   less than or equal to
   ``==``   equal to (note this is two not one ``=`` sign)
   ``!=``   not equal to
   ``>``    greater than
   ``>=``   greater than or equal to
   ======== ==============

   Numerical Comparison Operators

We can use these to select different words from a sentence of news text.
Here are some examples |mdash| only the operator is changed from one
line to the next.

    >>> [w for w in sent7 if len(w) < 4]
    [',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']
    >>> [w for w in sent7 if len(w) <= 4]
    [',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']
    >>> [w for w in sent7 if len(w) == 4]
    ['will', 'join', 'Nov.']
    >>> [w for w in sent7 if len(w) != 4]
    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board',
    'as', 'a', 'nonexecutive', 'director', '29', '.']
    >>> 

The above expressions involve numerical comparisons.  We can also
test various properties of words, using the functions listed in
Table word-tests_.

.. table:: word-tests

   ====================   =============================================
   Function               Meaning
   ====================   =============================================
   ``s.startswith(t)``    ``s`` starts with ``t``
   ``s.endswith(t)``      ``s`` ends with ``t``
   ``t in s``             ``t`` is contained inside ``s``
   ``s.islower()``        all cased characters in ``s`` are lowercase
   ``s.isupper()``        all cased characters in ``s`` are uppercase
   ``s.isalpha()``        all characters in ``s`` are alphabetic
   ``s.isalnum()``        all characters in ``s`` are alphanumeric
   ``s.isdigit()``        all characters in ``s`` are digits
   ``s.istitle()``        ``s`` is titlecased
   ====================   =============================================

   Some Word Comparison Operators

Here are some examples of these operators being used to
select words from our texts.

    >>> sorted(w for w in set(text1) if w.endswith("ableness"))
    ['comfortableness', 'honourableness', 'immutableness',
    'indispensableness', 'indomitableness', 'intolerableness',
    'palpableness', 'reasonableness', 'uncomfortableness']
    >>> sorted(term for term in set(text4) if "gnt" in term)
    ['Sovereignty', 'sovereignties', 'sovereignty']
    >>> sorted(item for item in set(sent7) if item.isdigit())
    ['29', '61']
    >>>

We can also use ``and``, ``or``, and ``not``:    

    >>> sorted(w for w in set(text7) if "-" in w and "index" in w)
    ['Stock-index', 'index-arbitrage', 'index-fund',
    'index-options', 'index-related', 'stock-index']
    >>> sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10)
    ['Abelmizraim', 'Allonbachuth', 'Beerlahairoi', 'Canaanitish',
    'Chedorlaomer', 'Girgashites', 'Hazarmaveth', ...]
    >>> sorted(w for w in set(sent7) if not w.islower())
    [',', '.', '29', '61', 'Nov.', 'Pierre', 'Vinken']
    >>> sorted(t for t in set(text2) if "cie" in t or "cei" in t)
    ['ancient', 'ceiling', 'conceit', 'conceited', 'conceive', 'conscience',
    'conscientious', 'conscientiously', 'deceitful', 'deceive', ...]

Control Structures
------------------

Most programming languages permit us to execute a block of code when a
`conditional expression`:dt:, or ``if`` statement, is satisfied.  In
the following program, we have created a variable called ``word``
containing the string value ``'cat'``. The ``if`` statement then
checks whether the condition ``len(word) < 5`` is true.  Because the
conditional expression is true, the body of the ``if`` statement is
invoked and the ``print`` statement is executed, and displays a
message to the user.  You should indent the print statement by
typing four spaces.

    >>> word = "cat"
    >>> if len(word) < 5:
    ...     print 'word length is less than 5'
    ... 
    word length is less than 5
    >>>

When we use the Python interpreter we have to have an extra blank line
in order for it to detect that the nested block is complete.

If we change the conditional expression to ``len(word) >= 5``,
to check that the length of ``word`` is greater than or equal to ``5``,
then the conditional expression will no longer be true.
This time, the body of the ``if`` statement will not be executed,
and no message is shown to the user:

    >>> if len(word) >= 5:
    ...   print 'word length is greater than or equal to 5'
    ... 
    >>>

An ``if`` statement is known as a `control structure`:dt:
because it controls whether the code in the indented block will be run.
Another control structure is the ``for`` loop.  Don't forget
the colon and the four spaces:

    >>> for word in ['Call', 'me', 'Ishmael', '.']:
    ...     print word
    ...
    Call
    me
    Ishmael
    .
    >>>

This is called a loop because Python executes the code in
circular fashion.  It starts by doing ``word = 'Call'``,
effectively using the ``word`` variable to name the first
item of the list.  Then it displays the value of ``word``
to the user.  Next, it goes back to the ``for`` statement,
and does ``word = 'me'``, before displaying this new value
to the user, and so on.  It continues in this fashion until
every item of the list has been processed.

Now we can combine the ``if`` and ``for`` statements.
We will loop over every item of the list, and only print
the item if it ends with the letter "l".  We'll pick another
name for the variable to demonstrate that Python doesn't
try to make sense of variable names.

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>> for xyzzy in sent1:
    ...     if xyzzy.endswith("l"):
    ...         print xyzzy
    ...
    Call
    Ishmael
    >>>

You will notice that ``if`` and ``for`` statements
have a colon at the end of the line,
before the indentation begins. In fact, all Python
control structures end with a colon.  The colon
indicates that the current statement relates to the
indented block that follows.

We can also specify an action to be taken if
the condition of the ``if`` statement is not met.
Here we see the ``elif`` "else if" statement, and
the ``else`` statement.  Notice that these also have
colons before the indented code.

    >>> for token in sent1:
    ...     if token.islower():
    ...         print "lowercase word"
    ...     elif token.istitle():
    ...         print "titlecase word"
    ...     else:
    ...         print "punctuation"
    ...
    titlecase word
    lowercase word
    titlecase word
    punctuation
    >>>

As you can see, even with this small amount of Python knowledge,
you can start to build multi-line Python programs.
Its important to develop such programs in pieces,
testing that each piece does what you expect before
combining them into a program.  This is why the Python
interactive interpreter is so invaluable, and why you should get
comfortable using it.

Finally, let's combine the idioms we've been exploring.
First we create a list of `cie`:lx: and `cei`:lx: words,
then we loop over each item and print it.  Notice the
comma at the end of the print statement, which tells
Python to produce its output on a single line.

    >>> confusing = sorted(w for w in set(text2) if "cie" in w or "cei" in w)
    >>> for word in confusing:
    ...     print word,
    ancient ceiling conceit conceited conceive conscience
    conscientious conscientiously deceitful deceive ...

Frequency Distributions
-----------------------

Some of the methods defined on NLTK frequency distributions are shown in Table freqdist_.
[More discussion and examples...]

.. table:: freqdist

   ============================  =======================================================
   Example                       Description
   ============================  =======================================================
   ``fdist['monstrous']``        count of the number of times a given sample occurred
   ``fdist.freq('monstrous')``   frequency of a given sample
   ``fdist.N()``                 total number of samples
   ``fdist.sorted()``            the samples sorted in order of decreasing frequency
   ``for sample in fdist:``      iterate over the samples
   ``fdist.max()``               sample with the greatest count
   ``fdist.plot()``              graphical plot of the frequency distribution
   ============================  =======================================================

   Methods Defined in the Frequency Distribution Module



Exercises
---------

#. |easy| Assign a new value to ``sent``, namely the sentence
   ``["she", "sells", "sea", "shells", "by", "the", "sea", "shore"]``,
   then write code to perform the following tasks:

   a) Print all words beginning with ``'sh'``:

   b) Print all words longer than 4 characters.

   c) Generate a new sentence that adds the popular
      hedge word ``'like'`` before every word
      beginning with ``'se'``.

#. |soso| What does the following Python do?  ``sum(len(w) for w in text1)``
   Can you use it to work out the average word length of a text?

#. |soso| What is the difference between the test ``w.isupper()`` and
   ``not w.islower()``?

------------------------------------------
Computing with Language: Semantic Networks
------------------------------------------

    >>> len(N)
    117798
    >>> dish = N['dish']
    >>> len(dish)
    6
    >>> dish[0].gloss
    'a piece of dishware normally used as a container for holding or serving food;
    "we gave them a set of dishes for a wedding present"'
    >>> dish[1].gloss
    'a particular item of prepared food; "she prepared a special dish for dinner"'
    >>> dish[4].gloss
    'directional antenna consisting of a parabolic reflector for microwave or
    radio frequency radiation'
    

-------------------------------------------
Natural Language Processing: A Brief Survey
-------------------------------------------

As we have seen, |NLP| is important
for scientific, economic, social, and cultural reasons.  |NLP| is
experiencing rapid growth as its theories and methods are deployed in
a variety of new language technologies.  For this reason it is
important for a wide range of people to have a working knowledge of |NLP|.
Within industry, it includes people in
`human-computer interaction`:idx:, `business information analysis`:idx:,
and `Web software development`:idx:.
Within academia, this includes people in areas from
`humanities computing`:idx: and `corpus linguistics`:idx:
through to `computer science`:idx: and `artificial intelligence`:idx:.
We hope that you, a member of this diverse
audience reading these materials, will come to appreciate the workings
of this rapidly growing field of |NLP| and will apply its techniques in
the solution of real-world problems.  

* scope; NLP vs CL
* key components: tagging, parsing, word-sense disambiguation
* complete systems: machine translation, summarization, question answering, dialogue

..
   Although existing search engines have been crucial to the growth and
   popularity of the Web, humans require skill, knowledge, and some luck,
   to extract answers to such questions as `What tourist sites can I
   visit between Philadelphia and Pittsburgh on a limited budget?`:lx:
   `What do expert critics say about digital SLR cameras?`:lx: `What
   predictions about the steel market were made by credible commentators
   in the past week?`:lx: Getting a computer to answer them automatically
   is a realistic long-term goal, but would involve a range of language
   processing tasks, including information extraction, inference, and
   summarization, and would need to be carried out on a scale and with a
   level of robustness that is still beyond our current capabilities.

Exploiting Patterns to Understand Language
------------------------------------------

For example, in `word sense disambiguation`:dt: we want to work out
which sense of a word was intended in a given context.  Consider the
ambiguous words `serve`:lx: and `dish`:lx:\ :

.. ex::
    .. ex:: `serve`:lx:\ : help with food or drink; hold an office; put ball into play
    .. ex:: `dish`:lx:\ : plate; course of a meal; communications device

|nopar|
Now, in a sentence containing the phrase: `he served the dish`:lx: we
can detect that both `serve`:lx: and `dish`:lx: are being used with
their food meanings.  Its unlikely that the topic of discussion
shifted from sports to communications between these words, since it
would result in bizarre interpretations like striking a satellite dish
with a tennis racquet:

.. ex::
    In a fit of fury, he tossed the portable satellite receiver in the air, and with
    a quick flick of his tennis racquet, he served the dish.

In other words, we automatically disambiguate words using context, exploiting
the simple fact that nearby words have closely related meanings.
As another example of this contextual effect, consider the word
`by`:lx:, which has three meanings: `the book by Chesterton`:lx: (agentive);
`the cup by the stove`:lx: (locative); and `submit by Friday`:lx: (temporal).
Observe in lost-children_ that the meaning of the italicized word helps us
interpret the meaning of `by`:lx:.

.. _lost-children:
.. ex::
   .. ex:: The lost children were found by the `searchers`:em:  (agentive)
   .. ex:: The lost children were found by the `mountain`:em:   (locative)
   .. ex:: The lost children were found by the `afternoon`:em:  (temporal)

A deeper kind of language understanding is to work out who did what to whom |mdash|
i.e. to detect the subjects and objects of verbs.
In the sentence `the thieves stole the paintings`:lx:
it is easy to tell who performed the stealing action.
Consider three possible following sentences in thieves_, and try to determine
what was sold, caught, and found (one case is ambiguous).
  
.. _thieves:
.. ex::
   .. ex:: The thieves stole the paintings.  They were subsequently `sold`:em:.
   .. ex:: The thieves stole the paintings.  They were subsequently `caught`:em:.
   .. ex:: The thieves stole the paintings.  They were subsequently `found`:em:.

|nopar|
Answering this question involves finding the `antecedent`:dt: of the pronoun `they`:lx:
(the theives or the paintings).  Computational techniques for solving this problem
fall under the heading of `semantic role labeling`:dt:.
If we can automatically solve such problems, we will have understood enough of the
text to perform some important language generation tasks, such as
`question answering`:idx: and `machine translation`:idx:.  In the first case,
a machine should be able to answer a user's questions relating to collection of texts:

.. _qa-application:
.. ex::
   .. ex:: *Text:* ... The thieves stole the paintings.  They were subsequently sold. ...
   .. ex:: *Human:* Who or what was sold?
   .. ex:: *Machine:* The paintings.

|nopar|
The machine's answer demonstrates that it has correctly worked out that `they`:lx:
refers to paintings and not to theives.  In the second case, the machine should
be able to produce a translation of the text into another language, accurately
conveying the meaning of the original text.  In translating the above text into French,
we are forced to choose the gender of the pronoun in the second sentence:
`ils`:lx: (masculine) if the thieves are sold, and `elles`:lx: (feminine) if
the paintings are sold.  Correct translation actually depends on correct understanding of
the pronoun.

.. _mt-application:
.. ex::
   .. ex:: The thieves stole the paintings.  They were subsequently sold.
   .. ex:: Les voleurs ont vol\ |eacute| les peintures. *Ils* ont |eacute|\ t\ |eacute| *vendus* plus tard.  (the thieves)
   .. ex:: Les voleurs ont vol\ |eacute| les peintures. *Elles* ont |eacute|\ t\ |eacute| *vendues* plus tard.  (the paintings)
    
In all of the above examples |mdash| working out the sense of a word, the subject of a verb, the 
antecedent of a pronoun |mdash| are steps in establishing the meaning of a sentence, things
we would expect a language understanding system to be able to do.  Interestingly, all of them
turn out to involve `classification`:dt:, tagging a word with a sense tag, a verb-argument tag, etc,
a topic we will return to in Chapter chap-data-intensive_.


-------
Summary
-------

* Texts are represented in Python using lists:
  ``['colorless', 'green', 'ideas']``.  We can use indexing, slicing
  and the ``len()`` function on lists.
* We get the vocabulary of a text ``t`` using ``sorted(set(t))``. 
* We process each word in a text using a ``for`` statement such
  as ``for w in t:`` or ``for word in text:``.  This must be followed by the colon character
  and an indented block of code, to be executed each time through the loop.
* We test a condition using an ``if`` statement: ``if len(word) < 5``.
  This must be followed by the colon character and an indented block of
  code, to be executed only if the condition is true.
* a frequency distribution is a collection of items along with their frequency counts
  (e.g. the words of a text and their frequency of appearance).
* A text corpus is a balanced collection of texts.  NLTK comes with many corpora,
  e.g. the Brown Corpus, ``nltk.corpus.brown``.
* A dictionary is used to map between arbitrary types of information,
  such as a string and a number: ``freq['cat'] = 12``.  We create
  dictionaries using the brace notation: ``pos = {}``,
  ``pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}``.
* WordNet is a semantically-oriented dictionary of English, consisting of synonym sets |mdash| or synsets |mdash|
  and organized into a hierarchical network.

---------------
Further Reading
---------------

Two freely available online texts are the following:

* Josh Cogliati, *Non-Programmer's Tutorial for Python*,
  http://en.wikibooks.org/wiki/Non-Programmer's_Tutorial_for_Python/Contents

*  Allen B. Downey, Jeffrey Elkner and Chris Meyers, 
   *How to Think Like a Computer Scientist: Learning with Python*,
   http://www.ibiblio.org/obp/thinkCSpy/

*An Introduction to Python*
[vanRossum2006IP]_ is a Python tutorial by Guido van
Rossum, the inventor of Python and Fred L. Drake, Jr., the official
editor of the Python documentation. It is available online at
http://docs.python.org/tut/tut.html. A more detailed but still
introductory text is [Lutz2003LP]_, which covers the essential
features of Python, and also provides an overview of the standard libraries.

..
    A more advanced text, [vanRossum2006IPLR]_ is the official reference
    for the Python language itself, and describes the syntax of Python and
    its built-in datatypes in depth. It is also available online at
    http://docs.python.org/ref/ref.html.

[Beazley2006PER]_ is a succinct reference book; although not suitable
as an introduction to Python, it is an excellent resource for
intermediate and advanced programmers.

Finally, it is always worth checking the official *Python
Documentation* at http://docs.python.org/.


.. include:: footer.rst
