.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: add exercises for Unicode section?
.. TODO: add bullet points on regular expressions to summary
.. TODO: update cspy reference to more recent book
.. TODO: add pointers to regexp toolkits (e.g. Kodos)
.. TODO: adopt simpler hacker example with only single character transpositions;
   move hacker example to later section (later chapter?)

.. _chap-programming:

=================================================
1. Introduction to Language Processing and Python
=================================================

----------------------
The Language Challenge
----------------------

Today, people from all walks of life |mdash| including professionals, students,
and the general population |mdash| are confronted by
unprecedented volumes of information, the vast bulk of which is stored
as unstructured text. In 2003, it was estimated that the annual
production of books amounted to 8 Terabytes. (A Terabyte is 1,000
Gigabytes, i.e., equivalent to 1,000 pickup trucks filled with books.)
It would take a human being about five years to read the new
scientific material that is produced every 24 hours.  Although these
estimates are based on printed materials, increasingly the information
is also available electronically. Indeed, there has been an explosion of text
and multimedia content on the World Wide Web.  For many people, a
large and growing fraction of work and leisure time is spent
navigating and accessing this universe of information.

.. 
   http://www2.sims.berkeley.edu/courses/is202/f00/lectures/Lecture2.ppt
   William Hayes

The presence of so much text in electronic form is a huge challenge to
|NLP|. Arguably, the only way for humans to cope with the information
explosion is to exploit computational techniques that can sift
through huge bodies of text. 

Although existing search engines have been crucial to the growth and
popularity of the Web, humans require skill, knowledge, and some luck,
to extract answers to such questions as `What tourist sites can I
visit between Philadelphia and Pittsburgh on a limited budget?`:lx:
`What do expert critics say about digital SLR cameras?`:lx: `What
predictions about the steel market were made by credible commentators
in the past week?`:lx: Getting a computer to answer them automatically
is a realistic long-term goal, but would involve a range of language
processing tasks, including information extraction, inference, and
summarization, and would need to be carried out on a scale and with a
level of robustness that is still beyond our current capabilities.

The Richness of Language
------------------------

Language is the chief manifestation of human intelligence.  Through
language we express basic needs and lofty aspirations, technical
know-how and flights of fantasy.  Ideas are shared over great
separations of distance and time.  The following samples from English
illustrate the richness of language:


.. ex::

  .. ex:: Overhead the day drives level and grey, hiding the sun by a flight
          of grey spears.  (William Faulkner, *As I Lay Dying*, 1935)
  .. ex:: When using the toaster please ensure that the exhaust fan is turned
          on. (sign in dormitory kitchen)
  .. ex:: Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated
          activities with Ki values of 45.1-271.6 |mu|\M (Medline, PMID: 10718780)
  .. ex:: Iraqi Head Seeks Arms (spoof news headline)
  .. ex:: The earnest prayer of a righteous man has great power and wonderful
          results. (James 5:16b)
  .. ex:: Twas brillig, and the slithy toves did gyre and gimble in the wabe
         (Lewis Carroll, *Jabberwocky*, 1872)
  .. ex:: There are two ways to do this, AFAIK :smile:  (internet discussion archive)

Thanks to this richness, the study of language is part of many
disciplines outside of linguistics, including translation, literary
criticism, philosophy, anthropology and psychology.  Many less obvious
disciplines investigate language use, such as law, hermeneutics,
forensics, telephony, pedagogy, archaeology, cryptanalysis and speech
pathology.  Each applies distinct methodologies to gather
observations, develop theories and test hypotheses.  Yet all serve to
deepen our understanding of language and of the intellect that is
manifested in language.

The importance of language to science and the arts is matched in
significance by the cultural treasure embodied in language.
Each of the world's ~7,000 human languages is rich in unique respects,
in its oral histories and creation legends, down to its grammatical
constructions and its very words and their nuances of meaning.
Threatened remnant cultures have words to distinguish plant subspecies
according to therapeutic uses that are unknown to science.  Languages
evolve over time as they come into contact with each other and they
provide a unique window onto human pre-history.  Technological change
gives rise to new words like `blog`:lx: and new morphemes like `e-`:lx: and
`cyber-`:lx:.  In many parts of the world, small linguistic variations
from one town to the next add up to a completely different language in
the space of a half-hour drive.  For its breathtaking complexity and
diversity, human language is as a colorful tapestry stretching
through time and space.

The Promise of |NLP|
--------------------

As we have seen, |NLP| is important
for scientific, economic, social, and cultural reasons.  |NLP| is
experiencing rapid growth as its theories and methods are deployed in
a variety of new language technologies.  For this reason it is
important for a wide range of people to have a working knowledge of
|NLP|.
Within industry, it includes people in
`human-computer interaction`:idx:, `business information analysis`:idx:,
and `Web software development`:idx:.
Within academia, this includes people in areas from
`humanities computing`:idx: and `corpus linguistics`:idx:
through to `computer science`:idx: and `artificial intelligence`:idx:.
We hope that you, a member of this diverse
audience reading these materials, will come to appreciate the workings
of this rapidly growing field of |NLP| and will apply its techniques in
the solution of real-world problems.  

This book presents a
carefully-balanced selection of theoretical foundations and practical
applications, and equips readers to work with large datasets, to create
robust models of linguistic phenomena, and to deploy them in working
language technologies.  By integrating all of this into the Natural
Language Toolkit (|NLTK|), we hope this book opens up the exciting
endeavor of practical natural language processing to a broader
audience than ever before.

The rest of this chapter provides a non-technical overview of Python and will
cover the basic programming knowledge needed for the rest of
the chapters in Part 1.  It contains many examples and exercises;
there is no better way to learn to program than to dive in and try
these yourself.  Before you know it you will be programming!

The goal of this chapter is to answer the following questions:

#. what can we achieve by combining simple programming techniques with large quantities of text?
#. how can we automatically extract representative words from a large text?
#. is the Python programming language suitable for such work?

Along the way you will be introduced to a selection of elementary concepts
in linguistics and computer science.  However, this is deliberately not
systematic, but only a taster, intended to give you the flavour of what
will come later, and motivate you to work through the more systematic
material that will follow.

.. _sect-computing-with-language:

----------------------------------------
Computing with Language: Texts and Words
----------------------------------------

As we will see, it is easy to get our hands on large quantities of text.
What can we do with it, assuming we can write some simple programs?
Here we will treat the text as data for the programs we write,
programs that manipulate and analyze it in a variety of interesting ways.
The first step is to get started with the Python interpreter.

Getting Started
---------------

One of the friendly things about Python is that it allows you
to type directly into the interactive `interpreter`:dt: |mdash|
the program that will be running your Python programs.
You can run the Python interpreter using a simple graphical interface
called the Interactive DeveLopment Environment (|IDLE|).
On a Mac you can find this under ``Applications -> MacPython``,
and on Windows under ``All Programs -> Python``.
Under Unix you can run Python from the shell by typing ``python``.
The interpreter will print a blurb about your Python version;
simply check that you are running Python 2.4 or greater (here it is 2.5):

.. doctest-ignore::
    Python 2.5 (r25:51918, Sep 19 2006, 08:49:13) 
    [GCC 4.0.1 (Apple Computer, Inc. build 5341)] on darwin
    Type "help", "copyright", "credits" or "license" for more information.
    >>>

.. note::
   If you are unable to run the Python interpreter, you probably don't
   have Python installed correctly.  Please visit |NLTK-URL| for
   detailed instructions.

The ``>>>`` prompt indicates that the Python interpreter is now waiting
for input.  When copying examples from this book be sure not to type
in the ``>>>`` prompt yourself.  Now, let's begin by using Python as a calculator:

    >>> 1 + 5 * 2 - 3
    8
    >>>

Once the interpreter has finished calculating the answer and displaying it, the
prompt reappears. This means the Python interpreter is waiting for another instruction.

Try a few more expressions of your own. You can use asterisk (``*``)
for multiplication and slash (``/``) for division, and parentheses for
bracketing expressions. One strange thing you might come across is
that division doesn't always behave as you might expect; it does integer
division or floating point division depending on how you specify the inputs:

    >>> 3/3
    1
    >>> 1/3
    0
    >>> 1.0/3.0
    0.33333333333333331
    >>>

These examples demonstrate how you can work interactively with the
interpreter, allowing you to experiment and explore.
As you will see later, your intuitions about numerical expressions
will be useful for manipulating language data in Python.

Now let's try a nonsensical expression to see how the interpreter handles it:

    >>> 1 +
    Traceback (most recent call last):
      File "<stdin>", line 1
        1 +
          ^
    SyntaxError: invalid syntax
    >>>

.. The real output doesn't contain the "Traceback ..." line, but
   doctest doesn't recognize it as an exception without it.

Here we have produced a `syntax error`:dt:.  It doesn't make sense
to end an instruction with a plus sign. The Python interpreter indicates
the line where the problem occurred.

Searching Text
--------------

Now that we can use the Python interpreter, let's see how we can harness its
power to process text.  The first step is to type a line of magic at the
Python prompt, telling the interpreter to load some texts for us to explore:
``from nltk.book import *`` (i.e. import all names from NLTK's book module).
After printing a welcome message, it loads
the text of several books, including *Moby Dick*.  Type the following,
taking care to get spelling and punctuation exactly right:

    >>> from nltk.book import *
    >>> text1
    <Text: Moby Dick by Herman Melville 1851>
    >>> text2
    <Text: Sense and Sensibility by Jane Austen 1811>
    >>>

We can examine the contents of a text in a variety
of ways.  A concordance view shows us a given word in its context.  Here we
look up the word `monstrous`:lx:.  Try seaching for other words; you can use the up-arrow
key to access the previous command and modify the word being searched.

    >>> text1.concordance("monstrous")
    mong the former , one was of a most monstrous size . ... This came towards us , o
    ION OF THE PSALMS . " Touching that monstrous bulk of the whale or ork we have re
    all over with a heathenish array of monstrous clubs and spears . Some were thickl
    ed as you gazed , and wondered what monstrous cannibal and savage could ever have
     that has survived the flood ; most monstrous and most mountainous ! That Himmale
     they might scout at Moby Dick as a monstrous fable , or still worse and more det
    ath of Radney .'" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere lo
    ling Scenes . In connexion with the monstrous pictures of whales , I am strongly 
    >>>

You can now try concordance searches on some of the other texts we have included.
For example, search *Sense and Sensibility* for the word
`affection`:lx:, using ``text2.concordance("affection")``.  Search the book of Genesis
to find out how long some people lived, using:
``text3.concordance("lived")``.  You could look at ``text4``, the
*US Presidential Inaugural Addresses* to see examples of English dating
back to 1789, and search for words like `nation`:lx:, `terror`:lx:, `god`:lx:.
We've also included ``text5``, the *NPS Chat Corpus*: search this for
unconventional words like `im`:lx:, `ur`:lx:, `lol`:lx:.
(Note that this corpus is uncensored!)

Once you've spent some time examining these texts, we hope you have a new
sense of the richness and diversity of language.  In the next chapter
you will learn how to access a broader range of text, including text in
languages other than English.

If we can find words in a text, we can also take note of their position within
the text.  We produce a dispersion plot, where each bar represents an instance
of a word and each row represents the entire text.  In Figure fig-inaugural_ we
see some striking patterns of word usage over the last 220 years.  You can
produce this plot as shown below (so long as you have Numpy and Pylab installed).
You might like to try different words, and different texts.  As before, take
care to get the quotes, commas, brackets and parentheses exactly right.

    >>> text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])
    >>>

.. _fig-inaugural:
.. figure:: ../images/inaugural.png
   :scale: 100

   Lexical Dispersion Plot for Words in Presidential Inaugural Addresses

A concordance permits us to see words in context, e.g. we saw that
`monstrous`:lx: appeared in the context `the monstrous pictures`:lx:.
What other words appear in the same contexts that `monstrous`:lx:
appears in?  We can find out as follows:

    >>> text1.similar("monstrous")
    subtly impalpable curious abundant perilous trustworthy untoward
    singular imperial few maddens loving mystifying christian exasperate
    puzzled fearless uncommon domineering candid
    >>> text2.similar("monstrous")
    great very so good vast a exceedingly heartily amazingly as sweet
    remarkably extremely
    >>>

Observe that we get different results for different books.

Now, just for fun, let's try generating some random text in the various
styles we have just seen.  To do this, we type the name of the text
followed by the "generate" function, e.g. ``text3.generate()``:

    >>> text3.generate()
    In the beginning of his brother is a hairy man , whose top may reach
    unto heaven ; and ye shall sow the land of Egypt there was no bread in
    all that he was taken out of the month , upon the earth . So shall thy
    wages be ? And they made their father ; and Isaac was old , and kissed
    him : and Laban with his cattle in the midst of the hands of Esau thy
    first born , and Phichol the chief butler unto his son Isaac , she
    >>>

Note that first time you run this, it is slow because it gathers statistics
about word sequences.  Each time you run it, you will get different output text.
Now try generating random text in the style of an inaugural address or an
internet chat room.

.. note:: When text is printed, punctuation has been split off
   from the previous word.  Although this is not correct formatting
   for English text, we do this to make it clear that punctuation does
   not belong to the word.  This is called "tokenization", and you will learn
   about it in Chapter chap-words_.
   

Counting Vocabulary
-------------------

The most obvious fact about texts that emerges from the previous section is that
they differ in the vocabulary they use.  In this section we will see how to use the
computer to count the words in a text, in a variety of useful ways.
As before you will jump right in and experiment with
the Python interpreter, even though you may not have studied Python systematically
yet.

Let's begin by finding out the length of a text from start to finish,
in terms of the words and punctuation symbols that appear.  We'll use
the text of *Moby Dick* again:

    >>> len(text1)
    260819
    >>>

That's a quarter of a million words long!  But how many distinct words does this text
contain?  To work this out in Python we have to pose the question slightly
differently.  The vocabulary of a text is just the *set* of words that it uses,
and in Python we can list the vocabulary of ``text3`` with the command: ``set(text3)``.
This will produce many screens of words.  Now try the following:

    >>> sorted(set(text3))
    ['!', "'", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)',
    'A', 'Abel', 'Abelmizraim', 'Abidah', 'Abide', 'Abimael', 'Abimelech',
    'Abr', 'Abrah', 'Abraham', 'Abram', 'Accad', 'Achbor', 'Adah', ...]
    >>> len(set(text3))
    2789
    >>> len(text3) / len(set(text3))
    16
    >>>

Here we can see a sorted list of vocabulary items, beginning with various
punctuation symbols and continuing with words starting with `A`:lx:.  Words
starting with `a`:lx: will appear much later, after the last "Z" word, `Zoroaster`:lx:.
We discover the size of the vocabulary indirectly, by asking
for the length of the set.  Finally, we can calculate a measure of the lexical
richness of the text and learn that each word is used 16 times on average.

Next, let's focus in on particular words.  We can count how often a word occurs
in a text, and compute what percentage
of the text is taken up by a specific word:

    >>> text3.count("smote")
    5
    >>> 100.0 * text4.count('a') / len(text4)
    1.4587672822333748
    >>>

You might like to repeat such calculations on several texts,
but it is tedious to keep retyping it for different texts.  Instead,
we can come up with our own name for this task, e.g. "score", and
define a `function`:dt: that can be re-used as often as we like:

    >>> def score(text):
    ...     return len(text) / len(set(text))
    ...
    >>> score(text3)
    16
    >>> score(text4)
    4
    >>>

.. note:: The Python interpreter changes the prompt from
   ``>>>`` to ``...`` after encountering the colon at the
   end of the first line.  The ``...`` prompt indicates
   that Python expects an indented code block to appear next.
   It is up to you to do the indentation, by typing four
   spaces.  To finish the indented block just enter a blank line.

Notice that we used the ``score`` function by typing its name, followed
by an open parenthesis, the name of the text, then a close parenthesis.
This is just what we did for the ``len`` and ``set`` functions earlier.
These parentheses will show up often: their role is to separate
the name of a task |mdash| such as ``score`` |mdash| from the data
that the task is to be performed on |mdash| such as ``text3``.
Functions are an advanced concept in programming and we only
mention them at the outset to give newcomers a sense of the
power and flexibility of programming.  We'll come back to them
towards the end of this chapter.

Exercises
---------

#. |easy| How many words are there in ``text2``?  How many
   distinct words are there?
   
#. |easy| Produce a dispersion plot of the four main protagonists in
   *Sense and Sensibility*: Elinor, Marianne, Edward, Willoughby.
   What can you observe about the different roles played by the males
   and females in this novel?  Can you identify the couples?

#. |easy| According to Strunk and White's *Elements of Style*,
   the word `however`:lx:, used at the start of a sentence,
   means "in whatever way" or "to whatever extent", and not
   "nevertheless".  They give this example of correct usage:
   `However you advise him, he will probably do as he thinks best.`:lx:
   (http://www.bartleby.com/141/strunk3.html)
   Use the concordance tool to study actual usage of this word
   in the various texts we have been considering.

#. |soso| Consider the following Python expression: ``len(set(text4))``.
   State the purpose of this expression.  Describe the two steps
   involved in performing this computation.

#. |soso| How many times does the word `lol`:lx: appear in ``text5``?
   How much is this as a percentage of the total number of words
   in this text? 

#. |soso| Pick a pair of texts and study the differences between them,
   in terms of vocabulary, vocabulary richness, genre, etc.  Can you
   find pairs of words which have quite different meanings across the
   two texts, such as `monstrous`:lx: in *Moby Dick* and in *Sense and Sensibility*?

#. |soso| Compare the frequency of use of the modal verbs `will`:lx: and
   `could`:lx: in ``text2`` (romance fiction) and ``text7`` (news).
   Which modal verb is more common in which genre? 

We will leave our quest for characteristic words of a text,
and explore a rather different approach that uses the
*ratio* of word frequencies.


-----------------------
A Closer Look at Python
-----------------------

You've seen some important building blocks of the Python programming language.
Here we take a break from language processing to take a closer look at Python.

Lists
-----

What is a text?  At one level, it is a sequence of symbols on a page, such
as this one.  At another level, it is a sequence of chapters, made up
of a sequence of sections, where each section is a sequence of paragraphs,
and so on.  However, for our purposes, we will think of a text as nothing
more than a sequence of words and punctuation.  Here's how we represent
text in Python, in this case the opening sentence of *Moby Dick*:

    >>> sent1 = ["Call", "me", "Ishmael", "."]
    >>>

After the prompt we've given a name we made up, ``sent1``, followed
by the equals sign, and then some quoted words, separated with
commas, and surrounded with brackets.  This bracketed material
is known as a `list`:dt: in Python: it is how we store a text.
Each individual word must be quoted, using double or single quotes,
like ``"this"`` or like ``'this'``.
(When using single quotes, use the *close* quote character at the start and the end.)
Here, we've given this list the name ``sent1``.  We can inspect
it by typing the name, and we can ask for its length:

    >>> sent1
    ['Call', 'me', 'Ishmael', '.']
    >>> len(sent1)
    4
    >>> score(sent1)
    1
    >>>

We can even apply our own "score" function to it.
Some more lists have been defined for you,
one for the opening sentence of each of our texts,
``sent2`` |ldots| ``sent8``.  We inspect two of them
here; you can see the rest for yourself using the Python interpreter.

    >>> sent2
    ["The", "family", "of", "Dashwood", "had", "long",
    "been", "settled", "in", "Sussex", "."]
    >>> sent3
    ["In", "the", "beginning", "God", "created", "the",
    "heaven", "and", "the", "earth", "."]

You can type these in or else make up a few sentences of your own.
Now let's repeat some of the other Python operations we saw above in
Section sect-computing-with-language_:

    >>> sorted(sent3)
    ['.', 'God', 'In', 'and', 'beginning', 'created', 'earth',
    'heaven', 'the', 'the', 'the']
    >>> len(set(sent3))
    9
    >>> sent3.count("the")
    3
    >>>

We can also do arithmetic operations with lists in Python.
Multiplying a list by a number, e.g. ``sent1 * 2``,
creates a longer list containing multiple
copies of the items in the original list.  Adding two
lists, e.g. ``sent4 + sent1``, creates a new list
containing everything from the first list, followed
by everything from the second list:

    >>> sent1 * 2
    ['Call', 'me', 'Ishmael', '.', 'Call', 'me', 'Ishmael', '.']
    >>> sent4 + sent1
    ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the',
    'House', 'of', 'Representatives', ':', 'Call', 'me', 'Ishmael', '.']
    >>>

This special use of the addition operation is called `concatenation`:dt:;
it links the lists together into a single list.  We can concatenate
sentences to build up a text.

Indexing
--------

As we have seen, a text in Python is just a list of words, represented
using a particular combination of brackets and quotes.  Just as with an
ordinary page of text, we can count up the total number of words
(``len(text1)``), and count the occurrences of a particular word
(``text1.count("heaven")).  And just as we can pick out the
first, tenth, or even 14,278th word in a printed text, we can identify
the elements of a list by their number, or `index`:dt:, by following
the name of the text with the index inside brackets.  We can
also find the index of the first occurrence of any word:

    >>> text4[173]
    'awaken'
    >>> text4.index("awaken")
    173
    >>>

Indexes turn out to be a common way to access the words of a text,
or |mdash| more generally |mdash| the elements of a list.
Python permits us to access sublists as well, extracting
manageable pieces of language from large texts, a technique
known as `slicing`:dt:.

    >>> text5[1040:1060]
    ['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is',
    'so', 'good', 'because', 'you', 'can', 'actually', 'play',
    'a', 'full', 'game', 'without', 'buying', 'it']
    
Indexes have some subtleties, and we'll explore these with
the help of an artificial sentence:

    >>> sent = ["word1", "word2", "word3", "word4", "word5",
                "word6", "word7", "word8", "word9", "word10",
                "word11", "word12", "word13", "word14", "word15",
                "word16", "word17", "word18", "word19", "word20"]
    >>> sent[0]
    'word1'
    >>> sent[19]
    'word20'
    >>>

Notice that our indexes start from zero:
``sent`` element zero, written ``sent[0]``,
is the first word, ``'word1'``, while
``sent`` element 19 is ``'word20'``. 
This is initially confusing,
but typical of modern programming languages.
(If you've mastered the system of counting
centuries where 19XY is a year in the 20th century,
or if you live in a country where walking up
1 flight of stairs puts you on level 2
of a building, you'll quickly get the hang of this.)
The moment Python accesses the content of a list from
the computer's memory, it is already at the first element;
we have to tell it how many elements forward to go.

Let's take a closer look at slicing, using our artificial sentence again:

    >>> sent[17:20]
    ['word18', 'word19', 'word20']
    >>> sent[17]
    'word18'
    >>> sent[18]
    'word19'
    >>> sent[19]
    'word20'
    >>>

Thus, the slice ``17:20`` includes ``sent`` elements 17, 18, and 19.
By convention, ``m:n`` means elements `m`:mathit:\ |dots|\ `n-1`:mathit:.
We can omit the first number if the slice begins at the start of the
list, and we can omit the second number if the slice goes to the end:

    >>> sent[:3]
    ['word1', 'word2', 'word3']
    >>> text2[141525:]
    ['among', 'the', 'merits', 'and', 'the', 'happiness', 'of', 'Elinor',
    'and', 'Marianne', ',', 'let', 'it', 'not', 'be', 'ranked', 'as', 'the',
    'least', 'considerable', ',', 'that', 'though', 'sisters', ',', 'and',
    'living', 'almost', 'within', 'sight', 'of', 'each', 'other', ',',
    'they', 'could', 'live', 'without', 'disagreement', 'between', 'themselves',
    ',', 'or', 'producing', 'coolness', 'between', 'their', 'husbands', '.',
    'THE', 'END']
    >>>

We can modify an element of a list by assigning to one of its index values,
e.g. putting ``sent[0]`` on the left of the equals sign.  We can also
replace an entire slice with new material:

    >>> sent[0] = "First Word"
    >>> sent[19] = "Last Word"
    >>> sent[1:19] = ["Second Word", "Third Word"]
    >>> sent
    ['First Word', 'Second Word', 'Third Word', 'Last Word']
    >>>


Variables
---------

From the start of Section sect-computing-with-language_, you have had
access texts called ``text1``, ``text2``, and so on.  It saved a lot
of typing to be able to refer to a 250,000-word book with a short name
like this!  In general, we can make up names for anything we care
to calculate.  We did this ourselves in the previous sections, e.g.
defining a `variable`:dt: ``sent1`` as follows:

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>>

Such lines have the form: *variable = expression*.  Python will evaluate
the expression, and save its result to the variable.  This process does
not generate any output; you have to type the variable on a line of its
own to inspect its contents.  The equals sign is slightly misleading,
since information is copied from the right side to the left.
The variable can be anything you like, e.g. ``my_sent``, ``sentence``, ``xyzzy``.
It must start with a letter, and can include numbers and underscores.
It cannot be any of Python's reserved words, such as ``if``, ``not``,
and ``import``.  Here are some examples:

    >>> mySent = ["The", "family", "of", "Dashwood", "had", "long",
    ...          "been", "settled", "in", "Sussex", "."]
    >>> noun_phrase = mySent[:4]
    >>> noun_phrase
    ['The', 'family', 'of', 'Dashwood']
    >>> wOrDs = sorted(noun_phrase)
    >>> wOrDs
    ['Dashwood', 'The', 'family', 'of']
    >>>

It is good to choose meaningful variable names to help you |mdash| and anyone
who reads your Python code |mdash| to understand what your code is meant to do.
Python does not try to make sense of the names; it blindly follows your instructions,
and does not object if you do something confusing, such as ``one = "two"`` or ``two = 3``.

We can use variables to hold intermediate steps of a computation.  This may make
the Python code easier to follow.  Thus ``len(set(text1))`` could also be written:

    >>> vocab = set(text1)
    >>> vocab_size = len(vocab)
    >>> vocab_size
    19317
    >>>

Exercises
---------

#. |easy| Create a variable ``phrase`` containing a list of words.
   Experiment with the operations described above, including addition,
   multiplication, indexing, slicing, and sorting. 

#. |easy| The index of `the`:lx in ``sent3`` is 1, because ``sent3[1]``
   gives us ``'the'``.  What are the indexes of the two other occurrences
   of this word in ``sent3``?

#. |easy| Our artificial sentence had 20 elements.  What does the interpreter
   do when you enter ``sent[20]``?  Why?

#. |easy| We can count backwards from the end of a list using negative indexes.
   The last element of a list always has index ``-1``.
   See what happens when you enter ``text2[-1]``.

#. |soso| Use ``text6.index(??)`` to find the index of the word `sunset`:lx:.
   By a process of trial and error, find the slice for the complete sentence that
   contains this word.

#. |soso| Use the addition, set, and sorted operations to compute the
   vocabulary of the sentences defined above (``sent1`` ...).

#. |soso| Write the slice expression to produces the last two
   words of ``text2``.


------------------------------------------
Computing with Language: Simple Statistics
------------------------------------------

Let's return to our exploration of the ways we can bring our computational
resources to bear on large quantities of text.  We began this discussion in
Section sect-computing-with-language_, and we saw how to search for words
in context, how to compile the vocabulary of a text, how to generate random
text in the same style, and so on.

In this section we pick up the question of what makes a text distinct,
and use automatic methods to find characteristic words and collocations
of a text.  As in Section sect-computing-with-language_, you will try
new features of the Python language by copying them into the interpreter,
and you'll learn about these features systematically in the following section.

Before continuing with this section, check your understanding of the
previous section by predicting the output of the following code, and using the
interpreter to check if you got it right.  If you found it difficult
to do this task, it would be a good idea to review the previous section
before continuing further.

.. doctest-ignore::

    >>> saying = ["After", "all", "is", "said", "and", "done", ",",
    ...           "more", "is", "said", "than", "done", "."]
    >>> words = set(saying)
    >>> words = sorted(words)
    >>> words[-2:]

Frequency Distributions
-----------------------

How could we automatically identify the words of a text that are most
informative about the topic and genre of the text?  Let's begin by
finding the most frequent words of the text.  Imagine how you might
go about finding the 50 most frequent words of a book.  One method
would be to keep a tally for each vocabulary item, like that shown in Figure tally_.
We would need thousands of counters and it would be a laborious process,
so laborious that we would rather assign the task to a machine.

.. _tally:
.. figure:: ../images/tally.png
   :scale: 100

   Counting Words Appearing in a Text

The table in Figure tally_ is known as a `frequency distribution`:dt:,
and it tells us the frequency of each vocabulary item in the text.  It is a "distribution"
since it tells us how the the total number of words in the text |mdash| 260,819
in the case of *Moby Dick* |mdash| are distributed across the vocabulary items.
Since we often need frequency distributions in language processing, NLTK
provides built-in support for them.  Let's use a ``FreqDist`` to find the
50 most frequent words of *Moby Dick*.  Be sure to try this for yourself,
taking care to use the correct parentheses and uppercase letters.
(This code assumes that you have already done
``from nltk.book import *`` during your Python session.)

    >>> fdist1 = FreqDist(text1)
    >>> fdist1
    <FreqDist with 260819 samples>
    >>> vocabulary1 = fdist1.sorted()
    >>> vocabulary1[:50]
    [',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that', "'", '-',
    'his', 'it', 'I', 's', 'is', 'he', 'with', 'was', 'as', '"', 'all', 'for',
    'this', '!', 'at', 'by', 'but', 'not', '--', 'him', 'from', 'be', 'on',
    'so', 'whale', 'one', 'you', 'had', 'have', 'there', 'But', 'or', 'were',
    'now', 'which', '?', 'me', 'like']
    >>> fdist1["whale"]
    906
    >>>

Do any words in the above list help us grasp the topic or genre of this text?
Only one word, `whale`:lx:, is slightly informative!  It occurs over 900 times.
This list tells us almost nothing about the text, but a lot about English.
What proportion of English text is taken up with such words?
We can generate a cumulative frequency plot for these words,
using ``fdist1.plot()``, to produce the graph shown in Figure fdist-moby_.
From this, it looks like these 50 words account for almost half the
words of the book!

.. _fdist-moby:
.. figure:: ../images/fdist-moby.png
   :scale: 25

   Cumulative Frequency Plot for 50 Most Frequent Words in *Moby Dick*

If the frequent words don't help us, how about the words that occur once
only, the so-called `hapaxes`:dt:.  See them using ``fdist1.hapaxes()``.
This list contains `lexicographer`:lx:, `cetological`:lx:,
`contraband`:lx:, `expostulations`:lx:, and about 9,000 others!
It seems that there's too many rare words, and without seeing the
context we probably can't guess what half of them mean in any case.

Next let's look at the *long* words of a text; perhaps these will be
more characteristic and informative.  For this we adapt some notation
from set theory.  We would like to find the words from the vocabulary
of the text that are more than than 15 characters long.  We can
express this in mathematical notation as follows:    

.. _ex-set-comprehension:

.. ex:: {`w`:math: | `w`:math: |element| `V`:math: & `P(w)`:math:\ },
   where `P(w)`:math: is true if and only if `w`:math: is more than 15 characters long.

In other words, we want to find all `w`:math: such that `w`:math:
is in the vocabulary and `w`:math: is longer than 15 characters.
We can translate this expression into Python as follows:

    >>> v = set(text1)
    >>> sorted(w for w in v if len(w) > 15)
    ['apprehensiveness', 'comprehensiveness', 'indiscriminately',
    'superstitiousness', 'circumnavigating', 'simultaneousness',
    'physiognomically', 'circumnavigation', 'hermaphroditical',
    'subterraneousness', 'uninterpenetratingly', 'irresistibleness',
    'responsibilities', 'uncompromisedness', 'uncomfortableness',
    'supernaturalness', 'characteristically', 'cannibalistically',
    'circumnavigations', 'indispensableness', 'preternaturalness',
    'CIRCUMNAVIGATION', 'undiscriminating', 'Physiognomically']

The expression ``w for w in v`` could have equally been written
``word for word in vocab``, and means "give me all words, where each
word is an element of the vocabulary set".  For each such word,
we check that its length is greater than 15; all other words will
be ignored.  We will discuss this more carefully later.  For now
you should simply try out the above statements in the Python interpreter,
and try changing the text, and changing the length condition.

Let's return to our task of finding words that characterize a text.
Notice that the long words in ``text4`` reflect its national focus:
`constitutionally`:lx:, `transcontinental`:lx:, while
those in ``text5`` reflect its informal content:
`boooooooooooglyyyyyy`:lx: and `yuuuuuuuuuuuummmmmmmmmmmm`:lx:.
Have we succeeded in automatically extracting words that typify
a text?  Well, these very long words are often hapaxes (i.e. unique)
and perhaps it would be better to find *frequently occurring*
long words.  This seems promising since it eliminates
frequent short words (e.g. `the`:lx:) and infrequent long words
like (`antiphilosophists`:lx:).
Here are all words from the chat corpus
that are longer than 5 characters, that occur more than 5 times:

    >>> fdist5 = FreqDist(text5)
    >>> sorted(w for w in set(text5) if len(w) > 5 and text5.count(w) > 5)
    ['#14-19teens', '<empty>', 'ACTION', 'anybody', 'anyone', 'around',
    'cute.-ass', 'everybody', 'everyone', 'female', 'listening', 'minutes',
    'people', 'played', 'player', 'really', 'seconds', 'should', 'something',
    'watching']

Notice how we have used two conditions: ``len(w) > 5`` ensures that the
words are longer than 5 letters, and ``text5.count(w) > 5`` ensures that
these words occur more than five times.  At last we have managed to
automatically identify the frequently-occuring content-bearing
words of the text.

Collocations
------------

Frequency distributions are very powerful.  Here we briefly explore
a more advanced application that uses word pairs, also known as `bigrams`:dt:.
We can convert a list of words to a list of bigrams as follows:

    >>> bigrams(["more", "is", "said", "than", "done"])
    [('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]
    >>>

Here we see that the pair of words `than-done`:lx: is a bigram, and we write
it in Python as ``('than', 'done')``.  Now, collocations are essentially
just frequent bigrams, except that we want to pay more attention to the
cases that involve rare words.  In particular, we want to find
bigrams that occur more often than we would expect based on
the frequency of individual words.  The ``collocations()`` function
does this for us (we will see how it works later).

    >>> text4.collocations()
    United States; fellow citizens; has been; have been; those who;
    Declaration Independence; Old World; Indian tribes;
    District Columbia; four years; Chief Magistrate; and the;
    the world; years ago; Santo Domingo; Vice President;
    the people; for the; specie payments; Western Hemisphere


..
   Statistics Over Secondary Data
   ------------------------------

   word lengths (multiple plots on one graph)

   eliminating words, normalizing words

..
   IDLE session:
   indexing, max()
   set.difference()?
   
..
   zipf curve of letters

..
   Forward pointers:
   dictionaries

 
 
.. FreqDists in detail, [] notation
   

--------------
Back to Python
--------------


So far, our simple programs have been able to manipulate sequences of
words, and perform some operation on each one.  We applied this to lists
consisting of a few words, but the approach works the same for lists of
arbitrary size, containing thousands of items.  Thus, such programs
have some interesting qualities: (i) the ability to work with
language, and (ii) the potential to save human effort through
automation.  Another useful feature of programs is their ability to
`make decisions`:em: on our behalf; this is our focus in this section.

Conditionals
------------

Python supports a wide range of operators like ``<`` and ``>=`` for
testing the relationship between values. The full set of these `relational
operators`:dt: are shown in Table inequalities_.

.. _inequalities:

   ======== ==============
   Operator Relationship
   ======== ==============
   ``<``    less than
   ``<=``   less than or equal to
   ``==``   equal to (note this is two not one ``=`` sign)
   ``!=``   not equal to
   ``>``    greater than
   ``>=``   greater than or equal to
   ======== ==============

   Numerical Comparison Operators

We can use these to select different words from a sentence of news text.
Here are some examples |mdash| only the operator is changed from one
line to the next.

    >>> [w for w in sent7 if len(w) < 4]
    [',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']
    >>> [w for w in sent7 if len(w) <= 4]
    [',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']
    >>> [w for w in sent7 if len(w) == 4]
    ['will', 'join', 'Nov.']
    >>> [w for w in sent7 if len(w) != 4]
    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board',
    'as', 'a', 'nonexecutive', 'director', '29', '.']
    >>> 

The above expressions involve numerical comparisons.  We can also
test various properties of words, using the functions listed in
Table word-tests_.

.. _word-tests:

   ====================   =============================================
   Function               Meaning
   ====================   =============================================
   ``s.startswith(t)``    ``s`` starts with ``t``
   ``s.endswith(t)``      ``s`` ends with ``t``
   ``t in s``             ``t`` is contained inside ``s``
   ``s.islower()``        all cased characters in ``s`` are lowercase
   ``s.isupper()``        all cased characters in ``s`` are uppercase
   ``s.isalpha()``        all characters in ``s`` are alphabetic
   ``s.isalnum()``        all characters in ``s`` are alphanumeric
   ``s.isdigit()``        all characters in ``s`` are digits
   ``s.istitle()``        ``s`` is titlecased
   ====================   =============================================

   Some Word Comparison Operators

Here are some examples of these operators being used to
select words from our texts.

    >>> sorted(w for w in set(text1) if w.endswith("ableness"))
    ['comfortableness', 'honourableness', 'immutableness',
    'indispensableness', 'indomitableness', 'intolerableness',
    'palpableness', 'reasonableness', 'uncomfortableness']
    >>> sorted(w for w in set(text4) if "gnt" in w)
    ['Sovereignty', 'sovereignties', 'sovereignty']
    >>> sorted(w for w in set(sent7) if w.isdigit())
    ['29', '61']
    >>>

We can also use ``and``, ``or``, and ``not``:    

    >>> sorted(w for w in set(text7) if "-" in w and "index" in w)
    ['Stock-index', 'index-arbitrage', 'index-fund',
    'index-options', 'index-related', 'stock-index']
    >>> sorted(w for w in set(text3) if w.istitle() and len(w) > 10)
    ['Abelmizraim', 'Allonbachuth', 'Beerlahairoi', 'Canaanitish',
    'Chedorlaomer', 'Girgashites', 'Hazarmaveth', ...]
    >>> sorted(w for w in set(sent7) if not w.islower())
    [',', '.', '29', '61', 'Nov.', 'Pierre', 'Vinken']
    >>> sorted(w for w in set(text2) if "cie" in w or "cei" in w)
    ['ancient', 'ceiling', 'conceit', 'conceited', 'conceive', 'conscience',
    'conscientious', 'conscientiously', 'deceitful', 'deceive', ...]

Control Structures
------------------

Most programming languages permit us to execute a block of code when a
`conditional expression`:dt:, or ``if`` statement, is satisfied.  In
the following program, we have created a variable called ``word``
containing the string value ``'cat'``. The ``if`` statement then
checks whether the condition ``len(word) < 5`` is true.  Because the
conditional expression is true, the body of the ``if`` statement is
invoked and the ``print`` statement is executed, and displays a
message to the user.

    >>> word = "cat"
    >>> if len(word) < 5:
    ...     print 'word length is less than 5'
    ... 
    word length is less than 5
    >>>

When we use the Python interpreter we have to have an extra blank line
in order for it to detect that the nested block is complete.

If we change the conditional expression to ``len(word) >= 5``,
to check that the length of ``word`` is greater than or equal to ``5``,
then the conditional expression will no longer be true.
This time, the body of the ``if`` statement will not be executed,
and no message is shown to the user:

    >>> if len(word) >= 5:
    ...   print 'word length is greater than or equal to 5'
    ... 
    >>>

An ``if`` statement is known as a `control structure`:dt:
because it controls whether the code in the indented block will be run.
Another control structure is the ``for`` loop:

    >>> for word in ['Call', 'me', 'Ishmael', '.']:
    ...     print word
    ...
    Call
    me
    Ishmael
    .
    >>>

This is called a loop because Python executes the code in
circular fashion.  It starts by doing ``word = 'Call'``,
effectively using the ``word`` variable to name the first
item of the list.  Then it displays the value of ``word``
to the user.  Next, it moves on to the second item of the
list, and so on.  It stops once every item of the list has
been processed.

Now we can combine the ``if`` and ``for`` statements.
We will loop over every item of the list, and only print
the item if it ends with the letter "l".  We'll pick another
name for the variable to demonstrate that Python doesn't
try to make sense of variable names.

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>> for xyzzy in sent1:
    ...     if xyzzy.endswith("l"):
    ...         print xyzzy
    ...
    Call
    Ishmael
    >>>

You will notice that ``if`` and ``for`` statements
have a colon at the end of the line,
before the indentation begins. In fact, all Python
control structures end with a colon.  The colon
indicates that the current statement relates to the
indented block that follows.

We can also specify an action to be taken if
the condition of the ``if`` statement is not met.
Here we see the ``elif`` "else if" statement, and
the ``else`` statement.  Notice that these also have
colons before the indented code.

    >>> for token in sent1:
    ...     if token.islower():
    ...         print "lowercase word"
    ...     elif token.istitle():
    ...         print "titlecase word"
    ...     else:
    ...         print "punctuation"
    ...
    titlecase word
    lowercase word
    titlecase word
    punctuation
    >>>

As you can see, even with this small amount of Python knowledge,
you can start to build multi-line Python programs.
Its important to develop such programs in pieces,
testing that each piece does what you expect before
combining them into a program.  This is why the Python
interactive interpreter is so invaluable, and why you should get
comfortable using it.

Finally, let's combine the idioms we've been exploring.
First we create a list of `cie`:lx: and `cei`:lx: words,
then we loop over each item and print it.  Notice the
comma at the end of the print statement, which tells
Python to produce its output on a single line.

    >>> confusing = sorted(w for w in set(text2) if "cie" in w or "cei" in w)
    >>> for word in confusing:
    ...     print word,
    ancient ceiling conceit conceited conceive conscience
    conscientious conscientiously deceitful deceive ...
    

Exercises
---------

#. |soso| What does the following Python do?  ``sum(len(w) for w in text1)``
   Can you use it to work out the average word length of a text?

#. |soso| What is the difference between the test ``w.isupper()`` and
   ``not w.islower()``?

#. |easy| Assign a new value to ``sent``, namely the sentence
   ``["she", "sells", "sea", "shells", "by", "the", "sea", "shore"]``,
   then write code to perform the following tasks:

   a) Print all words beginning with ``'sh'``:

   b) Print all words longer than 4 characters.

   c) Generate a new sentence that adds the popular
      hedge word ``'like'`` before every word
      beginning with ``'se'``.


.. _sec-extracting-text-from-corpora:

----------------------
Accessing NLTK Corpora
----------------------

|NLTK| is distributed with several corpora and corpus samples and many
are supported by the ``corpus`` package.  Here we use
a selection of texts from the `Project Gutenberg
<http://www.gutenberg.org/>`_ electronic text archive, and list the
files it contains.  (You may need to ``import nltk`` first.)

    >>> nltk.corpus.gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'blake-songs.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',
    'chesterton-thursday.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')
    >>>

We can use this information to access individual texts, e.g.:

    >>> emma = nltk.corpus.gutenberg.words("austen-emma.txt")
    >>> len(emma)
    192432
    >>>

The Brown Corpus
----------------

The Brown Corpus was the first million-word,
part-of-speech tagged electronic corpus of English, created in 1961 at
Brown University.  Each of the sections ``a`` through ``r`` represents
a different genre, as shown in Table brown-categories_.

.. table:: brown-categories

   ===  ================  ===  ==================  ===  ================
   Sec  Genre             Sec  Genre               Sec  Genre
   ===  ================  ===  ==================  ===  ================
   a    Press: Reportage  b    Press: Editorial    c    Press: Reviews
   d    Religion          e    Skill and Hobbies   f    Popular Lore
   g    Belles-Lettres    h    Government          j    Learned
   k    Fiction: General  k    Fiction: General    l    Fiction: Mystery
   m    Fiction: Science  n    Fiction: Adventure  p    Fiction: Romance
   r    Humor
   ===  ================  ===  ==================  ===  ================

   Sections of the Brown Corpus


We can access the corpus as a list of words, or a list of sentences (where each sentence
is itself just a list of words).  We can optionally specify a section of the corpus to
read:

    >>> nltk.corpus.brown.categories()
    ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'r']
    >>> nltk.corpus.brown.words(categories='a')
    ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
    >>> nltk.corpus.brown.sents(categories='a')
    [['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]
    >>>

Other Text Corpora
------------------

[list of English text corpora included with NLTK]


Corpora in Other Languages
--------------------------

NLTK comes with corpora for many languages, though in some cases
you will need to learn how to manipulate character encodings in Python
before using these corpora (see Appendix appendix-unicode_).

    >>> print nltk.corpus.nps_chat.words()
    ['now', 'im', 'left', 'with', 'this', 'gay', 'name', ...]
    >>> nltk.corpus.cess_esp.words()
    ['El', 'grupo', 'estatal', 'Electricit\xe9_de_France', ...]
    >>> nltk.corpus.floresta.words()
    ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]
    >>> nltk.corpus.udhr.files()
    ('Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',
    'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',
    'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...)
    >>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
    ['Saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', ...]
    >>> nltk.corpus.indian.words('hindi.pos')
    ['\xe0\xa4\xaa\xe0\xa5\x82\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa3',
    '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xac\xe0\xa4\x82\xe0\xa4\xa7', ...]




.. note:: The rest of this chapter is from a previous version, beginning
   not with texts and words, but with strings.  This material is in the
   process of being rewritten.


.. _getting-organized:
   
-----------------
Getting Organized
-----------------


Strings and lists are a simple way to organize data.  In particular,
they `map`:dt: from integers to values.  We can "look up" a character
in a string using an integer, and we can look up a word in a list of
words using an integer.  These cases are shown in  Figure maps01_.

.. _maps01:
.. figure:: ../images/maps01.png
   :scale: 25

   Sequence Look-up


However, we need a more flexible way to organize and access our data.
Consider the examples in Figure maps02_.

.. _maps02:
.. figure:: ../images/maps02.png
   :scale: 25

   Dictionary Look-up

In the case of a phone book, we look up an entry using a `name`:em:,
and get back a number.  When we type a domain name in a web browser,
the computer looks this up to get back an IP address.  A word
frequency table allows us to look up a word and find its frequency in
a text collection.  In all these cases, we are mapping from names to
numbers, rather than the other way round as with indexing into
sequences.  In general, we would like to be able to map between
arbitrary types of information.  Table linguistic-objects_ lists a variety
of linguistic objects, along with what they map.

.. _linguistic-objects:

    +--------------------+-------------------------------------------------+
    | Linguistic Object  |                      Maps                       |
    |                    +------------+------------------------------------+
    |                    |    from    | to                                 |
    +====================+============+====================================+
    |Document Index      |Word        |List of pages (where word is found) |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Thesaurus           |Word sense  |List of synonyms                    |
    +--------------------+------------+------------------------------------+
    |Dictionary          |Headword    |Entry (part of speech, sense        |
    |                    |            |definitions, etymology)             |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Comparative Wordlist|Gloss term  |Cognates (list of words, one per    |
    |                    |            |language)                           |
    +--------------------+------------+------------------------------------+
    |Morph Analyzer      |Surface form|Morphological analysis (list of     |
    |                    |            |component morphemes)                |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+

    Linguistic Objects as Mappings from Keys to Values

Most often, we are mapping from a string to some structured object.
For example, a document index maps from a word (which we can represent
as a string), to a list of pages (represented as a list of integers).
In this section, we will see how to represent such mappings in Python.

Accessing Data with Data
------------------------

Python provides a `dictionary`:dt: data type that can be used for
mapping between arbitrary types.

.. Note:: A Python dictionary is somewhat like a linguistic dictionary
   |mdash| they both give you a systematic means of looking things up,
   and so there is some potential for confusion. However, we hope that
   it will usually be clear from the context which kind of dictionary
   we are talking about.

Here we define ``pos`` to be an empty dictionary and then add three
entries to it, specifying the part-of-speech of some words.  We add
entries to a dictionary using the familiar square bracket notation:

    >>> pos = {}
    >>> pos['colorless'] = 'adj'
    >>> pos['furiously'] = 'adv'
    >>> pos['ideas'] = 'n'
    >>>

So, for example, ``pos['colorless'] = 'adj'`` says that the look-up
value of ``'colorless'`` in ``pos`` is the string ``'adj'``.

.. Monkey-patching to get our dict examples to print consistently:

    >>> from nltk import SortedDict
    >>> pos = SortedDict(pos)

To look up a value in ``pos``, we again use indexing notation, except
now the thing inside the square brackets is the item whose value we
want to recover:

    >>> pos['ideas']
    'n'
    >>> pos['colorless']
    'adj'
    >>>

The item used for look-up is called the `key`:dt:, and the
data that is returned is known as the `value`:dt:.  As with indexing
a list or string, we get an exception when we try to access the value
of a key that does not exist:

    >>> pos['missing']
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    KeyError: 'missing'
    >>>

This raises an important question.  Unlike lists and strings, where we
can use ``len()`` to work out which integers will be legal indices, how
do we work out the legal keys for a dictionary?  Fortunately, we can
check whether a key exists in a dictionary using the ``in`` operator:

    >>> 'colorless' in pos
    True
    >>> 'missing' in pos
    False
    >>> 'missing' not in pos
    True
    >>>

Notice that we can use ``not in`` to check if a key is `missing`:em:.  Be
careful with the ``in`` operator for dictionaries: it only applies to
the keys and not their values.  If we check for a value, e.g. ``'adj'
in pos``, the result is ``False``, since ``'adj'`` is not a key.
We can loop over all the entries in a dictionary using a ``for`` loop.

    >>> for word in pos:
    ...     print "%s (%s)" % (word, pos[word])
    ... 
    colorless (adj)
    furiously (adv)
    ideas (n)
    >>>

We can see what the contents of the dictionary look like by inspecting
the variable ``pos``.  Note the presence of the colon character to separate
each key from its corresponding value:

    >>> pos
    {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}
    >>>

Here, the contents of the dictionary are shown as `key-value
pairs`:dt:.  As you can see,  the order of the key-value pairs is different
from the order in which they were originally entered.  This is because
dictionaries are not sequences but mappings. The keys in a mapping
are not inherently ordered, and any ordering that we might want to impose on the keys
exists independently of the mapping.  As we shall see later, this
gives us a lot of flexibility.  

We can use the same key-value pair format to create a dictionary:

    >>> pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}
    >>>

.. Monkey-patching to get our dict examples to print consistently:

    >>> pos = SortedDict(pos)

Using the dictionary methods ``keys()``, ``values()`` and ``items()``,
we can access the keys and values as separate lists,
and also the key-value pairs:

    >>> pos.keys()
    ['colorless', 'furiously', 'ideas']
    >>> pos.values()
    ['adj', 'adv', 'n']
    >>> pos.items()
    [('colorless', 'adj'), ('furiously', 'adv'), ('ideas', 'n')]
    >>> for (key, val) in pos.items():
    ...     print "%s ==> %s" % (key, val)
    ...
    colorless ==> adj
    furiously ==> adv
    ideas ==> n
    >>>

Note that keys are forced to be unique.
Suppose we try to use a dictionary to store the fact that the
word `content`:lx: is both a noun and a verb:

    >>> pos['content'] = 'n'
    >>> pos['content'] = 'v'
    >>> pos
    {'content': 'v', 'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}
    >>>

Initially, ``pos['content']`` is given the value ``'n'``, and this is
immediately overwritten with the new value ``'v'``.
In other words, there is only one entry for ``'content'``.
If we wanted to store multiple values in that entry, we could use a list,
e.g. ``pos['content'] = ['n', 'v']``.

Scaling Up
----------

We can use dictionaries to count word occurrences.  For example, the
following code uses |NLTK|\ 's corpus reader to load
*Macbeth* and count the frequency of each word.
Before we can use |NLTK| we need to tell Python to load it, using
the statement ``import nltk``.

    >>> import nltk
    >>> count = nltk.defaultdict(int)                     # initialize a dictionary
    >>> for word in nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'): # tokenize Macbeth
    ...     word = word.lower()                           # normalize to lowercase
    ...     count[word] += 1                              # increment the counter
    ...
    >>>

You will learn more about accessing corpora in Section sec-extracting-text-from-corpora_.
For now, you just need to know that ``gutenberg.words()`` returns a list of words,
in this case from Shakespeare's play *Macbeth*, and we are iterating over this list
using a ``for`` loop.  We convert each word to lowercase using the string
method ``word.lower()``, and use a dictionary to maintain a set of
counters, one per word.  Now we can inspect the contents of the
dictionary to get counts for particular words:

    >>> count['scotland']
    12
    >>> count['the']
    692
    >>>

Exercises
---------

1. |talk| Review the mappings in Table linguistic-objects_.  Discuss any other
   examples of mappings you can think of.  What type of information do they map
   from and to?
   
#. |easy| Using the Python interpreter in interactive mode, experiment with
   the examples in this section.  Create a dictionary ``d``, and add
   some entries.  What happens if you try to access a non-existent
   entry, e.g. ``d['xyz']``?

#. |easy| Try deleting an element from a dictionary, using the syntax
   ``del d['abc']``.  Check that the item was deleted.

#. |easy| Create a dictionary ``e``, to represent a single lexical entry
   for some word of your choice.
   Define keys like ``headword``, ``part-of-speech``, ``sense``, and
   ``example``, and assign them suitable values.

-------
Summary
-------

* Text is represented in Python using strings, and we type these with
  single or double quotes: ``'Hello'``, ``"World"``.

* The characters of a string are accessed using indexes, counting from zero:
  ``'Hello World'[1]`` gives the value ``e``.  The length of a string is
  found using ``len()``.

* Substrings are accessed using slice notation: ``'Hello World'[1:5]``
  gives the value ``ello``.  If the start index is omitted, the
  substring begins at the start of the string; if the end index is omitted,
  the slice continues to the end of the string.

* Sequences of words are represented in Python using lists of strings:
  ``['colorless', 'green', 'ideas']``.  We can use indexing, slicing
  and the ``len()`` function on lists.

* Strings can be split into lists: ``'Hello World'.split()`` gives
  ``['Hello', 'World']``.  Lists can be joined into strings:
  ``'/'.join(['Hello', 'World'])`` gives ``'Hello/World'``.

* Lists can be sorted in-place: ``words.sort()``.  To produce a separate,
  sorted copy, use: ``sorted(words)``.

* We process each item in a string or list using a ``for`` statement:
  ``for word in phrase``.  This must be followed by the colon character
  and an indented block of code, to be executed each time through the loop.

* We test a condition using an ``if`` statement: ``if len(word) < 5``.
  This must be followed by the colon character and an indented block of
  code, to be executed only if the condition is true.

* A dictionary is used to map between arbitrary types of information,
  such as a string and a number: ``freq['cat'] = 12``.  We create
  dictionaries using the brace notation: ``pos = {}``,
  ``pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}``.

* Some functions are not available by default, but must be accessed using
  Python's ``import`` statement.

---------------
Further Reading
---------------

Two freely available online texts are the following:

* Josh Cogliati, *Non-Programmer's Tutorial for Python*,
  http://en.wikibooks.org/wiki/Non-Programmer's_Tutorial_for_Python/Contents

*  Allen B. Downey, Jeffrey Elkner and Chris Meyers, 
   *How to Think Like a Computer Scientist: Learning with Python*,
   http://www.ibiblio.org/obp/thinkCSpy/

*An Introduction to Python*
[vanRossum2006IP]_ is a Python tutorial by Guido van
Rossum, the inventor of Python and Fred L. Drake, Jr., the official
editor of the Python documentation. It is available online at
http://docs.python.org/tut/tut.html. A more detailed but still
introductory text is [Lutz2003LP]_, which covers the essential
features of Python, and also provides an overview of the standard libraries.

..
    A more advanced text, [vanRossum2006IPLR]_ is the official reference
    for the Python language itself, and describes the syntax of Python and
    its built-in datatypes in depth. It is also available online at
    http://docs.python.org/ref/ref.html.

[Beazley2006PER]_ is a succinct reference book; although not suitable
as an introduction to Python, it is an excellent resource for
intermediate and advanced programmers.

Finally, it is always worth checking the official *Python
Documentation* at http://docs.python.org/.


.. include:: footer.rst
