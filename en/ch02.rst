.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: add Mark Twain
.. TODO: discussion of resource rich/poor languages in section on corpora in other languages
         number of languages in the world, Ethnologue, etc
.. TODO: explain double vs single vs triple quotes for strings
.. TODO: negative indices of lists

.. _chap-corpora:

================================
2. Corpora: Large Bodies of Text
================================

[Introduction]

The goal of this chapter is to answer the following questions:

#. how can we write programs to access text from wider range
   of |NLTK| corpora?
#. how can we write programs to access text from local files and
   from the web, in order to get hold of an unlimited range of
   language material?
#. what new Python idioms are needed?

.. _sec-extracting-text-from-corpora:

-----------------------------------------------
Computing with Language: Accessing Text Corpora
-----------------------------------------------

A text corpus is a large body of text, containing a careful balance of material in
one or more genres.  We examined some small text collections in Chapter chap-introduction_,
such as the presidential inaugural addresses.  This particular corpus actually contains dozens of
individual texts |mdash| one per address |mdash| but we glued them end-to-end
and treated them like chapters of a book, i.e. as a single text.  In this
section we will examine a variety of text corpora and will see how to select
individual texts, and how to work with them.

The Gutenberg Corpus
--------------------

|NLTK| includes a small selection of texts from the `Project Gutenberg
<http://www.gutenberg.org/>`_ electronic text archive containing
some 25,000 free electronic books.  We begin
by getting the Python interpreter to load the |NLTK| package,
then ask to see ``nltk.corpus.gutenberg.files()``, the files in
|NLTK|\ 's corpus of Gutenberg texts:  

    >>> import nltk
    >>> nltk.corpus.gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's pick out the first of these texts |mdash| *Emma* by Jane Austen |mdash| and
give it a short name ``emma``, then find out how many words it contains: 

    >>> emma = nltk.corpus.gutenberg.words("austen-emma.txt")
    >>> len(emma)
    192432

.. note:: In |NLTK| 0.9.5 you cannot do concordancing (and other tasks from
   Section sect-computing-with-language-texts-and-words_) using a text
   defined this way.  Instead you have to do the following:

       >>> emma = nltk.Text(nltk.corpus.gutenberg.words("austen-emma.txt"))

The long name refers to the ``words()`` function of the ``gutenberg``
module in |NLTK|\ 's ``corpus`` package.
It gets cumbersome to type such long names all the time, so Python provides
another version of the import statement, as follows:

    >>> from nltk.corpus import gutenberg
    >>> gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's write a short program to display other information about each text:

    >>> for file in gutenberg.files():
    ...     num_chars = len(gutenberg.raw(file))
    ...     num_words = len(gutenberg.words(file))
    ...     num_sents = len(gutenberg.sents(file))
    ...     num_vocab = len(set(w.lower() for w in gutenberg.words(file)))
    ...     print num_chars/num_words, num_words/num_sents, num_words/num_vocab, file
    ... 
    4 21 24 austen-emma.txt
    4 23 16 austen-persuasion.txt
    4 24 20 austen-sense.txt
    4 33 73 bible-kjv.txt
    4 18 4 blake-poems.txt
    4 17 10 chesterton-ball.txt
    4 19 10 chesterton-brown.txt
    4 16 10 chesterton-thursday.txt
    4 24 13 melville-moby_dick.txt
    4 52 9 milton-paradise.txt
    4 12 7 shakespeare-caesar.txt
    4 13 6 shakespeare-hamlet.txt
    4 13 5 shakespeare-macbeth.txt
    4 35 10 whitman-leaves.txt

This program has displayed three statistics for each text:
average word length, average sentence length, and the number of times each vocabulary
item appears in the text on average (our lexical diversity score).
Observe that average word length appears to be a general property of English, since it is
always `4`:math:.  Average sentence length and lexical diversity
appear to be characteristics of particular authors.

This example also showed how we can access the "raw" text of the book,
not split up into words.  The ``raw()`` function gives us the contents of the file
without any linguistic processing.  So, for example, ``len(gutenberg.raw("blake-poems.txt")``
tells us how many *letters* occur in the text, including the spaces between words.
The ``sents()`` function divides the text up into its sentences, where each sentence is
a list of words:

    >>> macbeth_sentences = gutenberg.sents("shakespeare-macbeth.txt")
    >>> macbeth_sentences
    [['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',
    '1603', ']'], ['Actus', 'Primus', '.'], ...]
    >>> macbeth_sentences[1038]
    ['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',
    'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']

.. note:: Most |NLTK| corpus readers include a variety of access methods
   apart from ``words()``.  We access the raw file contents using ``raw()``,
   and get the content sentence by sentence using ``sents()``.  Richer
   linguistic content is available from some corpora, such as part-of-speech
   tags, dialogue tags, syntactic trees, and so forth; we will see these
   in later chapters.

Web Text
--------

Although Project Gutenberg contains thousands of books, it represents established
literature.  Its important to consider less formal language as well.  |NLTK|\ 's
small collection of web text includes content from a Firefox discussion forum,
conversations overheard in New York, the movie script of *Pirates of the Carribean*,
personal advertisements, and wine reviews:

    >>> for f in nltk.corpus.webtext.files():
    ...     print f, nltk.corpus.webtext.raw(f)[:70]
    ... 
    firefox.txt Cookie Manager: "Don't allow sites that set removed cookies to set fut
    overheard.txt White guy: So, do you have any plans for this evening? Asian girl: Yea
    pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Ros
    singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encounters.
    wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawberrie 

There is also a corpus of instant messaging chat sessions, originally collected
by the Naval Postgraduate School for research on automatic detection of internet predators.
The corpus contains over 10,000 posts, anonymized by replacing usernames with generic
names of the form "UserNNN", and manually edited to remove any other identifying information.
The corpus is organized into 15 files, where each file contains several hundred posts
collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a
generic adults chatroom).  The filename contains the date, chatroom,
and number of posts, e.g. ``10-19-20s_706posts.xml`` contains 706 posts gathered from
the 20s chat room on 10/19/2006.

    >>> chatroom = nltk.corpus.nps_chat.posts("10-19-20s_706posts.xml")
    >>> chatroom[123]
    ['i', 'do', "n't", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',
    'I', 'can', 'look', 'in', 'a', 'mirror', '.']

The Brown Corpus
----------------

The Brown Corpus was the first million-word electronic
corpus of English, created in 1961 at Brown University.
This corpus contains text from many sources, and the sources
have been categorized by genre, and given a label ``a`` through ``r``,
as set out in Table brown-categories_.

.. table:: brown-categories

   ===  =================  ===  ==================  ===  ================  ===  ================
   Sec  Genre              Sec  Genre               Sec  Genre             Sec  Genre
   ===  =================  ===  ==================  ===  ================  ===  ================
   a    Press: Reportage   b    Press: Editorial    c    Press: Reviews    d    Religion
   e    Skill and Hobbies  f    Popular Lore        g    Belles-Lettres    h    Government
   j    Learned            k    Fiction: General    k    Fiction: General  l    Fiction: Mystery
   m    Fiction: Science   n    Fiction: Adventure  p    Fiction: Romance  r    Humor
   ===  =================  ===  ==================  ===  ================  ===  ================

   Sections of the Brown Corpus

We can access the corpus as a list of words, or a list of sentences (where each sentence
is itself just a list of words).  We can optionally specify a section of the corpus to read:

    >>> brown = nltk.corpus.brown
    >>> brown.categories()
    ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'r']
    >>> brown.words(categories='a')
    ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
    >>> brown.sents(categories='a')
    [['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]

We can use the Brown Corpus to study systematic differences between
genres, a kind of linguistic inquiry known as `stylistics`:dt:.
For example, Table brown-modals_ was constructed by counting
the number of times various modal words appear in different sections of the corpus.

.. table:: brown-modals

   ==================  ===  =====  ===  =====  ====  ====
   Genre               can  could  may  might  must  will 
   ==================  ===  =====  ===  =====  ====  ====
   press: reportage    93   86     66   38     50    389
   religion            82   59     78   12     54    71
   skill and hobbies   268  58     131  22     83    264
   fiction: science    16   49     4    12     8     16    
   fiction: romance    74   193    11   51     45    43    
   humor               16   30     8    8      9     13               
   ==================  ===  =====  ===  =====  ====  ====

   Use of Modals in Brown Corpus, by Genre

|nopar|
Observe that the most frequent modal in the reportage genre is
`will`:lx:, suggesting a focus on the future, while the most frequent
modal in the romance genre is `could`:lx:, suggesting a focus on possibilities.

Table brown-modals_ can be thought of as a collection of frequency
distributions, one per row.  Let's use this idea to calculate the
values of the press row:

    >>> fdist = nltk.FreqDist(w.lower for w in brown.words(categories='a'))
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> [fdist[m] for m in modals]
    [94, 87, 93, 38, 53, 389]
    
We can compute all the numbers in Table brown-modals_ using |NLTK|\ 's
support for conditional frequency distributions, as follows:

    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> cfd = nltk.ConditionalFreqDist((g,w)
                  for g in brown.categories()
                  for w in brown.words(categories=g))
    >>> cfd.tabulate(conditions='adempr', samples=modals)
       can could  may might must will
    a   93   86   66   38   50  389
    d   82   59   78   12   54   71
    e  268   58  131   22   83  264
    m   16   49    4   12    8   16
    p   74  193   11   51   45   43
    r   16   30    8    8    9   13

Any text corpus that can be divided into components by genre, author, period, etc,
is amenable to exploration using conditional frequency distributions.  We
will present conditional frequency distributions systematically in
Section sec-getting-organized_.

Reuters Corpus
--------------

The Reuters Corpus contains 10,788 news documents totaling 1.3 million words.
The documents have been classified for relevance to 90 topics, and grouped
into two sets, called "training" and "test" (for training and testing algorithms
that automatically detect the topic of a document, as we will explore further
in Chapter chap-data-intensive_).

    >>> nltk.corpus.reuters.files()
    ('test/14826', 'test/14828', 'test/14829', 'test/14832', ...)
    >>> reuters.categories() 
    ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',
    'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',
    'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]

Unlike the Brown Corpus, categories in the Reuters corpus overlap with each
other, simply because a news story often covers multiple topics.  We can
ask for the topics covered by one or more documents, or for the documents
included in one or more categories:

    >>> reuters.categories('training/9865')
    ['barley', 'corn', 'grain', 'wheat']
    >>> reuters.categories(['training/9865', 'training/9880'])
    ['barley', 'corn', 'grain', 'money-fx', 'wheat']
    >>> reuters.files('barley') 
    ['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]
    >>> reuters.files(['barley', 'corn']) 
    ['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',
    'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]

.. note:: Many other English text corpora are provided with |NLTK|.  For a
   list see Appendix app-modules_.  For more examples of access |NLTK| corpora,
   please consult the online guide at ``http://nltk.org/doc/guides/corpus.html``.

Inaugural Address Corpus
------------------------

We saw this corpus in section sect-computing-with-language-texts-and-words_,
but treated it as a single text.  The graph in Figure fig-inaugural_,
used word offset as one of the axes, which is difficult to interpret.
However, this corpus is actually a collection of 55 texts,
one for each presidential address.  An interesting property of
this collection is its time dimension:

    >>> inaugural = nltk.corpus.inaugural
    >>> inaugural.files()
    ('1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', ...)
    >>> [file[:4] for file in inaugural.files()]
    ['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', '1825', ...]

Notice that the year of each text appears in its filename.  To get the year
out of the file name, we extracted the first four characters, using ``file[:4]``.

Let's look at how the words `America`:lx: and `citizen`:lx: are used over time.
The following code will count similar words, such as plurals of these words, or
the word `Citizens`:lx: as it would appear at the start of a sentence (how?).
The result is shown in Figure fig-inaugural2_.

    >>> cfd = nltk.ConditionalFreqDist((target, file[:4])
    ...           for file in inaugural.files()
    ...           for w in inaugural.words(file)
    ...           for target in ["america", "citizen"]
    ...           if w.lower().startswith(target))
    >>> cfd.plot()

.. _fig-inaugural2:
.. figure:: ../images/inaugural2.png
   :scale: 25

   Conditional Frequency Distribution for Two Words in the Inaugural Address Corpus

.. note::
   For more discussion of relative word frequencies in the Inaugural Address Corpus,
   see ``http://languagelog.ldc.upenn.edu/nll/?p=696``


Corpora in Other Languages
--------------------------

NLTK comes with corpora for many languages, though in some cases
you will need to learn how to manipulate character encodings in Python
before using these corpora (see Appendix app-unicode_).

    >>> print nltk.corpus.nps_chat.words()
    ['now', 'im', 'left', 'with', 'this', 'gay', 'name', ...]
    >>> nltk.corpus.cess_esp.words()
    ['El', 'grupo', 'estatal', 'Electricit\xe9_de_France', ...]
    >>> nltk.corpus.floresta.words()
    ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]
    >>> nltk.corpus.udhr.files()
    ('Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',
    'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',
    'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...)
    >>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
    ['Saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', ...]
    >>> nltk.corpus.indian.words('hindi.pos')
    ['\xe0\xa4\xaa\xe0\xa5\x82\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa3',
    '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xac\xe0\xa4\x82\xe0\xa4\xa7', ...]

The last of these corpora, ``udhr`` contains the Universal Declaration of Human Rights
in over 300 languages.  (Note that the names of the files in this corpus include
information about character encoding, and for now we will stick with texts in ISO Latin-1, or ASCII)

Let's use a conditional frequency distribution to examine the differences in word lengths,
for a selection of languages included in this corpus.
The output is shown in Figure fig-word-len-dist_ (run the program yourself to see a color plot).

    >>> languages = ['Chickasaw', 'English', 'German_Deutsch',
    ...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
    >>> cfd = nltk.ConditionalFreqDist((lang, len(word)) for lang in languages
    ...          for word in nltk.corpus.udhr.words(lang + "-Latin1"))
    >>> cfd.plot()

.. _fig-word-len-dist:
.. figure:: ../images/word-len-dist.png
   :scale: 25

   Cumulative Word Length Distributions for Several Languages


You will probably have other text sources, stored in files on your computer or accessible
via the web.  We'll discuss how to work with these in Chapter chap-words_.

.. _sec-defining-functions:

-------------------------
More Python: Reusing Code
-------------------------

By this time you've probably retyped a lot of code.  If you mess up when retyping a complex example you have
to enter it again.  Using the arrow keys to access and modify previous commands is helpful but only goes so
far.  In this section we see two important ways to reuse code: text editors and Python functions.

Creating Programs with a Text Editor
------------------------------------

The Python interative interpreter performs your instructions as soon as you type
them.  Often, it is better to compose a multi-line program using a text editor,
then ask Python to run the whole program at once.  Using |IDLE|, you can do
this by going to the ``File`` menu and opening a new window.  Try this now, and
enter the following one-line program:

::

     msg = 'Monty Python'

Save this program in a file called ``test.py``, then
go to the ``Run`` menu, and select the command ``Run Module``.
The result in the main |IDLE| window should look like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    >>>

Now, where is the output showing the value of ``msg``? The answer is
that the program in ``test.py`` will show a value only if you explicitly tell
it to, using the ``print`` command. So add another line to
``test.py`` so that it looks as follows:

::

     msg = 'Monty Python'
     print msg

Select ``Run Module`` again, and this time you should get output that
looks like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    Monty Python
    >>>

From now on, you have a choice of using the interactive interpreter or a
text editor to create your programs.  It is often convenient to test your ideas
using the interpreter, revising a line of code until it does what you expect,
and consulting the interactive help facility.  Once you're ready, you can paste
the code (minus any ``>>>`` prompts) into the text editor,
continue to expand it, and finally save the program
in a file so that you don't have to type it in again later.
Give the file a short but descriptive name, using all lowercase letters and separating
words with underscore, and using the ``.py`` filename extension, e.g. ``monty_python.py``.

.. note::
   Our inline code examples will continue to include the ``>>>`` and ``...`` prompts
   as if we are interacting directly with the interpreter.  As they get more complicated,
   you should instead type them into the editor, without the prompts, and run them
   from the editor as shown above.

Functions
---------

Suppose that you work on analyzing text involves different forms
of the same word, and that part of your program needs to work out
the plural form of a given singular noun.  Suppose it needs to do this
work in two places, once when it is processing some texts, and again
when it is processing user input.

Rather than repeating the same code several times over, it is more
efficient and reliable to localize this work inside a `function`:dt:.
A function is just a named block of code that performs some well-defined
task.  It usually has some inputs, also known as `parameters`:dt:,
and it may produce a result, also known as a `return value`:dt:.
We define a function using the keyword ``def`` followed by the
function name and any input parameters, followed by the body of the
function.  Here's the function we saw in section sect-computing-with-language-texts-and-words_:

    >>> def score(text):
    ...     return len(text) / len(set(text))

We use the keyword ``return`` to indicate the value that is
produced as output by the function.  In the above example,
all the work of the function is done in the ``return`` statement.
Here's an equivalent definition which does the same work
using multiple lines of code.  We'll change the parameter name
to remind you that this is an arbitrary choice:

    >>> def score(my_text_data):
    ...     word_count = len(my_text_data)
    ...     vocab_size = len(set(my_text_data))
    ...     richness_score = word_count / vocab_size
    ...     return richness_score

Notice that we've created some new variables inside the body of the function.
These are *local variables* and are not accessible outside the function.
Notice also that defining a function like this produces no output.
Functions do nothing until they are "called" (or "invoked").     

Let's return to our earlier scenario, and actually define a simple plural
function.  The function ``plural()`` in Figure plural_
takes a singular noun and generates a plural form (one which is not always
correct).

.. pylisting:: plural
   :caption: Example of a Python function

   def plural(word):
       if word.endswith('y'):
           return word[:-1] + 'ies'
       elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
           return word + 'es'
       elif word.endswith('an'):
           return word[:-2] + 'en'
       return word + 's'

   >>> plural('fairy')
   'fairies'
   >>> plural('woman')
   'women'

(There is much more to be said about functions, but
we will hold off until Section sec-functions_.)

Modules
-------

Over time you will find that you create a variety of useful little text processing functions,
and you end up copy-pasting them from old programs to new ones.  While file contains the
latest version of the function you want to use?
It makes life a lot easier if you can collect your work into a single place, and
access previously defined functions without any copying and pasting.

To do this, save your function(s) in a file called (say) ``textproc.py``.
Now, you can access your work simply by importing it from the file:

.. doctest-ignore::
    >>> from textproc import plural
    >>> plural('wish')
    wishes
    >>> plural('fan')
    fen

Our plural function has an error, and we'll need to fix it.  This time, we won't
produce another version, but instead we'll fix the existing one.  Thus, at every
stage, there is only one version of our plural function, and no confusion about
which one we should use.

A collection of variable and function definitions in a file is called a Python
`module`:dt:.  A collection of related modules is called a `package`:dt:.
|NLTK|\ 's code for processing the Brown Corpus is an example of a module,
and its collection of code for processing all the different corpora is
an example of a package.  |NLTK| itself is a set of packages, sometimes
called a `library`:dt:.

-----------------
Lexical Resources
-----------------

A lexicon is a collection of words and/or phrases along with associated information such
as part of speech and sense definitions.  The simplest kind of lexicon is nothing more
than a sorted list of words.  Sophisticated lexicons include complex structure within and
across the individual entries.  In this section we'll look at some lexical resources
included with |NLTK|.   

Wordlist Corpora
----------------

|NLTK| includes some corpora that are nothing more than wordlists.
The Words corpus is the ``/usr/dict/words`` file from Unix, used by
some spell checkers.  We can use it to find unusual or mis-spelt
words in a text corpus:

    >>> def unusual_words(text):
    ...     text_vocab = set(w.lower() for w in text if w.isalpha())
    ...     english_vocab = set(w.lower() for w in nltk.corpus.words.words())
    ...     unusual = text_vocab.difference(english_vocab)
    ...     return sorted(unusual)
    ...
    >>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))
    ['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary', 'adieus',
    'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham', 'amiably', 'annamaria',
    'annuities', 'apologising', 'arbour', 'archness', 'ardour', 'artlessness', 'assiduities', ...]
    >>> unusual_words(nltk.corpus.nps_chat.words())
    ['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros', 'actualy', 'adduser',
    'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk', 'agaibn', 'agurlwithbigguns',
    'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', 'ahhah', 'ahhahahaha', 'ahhh', 'ahhhh', ...]

There is a corpus of `stopwords`:dt:, high-frequency glue words that we sometimes
want to filter out of a text before further processing.

    >>> nltk.corpus.stopwords.words('english')
    ['a', "a's", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually',
    'after', 'afterwards', 'again', 'against', "ain't", 'all', 'allow', 'allows', 'almost',
    'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', ...]

Let's define a function to compute what fraction of words in a text are *not* in the
stopwords list:

    >>> def content_fraction(text):
    ...     stopwords = nltk.corpus.stopwords.words('english')
    ...     content = [w for w in text if w.lower() not in stopwords]
    ...     return 1.0 * len(content) / len(text)
    ...    
    >>> content_fraction(nltk.corpus.reuters.words())
    0.65997695393285261

Thus, with the help of stopwords we filter out a third of the words of the text.

One more wordlist corpus is the Names corpus, containing 8,000 names categorized by gender.
The male and female names are stored in separate files.  Let's find names which appear
in both files, i.e. names that are ambiguous for gender:

    >>> names = nltk.corpus.names
    >>> names.files()
    ('female.txt', 'male.txt')
    >>> [w for w in names.words('male.txt') if w in names.words('female.txt')]
    ['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',
    'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',
    'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ..]

It is well known that names ending in the letter `a`:lx: are almost always female.
We can see this, and some other patterns, in the graph in Figure fig-cfd-gender_,
which is produced by the following code:

    >>> cfd = nltk.ConditionalFreqDist((file, name[-1])
    ...           for file in names.files()
    ...           for name in names.words(file))
    >>> cfd.plot()

.. _fig-cfd-gender:
.. figure:: ../images/cfd-gender.png
   :scale: 25

   Frequency of Final Letter of Female vs Male Names


Tabular Lexicons
----------------

As we have seen, the entries in a wordlist lack internal structure |mdash| they are just words.
A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word
plus some properties in each row.  |NLTK| includes the CMU Pronouncing Dictionary,
used by speech synthesizers. 

    >>> entries = nltk.corpus.cmudict.entries()
    >>> len(entries)
    127069
    >>> for entry in entries[40000:40010]:
    ...     print entry
    ... 
    ('fir', 1, ('F', 'ER1'))
    ('fire', 1, ('F', 'AY1', 'ER0'))
    ("fire's", 1, ('F', 'AY1', 'ER0', 'Z'))
    ('fire', 2, ('F', 'AY1', 'R'))
    ('firearm', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M'))
    ('firearm', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M'))
    ('firearms', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'))
    ('firearms', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'))
    ('fireball', 1, ('F', 'AY1', 'ER0', 'B', 'AO2', 'L'))
    ('fireball', 2, ('F', 'AY1', 'R', 'B', 'AO2', 'L'))

For each word, it provides a pronunciation number and a list of phonetic
codes |mdash| distinct labels for each contrastive sound |mdash|
known as `phones`:lx:.  Observe that `fire`:lx: has two pronunciations:
the one-syllable ``F AY1 R``, and the two-syllable ``F AY1 ER0``.
The symbols in the CMU Pronouncing Dictionary are from the *Arpabet*,
described in more detail at ``http://en.wikipedia.org/wiki/Arpabet``

Each entry consists of three pieces of information, and we can
process these individually by complicating the ``for`` statement
slightly.  Instead of writing ``for entry in entries:``, we replace
``entry`` with *three* variable names.  Each of the pieces of a single
entry are assigned to each of the three variables.  (Underscore
is a legal variable name, and we'll use this name when we don't plan to do
anything with the variable later.)

    >>> for w, _, pron in entries:
    ...     if len(pron) == 3:
    ...         ph1, ph2, ph3 = pron
    ...         if ph1 == 'P' and ph3 == 'T':
    ...             print w, ph2,
    ...
    pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1
    pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1
    pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1

The above program scans the lexicon for entries with a pronunciation consisting of
three phones (``len(pron) == 3``).  If the condition is true, we assign the contents
of ``pron`` to three new variables ``ph1``, ``ph2`` and ``ph3``.  Notice the unusual
form of the statement which does that work.

Here's another example of the same ``for`` statement, this time used inside a list
comprehension.  This program finds all words whose pronunciation ends with a syllable
sounding like `nicks`:lx:.  You could use this method to find rhyming words.

    >>> syllable = ('N', 'IH0', 'K', 'S')
    >>> [w for w, _, pron in entries if pron[-4:] == syllable]
    ["atlantic's", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',
    'chetniks', "clinic's", 'clinics', 'conics', 'cynics', 'diasonics', "dominic's",
    'ebonics', 'electronics', "electronics'", 'endotronics', "endotronics'", 'enix', ...]

Notice that the one pronunciation is spelt in several ways: `nics`:lx:, `niks`:lx:, `nix`:lx:,
even `ntic's`:lx: with a silent `t`:lx, for the word `atlantic's`:lx:.  Let's look for some other
mismatches between pronunciation and writing.  Can you summarize the purpose of
the following examples and explain how they work?

    >>> [w for w, _, pron in entries if pron[-1] == 'M' and w[-1] == 'n']
    ['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']
    >>> sorted(set(w[:2] for w, _, pron in entries if pron[0] == 'N' and w[0] != 'n'))
    ['gn', 'kn', 'mn', 'pn']

The phones contain digits, to represent 
primary stress (``1``), secondary stress (``2``) and no stress (``0``).
As our final example, we define a function to extract the stress digits
and then scan our lexicon to find words having a particular stress pattern.

    >>> def stress(pron):
    ...     return [int(char) for phoneme in pron for char in phoneme if char.isdigit()] 
    >>> [w for w, _, pron in entries if stress(pron) == [0, 1, 0, 2, 0]]
    ['abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator',
    'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative',
    'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators', ...]
    >>> [w for w, _, pron in entries if stress(pron) == [0, 2, 0, 1, 0]]
    ['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',
    'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',
    'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]

Note that this example has a user-defined function inside the condition of
a list comprehension.

[Summary of tabular lexicons; forward reference to discussion about processing CSV files]

Shoebox and Toolbox Lexicons
----------------------------

[basic introduction from ch 13]


WordNet
-------

`WordNet`:idx: is a semantically-oriented dictionary of English,
similar to a traditional thesaurus but with a richer structure.
WordNet groups words into synonym sets, or `synsets`:dt:, each with
its own definition and with links to other synsets.
WordNet 3.0 data is distributed with NLTK, and includes 117,659 synsets.

Although WordNet was originally developed for research
in psycholinguistics, it is widely used in NLP and Information Retrieval.
WordNets are being developed for many other languages, as documented
at ``http://www.globalwordnet.org/``.

Senses and Synonyms
-------------------

Consider the following sentence:

.. _carex1:
.. ex::
   Benz is credited with the invention of the motorcar.

If we replace `motorcar`:lx: in carex1_ by `automobile`:lx:, the
meaning of the sentence stays pretty much the same:

.. _carex2:
.. ex::
   Benz is credited with the invention of the automobile.

Since everything else in the sentence has remained unchanged, we can
conclude that the words `motorcar`:lx: and `automobile`:lx: have the
same meaning, i.e. they are `synonyms`:dt:. 

In order to look up the senses of a word, we need to pick
a part of speech for the word.  WordNet contains four dictionaries: ``N``
(nouns), ``V`` (verbs), ``ADJ`` (adjectives), and ``ADV``
(adverbs). To simplify our discussion, we will focus on the ``N``
dictionary here.  Let's look up `motorcar`:lx: in the ``N`` dictionary.

    >>> from nltk import wordnet
    >>> car = wordnet.N['motorcar']
    >>> car
    motorcar (noun)

The variable ``car`` is now bound to a ``Word`` object.
Words will often have more than sense, where each
sense is represented by a synset. However,
`motorcar`:lx: only has one sense in WordNet, as we can discover
using ``len()``.  We can then find the synset (a set of
lemmas), the words it contains, and a gloss.

    >>> len(car)
    1
    >>> car[0]
    {noun: car, auto, automobile, machine, motorcar}
    >>> list(car[0])
    ['car', 'auto', 'automobile', 'machine', 'motorcar']
    >>> car[0].gloss
    'a motor vehicle with four wheels; usually propelled by an
    internal combustion engine; 
    "he needs a car to get to work"'

The ``wordnet`` module also defines ``Synset``\ s.
Let's look at a word which is `polysemous`:dt:\ ;
that is, which has multiple synsets:

    >>> poly = wordnet.N['pupil']
    >>> for synset in poly:
    ...     print synset
    {noun: student, pupil, educatee}
    {noun: pupil}
    {noun: schoolchild, school-age_child, pupil}
    >>> poly[1].gloss
    'the contractile aperture in the center of the iris of the eye;
    resembles a large black dot'

The WordNet Hierarchy
---------------------

WordNet synsets correspond to abstract concepts, which may or may not
have corresponding words in English.  These concepts are linked together in a hierarchy.
Some are very general, such as *Entity*, *State*, *Event* |mdash| these are called
`unique beginners`:dt:.  Others, such as *gas guzzler* and
*hatchback*, are much more specific. A small portion of a concept
hierarchy is illustrated in Figure wn-hierarchy_. The edges between nodes
indicate the hypernym/hyponym relation; the dotted line at the top is
intended to indicate that *artifact* is a non-immediate hypernym of *motorcar*. 

.. _wn-hierarchy:
.. figure:: ../images/wordnet-hierarchy.png
   :scale: 15

   Fragment of WordNet Concept Hierarchy

WordNet makes it easy to navigate between concepts.
For example, given a concept like *motorcar*,
we can look at the concepts that are more specific;
the (immediate) `hyponyms`:dt:. Here is one way to carry out this
navigation:

    >>> for concept in car[0][wordnet.HYPONYM][:10]:
    ...     print concept
    {noun: ambulance}
    {noun: beach_wagon, station_wagon, wagon, estate_car, beach_waggon, station_waggon, waggon}
    {noun: bus, jalopy, heap}
    {noun: cab, hack, taxi, taxicab}
    {noun: compact, compact_car}
    {noun: convertible}
    {noun: coupe}
    {noun: cruiser, police_cruiser, patrol_car, police_car, prowl_car, squad_car}
    {noun: electric, electric_automobile, electric_car}
    {noun: gas_guzzler}

|nopar|
We can also move up the hierarchy, by looking at broader concepts than
*motorcar*, e.g. the immediate `hypernym`:dt: of a concept:

    >>> car[0][wordnet.HYPERNYM]
    [{noun: motor_vehicle, automotive_vehicle}]

We can also look for the hypernyms of hypernyms.  In fact, from any
synset we can trace (multiple) paths back to a unique beginner.
Synsets have a method for doing this, called ``tree()``,
which produces a nested list structure.

    >>> pprint.pprint(wordnet.N['car'][0].tree(wordnet.HYPERNYM))
    [{noun: car, auto, automobile, machine, motorcar},
     [{noun: motor_vehicle, automotive_vehicle},
      [{noun: self-propelled_vehicle},
       [{noun: wheeled_vehicle},
        [{noun: vehicle},
         [{noun: conveyance, transport},
          [{noun: instrumentality, instrumentation},
           [{noun: artifact, artefact},
            [{noun: whole, unit},
             [{noun: object, physical_object},
              [{noun: physical_entity}, [{noun: entity}]]]]]]]],
        [{noun: container},
         [{noun: instrumentality, instrumentation},
          [{noun: artifact, artefact},
           [{noun: whole, unit},
            [{noun: object, physical_object},
             [{noun: physical_entity}, [{noun: entity}]]]]]]]]]]]

A related method ``closure()`` produces a flat version of this structure,
with repeats eliminated.
Both of these functions take an optional ``depth`` argument that permits
us to limit the number of steps to take.
(This is important when using unbounded relations like ``SIMILAR``.)
Table wordnet-rel_ lists the most important lexical relations supported
by WordNet; see ``dir(wordnet)`` for a full list.

.. table:: wordnet-rel

   ===========  ================  ==============================================
   Hypernym     more general      `animal`:lx: is a hypernym of `dog`:lx:
   Hyponym      more specific     `dog`:lx: is a hyponym of `animal`:lx:
   Meronym      part of           `door`:lx: is a meronym of `house`:lx:
   Holonym      has part          `house`:lx: is a holonym of `door`:lx:
   Synonym      similar meaning   `car`:lx: is a synonym of `automobile`:lx:
   Antonym      opposite meaning  `like` is an antonym of `dislike`:lx:
   Entailment   necessary action  `step` is an entailment of `walk`:lx:
   ===========  ================  ==============================================

   Major WordNet Lexical Relations

Recall that we can iterate over the words of a synset, with ``for word in synset``.
We can also test if a word is in a dictionary, e.g. ``if word in wordnet.V``.
As our last task, let's put these together to find "animal words" that are used as verbs.
Since there are a lot of these, we will cut this off at depth 4.
Can you think of the animal and verb sense of each word?

    >>> animals = wordnet.N['animal'][0].closure(wordnet.HYPONYM, depth=4)
    >>> [word for synset in animals for word in synset if word in wordnet.V]
    ['pet', 'stunt', 'prey', 'quarry', 'game', 'mate', 'head', 'dog',
     'stray', 'dam', 'sire', 'steer', 'orphan', 'spat', 'sponge',
     'worm', 'grub', 'pooch', 'toy', 'queen', 'baby', 'pup', 'whelp',
     'cub', 'kit', 'kitten', 'foal', 'lamb', 'fawn', 'bird', 'grouse',
     'hound', 'bulldog', 'stud', 'hog', 'baby', 'fish', 'cock', 'parrot',
     'frog', 'beetle', 'bug', 'bug', 'queen', 'leech', 'snail', 'slug',
     'clam', 'cockle', 'oyster', 'scallop', 'scollop', 'escallop', 'quail']

NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet.
It can be accessed with ``nltk.corpus.verbnet``.

.. _sec-getting-organized:

------------------------------
More Python: Getting Organized
------------------------------

A text, as we have seen, is treated in Python as a list of words.
An important property of lists is that we can "look up" a particular
item by giving its index, e.g. ``text1[100]``.  Notice how we specify
a number, and get back a word.  We can think of a list as a simple
kind of table, as shown in Figure maps01_.

.. _maps01:
.. figure:: ../images/maps01.png
   :scale: 25

   List Look-up

Contrast this situation with frequency distributions (section computing-with-language-simple-statistics_),
where we specify a word in order to get back a number, e.g. ``fdist['monstrous']``, which
tells us the number of times a given word has occurred in a text.  Look-up using words is
familiar to anyone who has used a dictionary.  Some more examples are shown in
Figure maps02_.

.. _maps02:
.. figure:: ../images/maps02.png
   :scale: 22

   Dictionary Look-up

In the case of a phone book, we look up an entry using a `name`:em:,
and get back a number.  When we type a domain name in a web browser,
the computer looks this up to get back an IP address.  A word
frequency table allows us to look up a word and find its frequency in
a text collection.  In all these cases, we are mapping from names to
numbers, rather than the other way round.
In general, we would like to be able to map between
arbitrary types of information.  Table linguistic-objects_ lists a variety
of linguistic objects, along with what they map.

.. table:: linguistic-objects

    +--------------------+-------------------------------------------------+
    | Linguistic Object  |                      Maps                       |
    |                    +------------+------------------------------------+
    |                    |    from    | to                                 |
    +====================+============+====================================+
    |Document Index      |Word        |List of pages (where word is found) |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Thesaurus           |Word sense  |List of synonyms                    |
    +--------------------+------------+------------------------------------+
    |Dictionary          |Headword    |Entry (part of speech, sense        |
    |                    |            |definitions, etymology)             |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Comparative Wordlist|Gloss term  |Cognates (list of words, one per    |
    |                    |            |language)                           |
    +--------------------+------------+------------------------------------+
    |Morph Analyzer      |Surface form|Morphological analysis (list of     |
    |                    |            |component morphemes)                |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+

    Linguistic Objects as Mappings from Keys to Values

Most often, we are mapping from a "word" to some structured object.
For example, a document index maps from a word (which we can represent
as a string), to a list of pages (represented as a list of integers).
In this section, we will see how to represent such mappings in Python.

Accessing Data with Data
------------------------

Python provides a `dictionary`:dt: data type that can be used for
mapping between arbitrary types.

.. Note:: A Python dictionary is somewhat like a linguistic dictionary
   |mdash| they both give you a systematic means of looking things up,
   and so there is some potential for confusion. However, we hope that
   it will usually be clear from the context which kind of dictionary
   we are talking about.

Here we define ``pos`` to be an empty dictionary and then add three
entries to it, specifying the part-of-speech of some words.  We add
entries to a dictionary using the familiar square bracket notation:

    >>> pos = {}
    >>> pos['colorless'] = 'adj'
    >>> pos['furiously'] = 'adv'
    >>> pos['ideas'] = 'n'

So, for example, ``pos['colorless'] = 'adj'`` says that the look-up
value of ``'colorless'`` in ``pos`` is the string ``'adj'``.

.. Monkey-patching to get our dict examples to print consistently:

    >>> from nltk import SortedDict
    >>> pos = SortedDict(pos)

To look up a value in ``pos``, we again use indexing notation, except
now the thing inside the square brackets is the item whose value we
want to recover:

    >>> pos['ideas']
    'n'
    >>> pos['colorless']
    'adj'

The item used for look-up is called the `key`:dt:, and the
data that is returned is known as the `value`:dt:.  As with indexing
a list or string, we get an exception when we try to access the value
of a key that does not exist:

    >>> pos['missing']
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    KeyError: 'missing'

This raises an important question.  Unlike lists and strings, where we
can use ``len()`` to work out which integers will be legal indices, how
do we work out the legal keys for a dictionary?  Fortunately, we can
check whether a key exists in a dictionary using the ``in`` operator:

    >>> 'colorless' in pos
    True
    >>> 'missing' in pos
    False
    >>> 'missing' not in pos
    True

Notice that we can use ``not in`` to check if a key is `missing`:em:.  Be
careful with the ``in`` operator for dictionaries: it only applies to
the keys and not their values.  If we check for a value, e.g. ``'adj'
in pos``, the result is ``False``, since ``'adj'`` is not a key.
We can loop over all the entries in a dictionary using a ``for`` loop.

    >>> for word in pos:
    ...     print word, pos[word]
    ... 
    colorless adj
    furiously adv
    ideas n

We can see what the contents of the dictionary look like by inspecting
the variable ``pos``.  Note the presence of the colon character to separate
each key from its corresponding value:

    >>> pos
    {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}

Here, the contents of the dictionary are shown as `key-value
pairs`:dt:.  As you can see,  the order of the key-value pairs is different
from the order in which they were originally entered.  This is because
dictionaries are not sequences but mappings. The keys in a mapping
are not inherently ordered, and any ordering that we might want to impose on the keys
exists independently of the mapping.  As we shall see later, this
gives us a lot of flexibility.  

We can use the same key-value pair format to create a dictionary:

    >>> pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}

.. Monkey-patching to get our dict examples to print consistently:

    >>> pos = SortedDict(pos)

Using the dictionary methods ``keys()``, ``values()`` and ``items()``,
we can access the keys and values as separate lists,
and also the key-value pairs:

    >>> pos.keys()
    ['colorless', 'furiously', 'ideas']
    >>> pos.values()
    ['adj', 'adv', 'n']
    >>> pos.items()
    [('colorless', 'adj'), ('furiously', 'adv'), ('ideas', 'n')]
    >>> for (key, val) in pos.items():
    ...     print key, "==>", val
    ...
    colorless ==> adj
    furiously ==> adv
    ideas ==> n

Note that keys are forced to be unique.
Suppose we try to use a dictionary to store the fact that the
word `content`:lx: is both a noun and a verb:

    >>> pos['content'] = 'n'
    >>> pos['content'] = 'v'
    >>> pos
    {'content': 'v', 'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}

Initially, ``pos['content']`` is given the value ``'n'``, and this is
immediately overwritten with the new value ``'v'``.
In other words, there is only one entry for ``'content'``.
If we wanted to store multiple values in that entry, we could use a list,
e.g. ``pos['content'] = ['n', 'v']``.

Sorting Dictionaries by Value
-----------------------------

    >>> from operator import itemgetter
    >>> pos.items()
    >>> sorted(pos.items, key=itemgetter(1), reverse=True)


Dictionaries vs Frequency Distributions
---------------------------------------

|NLTK|\ 's frequency distributions (``nltk.FreqDist``) is just
a special kind of dictionary which has additional support for sorting
and plotting that are needed in language processing.

We can use dictionaries to count word occurrences, emulating the
method used for tallying words (Figure tally_).  
We begin by initializing
an empty dictionary, then process each word of the text.  If the word hasn't
been seen before, we add it to our list with a zero count.  If we've
seen it before we increment its count using the ``+=`` operator.
If we ask for the keys we get them in an arbitrary order.

    >>> counts = {}
    >>> for word in nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'):
    ...     if word not in counts:
    ...         counts[word] = 0
    ...     counts[word] += 1
    ...
    >>> counts['Scotland']
    12
    >>> counts['the']
    692
    >>> counts.keys()
    ['Lead', 'doubts', 'felt', 'hath', 'protest', 'sleep', 'thirst', 'Barke', 'hate',
    'goodnesse', 'forget', 'whose', 'Hose', 'solliciting', 'euery', 'Keepes', ...]

However, it is preferable to use |NLTK|\ 's support for frequency distributions,
since it is more compact and has more functionality.  If we ask to see the keys
we get them in order of decreasing frequency.

    >>> counts = nltk.FreqDist(nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'))
    >>> counts['Scotland']
    12
    >>> counts.keys()
    [',', '.', "'", 'the', ':', 'and', 'I', 'of', 'to', '?', 'd', 'a', 'you', 'in', 'my',
    'And', 'is', 'that', 'not', 'it', 'Macb', 'with', 's', 'his', 'be', 'The', 'haue', ...]
    
Conditional Frequency Distributions
-----------------------------------

We have used frequency distributions to count the number of occurrences of
each word in a text.  Here we will generalize this idea to look at the
distribution of words in a given context.
A `conditional frequency distribution`:dt: is a collection
of frequency distributions, each one for a different condition.
Here the condition will be the preceding word.

Generating Random Text with Style
---------------------------------

In Figure random_, we've defined a function ``train_model()`` that
uses ``ConditionalFreqDist()`` to count words as they appear
relative to the context defined by the preceding word (stored in ``prev``).
It scans the corpus, incrementing the appropriate counter, and
updating the value of ``prev``.  The function ``generate_model()``
contains a simple loop to generate text: we set an initial
context, pick the most likely token in that context as our next
word (using ``max()``), and then use that word as our new context.
This simple approach to text generation tends to get stuck in loops;
another method would be to randomly choose the next word from among
the available words.

.. pylisting:: random
   :caption: Generating Random Text in the Style of Genesis

   def train_model(text):
       cfdist = nltk.ConditionalFreqDist()
       prev = None
       for word in text:
           cfdist[prev].inc(word)
           prev = word
       return cfdist

   def generate_model(cfdist, word, num=15):
       for i in range(num):
           print word,
           word = cfdist[word].max()

   >>> model = train_model(nltk.corpus.genesis.words('english-kjv.txt'))
   >>> model['living']
   <FreqDist with 16 samples>
   >>> list(model['living'])
   ['substance', ',', '.', 'thing', 'soul', 'creature']
   >>> generate_model(model, 'living')
   living creature that he said , and the land of the land of the land

---------------------
Low-Density Languages
---------------------

* ethnologue info


-------
Summary
-------

* A text corpus is a balanced collection of texts.  NLTK comes with many corpora,
  e.g. the Brown Corpus, ``nltk.corpus.brown``.
* A dictionary is used to map between arbitrary types of information,
  such as a string and a number: ``freq['cat'] = 12``.  We create
  dictionaries using the brace notation: ``pos = {}``,
  ``pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}``.

---------
Exercises
---------

#. |easy| Define a frequency distribution over the letters of a text, e.g. 
   ``fdist = nltk.FreqDist(gutenberg.raw("blake-poems.txt"))``.  Now plot
   this distribution, using ``fdist.plot()``, to see which letters are more
   frequent.  Now do this using texts from the Universal Declaration of
   Human Rights, to see how languages differ in their frequency of use of
   different letters.

#. |soso| **Exploring text genres:**
   Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?

#. |soso| The CMU Pronunciation Dictionary contains multiple pronunciations
   for certain words.  How many distinct words does it contain?  What fraction
   of words in this dictionary have more than one possible pronunciation?

#. |easy| Familiarize yourself with the WordNet interface, by reading the
   documentation available via ``help(wordnet)``.  Try out the text-based
   browser, ``wordnet.browse()``.

#. |easy| Investigate the holonym / meronym relations for some nouns.  Note that there
   are three kinds (member, part, substance), so access is more specific,
   e.g., ``wordnet.MEMBER_MERONYM``, ``wordnet.SUBSTANCE_HOLONYM``.

.. Currently too hard since we can't iterate over synsets easily
   #. |soso| What percentage of noun synsets have no hyponyms?

#. |easy| The polysemy of a word is the number of senses it has.
   Using WordNet, we can determine that the noun *dog* has 7 senses
   with: ``len(nltk.wordnet.N['dog'])``.
   Compute the average polysemy of nouns, verbs, adjectives and
   adverbs according to WordNet.

#. |soso| What is the branching factor of the noun hypernym hierarchy?
   (For all noun synsets that have hyponyms, how many do they have on average?)

#. |soso| Define a function ``supergloss(s)`` that takes a synset ``s`` as its argument
   and returns a string consisting of the concatenation of the glosses of ``s``, all
   hypernyms of ``s``, and all hyponyms of ``s``.

#. |talk| Review the mappings in Table linguistic-objects_.  Discuss any other
   examples of mappings you can think of.  What type of information do they map
   from and to?

#. |easy| Using the Python interpreter in interactive mode, experiment with
   the examples in this section.  Create a dictionary ``d``, and add
   some entries.  What happens if you try to access a non-existent
   entry, e.g. ``d['xyz']``?

#. |easy| Try deleting an element from a dictionary, using the syntax
   ``del d['abc']``.  Check that the item was deleted.

#. |easy| Create a dictionary ``e``, to represent a single lexical entry
   for some word of your choice.
   Define keys like ``headword``, ``part-of-speech``, ``sense``, and
   ``example``, and assign them suitable values.

#. |soso| Write a program to find all words that occur at least three times in the Brown Corpus.

#. |soso| Write a program to generate a table of token/type ratios, as we saw in
   Table brown-types_.  Include the full set of Brown Corpus genres (``nltk.corpus.brown.categories()``).
   Which genre has the lowest diversity (greatest number of tokens per type)?
   Is this what you would have expected?

#. |soso| Modify the text generation program in Figure random_ further, to
   do the following tasks:

   a) Store the *n* most likely words in a list ``lwords`` then randomly
      choose a word from the list using ``random.choice()``.

   b) Select a particular genre, such as a section of the Brown Corpus,
      or a genesis translation, one of the Gutenberg texts, or one of the Web texts.  Train
      the model on this corpus and get it to generate random text.  You
      may have to experiment with different start words. How intelligible
      is the text?  Discuss the strengths and weaknesses of this method of
      generating random text.

   c) Now train your system using two distinct genres and experiment
      with generating text in the hybrid genre.  Discuss your observations.

#. |soso| Write a program to print the most frequent bigrams
   (pairs of adjacent words) of a text,
   omitting non-content words, in order of decreasing frequency.

#. |soso| Write a program to create a table of word frequencies by genre,
   like the one given above for modals.  Choose your own words and
   try to find words whose presence (or absence) is typical of a genre.
   Discuss your findings.

#. |hard| **Zipf's Law**:
   Let *f(w)* be the frequency of a word *w* in free text. Suppose that
   all the words of a text are ranked according to their frequency,
   with the most frequent word first. Zipf's law states that the
   frequency of a word type is inversely proportional to its rank
   (i.e. *f.r = k*, for some constant *k*). For example, the 50th most
   common word type should occur three times as frequently as the
   150th most common word type.

   a) Write a function to process a large text and plot word
      frequency against word rank using ``pylab.plot``. Do
      you confirm Zipf's law? (Hint: it helps to use a logarithmic scale).
      What is going on at the extreme ends of the plotted line?

   #) Generate random text, e.g. using ``random.choice("abcdefg ")``,
      taking care to include the space character.  You will need to
      ``import random`` first.  Use the string
      concatenation operator to accumulate characters into a (very)
      long string.  Then tokenize this string, and generate the Zipf
      plot as before, and compare the two plots.  What do you make of
      Zipf's Law in the light of this?

#. |hard| Modify the ``generate_model()`` function in Figure random_ to use Python's
   ``random.choose()`` method to randomly pick the next word from
   the available set of words.

#. |soso| Write a function ``tf()`` that takes a word and the name of a section
   of the Brown Corpus as arguments, and computes the text frequency of the word
   in that section of the corpus.

#. |easy| Try the examples in this section, then try the following.

   a) Create a variable called ``msg`` and put a message
      of your own in this variable.  Remember that strings need
      to be quoted, so you will need to type something like:
      ``msg = "I like NLP!"``
   b) Now print the contents of this variable in two ways, first
      by simply typing the variable name and pressing enter, then
      by using the ``print`` command.
   c) Try various arithmetic expressions using this string, e.g.
      ``msg + msg``, and ``5 * msg``.
   d) Define a new string ``hello``, and then try ``hello + msg``.
      Change the ``hello`` string so that it ends with a space
      character, and then try ``hello + msg`` again.

#. |easy| Consider the following two expressions which have the same
   result.  Which one will typically be more relevant in |NLP|?  Why?

   a) ``"Monty Python"[6:12]``
   b) ``["Monty", "Python"][1]``

#. |easy| Define a string ``s = 'colorless'``.  Write a Python statement
   that changes this to "colourless" using only the slice and
   concatenation operations.

#. |easy| Try the slice examples from this section using the interactive
   interpreter.  Then try some more of your own.  Guess what the result
   will be before executing the command.

#. |easy| We can use the slice notation to remove morphological endings on
   words.  For example, ``'dogs'[:-1]`` removes the last character of
   ``dogs``, leaving ``dog``.  Use slice notation to remove the
   affixes from these words (we've inserted a hyphen to
   indicate the affix boundary, but omit this from your strings):
   ``dish-es``, ``run-ning``, ``nation-ality``, ``un-do``,
   ``pre-heat``.

#. |easy| We saw how we can generate an ``IndexError`` by indexing beyond the end
   of a string.  Is it possible to construct an index that goes too far to
   the left, before the start of the string?

#. |easy| We can also specify a "step" size for the slice. The following
   returns every second character within the slice: ``msg[6:11:2]``.
   It also works in the reverse direction: ``msg[10:5:-2]``
   Try these for yourself, then experiment with different step values.

#. |easy| What happens if you ask the interpreter to evaluate ``msg[::-1]``?
   Explain why this is a reasonable result.

#. |easy| Define a conditional frequency distribution over the Names corpus
   that allows you to see which initial letters are more frequent for males
   vs females (cf. Figure fig-cfd-gender_).

#. |soso| Write a function that finds the 50 most frequently occurring words
   of a text that are not stopwords.


.. include:: footer.rst

