.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: add Mark Twain
.. TODO: discussion of resource rich/poor languages in section on corpora in other languages
         number of languages in the world, Ethnologue, etc
.. TODO: explain double vs single vs triple quotes for strings
.. TODO: negative indices of lists
.. TODO: TextCollection
.. TODO: extracting dates from a tokenized text
.. TODO: finding a sequence of words matching some pattern (including doubled words, e.g. "the thing is is that")
.. TODO: non-mutability of dictionary keys
.. TODO: The Lexicon:
   * words are more than just the output of tokenization
   * explore what it means for a document to contain a word
   * ways this can fail: mis-spelling; different endings; synonyms; homonyms
   * type vs token distinction; connection of types to lemmas
   * concept of "word", many-to-many mapping between forms and meanings
   * why the lexicon is an open set, lexical productivity and challenge for NLP
   * morphology


.. _chap-corpora:

=====================================
2. Text Corpora and Lexical Resources
=====================================

Practical work in Natural Language Processing usually involves
a variety of established bodies of linguistic data. Such a body of
text is called a `corpus`:dt: (plural `corpora`:dt:).
The goal of this chapter is to answer the following questions:

#. What are some useful text corpora and lexical resources, and how can we access them with Python?
#. Which Python constructs are most helpful for this work?
#. How do we re-use code effectively?

This chapter continues to present programming concepts by example, in the
context of a linguistic processing task. We will wait till later before
exploring each Python construct systematically.  Don't worry if you see
an example that contains something unfamiliar; simply try it out and see
what it does, and |mdash| if you're game |mdash| modify it by substituting
some part of the code with a different text or word.  This way you will
associate a task with a programming idiom, and learn the hows and whys later.

.. _sec-extracting-text-from-corpora:

----------------------
Accessing Text Corpora
----------------------

As just mentioned, a text corpus is any large body of text. Many, but
not all, corpora are designed to contain a careful balance of material
in one or more genres.  We examined some small text collections in
Chapter chap-introduction_, such as the speeches known as the US Presidential
Inaugural Addresses.  This particular corpus actually contains dozens
of individual texts |mdash| one per address |mdash| but we glued them
end-to-end and treated them as a single text.  In this section we will
examine a variety of text corpora and will see how to select
individual texts, and how to work with them.

The Gutenberg Corpus
--------------------

|NLTK| includes a small selection of texts from the Project Gutenberg
`<http://www.gutenberg.org/>`_ electronic text archive containing
some 25,000 free electronic books.  We begin
by getting the Python interpreter to load the |NLTK| package,
then ask to see ``nltk.corpus.gutenberg.files()``, the files in
|NLTK|\ 's corpus of Gutenberg texts:  

    >>> import nltk
    >>> nltk.corpus.gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's pick out the first of these texts |mdash| *Emma* by Jane Austen |mdash| and
give it a short name ``emma``, then find out how many words it contains: 

    >>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')
    >>> len(emma)
    192432

.. note:: In |NLTK| 0.9.5 you cannot carry out concordancing (and other tasks from
   Section sect-computing-with-language-texts-and-words_) using a text
   defined this way.  Instead you have to make the following statement:

       >>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))

When we defined ``emma``, we invoked the ``words()`` function of the ``gutenberg``
module in |NLTK|\ 's ``corpus`` package.
But since it is cumbersome to type such long names all the time, so Python provides
another version of the ``import`` statement, as follows:

    >>> from nltk.corpus import gutenberg
    >>> gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's write a short program to display other information about each text:

    >>> for file in gutenberg.files():
    ...     num_chars = len(gutenberg.raw(file))
    ...     num_words = len(gutenberg.words(file))
    ...     num_sents = len(gutenberg.sents(file))
    ...     num_vocab = len(set(w.lower() for w in gutenberg.words(file)))
    ...     print num_chars/num_words, num_words/num_sents, num_words/num_vocab, file
    ... 
    4 21 24 austen-emma.txt
    4 23 16 austen-persuasion.txt
    4 24 20 austen-sense.txt
    4 33 73 bible-kjv.txt
    4 18 4 blake-poems.txt
    4 17 10 chesterton-ball.txt
    4 19 10 chesterton-brown.txt
    4 16 10 chesterton-thursday.txt
    4 24 13 melville-moby_dick.txt
    4 52 9 milton-paradise.txt
    4 12 7 shakespeare-caesar.txt
    4 13 6 shakespeare-hamlet.txt
    4 13 5 shakespeare-macbeth.txt
    4 35 10 whitman-leaves.txt

This program has displayed three statistics for each text:
average word length, average sentence length, and the number of times each vocabulary
item appears in the text on average (our lexical diversity score).
Observe that average word length appears to be a general property of English, since it is
always `4`:math:.  Average sentence length and lexical diversity
appear to be characteristics of particular authors.

This example also showed how we can access the "raw" text of the book,
not split up into words.  The ``raw()`` function gives us the contents of the file
without any linguistic processing.  So, for example, ``len(gutenberg.raw('blake-poems.txt')``
tells us how many *letters* occur in the text, including the spaces between words.
The ``sents()`` function divides the text up into its sentences, where each sentence is
a list of words:

    >>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')
    >>> macbeth_sentences
    [['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',
    '1603', ']'], ['Actus', 'Primus', '.'], ...]
    >>> macbeth_sentences[1038]
    ['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',
    'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']
    >>> longest_len = max(len(s) for s in macbeth_sentences)
    >>> [s for s in macbeth_sentences if len(s) == longest_len]
    [['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that',
    'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The',
    'mercilesse', 'Macdonwald', ...], ...]

.. note:: Most |NLTK| corpus readers include a variety of access methods
   apart from ``words()``.  We access the raw file contents using ``raw()``,
   and get the content sentence by sentence using ``sents()``.  Richer
   linguistic content is available from some corpora, such as part-of-speech
   tags, dialogue tags, syntactic trees, and so forth; we will see these
   in later chapters.

Web and Chat Text
-----------------

Although Project Gutenberg contains thousands of books, it represents established
literature.  It is important to consider less formal language as well.  |NLTK|\ 's
small collection of web text includes content from a Firefox discussion forum,
conversations overheard in New York, the movie script of *Pirates of the Carribean*,
personal advertisements, and wine reviews:

    >>> from nltk.corpus import webtext
    >>> for f in webtext.files():
    ...     print f, webtext.raw(f)[:70]
    ... 
    firefox.txt Cookie Manager: "Don't allow sites that set removed cookies to set fut
    grail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop clop
    overheard.txt White guy: So, do you have any plans for this evening? Asian girl: Yea
    pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Ros
    singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encounters.
    wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawberrie 

There is also a corpus of instant messaging chat sessions, originally collected
by the Naval Postgraduate School for research on automatic detection of internet predators.
The corpus contains over 10,000 posts, anonymized by replacing usernames with generic
names of the form "UserNNN", and manually edited to remove any other identifying information.
The corpus is organized into 15 files, where each file contains several hundred posts
collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a
generic adults chatroom).  The filename contains the date, chatroom,
and number of posts, e.g. ``10-19-20s_706posts.xml`` contains 706 posts gathered from
the 20s chat room on 10/19/2006.

    >>> from nltk.corpus import nps_chat 
    >>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')
    >>> chatroom[123]
    ['i', 'do', "n't", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',
    'I', 'can', 'look', 'in', 'a', 'mirror', '.']

The Brown Corpus
----------------

The Brown Corpus was the first million-word electronic
corpus of English, created in 1961 at Brown University.
This corpus contains text from 500 sources, and the sources
have been categorized by genre, such as *news*, *editorial*, and so on.
Table brown-sources_ gives an example of each genre
(for a complete list, see ``http://icame.uib.no/brown/bcm-los.html``).

.. table:: brown-sources

  ===  ========  ===============  =========================================================================
  ID   File      Genre            Description
  ===  ========  ===============  =========================================================================
  A16  ``ca16``  news             Chicago Tribune: *Society Reportage*
  B02  ``cb02``  editorial        Christian Science Monitor: *Editorials*
  C17  ``cc17``  reviews          Time Magazine: *Reviews*
  D12  ``cd12``  religion         Underwood: *Probing the Ethics of Realtors*
  E36  ``ce36``  hobbies          Norling: *Renting a Car in Europe*
  F25  ``cf25``  lore             Boroff: *Jewish Teenage Culture*
  G22  ``cg22``  belles_lettres   Reiner: *Coping with Runaway Technology*
  H15  ``ch15``  government       US Office of Civil and Defence Mobilization: *The Family Fallout Shelter*
  J17  ``cj19``  learned          Mosteller: *Probability with Statistical Applications*
  K04  ``ck04``  fiction          W.E.B. Du Bois: *Worlds of Color*
  L13  ``cl13``  mystery          Hitchens: *Footsteps in the Night*
  M01  ``cm01``  science_fiction  Heinlein: *Stranger in a Strange Land*
  N14  ``cn15``  adventure        Field: *Rattlesnake Ridge*
  P12  ``cp12``  romance          Callaghan: *A Passion in Rome*
  R06  ``cr06``  humor            Thurber: *The Future, If Any, of Comedy*
  ===  ========  ===============  =========================================================================
  
  Example Document for Each Section of the Brown Corpus

We can access the corpus as a list of words, or a list of sentences (where each sentence
is itself just a list of words).  We can optionally specify particular categories or files to read:

    >>> from nltk.corpus import brown
    >>> brown.categories()
    ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',
    'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',
    'science_fiction']
    >>> brown.words(categories='news')
    ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
    >>> brown.words(files=['cg22'])
    ['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]
    >>> brown.sents(categories=['news', 'editorial', 'reviews'])
    [['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]

We can use the Brown Corpus to study systematic differences between
genres, a kind of linguistic inquiry known as `stylistics`:dt:.
Let's compare genres in their usage of modal verbs.  The first step
is to produce the counts for a particular genre:

    >>> news_text = brown.words(categories='news')
    >>> fdist = nltk.FreqDist(w.lower() for w in news_text)
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> for m in modals:
    ...     print m + ':' + fdist[m],
    ...
    can:94 could:87 may:93 might:38 must:53 will:389

.. note:: |TRY|
   Choose a different section of the Brown Corpus, and adapt the above
   method to count a selection of `wh`:lx: words, such as `what`:lx:,
   `when`:lx:, `where`:lx:, `who`:lx: and `why`:lx:.

Next, we need to obtain counts for each genre of interest.  To save re-typing,
we can put the above code into a function, and use the function several
times over. (We discuss functions in more detail in Section sec-defining-functions_.)
However, there is an even better way, using
|NLTK|\ 's support for conditional frequency distributions
(Section sec-conditional-frequency-distributions_), as follows:

    >>> cfd = nltk.ConditionalFreqDist((g,w)
                  for g in brown.categories()
                  for w in brown.words(categories=g))
    >>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> cfd.tabulate(conditions=genres, samples=modals)
                     can could  may might must will
               news   93   86   66   38   50  389
           religion   82   59   78   12   54   71
            hobbies  268   58  131   22   83  264
    science_fiction   16   49    4   12    8   16
            romance   74  193   11   51   45   43
              humor   16   30    8    8    9   13

Observe that the most frequent modal in the news genre is
`will`:lx:, suggesting a focus on the future, while the most frequent
modal in the romance genre is `could`:lx:, suggesting a focus on possibilities.

Reuters Corpus
--------------

The Reuters Corpus contains 10,788 news documents totaling 1.3 million words.
The documents have been classified into 90 topics, and grouped
into two sets, called "training" and "test" (for training and testing algorithms
that automatically detect the topic of a document, as we will explore further
in Chapter chap-data-intensive_).

    >>> from nltk.corpus import reuters
    ('test/14826', 'test/14828', 'test/14829', 'test/14832', ...)
    >>> reuters.categories() 
    ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',
    'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',
    'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]

Unlike the Brown Corpus, categories in the Reuters corpus overlap with
each other, simply because a news story often covers multiple topics.
We can ask for the topics covered by one or more documents, or for the
documents included in one or more categories. For convenience, the
corpus methods accept a single name or a list of names.

    >>> reuters.categories('training/9865')
    ['barley', 'corn', 'grain', 'wheat']
    >>> reuters.categories(['training/9865', 'training/9880'])
    ['barley', 'corn', 'grain', 'money-fx', 'wheat']
    >>> reuters.files('barley') 
    ['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]
    >>> reuters.files(['barley', 'corn']) 
    ['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',
    'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]


Similarly, we can specify the words or sentences we want in terms of
files or categories. The first handful of words in each of these texts are the
titles, which by convention are stored as upper case.

    >>> reuters.words('training/9865')[:14]
    ['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', 'BIDS',
    'DETAILED', 'French', 'operators', 'have', 'requested', 'licences', 'to', 'export'] 
    >>> reuters.words(['training/9865', 'training/9880'])
    ['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]
    >>> reuters.words(categories='barley')
    ['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]
    >>> reuters.words(categories=['barley', 'corn'])
    ['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]

.. note:: Many other English text corpora are provided with |NLTK|.  For a
   list see Appendix app-modules_.  For more examples of how to access |NLTK| corpora,
   please consult the online guide at ``http://nltk.org/doc/guides/corpus.html``.

US Presidential Inaugural Addresses 
-----------------------------------

In section sect-computing-with-language-texts-and-words_, we looked at
the US Presidential Inaugural Addresses corpus,
but treated it as a single text.  The graph in Figure fig-inaugural_,
used word offset as one of the axes, but this is difficult to interpret.
However, the corpus is actually a collection of 55 texts,
one for each presidential address.  An interesting property of
this collection is its time dimension:

    >>> from nltk.corpus import inaugural
    >>> inaugural.files()
    ('1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...)
    >>> [file[:4] for file in inaugural.files()]
    ['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]

Notice that the year of each text appears in its filename.  To get the year
out of the file name, we extracted the first four characters, using ``file[:4]``.

Let's look at how the words `America`:lx: and `citizen`:lx: are used over time.
The following code will count similar words, such as plurals of these words, or
the word `Citizens`:lx: as it would appear at the start of a sentence (how?).
The result is shown in Figure fig-inaugural2_.

    >>> cfd = nltk.ConditionalFreqDist((target, file[:4])
    ...           for file in inaugural.files()
    ...           for w in inaugural.words(file)
    ...           for target in ['america', 'citizen']
    ...           if w.lower().startswith(target))
    >>> cfd.plot()

.. _fig-inaugural2:
.. figure:: ../images/inaugural2.png
   :scale: 20

   Conditional Frequency Distribution for Two Words in the Inaugural Address Corpus


Corpora in Other Languages
--------------------------

NLTK comes with corpora for many languages, though in some cases
you will need to learn how to manipulate character encodings in Python
before using these corpora (see Appendix app-unicode_).

    >>> nltk.corpus.cess_esp.words()
    ['El', 'grupo', 'estatal', 'Electricit\xe9_de_France', ...]
    >>> nltk.corpus.floresta.words()
    ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]
    >>> nltk.corpus.udhr.files()
    ('Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',
    'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',
    'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...)
    >>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
    ['Saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', ...]
    >>> nltk.corpus.indian.words('hindi.pos')
    ['\xe0\xa4\xaa\xe0\xa5\x82\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa3',
    '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xac\xe0\xa4\x82\xe0\xa4\xa7', ...]

The last of these corpora, ``udhr``, contains the Universal Declaration of Human Rights
in over 300 languages.  (Note that the names of the files in this corpus include
information about character encoding, and for now we will stick with texts in ISO Latin-1, or ASCII)

Let's use a conditional frequency distribution to examine the differences in word lengths,
for a selection of languages included in this corpus.
The output is shown in Figure fig-word-len-dist_ (run the program yourself to see a color plot).

    >>> from nltk.corpus import udhr
    >>> languages = ['Chickasaw', 'English', 'German_Deutsch',
    ...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
    >>> cfd = nltk.ConditionalFreqDist((lang, len(word))
    ...          for lang in languages
    ...          for word in udhr.words(lang + '-Latin1'))
    >>> cfd.plot()

.. _fig-word-len-dist:
.. figure:: ../images/word-len-dist.png
   :scale: 25

   Cumulative Word Length Distributions for Several Languages

.. note:: |TRY|
   Pick a language of interest in ``udhr.files()``, and define a variable
   ``raw_text = udhr.raw('Language-Latin1')``.  Now plot a frequency
   distribution of the letters of the text using ``nltk.FreqDist(raw_text).plot()``.

Unfortunately, for many languages, substantial corpora are not yet available.  Often there is
no government or industrial support for developing language resources, and individual
efforts are piecemeal and hard to discover or re-use.  Some languages have no
established writing system, or are endangered.  A good place to check
is the search service of the *Open Language Archives Community*, at
``http://www.language-archives.org/``. 
This service indexes the catalogs of dozens of language resource archives and publishers.

.. note::
   The most complete inventory of the world's languages is *Ethnologue*, ``http://www.ethnologue.com/``.

Text Corpus Structure
---------------------

The corpora we have seen exemplify a variety of common corpus structures, summarized in Figure fig-text-corpus-structure_.
The simplest kind lacks any structure: it is just a collection of texts.
Often, texts are grouped into categories that might correspond to genre, source, author, language, etc.
Sometimes these categories overlap, notably in the case of topical categories, since a text can be
relevant to more than one topic.  Occasionally, text collections have temporal structure,
news collections being the most common.


.. _fig-text-corpus-structure:
.. figure:: ../images/text-corpus-structure.png
   :scale: 150

   Common Structures for Text Corpora (one point per text)

|NLTK|\ 's corpus readers support efficient access to a variety of corpora, and can
easily be extended to work with new corpora [REF].  Table tab-corpus_ lists the
basic methods provided by the corpus readers. 

.. table:: tab-corpus

   ===============================  ==========================================================
   Example                          Description
   ===============================  ==========================================================
   ``files()``                      the files of the corpus
   ``categories()``                 the categories of the corpus
   ``abspath(file)``                the location of the given file on disk
   ``words()``                      the words of the whole corpus
   ``words(files=[f1,f2,f3])``      the words of the specified files                  
   ``words(categories=[c1,c2])``    the words of the specified categories                  
   ``sents()``                      the sentences of the specified categories
   ``sents(files=[f1,f2,f3])``      the sentences of the specified files                  
   ``sents(categories=[c1,c2])``    the sentences of the specified categories                  
   ===============================  ==========================================================

   Basic Methods Defined in |NLTK|\ 's Corpus Package
   
.. note::
   For more information about |NLTK|\ 's Corpus Package, type ``help(nltk.corpus.reader)``
   at the Python prompt, or see ``http://nltk.org/doc/guides/corpus.html``.
   You will probably have other text sources, stored in files on your computer or accessible
   via the web.  We'll discuss how to work with these in Chapter chap-words_.

Loading your own Corpus
-----------------------

If you have a collection of text files that you would like to access using
the above methods, you can easily load them with the help of |NLTK|\ 's
``PlaintextCorpusReader`` as follows:

    >>> from nltk.corpus import PlaintextCorpusReader
    >>> corpus_root = '/usr/share/dict'
    >>> wordlists = PlaintextCorpusReader(corpus_root, '.*')
    >>> wordlists.files()
    ('README', 'connectives', 'propernames', 'web2', 'web2a', 'words')
    >>> wordlists.words('connectives')
    ['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]

The second parameter of the ``PlaintextCorpusReader`` can be a list
of file pathnames, like ``['a.txt', 'test/b.txt']``, 
or a pattern that matches all file pathnames, like ``'[abc]/.*\.txt'``
(see Section sec-regular-expressions-word-patterns_ for information
about regular expressions).

.. _sec-conditional-frequency-distributions:

-----------------------------------
Conditional Frequency Distributions
-----------------------------------

We introduced frequency distributions in Chapter chap-introduction_,
and saw that given some list ``mylist`` of words or other items,
``FreqDist(mylist)`` would compute the number of occurrences of each
item in the list.  When the texts of a corpus are divided into several
categories, by genre, topic, author, etc, we can maintain separate
frequency distributions for each category to enable study of
systematic differences between the categories.  In the previous
section we achieved this using |NLTK|\ 's ``ConditionalFreqDist`` data
type.  A `conditional frequency distribution`:dt: is a collection of
frequency distributions, each one for a different "condition".  The
condition will often be the category of the text.  Figure tally2_
depicts a fragment of a conditional frequency distribution having just
two conditions, one for news text and one for romance text.

.. _tally2:
.. figure:: ../images/tally2.png
   :scale: 70

   Counting Words Appearing in a Text Collection (a conditional frequency distribution)

Conditions and Events
---------------------

As we saw in Chapter chap-introduction_, a frequency distribution counts
observable events, such as the appearance of words in a text.  A conditional
frequency distribution needs to pair each such event with a condition.
So instead of processing a text (a sequence of words), we have to
process a sequence of pairs:

    >>> text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
    >>> pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]

Each pair has the form ``(condition, event)``.  If we were processing the
entire Brown Corpus by genre there would be 15 conditions (one for each genre),
and 1,161,192 events (one for each word).

[TUPLES]

.. In Python we use the `tuple`:dt: data type, which ...

.. How it differs from a list...

Counting Words by Genre
-----------------------

In section sec-extracting-text-from-corpora_ we saw a conditional
frequency distribution where the condition was the section of the
Brown Corpus, and for each condition we counted words. Whereas
``FreqDist()`` takes a simple list as input, ``ConditionalFreqDist()``
takes a list of pairs.

    >>> cfd = nltk.ConditionalFreqDist((g,w)
    ...                                for g in brown.categories()
    ...                                for w in brown.words(categories=g))

Let's break this
down, and look at just two genres, news and romance.  For each genre,
we loop over every word in the genre, producing pairs consisting of
the genre and the word:

    >>> genre_word = [(g,w) for g in ['news', 'romance'] for w in brown.words(categories=g)]
    >>> len(genre_word)
    170576

So pairs at the beginning of the list ``genre_word`` will be of the form
(``'news'``, *word*) while those at the end will be of the form
(``'romance'``, *word*). (Recall that ``[-4:]`` gives us a slice
consisting of the last four items of a sequence.)

    >>> genre_word[:4]
    [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')]
    >>> genre_word[-4:]
    [('romance', 'afraid'), ('romance', 'not'), ('romance', "''"), ('romance', '.')]

We can now use this list of pairs to create a ``ConditionalFreqDist``, and
save it in a variable ``cfd``.  As usual, we can type the name of the
variable to inspect it, and verify it has two conditions:   

    >>> cfd = nltk.ConditionalFreqDist(genre_word)
    >>> cfd
    <ConditionalFreqDist with 2 conditions>
    >>> cfd.conditions()
    ['news', 'romance']

Let's access the two conditions, and satisfy ourselves that each is just
a frequency distribution:

    >>> cfd['news']
    <FreqDist with 100554 samples>
    >>> cfd['romance']
    <FreqDist with 70022 samples>
    >>> list(cfd['romance'])
    [',', '.', 'the', 'and', 'to', 'a', 'of', '``', "''", 'was', 'I', 'in', 'he', 'had',
    '?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him',
    'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', ...]
    >>> cfd['romance']['could']
    193

Apart from combining two or more frequency distributions, and being easy to initialize,
a ``ConditionalFreqDist`` provides some useful methods for tabulation and plotting.
We can optionally specify which conditions to display with a ``conditions=`` parameter.
When we omit it, we get all the conditions.

.. note:: |TRY|
   Find out which days of the week are most newsworthy, and which are most romantic.
   Define a variable called ``days`` containing a list of days of the week, i.e.
   ``['Monday', ...]``.  Now tabulate the counts for these words using
   ``cfd.tabulate(samples=days)``.  Now try the same thing using ``plot`` in place of ``tabulate``.

Other Conditions
----------------

The plot in Figure fig-word-len-dist_ is based on a conditional frequency distribution
where the condition is the name of the language
and the counts being plotted are derived from word lengths.
It exploits the fact that the filename for each language is the language name followed
by``'-Latin1'`` (the character encoding).
 
    >>> cfd = nltk.ConditionalFreqDist((lang, len(word))
    ...          for lang in languages
    ...          for word in udhr.words(lang + '-Latin1'))

The plot in Figure fig-inaugural2_ is based on a conditional frequency distribution
where the condition is either of two words `america`:lx: or `citizen`:lx:, and the
counts being plotted are the number of times the word occurs in a particular speech.
It expoits the fact that the filename for each speech, e.g. ``1865-Lincoln.txt``
contains the year as the first four characters.

    >>> cfd = nltk.ConditionalFreqDist((target, file[:4])
    ...           for file in inaugural.files()
    ...           for w in inaugural.words(file)
    ...           for target in ['america', 'citizen']
    ...           if w.lower().startswith(target))

This code will generate the tuple ``('america', '1865')`` for
every instance of a word whose lowercased form starts with "america"
|mdash| such as "Americans" |mdash| in the file ``1865-Lincoln.txt``.

Generating Random Text with Bigrams
-----------------------------------

We can use a conditional frequency distribution to create a table of
bigrams (word pairs). (We introducted bigrams in Section
computing-with-language-simple-statistics_.)
The ``bigrams()`` function takes a list of
words and builds a list of consecutive word pairs:

    >>> sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',
    ...   'and', 'the', 'earth', '.']
    >>> nltk.bigrams(sent)
    [('In', 'the'), ('the', 'beginning'), ('beginning', 'God'), ('God', 'created'),
    ('created', 'the'), ('the', 'heaven'), ('heaven', 'and'), ('and', 'the'),
    ('the', 'earth'), ('earth', '.')]
    
In Figure random_, we treat each word as a condition, and for each one
we effectively create a frequency distribution over the following
words.  The function ``generate_model()`` contains a simple loop to
generate text. When we call the function, we choose a word (such as
``'living'``) as our initial context, then once inside the loop, we
print the current value of the variable ``word``, and reset ``word``
to be the most likely token in that context (using ``max()``); next
time through the loop, we use that word as our new context.  As you
can see by inspecting the output, this simple approach to text
generation tends to get stuck in loops; another method would be to
randomly choose the next word from among the available words.

.. pylisting:: random
   :caption: Generating Random Text in the Style of Genesis

   def generate_model(cfdist, word, num=15):
       for i in range(num):
           print word,
           word = cfdist[word].max()

   >>> bigrams = nltk.bigrams(nltk.corpus.genesis.words('english-kjv.txt'))
   >>> cfd = nltk.ConditionalFreqDist(bigrams)
   >>> print cfd['living']
   <FreqDist: 'creature': 7, 'thing': 4, 'substance': 2, ',': 1, '.': 1, 'soul': 1>
   >>> generate_model(cfd, 'living')
   living creature that he said , and the land of the land of the land

Summary
-------

.. table:: conditionalfreqdist

   =======================================  ================================================================
   Example                                  Description
   =======================================  ================================================================
   ``cfdist = ConditionalFreqDist(pairs)``  create a conditional frequency distribution
   ``cfdist.conditions()``                  alphabetically sorted list of conditions
   ``cfdist[condition]``                    the frequency distribution for this condition
   ``cfdist[condition][sample]``            frequency for the given sample for this condition
   ``cfdist.tabulate()``                    tabulate the conditional frequency distribution
   ``cfdist.plot()``                        graphical plot of the conditional frequency distribution
   ``cfdist1 < cfdist2``                    samples in ``cfdist1`` occur less frequently than in ``cfdist2``
   =======================================  ================================================================

   Methods Defined for |NLTK|\ 's Conditional Frequency Distributions

.. _sec-defining-functions:

-------------------------
More Python: Reusing Code
-------------------------

By this time you've probably retyped a lot of code.  If you mess up when retyping a complex example you have
to enter it again.  Using the arrow keys to access and modify previous commands is helpful but only goes so
far.  In this section we see two important ways to reuse code: text editors and Python functions.

Creating Programs with a Text Editor
------------------------------------

The Python interative interpreter performs your instructions as soon as you type
them.  Often, it is better to compose a multi-line program using a text editor,
then ask Python to run the whole program at once.  Using |IDLE|, you can do
this by going to the ``File`` menu and opening a new window.  Try this now, and
enter the following one-line program:

::

     msg = 'Monty Python'

Save this program in a file called ``test.py``, then
go to the ``Run`` menu, and select the command ``Run Module``.
The result in the main |IDLE| window should look like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    >>>

Now, where is the output showing the value of ``msg``? The answer is
that the program in ``test.py`` will show a value only if you explicitly tell
it to, using the ``print`` statement. So add another line to
``test.py`` so that it looks as follows:

::

     msg = 'Monty Python'
     print msg

Select ``Run Module`` again, and this time you should get output that
looks like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    Monty Python
    >>>

From now on, you have a choice of using the interactive interpreter or a
text editor to create your programs.  It is often convenient to test your ideas
using the interpreter, revising a line of code until it does what you expect,
and consulting the interactive help facility.  Once you're ready, you can paste
the code (minus any ``>>>`` prompts) into the text editor,
continue to expand it, and finally save the program
in a file so that you don't have to type it in again later.
Give the file a short but descriptive name, using all lowercase letters and separating
words with underscore, and using the ``.py`` filename extension, e.g. ``monty_python.py``.

.. note::
   Our inline code examples will continue to include the ``>>>`` and ``...`` prompts
   as if we are interacting directly with the interpreter.  As they get more complicated,
   you should instead type them into the editor, without the prompts, and run them
   from the editor as shown above.

Functions
---------

Suppose that you work on analyzing text that involves different forms
of the same word, and that part of your program needs to work out
the plural form of a given singular noun.  Suppose it needs to do this
work in two places, once when it is processing some texts, and again
when it is processing user input.

Rather than repeating the same code several times over, it is more
efficient and reliable to localize this work inside a `function`:dt:.
A function is just a named block of code that performs some well-defined
task.  It usually has some inputs, also known as `parameters`:dt:,
and it may produce a result, also known as a `return value`:dt:.
We define a function using the keyword ``def`` followed by the
function name and any input parameters, followed by the body of the
function.  Here's the function we saw in section sect-computing-with-language-texts-and-words_:

    >>> def score(text):
    ...     return len(text) / len(set(text))

We use the keyword ``return`` to indicate the value that is
produced as output by the function.  In the above example,
all the work of the function is done in the ``return`` statement.
Here's an equivalent definition which does the same work
using multiple lines of code.  We'll change the parameter name
to remind you that this is an arbitrary choice:

    >>> def score(my_text_data):
    ...     word_count = len(my_text_data)
    ...     vocab_size = len(set(my_text_data))
    ...     richness_score = word_count / vocab_size
    ...     return richness_score

Notice that we've created some new variables inside the body of the function.
These are *local variables* and are not accessible outside the function.
Notice also that defining a function like this produces no output.
Functions do nothing until they are "called" (or "invoked").     

Let's return to our earlier scenario, and actually define a simple plural
function.  The function ``plural()`` in Figure plural_
takes a singular noun and generates a plural form (one which is not always
correct).

.. pylisting:: plural
   :caption: Example of a Python function

   def plural(word):
       if word.endswith('y'):
           return word[:-1] + 'ies'
       elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
           return word + 'es'
       elif word.endswith('an'):
           return word[:-2] + 'en'
       return word + 's'

   >>> plural('fairy')
   'fairies'
   >>> plural('woman')
   'women'

(There is much more to be said about functions, but
we will hold off until Section sec-functions_.)

Modules
-------

Over time you will find that you create a variety of useful little text processing functions,
and you end up copy-pasting them from old programs to new ones.  Which file contains the
latest version of the function you want to use?
It makes life a lot easier if you can collect your work into a single place, and
access previously defined functions without any copying and pasting.

To do this, save your function(s) in a file called (say) ``textproc.py``.
Now, you can access your work simply by importing it from the file:

.. doctest-ignore::
    >>> from textproc import plural
    >>> plural('wish')
    wishes
    >>> plural('fan')
    fen

Our plural function has an error, and we'll need to fix it.  This time, we won't
produce another version, but instead we'll fix the existing one.  Thus, at every
stage, there is only one version of our plural function, and no confusion about
which one we should use.

A collection of variable and function definitions in a file is called a Python
`module`:dt:.  A collection of related modules is called a `package`:dt:.
|NLTK|\ 's code for processing the Brown Corpus is an example of a module,
and its collection of code for processing all the different corpora is
an example of a package.  |NLTK| itself is a set of packages, sometimes
called a `library`:dt:.

[Work in somewhere: In general, we use ``import`` statements when we want to get
access to Python code that doesn't already come as part of core
Python. This code will exist somewhere as one or more files. Each such
file corresponds to a Python `module`:dt: |mdash| this is a way of
grouping together code and data that we regard as reusable. When you
write down some Python statements in a file, you are in effect
creating a new Python module. And you can make your code depend on
another module by using the ``import`` statement.]

.. caution:: If you are creating a file to contain some of your Python
   code, do *not* name your file ``nltk.py``: it may get imported in
   place of the "real" NLTK package. (When it imports modules, Python
   first looks in the current folder / directory.)

.. _sec-lexical-resources:

-----------------
Lexical Resources
-----------------

A lexicon, or lexical resource, is a collection of words and/or phrases along
with associated information such as part of speech and sense definitions.
Lexical resources are secondary to texts, and are usually created and enriched with the help
of texts.  For example, if we have a defined a text ``my_text``, then
``vocab = sorted(set(my_text))`` builds the vocabulary of ``my_text``,
while ``word_freq = FreqDist(my_text)`` counts the frequency of each word in the text.  Both
of ``vocab`` and ``word_freq`` are simple lexical resources.  Similarly, a concordance 
(Section sect-computing-with-language-texts-and-words_)
gives us information about word usage that might help in the preparation of
a dictionary.  

Standard terminology for lexicons is illustrated in Figure fig-lexicon_.

.. _fig-lexicon:
.. figure:: ../images/lexicon.png
   :scale: 50
   
   Lexicon Terminology

The simplest kind of lexicon is nothing more than a sorted list of words.
Sophisticated lexicons include complex structure within and across
the individual entries.  In this section we'll look at some lexical resources
included with |NLTK|.

Wordlist Corpora
----------------

|NLTK| includes some corpora that are nothing more than wordlists.
The Words corpus is the ``/usr/dict/words`` file from Unix, used by
some spell checkers.  We can use it to find unusual or mis-spelt
words in a text corpus, as shown in Figure unusual_.

.. pylisting:: unusual
   :caption: Using a Lexical Resource to Filter a Text

    def unusual_words(text):
        text_vocab = set(w.lower() for w in text if w.isalpha())
        english_vocab = set(w.lower() for w in nltk.corpus.words.words())
        unusual = text_vocab.difference(english_vocab)
        return sorted(unusual)
    
    >>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))
    ['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary',
    'adieus', 'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham',
    'amiably', 'annamaria', 'annuities', 'apologising', 'arbour', 'archness', ...]
    >>> unusual_words(nltk.corpus.nps_chat.words())
    ['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros',
    'actualy', 'adduser', 'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk',
    'agaibn', 'agurlwithbigguns', 'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', ...]

There is also a corpus of `stopwords`:dt:, that is, high-frequency
words like `the`:lx:, `to`:lx: and `also`:lx: that we sometimes
want to filter out of a document before further processing. Stopwords
usually have little lexical content, and their presence in a text fail
to distinguish it from other texts.

    >>> from nltk.corpus import stopwords
    >>> stopwords.words('english')
    ['a', "a's", 'able', 'about', 'above', 'according', 'accordingly', 'across',
    'actually', 'after', 'afterwards', 'again', 'against', "ain't", 'all', 'allow',
    'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', ...]

Let's define a function to compute what fraction of words in a text are *not* in the
stopwords list:

    >>> def content_fraction(text):
    ...     stopwords = nltk.corpus.stopwords.words('english')
    ...     content = [w for w in text if w.lower() not in stopwords]
    ...     return 1.0 * len(content) / len(text)
    ...    
    >>> content_fraction(nltk.corpus.reuters.words())
    0.65997695393285261

Thus, with the help of stopwords we filter out a third of the words of the text.
Notice that we've combined two different kinds of corpus here, using a lexical
resource to filter the content of a text corpus.

.. _fig-target:
.. figure:: ../images/target.png
   :scale: 36

   A Word Puzzle Known as "Target" 

A wordlist is useful for solving word puzzles, such as the one in Figure fig-target_.
Our program iterates through every word and, for each one, checks whether it meets
the conditions.  The obligatory letter and length constraint are easy to check (and we'll
only look for words with six or more letters here).
It is trickier to check that candidate solutions only use combinations of the
supplied letters, especially since some of the latter appear twice (here, the letter `v`:lx:).
We use the ``FreqDist`` comparison method to check that the frequency of each
*letter* in the candidate word is less than or equal to the frequency of the
corresponding letter in the puzzle.

    >>> puzzle_letters = nltk.FreqDist('egivrvonl')
    >>> obligatory = 'r'
    >>> wordlist = nltk.corpus.words.words()
    >>> [w for w in wordlist if len(w) >= 6
                             and obligatory in w
                             and nltk.FreqDist(w) <= puzzle_letters]
    ['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor',
    'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi',
    'revolving', 'ringle', 'roving', 'violer', 'virole']

.. note:: |TRY|
   Can you think of an English word that contains `gnt`:lx:?  Write Python code
   to find any such words in the wordlist.

One more wordlist corpus is the Names corpus, containing 8,000 first names categorized by gender.
The male and female names are stored in separate files.  Let's find names which appear
in both files, i.e. names that are ambiguous for gender:

    >>> names = nltk.corpus.names
    >>> names.files()
    ('female.txt', 'male.txt')
    >>> male_names = names.words('male.txt')
    >>> female_names = names.words('female.txt')
    >>> [w for w in male_names if w in female_names]
    ['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',
    'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',
    'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ..]

It is well known that names ending in the letter `a`:lx: are almost always female.
We can see this and some other patterns in the graph in Figure fig-cfd-gender_,
produced by the following code:

    >>> cfd = nltk.ConditionalFreqDist((file, name[-1])
    ...           for file in names.files()
    ...           for name in names.words(file))
    >>> cfd.plot()

.. _fig-cfd-gender:
.. figure:: ../images/cfd-gender.png
   :scale: 25

   Frequency of Final Letter of Female vs Male Names


A Pronouncing Dictionary
------------------------

As we have seen, the entries in a wordlist lack internal structure |mdash| they are just words.
A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word
plus some properties in each row.  |NLTK| includes the CMU Pronouncing
Dictionary for US English, which was designed for
use by speech synthesizers. 

    >>> entries = nltk.corpus.cmudict.entries()
    >>> len(entries)
    127069
    >>> for entry in entries[40000:40010]:
    ...     print entry
    ... 
    ('fir', 1, ('F', 'ER1'))
    ('fire', 1, ('F', 'AY1', 'ER0'))
    ("fire's", 1, ('F', 'AY1', 'ER0', 'Z'))
    ('fire', 2, ('F', 'AY1', 'R'))
    ('firearm', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M'))
    ('firearm', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M'))
    ('firearms', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'))
    ('firearms', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'))
    ('fireball', 1, ('F', 'AY1', 'ER0', 'B', 'AO2', 'L'))
    ('fireball', 2, ('F', 'AY1', 'R', 'B', 'AO2', 'L'))

For each word, this lexicon provides a pronunciation number and a list of phonetic
codes |mdash| distinct labels for each contrastive sound |mdash|
known as `phones`:lx:.  Observe that `fire`:lx: has two pronunciations
(in US English):
the one-syllable ``F AY1 R``, and the two-syllable ``F AY1 ER0``.
The symbols in the CMU Pronouncing Dictionary are from the *Arpabet*,
described in more detail at ``http://en.wikipedia.org/wiki/Arpabet``

Each entry consists of three pieces of information, and we can
process these individually, using a more complex version of the ``for`` statement.
Instead of writing ``for entry in entries:``, we replace
``entry`` with *three* variable names.  The three pieces of a single
entry are assigned to each of the three variables.  One of the
variable names is "_" and we'll use this name when we don't
plan to do anything with the variable later.

    >>> for w, _, pron in entries:
    ...     if len(pron) == 3:
    ...         ph1, ph2, ph3 = pron
    ...         if ph1 == 'P' and ph3 == 'T':
    ...             print w, ph2,
    ...
    pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1
    pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1
    pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1

The above program scans the lexicon for entries with a pronunciation consisting of
three phones (``len(pron) == 3``).  If the condition is true, we assign the contents
of ``pron`` to three new variables ``ph1``, ``ph2`` and ``ph3``.  Notice the unusual
form of the statement which does that work: ``ph1, ph2, ph3 = pron``.

Here's another example of the same ``for`` statement, this time used inside a list
comprehension.  This program finds all words whose pronunciation ends with a syllable
sounding like `nicks`:lx:.  You could use this method to find rhyming words.

    >>> syllable = ('N', 'IH0', 'K', 'S')
    >>> [w for w, _, pron in entries if pron[-4:] == syllable]
    ["atlantic's", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',
    'chetniks', "clinic's", 'clinics', 'conics', 'cynics', 'diasonics', "dominic's",
    'ebonics', 'electronics', "electronics'", 'endotronics', "endotronics'", 'enix', ...]

Notice that the one pronunciation is spelt in several ways: `nics`:lx:, `niks`:lx:, `nix`:lx:,
even `ntic's`:lx: with a silent `t`:lx:, for the word `atlantic's`:lx:.  Let's look for some other
mismatches between pronunciation and writing.  Can you summarize the purpose of
the following examples and explain how they work?

    >>> [w for w, _, pron in entries if pron[-1] == 'M' and w[-1] == 'n']
    ['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']
    >>> sorted(set(w[:2] for w, _, pron in entries if pron[0] == 'N' and w[0] != 'n'))
    ['gn', 'kn', 'mn', 'pn']

The phones contain digits, to represent 
primary stress (``1``), secondary stress (``2``) and no stress (``0``).
As our final example, we define a function to extract the stress digits
and then scan our lexicon to find words having a particular stress pattern.

    >>> def stress(pron):
    ...     return [int(char) for phone in pron for char in phone if char.isdigit()] 
    >>> [w for w, _, pron in entries if stress(pron) == [0, 1, 0, 2, 0]]
    ['abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator',
    'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative',
    'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators', ...]
    >>> [w for w, _, pron in entries if stress(pron) == [0, 2, 0, 1, 0]]
    ['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',
    'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',
    'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]

Note that this example has a user-defined function inside the condition of
a list comprehension.

We can use any lexical resource to process a text, e.g. to filter out words having
some lexical property (like nouns), or mapping every word of the text.
For example, the following text-to-speech function looks up each word
of the text in the pronunciation dictionary.

    >>> prondict = nltk.corpus.cmudict.transcriptions()
    >>> text = ['natural', 'language', 'processing']
    >>> [ph for w in text for ph in prondict[w][0]]
    ['N', 'AE1', 'CH', 'ER0', 'AH0', 'L', 'L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH',
    'P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']

Rather than iterating over the whole dictionary, we can also access it
by looking up particular words.  (This uses Python's dictionary data
structure, which we will study in Section sec-dictionaries_.)

    >>> prondict = nltk.corpus.cmudict.transcriptions()
    >>> prondict['fire']
    [('F', 'AY1', 'ER0'), ('F', 'AY1', 'R')]
    >>> prondict['blog']
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    KeyError: 'blog'
    >>> prondict['blog'] = [('B', 'L', 'AA1', 'G')]
    >>> prondict['blog']
    [('B', 'L', 'AA1', 'G')]

We look up a dictionary by specifying its name, followed by a `key`:dt:
(such as the word `fire`:lx:) inside square brackets: ``prondict['fire']``.
If we try to look up a non-existent key, we get a ``KeyError``,
as we did when indexing a list with an integer that was too large.
The word `blog`:lx: is missing from the pronouncing dictionary,
so we tweak our version by assigning a value for this key
(this has no effect on the |NLTK| corpus; next time we access it,
`blog`:lx: will still be absent).

[Summary of tabular lexicons; forward reference to discussion about processing CSV files]

Comparative Wordlists
---------------------

Another example of a tabular lexicon is the `comparative wordlist`:dt:, a spreadsheet
with one row for each word, and a column for each of several different languages.
|NLTK| includes so-called `Swadesh wordlists`:dt:, lists of about 200 common words
in several languages.  The languages are identified using an ISO 639 two-letter code.

    >>> from nltk.corpus import swadesh
    >>> swadesh.files()
    ('be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk',
    'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk')
    >>> swadesh.words('en')
    ['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that',
    'here', 'there', 'who', 'what', 'where', 'when', 'how', 'not', 'all', 'many', 'some',
    'few', 'other', 'one', 'two', 'three', 'four', 'five', 'big', 'long', 'wide', ...] 

We can also access cognate words from multiple languages using the ``entries()`` method,
specifying a list of languages.  With one further step we can convert this into a simple dictionary.

    >>> fr2en = swadesh.entries(['fr', 'en'])
    >>> fr2en
    [('je', 'I'), ('tu, vous', 'you (singular), thou'), ('il', 'he'), ('nous', 'we'), ...]
    >>> translate = dict(fr2en)
    >>> translate['chien']
    'dog'
    >>> translate['jeter']
    'throw'

We can make our simple translator more useful by adding other source languages.
Let's get the German-English and Spanish-English pairs, convert each to a
dictionary, then *update* our original ``translate`` dictionary with these
additional mappings:

    >>> de2en = swadesh.entries(['de', 'en'])    # German-English
    >>> es2en = swadehs.entries(['es', 'en'])    # Spanish-English
    >>> translate.update(dict(de2en))
    >>> translate.update(dict(es2en))
    >>> translate['Hund']
    'dog'
    >>> translate['perro']
    'dog'
    
(We will return to Python's dictionary data type ``dict()`` in Section sec-dictionaries_.)
We can compare words in various Germanic and Romance languages:

    >>> languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'it', 'la']
    >>> for i in [139, 140, 141, 142]:
    ...     print swadesh.entries(languages)[i]
    ... 
    ('say', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dire', 'dicere')
    ('sing', 'singen', 'zingen', 'cantar', 'chanter', 'cantar', 'cantare', 'canere')
    ('play', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar, brincar', 'giocare', 'ludere')
    ('float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar, boiar', 'galleggiare', 'fluctuare')

Shoebox and Toolbox Lexicons
----------------------------

Perhaps the single most popular tool used by linguists for managing data
is *Toolbox*, previously known as *Shoebox*
(freely downloadable from ``http://www.sil.org/computing/toolbox/``).
A Toolbox file consists of a collection of entries,
where each entry is made up of one or more fields.
Most fields are optional or repeatable, which means that this kind of
lexical resource cannot be treated as a table or spreadsheet.

Here is an example of an entry in the dictionary of the Rotokas language, for
the word `kaa`:lx: meaning "to gag":

    >>> from nltk.corpus import toolbox
    >>> toolbox.entries('rotokas.dic')
    [('kaa', [('ps', 'V'), ('pt', 'A'), ('ge', 'gag'), ('tkp', 'nek i pas'), ('dcsv', 'true'),
    ('vx', '1'), ('sc', '???'), ('dt', '29/Oct/2005'),
    ('ex', 'Apoka ira kaaroi aioa-ia reoreopaoro.'),
    ('xp', 'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.'),
    ('xe', 'Apoka is gagging from food while talking.')]), ...]

Each field has a label such as ``ps`` (part of speech) and contents, such as ``V`` (verb).
The ``ge`` field contains the English gloss, and the last three fields contain
an example sentence in Rotokas and its translations into Tok Pisin and English.

The loose structure of Toolbox files makes it hard for us to do much more with them
at this stage.  XML provides a powerful way to process this kind of corpus and
we will return to this topic in Chapter chap-data_.


.. note::
   The Rotokas language is spoken on the island of Bougainville, Papua New Guinea.
   This lexicon was contributed to |NLTK| by Stuart Robinson.   
   Rotokas is notable for having the smallest alphabet of any written language
   (12 letters), ``http://en.wikipedia.org/wiki/Rotokas_language``
    
.. _wordnet:   

-------
WordNet
-------

`WordNet`:idx: is a semantically-oriented dictionary of English,
similar to a traditional thesaurus but with a richer structure.
|NLTK| includes the English WordNet, with 155,287 words
and 117,659 "synonym sets" or synsets.  It is
accessible to you in the variable ``wordnet`` (so long as you have already imported
the book module, using ``from nltk.book import *``).

    >>> for synset in wordnet.synsets('dish'):
    ...     print synset.name, synset.definition
    ...
    dish.n.01 a piece of dishware normally used as a container for holding or serving food
    dish.n.02 a particular item of prepared food
    dish.n.03 the quantity that a dish will hold
    smasher.n.02 a very attractive or seductive looking woman
    dish.n.05 directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation
    cup_of_tea.n.01 an activity that you like or at which you are superior
    serve.v.06 provide (usually but not necessarily food)
    dish.v.02 make concave; shape like a dish
    >>>

Each synset is characterized by a set of synonymous words:
    
    >>> wn.synset('dish.n.04').lemma_names
    ['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart',
    'peach', 'lulu', 'looker', 'mantrap', 'dish']
    >>>

We can access semantically related forms.  The `hypernyms`:dt: of a word sense are more general terms... 

    >>> wordnet.synset('dish.n.01').hypernyms()
    [Synset('container.n.01'), Synset('crockery.n.01')]
    >>> wordnet.synset('crockery.n.01').hypernyms()
    [Synset('tableware.n.01')]
    >>>

.. note::
   |NLTK| includes a convenient web-browser interface to WordNet
   ``nltk.wordnet.browser()``

Senses and Synonyms
-------------------

.. senses in order of decreasing frequency?
.. how to access frequency?

Consider the sentence in carex1_.
If we replace the word `motorcar`:lx: in carex1_ by `automobile`:lx:,
to get carex2_, the meaning of the sentence stays pretty much the same:

.. ex::
   .. _carex1:
   .. ex::
      Benz is credited with the invention of the motorcar.

   .. _carex2:
   .. ex::
      Benz is credited with the invention of the automobile.

Since everything else in the sentence has remained unchanged, we can
conclude that the words `motorcar`:lx: and `automobile`:lx: have the
same meaning, i.e. they are `synonyms`:dt:.  Let's explore these
words with the help of WordNet: 

    >>> from nltk.corpus import wordnet as wn
    >>> wn.synsets('motorcar')
    [Synset('car.n.01')]

Thus, `motorcar`:lx: has just one possible meaning and it is identified as ``car.n.01``,
the first noun sense of `car`:lx:.  The entity ``car.n.01`` is called a `synset`:dt:,
or "synonym set", a collection of synonymous words (or "lemmas"):

    >>> wn.synset('car.n.01').lemma_names
    ['car', 'auto', 'automobile', 'machine', 'motorcar']

Each word of a synset can have several meanings, e.g. `car`:lx: can also signify
a train carriage, a gondola, or an elevator car.  However, we are only interested
in the single meaning that is common to all words of the synset.  Each synset
also comes with a prose definition and some example sentences:

    >>> wn.synset('car.n.01').definition
    'a motor vehicle with four wheels; usually propelled by an internal combustion engine'
    >>> wn.synset('car.n.01').examples
    ['he needs a car to get to work']

Although these help humans understand the intended meaning of a synset,
the words of the synset are often more useful for our programs.
To eliminate ambiguity, we will identify these words as
``car.n.01.automobile``, ``car.n.01.motorcar``, and so on.
This pairing of a synset with a word is called a lemma,
and here's how to access them:

    >>> wn.synset('car.n.01').lemmas
    [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'),
    Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]
    >>> wn.lemma('car.n.01.automobile')
    Lemma('car.n.01.automobile')
    >>> wn.lemma('car.n.01.automobile').synset
    'car.n.01'
    >>> wn.lemma('car.n.01.automobile').name
    'automobile'

Unlike the words `automobile`:lx: and `motorcar`:lx:, the word `car`:lx: is
ambiguous:

    >>> wn.synsets('car')
    [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'),
    Synset('cable_car.n.01')]

Each of these five synsets has its own lemmas: 

    >>> for synset in wn.synsets('car'):
    ...     print synset.lemma_names
    ... 
    ['car', 'auto', 'automobile', 'machine', 'motorcar']
    ['car', 'railcar', 'railway_car', 'railroad_car']
    ['car', 'gondola']
    ['car', 'elevator_car']
    ['cable_car', 'car']

For good measure, we can access all the lemmas involving the word `car`:lx:
as follows:

    >>> wn.lemmas('car')
    [Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'),
    Lemma('car.n.04.car'), Lemma('cable_car.n.01.car')]


The WordNet Hierarchy
---------------------

WordNet synsets correspond to abstract concepts, and they don't always
have corresponding words in English.  These concepts are linked together in a hierarchy.
Some concepts are very general, such as *Entity*, *State*, *Event* |mdash| these are called
`unique beginners`:dt:.  Others, such as *gas guzzler* and
*hatchback*, are much more specific. A small portion of a concept
hierarchy is illustrated in Figure wn-hierarchy_. The edges between nodes
indicate the hypernym/hyponym relation...

.. _wn-hierarchy:
.. figure:: ../images/wordnet-hierarchy.png
   :scale: 15

   Fragment of WordNet Concept Hierarchy

WordNet makes it easy to navigate between concepts.
For example, given a concept like *motorcar*,
we can look at the concepts that are more specific;
the (immediate) `hyponyms`:dt:.

    >>> motorcar = wn.synset('car.n.01')
    >>> types_of_motorcar = motorcar.hyponyms()
    >>> types_of_motorcar[26]
    Synset('ambulance.n.01')
    >>> sorted([lemma.name for synset in types_of_motorcar for lemma in synset.lemmas])
    ['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon',
    'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible',
    'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car',
    'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap',
    'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover',
    'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car',
    'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer',
    'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan',
    'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car',
    'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car',
    'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon', 'wagon']

We can also navigate up the hierarchy by visiting hypernyms.  Some words
have multiple paths, because they can be classified in more than one way.
There are two paths between ``car.n.01`` and ``entity.n.01`` because
``wheeled_vehicle.n.01`` can be classified either as a vehicle or as a container.
 
    >>> motorcar.hypernyms()
    [Synset('motor_vehicle.n.01')]
    >>> [synset.name for synset in motorcar.hypernym_paths()[1]]
    ['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02',
    'artifact.n.01', 'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n.01',
    'wheeled_vehicle.n.01', 'self-propelled_vehicle.n.01', 'motor_vehicle.n.01',
    'car.n.01']]

More Lexical Relations
----------------------

Hypernyms and hyponyms are called lexical "relations" because they relate one
synset to another.  These two relations navigate up and down the "is-a" hierarchy.
Another important way to navigate the WordNet network is from items to their
components (`meronyms`:dt:) or to the things they are contained in (`holonyms`:lx:).
For example, the parts of a `tree`:lx: are its `trunk`:lx:, `crown`:lx:, and so on;
the ``part_meronyms()``.
The *substance* a tree is made of include `heartwood`:lx: and `sapwood`:lx:;
the ``substance_meronyms()``.
A collection of trees forms a `forest`:lx:; the ``member_holonyms()``:

    >>> wn.synset('tree.n.01').part_meronyms()
    [Synset('burl.n.02'), Synset('crown.n.07'), Synset('stump.n.01'),
    Synset('trunk.n.01'), Synset('limb.n.02')]
    >>> wn.synset('tree.n.01').substance_meronyms()
    [Synset('heartwood.n.01'), Synset('sapwood.n.01')]
    >>> wn.synset('tree.n.01').member_holonyms()
    [Synset('forest.n.01')]

To see just how intricate things can get, consider the word `mint`:lx:, which
has several closely-related senses.  We can see that ``mint.n.04`` is part of
``mint.n.02`` and the substance from which ``mint.n.05`` is made.

    >>> for synset in wn.synsets('mint', wn.NOUN):
    ...     print synset.name + ':', synset.definition
    ... 
    batch.n.02: (often followed by `of') a large number or amount or extent
    mint.n.02: any north temperate plant of the genus Mentha with aromatic leaves and small mauve flowers
    mint.n.03: any member of the mint family of plants
    mint.n.04: the leaves of a mint plant used fresh or candied
    mint.n.05: a candy that is flavored with a mint oil
    mint.n.06: a plant where money is coined by authority of the government
    >>> wn.synset('mint.n.04').part_holonyms()
    [Synset('mint.n.02')]
    >>> wn.synset('mint.n.04').substance_holonyms()
    [Synset('mint.n.05')]

Many verbs have entailments:

    >>> wn.synset('walk.v.01').entailments()
    [Synset('step.v.01')]
    >>> wn.synset('eat.v.01').entailments()
    [Synset('swallow.v.01'), Synset('chew.v.01')]
    >>> wn.synset('tease.v.03').entailments()
    [Synset('arouse.v.07'), Synset('disappoint.v.01')]

[Some adjectives have antonyms...]

Semantic Similarity
-------------------

.. TODO: discuss WSD, mention Semcor, give pine cone example

The 117,659 synsets of WordNet are linked by a complex network of
lexical relations.  We can use this network to discover how
closely related any pair of concepts are.  A simple method is
to find the most specific hypernym(s) that are shared by
two synsets.  In the following examples, we see that 

    >>> grail = wn.synset('grail.n.01')
    >>> swallow = wn.synset('swallow.n.03')
    >>> python = wn.synset('python.n.01')
    >>> comedy = wn.synset('comedy.n.01')
    >>> comedy.lowest_common_hypernyms(grail)
    [Synset('abstraction.n.06')]
    >>> wn.synset('abstraction.n.06').min_depth()
    1
    >>> grail.lowest_common_hypernyms(swallow)
    [Synset('entity.n.01')]
    >>> wn.synset('entity.n.01').min_depth()
    0
    >>> swallow.lowest_common_hypernyms(python)
    [Synset('vertebrate.n.01')]
    >>> wn.synset('vertebrate.n.01').min_depth()
    8

From this we might conclude that `swallow`:lx: is
more closely related to `python`:lx: than to `grail`:lx:.

The WordNet package includes a variety of sophisticated measures
that incorporate this basic insight.  For example,
``path_similarity`` assigns a score in the range ``0``\ |ndash|\
``1``, based on the shortest path that connects the concepts in the hypernym
hierarchy (``-1`` is returned in those cases where a path cannot be
found).  Comparing a synset with itself will return ``1``.

    >>> comedy.path_similarity(grail)
    0.083333333333333329
    >>> grail.path_similarity(swallow)
    0.050000000000000003
    >>> swallow.path_similarity(python)
    0.090909090909090912

This is a convenient interface, and gives us the same relative ordering as before.

NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet.
It can be accessed with ``nltk.corpus.verbnet``.

-------
Summary
-------

* A text corpus is a balanced collection of texts.  NLTK comes with many corpora,
  e.g. the Brown Corpus, ``nltk.corpus.brown``.
* Some text corpora are categorized, e.g. by genre or topic; sometimes the
  categories of a corpus overlap each other.
* To find out about some variable ``v`` that you have created,
  type ``help(v)`` to read the help entry for this kind of object.
* Some functions are not available by default, but must be accessed using
  Python's ``import`` statement.

-----------------------
Further Reading (NOTES)
-----------------------

Natural Language Processing
---------------------------

Several websites have useful information about |NLP|, including
conferences, resources, and special-interest groups, e.g.
``www.lt-world.org``, ``www.aclweb.org``, ``www.elsnet.org``.
The website of the *Association for Computational Linguistics*,
at ``www.aclweb.org``, contains an overview of computational linguistics,
including copies of introductory chapters from recent textbooks.
Wikipedia has entries for |NLP| and its subfields
(but don't confuse natural language processing with
the other |NLP|\ : neuro-linguistic programming.)
The new, second edition of *Speech and Language Processing*, is
a more advanced textbook that builds on the material presented here.
Three books provide comprehensive surveys of the field:
[Cole97]_, [Dale00handbook]_, [Mitkov02handbook]_.
Several |NLP| systems have online interfaces that you might like to
experiment with, e.g.:

* WordNet: ``http://wordnet.princeton.edu/``
* Translation: ``http://world.altavista.com/``
* ChatterBots: ``http://www.loebner.net/Prizef/loebner-prize.html``
* Question Answering: ``http://www.answerbus.com/``
* Summarization: ``http://newsblaster.cs.columbia.edu/``

Python
------

[vanRossum2006IP]_ is a Python tutorial by Guido van
Rossum, the inventor of Python and Fred Drake, the official
editor of the Python documentation. It is available online at
``http://docs.python.org/tut/tut.html``. A more detailed but still
introductory text is [Lutz2003LP]_, which covers the essential
features of Python, and also provides an overview of the standard libraries.
A more advanced text, [vanRossum2006PLR]_ is the official reference
for the Python language itself, and describes the syntax of Python and
its built-in datatypes in depth. It is also available online at
``http://docs.python.org/ref/ref.html.``
[Beazley2006PER]_ is a succinct reference book; although not suitable
as an introduction to Python, it is an excellent resource for
intermediate and advanced programmers.
Finally, it is always worth checking the official *Python
Documentation* at http://docs.python.org/.

Two freely available online texts are the following:

* Josh Cogliati, *Non-Programmer's Tutorial for Python*,
  ``http://en.wikibooks.org/wiki/Non-Programmer's_Tutorial_for_Python/Contents``

*  Jeffrey Elkner, Allen B. Downey and Chris Meyers, 
   *How to Think Like a Computer Scientist: Learning with Python* (Second Edition),
   ``http://openbookproject.net/thinkCSpy/``

Learn more about functions in Python by reading Chapter 4 of
[Lutz2003LP]_.

Archives of the CORPORA mailing list.

[Woods86]_

LDC, ELRA

The online documentation at |NLTK-API| contains extensive reference material
for all |NLTK| modules.

Although WordNet was originally developed for research
in psycholinguistics, it is widely used in NLP and Information Retrieval.
WordNets are being developed for many other languages, as documented
at ``http://www.globalwordnet.org/``.

For a detailed comparison of wordnet similarity measures, see [Budanitsky2006EWB]_.

---------
Exercises
---------

#. |easy| How many words are there in ``text2``?  How many
   distinct words are there?

#. |easy| Compare the lexical diversity scores for humor
   and romance fiction in Table brown-types_.  Which genre is
   more lexically diverse?

#. |easy| Produce a dispersion plot of the four main protagonists in
   *Sense and Sensibility*: Elinor, Marianne, Edward, Willoughby.
   What can you observe about the different roles played by the males
   and females in this novel?  Can you identify the couples?

#. |easy| According to Strunk and White's *Elements of Style*,
   the word `however`:lx:, used at the start of a sentence,
   means "in whatever way" or "to whatever extent", and not
   "nevertheless".  They give this example of correct usage:
   `However you advise him, he will probably do as he thinks best.`:lx:
   (http://www.bartleby.com/141/strunk3.html)
   Use the concordance tool to study actual usage of this word
   in the various texts we have been considering.

#. |easy| Create a variable ``phrase`` containing a list of words.
   Experiment with the operations described in this chapter, including addition,
   multiplication, indexing, slicing, and sorting. 

#. |easy| The first sentence of ``text3`` is provided to you in the
   variable ``sent3``.  The index of `the`:lx: in ``sent3`` is 1, because ``sent3[1]``
   gives us ``'the'``.  What are the indexes of the two other occurrences
   of this word in ``sent3``?

#. |easy| Using the Python interactive interpreter, experiment with
   the examples in this section.  Think of a short phrase and
   represent it as a list of strings, e.g. ['Monty', 'Python'].
   Try the various operations for indexing, slicing and sorting the elements
   of your list.

#. |easy| Investigate the holonym / meronym relations for some nouns.  Note that there
   are three kinds (member, part, substance), so access is more specific,
   e.g., ``wordnet.MEMBER_MERONYM``, ``wordnet.SUBSTANCE_HOLONYM``.

#. |easy| The polysemy of a word is the number of senses it has.
   Using WordNet, we can determine that the noun *dog* has 7 senses
   with: ``len(nltk.wordnet.N['dog'])``.
   Compute the average polysemy of nouns, verbs, adjectives and
   adverbs according to WordNet.

#. |easy| Using the Python interpreter in interactive mode, experiment with
   the dictionary examples in this chapter.  Create a dictionary ``d``, and add
   some entries.  What happens if you try to access a non-existent
   entry, e.g. ``d['xyz']``?

#. |easy| Try deleting an element from a dictionary, using the syntax
   ``del d['abc']``.  Check that the item was deleted.

#. |easy| Create a dictionary ``e``, to represent a single lexical entry
   for some word of your choice.
   Define keys like ``headword``, ``part-of-speech``, ``sense``, and
   ``example``, and assign them suitable values.

#. |easy| Try the examples in this section, then try the following.

   a) Create a variable called ``msg`` and put a message
      of your own in this variable.  Remember that strings need
      to be quoted, so you will need to type something like:
      ``msg = "I like NLP!"``
   b) Now print the contents of this variable in two ways, first
      by simply typing the variable name and pressing enter, then
      by using the ``print`` statement.
   c) Try various arithmetic expressions using this string, e.g.
      ``msg + msg``, and ``5 * msg``.
   d) Define a new string ``hello``, and then try ``hello + msg``.
      Change the ``hello`` string so that it ends with a space
      character, and then try ``hello + msg`` again.

#. |easy| Consider the following two expressions which have the same
   result.  Which one will typically be more relevant in |NLP|?  Why?

   a) ``"Monty Python"[6:12]``
   b) ``["Monty", "Python"][1]``

#. |easy| Define a string ``s = 'colorless'``.  Write a Python statement
   that changes this to "colourless" using only the slice and
   concatenation operations.

#. |easy| Try the slice examples from this section using the interactive
   interpreter.  Then try some more of your own.  Guess what the result
   will be before executing the command.

#. |easy| We can use the slice notation to remove morphological endings on
   words.  For example, ``'dogs'[:-1]`` removes the last character of
   ``dogs``, leaving ``dog``.  Use slice notation to remove the
   affixes from these words (we've inserted a hyphen to
   indicate the affix boundary, but omit this from your strings):
   ``dish-es``, ``run-ning``, ``nation-ality``, ``un-do``,
   ``pre-heat``.

#. |easy| We saw how we can generate an ``IndexError`` by indexing beyond the end
   of a string.  Is it possible to construct an index that goes too far to
   the left, before the start of the string?

#. |easy| We can also specify a "step" size for the slice. The following
   returns every second character within the slice: ``msg[6:11:2]``.
   It also works in the reverse direction: ``msg[10:5:-2]``
   Try these for yourself, then experiment with different step values.

#. |easy| What happens if you ask the interpreter to evaluate ``msg[::-1]``?
   Explain why this is a reasonable result.

#. |easy| Define a conditional frequency distribution over the Names corpus
   that allows you to see which initial letters are more frequent for males
   vs females (cf. Figure fig-cfd-gender_).

#. |easy| Use the corpus module to read ``austen-persuasion.txt``.
   How many word tokens does this book have?  How many word types?

#. |easy| Use the Brown corpus reader ``nltk.corpus.brown.words()`` or the Web text corpus
   reader ``nltk.corpus.webtext.words()`` to access some sample text in two different genres.

#. |easy| Read in the texts of the *State of the Union* addresses, using the
   ``state_union`` corpus reader.  Count occurrences of ``men``, ``women``,
   and ``people`` in each document.  What has happened to the usage of these
   words over time?

#. |soso| Consider the following Python expression: ``len(set(text4))``.
   State the purpose of this expression.  Describe the two steps
   involved in performing this computation.

#. |soso| Pick a pair of texts and study the differences between them,
   in terms of vocabulary, vocabulary richness, genre, etc.  Can you
   find pairs of words which have quite different meanings across the
   two texts, such as `monstrous`:lx: in *Moby Dick* and in *Sense and Sensibility*?

#. |soso| Use ``text9.index(??)`` to find the index of the word `sunset`:lx:.
   By a process of trial and error, find the slice for the complete sentence that
   contains this word.

#. |soso| Using list addition, and the ``set`` and ``sorted`` operations, compute the
   vocabulary of the sentences ``sent1`` ... ``sent8``.

#. |soso| What is the difference between ``sorted(set(w.lower() for w in text1))``
   and ``sorted(w.lower() for w in set(text1))``?  Which one will gives
   a larger value?  Will this be the case for other texts?

#. |soso| Write the slice expression to produces the last two
   words of ``text2``.

#. |soso| Read the BBC News article: *UK's Vicky Pollards 'left behind'* ``http://news.bbc.co.uk/1/hi/education/6173441.stm``.
   The article gives the following statistic about teen language:
   "the top 20 words used, including yeah, no, but and like, account for around a third of all words."
   How many word types account for a third
   of all word tokens, for a variety of text sources?  What do you conclude about this statistic?
   Read more about this on *LanguageLog*, at ``http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html``.

#. |soso| Assign a new value to ``sent``, namely the sentence
   ``['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']``,
   then write code to perform the following tasks:

   a) Print all words beginning with ``'sh'``:

   b) Print all words longer than 4 characters.

#. |soso| What does the following Python code do?  ``sum(len(w) for w in text1)``
   Can you use it to work out the average word length of a text?

#. |soso| What is the difference between the following two tests:
   ``w.isupper()``, ``not w.islower()``?

#. |soso| Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?

#. |soso| The CMU Pronouncing Dictionary contains multiple pronunciations
   for certain words.  How many distinct words does it contain?  What fraction
   of words in this dictionary have more than one possible pronunciation?

#. |soso| What is the branching factor of the noun hypernym hierarchy?
   (For all noun synsets that have hyponyms, how many do they have on average?)

#. |soso| Define a function ``supergloss(s)`` that takes a synset ``s`` as its argument
   and returns a string consisting of the concatenation of the glosses of ``s``, all
   hypernyms of ``s``, and all hyponyms of ``s``.

#. |talk| Review the mappings in Table linguistic-objects_.  Discuss any other
   examples of mappings you can think of.  What type of information do they map
   from and to?

#. |soso| Write a program to find all words that occur at least three times in the Brown Corpus.

#. |soso| Write a program to generate a table of token/type ratios, as we saw in
   Table brown-types_.  Include the full set of Brown Corpus genres (``nltk.corpus.brown.categories()``).
   Which genre has the lowest diversity (greatest number of tokens per type)?
   Is this what you would have expected?

#. |soso| Modify the text generation program in Figure random_ further, to
   do the following tasks:
   
   a) Store the *n* most likely words in a list ``lwords`` then randomly
      choose a word from the list using ``random.choice()``.
      
   b) Select a particular genre, such as a section of the Brown Corpus,
      or a genesis translation, one of the Gutenberg texts, or one of the Web texts.  Train
      the model on this corpus and get it to generate random text.  You
      may have to experiment with different start words. How intelligible
      is the text?  Discuss the strengths and weaknesses of this method of
      generating random text.
      
   c) Now train your system using two distinct genres and experiment
      with generating text in the hybrid genre.  Discuss your observations.

#. |soso| Write a program to print the most frequent bigrams
   (pairs of adjacent words) of a text,
   omitting non-content words, in order of decreasing frequency.

#. |soso| Write a program to create a table of word frequencies by genre,
   like the one given above for modals.  Choose your own words and
   try to find words whose presence (or absence) is typical of a genre.
   Discuss your findings.

#. |soso| Write a function that finds the 50 most frequently occurring words
   of a text that are not stopwords.

#. |soso| Write a function ``tf()`` that takes a word and the name of a section
   of the Brown Corpus as arguments, and computes the text frequency of the word
   in that section of the corpus.

#. |soso| Write a program to guess the number of syllables contained in a text,
   making use of the CMU Pronouncing Dictionary.

#. |soso| Define a function ``hedge(text)`` which processes a
   text and produces a new version with the word
   ``'like'`` between every third word.

#. |hard| **Zipf's Law**:
   Let *f(w)* be the frequency of a word *w* in free text. Suppose that
   all the words of a text are ranked according to their frequency,
   with the most frequent word first. Zipf's law states that the
   frequency of a word type is inversely proportional to its rank
   (i.e. *f.r = k*, for some constant *k*). For example, the 50th most
   common word type should occur three times as frequently as the
   150th most common word type.

   a) Write a function to process a large text and plot word
      frequency against word rank using ``pylab.plot``. Do
      you confirm Zipf's law? (Hint: it helps to use a logarithmic scale).
      What is going on at the extreme ends of the plotted line?

   #) Generate random text, e.g. using ``random.choice("abcdefg ")``,
      taking care to include the space character.  You will need to
      ``import random`` first.  Use the string
      concatenation operator to accumulate characters into a (very)
      long string.  Then tokenize this string, and generate the Zipf
      plot as before, and compare the two plots.  What do you make of
      Zipf's Law in the light of this?

#. |hard| Modify the ``generate_model()`` function in Figure random_ to use Python's
   ``random.choose()`` method to randomly pick the next word from
   the available set of words.

#. |hard| Define a function ``find_language()`` that takes a string
   as its argument, and returns a list of languages that have that
   string as a word.  Use the ``udhr`` corpus and limit your searches
   to files in the Latin-1 encoding.

#. |hard| Use one of the predefined similarity measures to score
   the similarity of each of the following pairs of words.
   Rank the pairs in order of decreasing similarity.
   How close is your ranking to the order given here?
   (Note that this order was established experimentally
   by [MillerCharles1998]_.)

::
      car-automobile, gem-jewel, journey-voyage, boy-lad,
      coast-shore, asylum-madhouse, magician-wizard, midday-noon,
      furnace-stove, food-fruit, bird-cock, bird-crane, tool-implement,
      brother-monk, lad-brother, crane-implement, journey-car,
      monk-oracle, cemetery-woodland, food-rooster, coast-hill,
      forest-graveyard, shore-woodland, monk-slave, coast-forest,
      lad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string.      

.. Currently too hard since we can't iterate over synsets easily
   #. |soso| What percentage of noun synsets have no hyponyms?

.. include:: footer.rst

