.. -*- mode: rst -*-
.. include:: ../definitions.txt

.. _chap-advanced-parsing:

===================
8. Advanced Parsing
===================


------------
Introduction
------------

As we have seen, parsing builds trees over sentences, according to a
phrase structure grammar.  However, as the coverage of the grammar
increases and the length of the input sentence grows, the number of
parse trees grows rapidly.  In fact, it grows at an astronomical rate.

Let's explore this issue with the help of a simple example.
The word `fish`:lx: is both a noun and a verb.  We can make up the
nonsense sentence `fish fish fish`:lx:, meaning *fish like to fish for
other fish*.  (Try this with `police`:lx: if you prefer something more
sensible.)  Here is a toy grammar for the 'fish' sentences.

    >>> from nltk_lite.parse import cfg, chart
    >>> grammar = cfg.parse_grammar("""
    ... S -> NP V NP
    ... NP -> NP Sbar
    ... Sbar -> NP V | V NP
    ... NP -> 'fish'
    ... V -> 'fish'
    ... """)

Now we can try parsing a longer sentence, `fish fish fish fish
fish`:lx:, which amongst other things, means *fish that are fished by
other fish are in the habit of fishing fish themselves*. We use the
|NLTK| chart parser, which is presented later on in this chapter.
This sentence has four readings.

    >>> tokens = ["fish"] * 5
    >>> cp = chart.ChartParse(grammar, chart.TD_STRATEGY)
    >>> for tree in cp.get_parse_list(tokens):
    ...     print tree
    (S:
      (NP: (NP: 'fish') (Sbar: (V: 'fish') (NP: 'fish')))
      (V: 'fish')
      (NP: 'fish'))
    (S:
      (NP: (NP: 'fish') (Sbar: (NP: 'fish') (V: 'fish')))
      (V: 'fish')
      (NP: 'fish'))
    (S:
      (NP: 'fish')
      (V: 'fish')
      (NP: (NP: 'fish') (Sbar: (V: 'fish') (NP: 'fish'))))
    (S:
      (NP: 'fish')
      (V: 'fish')
      (NP: (NP: 'fish') (Sbar: (NP: 'fish') (V: 'fish'))))

As the length of this sentence goes up (3, 5, 7, ...) we get the
following numbers of parse trees: 1; 4; 20; 112; 672; 4,224; 27,456;
183,040; 1,244,672; 8,599,552; 60,196,864; 426,008,576.  The last of
these |mdash| a figure of the order of 10\ `8`:superscript: |mdash| is
for a sentence of length 23, the average length of sentences in the
WSJ section of Penn Treebank.  (This growth is *super-exponential*
(equal to 2\ `n`:superscript:\ .C(n+1), where C(n) is the *n*\ th
Catalan number, (2n)!/(n!(n+1)!).)  No practical |NLP| system could
construct 10\ `8`:superscript: trees for a typical sentence, much less
choose the appropriate one in the context.  It's clear that humans
don't do this either!
Note that the problem is not with our choice of example. 
[ChurchPatil1982]_ point out that the syntactic ambiguity of `pp`:gc:
attachment in sentences like pp_ also grows in proportion to the Catalan
numbers.

.. _pp:
.. ex:: Put the block in the box on the table.

Moreover, as soon as we try to construct a broad-coverage grammar, we
are forced to make lexical entries highly ambiguous for their part of
speech.  In a toy grammar, `a`:lx: is only a determiner, `dog`:lx: is
only a noun, and `runs`:lx: is only a verb.  However, in a
broad-coverage grammar, `a` is also a noun (e.g. `part a`:lx:),
`dog`:lx: is also a verb (meaning to follow closely), and `runs`:lx:
is also a noun (e.g. `ski runs`:lx:).  In fact, all words can be
referred to by name: e.g. `the verb 'ate' is spelled with three
letters`:lx:; in speech we do not need to supply quotation marks.
Furthermore, it is possible to *verb* most nouns.  Thus a parser for a
broad-coverage grammar will be overwhelmed with ambiguity.  Even
complete gibberish will often have a reading, e.g. `the a are of
I`:lx:.  As [Abney1996]_ has pointed out, this is not word salad but a
grammatical noun phrase, in which `are`:lx: is a noun meaning a
hundredth of a hectare (or 100 sq m), and `a`:lx: and `I`:lx: are
nouns designating coordinates:

.. figure:: ../images/are.png
   :scale: 25

   The a are of I

|nopar| Given this unlikely phrase, a broad-coverage parser should
find this surprising reading.  Similarly, sentences which seem to be
unambiguous, such as `John saw Mary`:lx:, turn out to have other
readings we would not have anticipated (as Abney explains).  This
ambiguity is unavoidable, and leads to horrendous inefficiency in
parsing seemingly inoccuous sentences. 

Let's look more closely at the
issue of efficiency. The top-down recursive-descent parser presented
in Chapter chap-parse_ can be very inefficient, since it often builds
and discards the same sub-structure many times over.

.. Note::
   You should try the recursive-descent parser demo if you haven't
   already:
   
   .. doctest-ignore::
       >>> from nltk_lite.draw import srparser
       >>> srparser.demo()

+-------------------------------------------------------+
|                                                       |
|     Backtracking and Repeated Parsing of Subtrees     |
+=============================+=========================+
| a. Initial stage            | b. Backtracking         |
|                             |                         |
| |findtheblock1|             | |findtheblock2|         |
+-----------------------------+-------------------------+
| c. Failing to match `on`:lx:| d. Completed parse      |
|                             |                         |
| |findtheblock3|             | |findtheblock4|         |
+-----------------------------+-------------------------+

.. |findtheblock1| image:: ../images/findtheblock1.png
   :scale: 85
.. |findtheblock2| image:: ../images/findtheblock2.png
   :scale: 85
.. |findtheblock3| image:: ../images/findtheblock3.png
   :scale: 85
.. |findtheblock4| image:: ../images/findtheblock4.png
   :scale: 85



In this chapter, we will
present two, independent, methods for dealing with ambiguity:

Dynamic Programming and Chart Parsing:
  These techniques allow us to derive the parses of an ambiguous
  sentence more `efficiently`:em:.

Probabilistic Parsing:
  These techniques allow us to `rank`:em: the parses of an ambiguous
  sentence on the basis of evidence from corpora.


-------------------
Dynamic programming
-------------------

The simple parsers discussed in Chapter chap-parse_ have significant
limitations.  The bottom-up shift-reduce parser can only find one
parse, and it often fails to find a parse even if one exists.  As just
pointed out, the top-down recursive-descent parser can be very
inefficient, and if the grammar contains left-recursive rules, it can
enter into an infinite loop.  In order to address these problems of
completeness and efficiency, we will explore a technique called
`dynamic programming`:dt:, which stores intermediate results, and
re-uses them when appropriate.

Dynamic programming is a general technique for designing algorithms
which is widely used in natural language processing.  The term
'programming' is used in a different sense to what you might expect,
to mean planning or scheduling.  Dynamic programming is used when a
problem contains overlapping sub-problems.  Instead of computing
solutions to these sub-problems repeatedly, we simply store them in a
lookup table.

In the remainder of this section we will introduce dynamic programming,
but in a rather different context to syntactic parsing.

Sanscrit Meter
--------------

Pingala was an Indian author who lived around the 5th century B.C.,
and wrote a treatise on Sanscrit prosody called the *Chandas Shastra*.
Virahanka extended this work around the 6th century A.D., studying the
number of ways of combining short and long syllables to create a meter
of length *n*.  He found, for example, that there are five ways to
construct a meter of length 4: *V*\ :subscript:`4` = *{LL, SSL, SLS,
LSS, SSSS}*.  In general, we can split *V*\ :subscript:`n` into two
subsets, those starting with *L: {LL, LSS}*, and those starting with
*S: {SSL, SLS, SSSS}*.  This is the clue for decomposing the problem:

|  *V*\ :subscript:`4` =
|    LL, LSS; or L prefixed to each item of *V*\ :subscript:`2` = {L, SS}
|    SSL, SLS, SSSS; or S prefixed to each item of *V*\ :subscript:`3` = {SL, LS, SSS}

|nopar| 
With this observation, we can write a little recursive function to
compute these meters: 

    >>> def virahanka1(n):
    ...     if n == 0:
    ...         return [""]
    ...     elif n == 1:
    ...         return ["S"]
    ...     else:
    ...         s = ["S" + prosody for prosody in virahanka1(n-1)]
    ...         l = ["L" + prosody for prosody in virahanka1(n-2)]
    ...         return s + l
    >>> virahanka1(4)
    ['SSSS', 'SSL', 'SLS', 'LSS', 'LL']  

Notice that, in order to compute *V*\ :subscript:`4` we first compute
*V*\ :subscript:`3` and *V*\ :subscript:`2`.  But to compute *V*\ :subscript:`3`,
we need to first compute *V*\ :subscript:`2` and *V*\ :subscript:`1`.  This `call
structure`:dt: is depicted in the following tree:

.. ex:: 
   .. tree:: (V4 (V3 (V2 V1 V0) V1) (V2 V1 V0))

|nopar| 
As you can see, *V*\ :subscript:`2` is computed twice.
This might not seem like a significant problem, but 
it turns out to be rather wasteful as *n* gets large:
to compute *V*\ :subscript:`10` using this recursive technique, we
would computes
*V*\ :subscript:`2` 34 times;
for *V*\ :subscript:`20` we would compute *V*\ :subscript:`2` 4,181 times;
for *V*\ :subscript:`30` we would compute *V*\ :subscript:`2` 514,229 times;
and
for *V*\ :subscript:`40` we would compute *V*\ :subscript:`2` 63,245,986 times!
A much better alternative is to store the value of *V*\ :subscript:`2` in a table
and look it up whenever we need it.  The same goes for other values, such
as *V*\ :subscript:`3` and so on.  Here is a dynamic programming
approach which computes the same result as the earlier program, only
much more efficiently.  It uses some auxiliary storage, a table
called ``lookup``:

    >>> lookup = [None] * 100
    >>> lookup[0] = [""]
    >>> lookup[1] = ["S"]
    >>> def virahanka2(n):
    ...     for i in range(n-1):
    ...         s = ["S" + prosody for prosody in lookup[i]]
    ...         l = ["L" + prosody for prosody in lookup[i+1]]
    ...         lookup[i+2] = s + l
    ...     return lookup[n]

|nopar|
This is the classic `bottom-up`:dt: approach to dynamic programming, where
we fill up a table with solutions to all smaller sub-problems, then simply
read off the result we are interested in.  Notice that each sub-problem
is only ever solved once.  

However, this method is still wasteful for some applications, because
it may compute solutions to sub-problems that are never used in
solving the main problem.  This wasted computation can be avoided
using the *top-down* approach to dynamic programming:

    >>> lookup = [None] * 100
    >>> lookup[0] = [""]
    >>> lookup[1] = ["S"]
    >>> def virahanka3(n):
    ...     if not lookup[n]:
    ...         s = ["S" + prosody for prosody in virahanka3(n-1)]
    ...         l = ["L" + prosody for prosody in virahanka3(n-2)]
    ...         lookup[n] = s + l
    ...     return lookup[n]

|nopar|
Unlike the bottom-up approach, this approach is recursive.  It avoids
the huge wastage of our first version by checking whether it has
previously stored the result.  If not, it computes the result
recursively and stores it in the table.  The last step is to return
the stored result.

This concludes our introduction to dynamic programming.  In the next
section we will see this method applied to syntactic parsing.

-------------
Chart Parsing
-------------



Well-formed Substring Tables
----------------------------

We start off by defining a simple grammar.

    >>> from nltk_lite.parse import cfg
    >>> grammar = cfg.parse_grammar("""
    ... S -> NP VP
    ... PP -> P NP
    ... NP -> Det N | NP PP
    ... VP -> V NP | VP PP
    ... Det -> 'the'
    ... N -> 'kids' | 'box' | 'floor'
    ... V -> 'opened' 
    ... P -> 'on' 
    ... """)

As you can see, this grammar allows the `vp`:gc: 
`opened the box on the floor`:lx: to be analysed in two ways,
depending on where the `pp`:gc: is attached.

.. ex::
  .. ex::
    .. tree:: (S (NP (Det the) (N kids)) (VP (V opened)(NP (NP (Det the) (N box))(PP (P on (NP (Det the)(N floor)))))))
 
  .. ex::
    .. tree:: (S (NP (Det the) (N kids)) (VP (VP (V opened)(NP (Det the) (N box)))(PP (P on (NP (Det the)(N floor))))))

Dynamic programming allows us to build the `pp`:gc: `on the floor`:lx:
just once; we enter the information into a table, and then look it
up when we need to use it as a subconstituent of either the object `np`:gc: or
the higher `vp`:gc:. The table that holds this information is often called a
`well-formed substring table`:dt: (or |WFST| for short). 
We will show how to construct the |WFST| bottom-up so as to systematically record
what syntactic constituents have been found.

Let's set our input to be the sentence `the kids opened the box on the
floor`:lx:. It is helpful to think of the input as being indexed like a Python
list. We have illustrated this in Figure stringpos_.

.. _stringpos:
.. figure:: ../images/chart_positions.png
   :scale: 25

   Slice Points in the Input String

|nopar|
This allows us to say that, for instance, the word `opened`:lx: can be
found at position `2, 3` in the input. 
This of course is what we get when we split an input string into a list of tokens.

    >>> sent = "the kids opened the box on the floor"
    >>> tokens = sent.split()
    >>> tokens[2:3]
    ['opened']

|nopar| We can also just use the start position of a token to refer to
it. So we can also get hold of `opened`:lx: as ``tokens[2]``. 
In a |WFST|, we record the
position of the words by filling in cells in a triangular matrix.  We
will take the vertical axis of the matrix as giving the start
position of a substring, and the horizontal axis as giving the end
position. In such a matrix, therefore, we might expect to find the
word `opened`:lx: in the cell with coordinates (2, 3). To make our
life easier, however, we will look up the lexical category of each word, and
place that category in the matrix instead. So  cell (2, 3) will
contain the entry `v`:gc:. More generally, if our input string is
`a`:sub:`1`\ `a`:sub:`2`\ |cdots|\ `a`:sub:`n`, and our grammar
contains a rule of the form *A* |rarr| `a`:sub:`i`, then we add *A* to
the cell (`i`-1, `i`).


So, for every word in ``tokens``, we can look up in our grammar productions what
category it belongs to. (We make the simplifying assumption for now
that there is no lexical ambiguity.)

    >>> grammar.productions(rhs=tokens[2])
    [V -> 'opened']

|nopar| 
For our |WFST|, we create an *n* |times| *n* matrix (where *n* is the number of
tokens plus 1) as a list of lists in Python, and we initialize the matrix
with the lexical categories of each token in the input.
    
    >>> def init_wfst(tokens, grammar):
    ...     numtokens = len(tokens)
    ...     wfst = [['' for i in range(numtokens+1)] for j in range(numtokens+1)]
    ...     for i in range(numtokens):
    ...         prod_rhs = grammar.productions(rhs=tokens[i])
    ...         wfst[i][i+1] = prod_rhs[0].lhs()
    ...     return wfst

|nopar| 
We use the following utility function to pretty-print the
|WFST| for us.

    >>> def display(wfst, tokens):
    ...     numfields = len(wfst)
    ...     hline = "   " + "=" * 5 * len(tokens)
    ...     print
    ...     print '    ' + ' '.join([("%-4d" % i) for i in range(1, numfields)])
    ...     print hline
    ...     for i in range(numfields-1):
    ...         rownum = i
    ...         print "%d| " % rownum,
    ...         for j in range(1, numfields):
    ...             print "%-4s" % wfst[i][j],
    ...         print "|"
    ...     print hline

|nopar| We create the initialized |WFST| and display it:

    >>> wfst0 = init_wfst(tokens, grammar)
    >>> display(wfst0, tokens)
	1    2    3    4    5    6    7    8   
       ========================================
    0|  Det                                     |
    1|       N                                  |
    2|            V                             |
    3|                 Det                      |
    4|                      N                   |
    5|                           P              |
    6|                                Det       |
    7|                                     N    |
       ========================================

|nopar| As promised, there is a `v`:gc: in cell (2, 3).
The same information can be represented in a directed acyclic graph,
as shown in Figure chartinit0_. This graph is usually called a `chart`:dt:.

.. _chartinit0:
.. figure:: ../images/chart_init0.png
   :scale: 25

   A Graph Representation of the Initialized |WFST|

Returning to our tabular representation, given that we have `det`:gc:
in cell (0, 1), and `n`:gc: in cell (1, 2), what should we put into
cell (0, 2)? In other words, what syntactic category derives the
string `the kids`:lx:? We have already established that `Det`:gc:
derives `the`:lx: and `n`:gc: derives `kids`:lx:, so we need to find a
rule of the form *A* |rarr| `det`:gc: `n`:gc:, that is, a rule whose
righthand side matches the categories in the cells we have already
found. To help us easily retrieve production rules by their righthand
sides, we create an index for the grammar.

    >>> index = {}
    >>> for prod in grammar.productions():
    ...     index[prod.rhs()] = prod.lhs()
    >>> r = grammar.productions()[2].rhs()
    >>> r
    (<Det>, <N>)
    >>> index[r]
    <NP>

|nopar| 
This is an example of a space-time trade-off: we do a reverse lookup
on the grammar, instead of having to check through entire list of
productions each time we want to look up via the right-hand side.  In
the case at hand, we know that we can enter `n`:gc: in cell (0,2).
Figure chartinit1_ is the corresponding graph representation, where
we add a new edge labeled `n`:gc: to cover the input from `0` to `2`.

.. _chartinit1:
.. figure:: ../images/chart_init1.png
   :scale: 25

   Adding an `np`:gc: Edge to the Chart

More generally, given an input `a`:sub:`1`\ `a`:sub:`2`\ |cdots|\
`a`:sub:`n`, we need to decide what to enter into cell (`i, j`), where
0 |leq| `i` < `j` |leq| `n`.  We can enter *A* in (`i, j`) if we find
nonterminals *B*, *C*, and an integer `k` such that:

 - `i` < `k` < `j`;
 - *B* is in cell (`i, k`);
 - *C* is in cell (`k, j`); and
 - *A* |rarr| *B* *C* is a production of the grammar.

The following function uses this inference step to complete the
|WFST|.

    >>> def complete_wfst(wfst, tokens, index, trace=False):
    ...     numtokens = len(tokens)
    ...     for span in range(2, numtokens+1):
    ...         for start in range(numtokens+1-span):
    ...             end = start + span
    ...             for mid in range(start+1, end):
    ...                 nt1 = wfst[start][mid]
    ...                 nt2 = wfst[mid][end]
    ...                 if (nt1,nt2) in index:
    ...                     nt3 = index[(nt1,nt2)]
    ...                     if trace:
    ...                         print "[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]" % \
    ...                         (start, nt1, mid, nt2, end, start, nt3, end)
    ...                     wfst[start][end] = index[(nt1,nt2)]
    ...     return wfst
    ...
    >>> wfst1 = complete_wfst(wfst0, tokens, index)
    >>> display(wfst1, tokens)
	1    2    3    4    5    6    7    8   
       ========================================
    0|  Det  NP             S              S    |
    1|       N                                  |
    2|            V         VP             VP   |
    3|                 Det  NP             NP   |
    4|                      N                   |
    5|                           P         PP   |
    6|                                Det  NP   |
    7|                                     N    |
       ========================================

|nopar| The tabular algorithm above gives an intuitive idea of why
parsing context-free grammar is proportional to `n`:sup:`3`: there is a triply nested
loop, and the size of each loop is based
on the length of the input.

By setting ``trace`` to ``True`` when calling the function ``complete_wfst()``, we get 
additional output that is intended to be reminiscent of a chart representation.

    >>> wfst1 = complete_wfst(wfst0, tokens, index, trace=True)
    [0] Det [1]   N [2] ==> [0]  NP [2]
    [3] Det [4]   N [5] ==> [3]  NP [5]
    [6] Det [7]   N [8] ==> [6]  NP [8]
    [2]   V [3]  NP [5] ==> [2]  VP [5]
    [5]   P [6]  NP [8] ==> [5]  PP [8]
    [0]  NP [2]  VP [5] ==> [0]   S [5]
    [3]  NP [5]  PP [8] ==> [3]  NP [8]
    [2]   V [3]  NP [8] ==> [2]  VP [8]
    [2]  VP [5]  PP [8] ==> [2]  VP [8]
    [0]  NP [2]  VP [8] ==> [0]   S [8]

|nopar| For example, this says that since we found ``Det`` at
``wfst[0][1]`` and ``N`` at ``wfst[1][2]``, we can add ``NP`` to
``wfst[0][2]``. 

We conclude that there is a parse for the whole input string once
we have constructed an `s`:gc: node that covers the whole input, from
position `0` to position `8`; i.e., we can conclude that `s`:gc:
|DoubleRightArrow|\ * `a`:sub:`1`\ `a`:sub:`2`\ |cdots|\ `a`:sub:`n`. 

A |WFST| is a data structure that can be used by a variety of parsing
algorithms. The particular method for constructing a |WFST| that we
have just presented is based on the |CYK| algorithm [ref?]. As you can
see, the |WFST| is not itself a parse tree, so the technique is
strictly speaking `recognizing`:dt that a sentence is admitted by a
grammar, rather than parsing it. One significant limitation of |CYK|
is that it requires every non-lexical grammar production to be
`binary`:em:. A grammar where this holds is said to be in `Chomsky
Normal Form`:dt:. Although it is possible to convert an arbitrary
|CFG| into Chomsky Normal Form (cf. [JurafskyMartin]_), we would
prefer to use an approach without such a requirement.

Exercises
---------

1. Read about string edit distance and the Levenshtein Algorithm.
   Try the implementation provided in ``nltk_lite.utilities.edit_dist``.
   How is this using dynamic programming?  Does it use the bottom-up or
   top-down approach?

#. Modify the functions ``init_wfst()`` and ``complete_wfst()`` so
   that the contents of each cell in the |WFST| is a set of
   non-terminal symbols rather than a single non-terminal. *Hint*. Something about
   avoiding deep copies.

#. Modify the functions ``init_wfst()`` and ``complete_wfst()`` so
   that when a non-terminal symbol is added to a cell in the |WFST|, it includes
   a record of the cells from which it was derived. Implement a
   function which will convert a |WFST| in this form to a parse tree.



Active Charts
-------------

One important aspect of the tabular approach to parsing can be seen more
clearly if we look at the graph representation: given our grammar, there are two
different ways to derive a top-level `vp`:gc: for the input, as shown in
Figures chartnp0_ and chartnp1_.

.. _chartnp0:
.. figure:: ../images/chart_np0.png
   :scale: 25

   `vp`:gc: |rarr| `v`:gc: `np`:gc:

.. _chartnp1:
.. figure:: ../images/chart_np1.png
   :scale: 25

   `vp`:gc: |rarr| `vp`:gc: `pp`:gc:

|nopar| For the sake of simplicity, we did not try to show this
structural ambiguity in our
matrix representation. Nevertheless, it is possible to extend the
approch so that each cell contains a `set`:em: of non-terminals rather
than a single one. In our graph representation, we simply merge the
two sets of edges in Figures chartnp0_ and chartnp1_ to yield Figure
chartnp2_ (which also contains the final `s`:gc: edge).

.. _chartnp2:
.. figure:: ../images/chart_np2.png
   :scale: 25

|nopar| However, we cannot necessarily read off from the |WFST| what
the justification was for adding a specific edge. For example, in
chartnp1_, the longest `vp`:gc: edge might be interpreted as having
arisen by virtue of a rule `vp`:gc: |rarr| `v np pp`:gc:. Unlike
phrase structure trees, a |WFST| does not encode a relation of
immediate dominance. In order to make such information available, we
can label edges not just with a non-terminal category, but with the
whole production which justified the addition of the edge. This is
illustrated in Figure chartprods_.

.. _chartprods:
.. figure:: ../images/chart_prods.png
   :scale: 25


In general, a parser hypothesizes constituents based on the grammar
and its current knowledge about the tokens it has seen and the
constituents it has already found.  Any constituent that is consistent
with the current knowledge can be hypothesized; but many of these
hypothesized constituents may not be used in complete parses. We can
view a |WFST| as recording these hypotheses.

All of the edges that we've seen so far represent complete
constituents.  However, it can also be helpful to hypothesize
`incomplete`:dt: constituents.  For example, much of the work done by a
parser in processing the production *VP* |rarr| *V* *NP* is also useful
when processing *VP* |rarr| *V* *NP* *PP*.
Thus, we might want to record the
hypothesis that "the `v`:gc: constituent `likes`:lx: forms the beginning of a
`vp`:gc:."

We can record hypotheses of this form by adding a `dot`:dt: to the
edge's right hand side.  The children to the left of the dot specify
what children the constituent starts with; and the children to the
right of the dot specify what children still need to be found in order
to form a complete constituent.  For example, the edge in the
Figure dottededge_ records the hypothesis that "a `vp`:gc: starts with the `v`:gc:
`likes`:lx:, but still needs an `np`:gc: to become complete":

.. _dottededge:
.. figure:: ../images/chart_intro_dottededge.png
   :scale: 30

|nopar| These `dotted edges`:dt: are used to record all of the hypotheses that a
chart parser makes about constituents in a sentence.  
We write dotted edges as a pair of the form [`A` |rarr| `c`:sub:`1`
|dots| `c`:sub:`d` |dot| `c`:sub:`d+1` |dots| `c`:sub:`n`, (*i*,
*j*)], where `A` |rarr| `c`:sub:`1`
|dots| `c`:sub:`d` |dot| `c`:sub:`d+1` |dots| `c`:sub:`n` is a
production annotated with a dot, and (*i*,
*j*) is a pair of indices that record the input words which are
covered by the production.
More formally, we 
define a dotted edge as follows:

A dotted edge [`A` |rarr| `c`:sub:`1` |dots| `c`:sub:`d` |dot| `c`:sub:`d+1` |dots| `c`:sub:`n`, (*i*, *j*)] 
  records the hypothesis that
  a constituent of type `A` starts with children `c`:sub:`1` |dots|
  `c`:sub:`d` covering words `w`:sub:`i` |dots| `w`:sub:`j`, but still
  needs children `c`:sub:`d+1` |dots| `c`:sub:`n` to be complete
  (where both `c`:sub:`1` |dots| `c`:sub:`d` and
  `c`:sub:`d+1` |dots| `c`:sub:`n` may be empty.)

|nopar| If `d = n`:math: (i.e., if `c`:sub:`d+1` |dots| `c`:sub:`n` is
empty) then the edge represents a complete constituent, and is called
a `complete edge`:dt:.  Otherwise, the edge represents an incomplete
constituent, and is called an `incomplete edge`:dt:.  In Figure
incomplete_, [`vp`:gc: |rarr| `v`:gc: `np`:gc: |dot|, (1, 3)] is a
complete edge, and [`vp`:gc: |rarr| `v`:gc: |dot| `np`:gc:, (1, 2)] is
an incomplete edge.

.. _incomplete:   
.. figure:: ../images/chart_intro_incomplete.png
   :scale: 30
    
If `n = 0`:math: (i.e., if `c`:sub:`1` |dots| `c`:sub:`n` is empty),
then the edge is called a `self-loop edge`:dt:.  
This is illustrated in Figure selfloop_.

.. _selfloop:
.. figure:: ../images/chart_intro_selfloop.png
   :scale: 30

If a complete edge spans the entire sentence, and has the grammar's
start symbol as its left-hand side, then the edge is called a `parse
edge`:dt:, and it encodes one or more parse trees for the sentence.  In
Figure parseedge_, [`s`:gc: |rarr| `np`:gc: `vp`:gc: |dot|, (0, 3)] is a parse edge.

.. _parseedge:      
.. figure:: ../images/chart_intro_parseedge.png
   :scale: 30

The Chart Parser
----------------

To parse a sentence, a chart parser first creates an empty chart
spanning the sentence.  It then finds edges that are licensed by its
knowledge about the sentence, and adds them to the chart one at a time
until one or more parse edges are found.  The edges that it adds can
be licensed in one of three ways:
      
1. The *input* can license an edge.  In particular, each word `w`:sub:`i`
   in the input licenses the complete edge [`w`:sub:`i` |rarr|
   |dot|, (*i*, *i*\ +1)].

#. The *grammar* can license an edge.  In particular, each grammar
   production A |rarr| |alpha| licenses the self-loop edge [*A* |rarr|
   |dot| |alpha|, (*i*, *i*)] for every *i*, 0 |leq| *i* < *n*.

#. The *current chart contents* can license an edge.
          
However, it is not wise to add `all`:em: licensed edges to the chart,
since many of them will not be used in any complete parse.  For
example, even though the edge in the following chart is licensed (by
the grammar), it will never be used in a complete parse:

.. _uselessedge:        
.. figure:: ../images/chart_useless_edge.png
   :scale: 30

Chart parsers therefore use a set of `rules`:dt: to heuristically decide
when an edge should be added to a chart.  This set of rules, along
with a specification of when they should be applied, forms a
`strategy`:dt:.

The Fundamental Rule
--------------------

One rule is particularly important, since it is used by every chart
parser: the `Fundamental Rule`:dt:.\ [#]_  
This rule is used to combine an
incomplete edge that's expecting a nonterminal *B* with a complete
edge immediately following it whose left hand side is *B*.  Formally,
it states that if the chart contains the edges:

1. [*A* |rarr| |alpha| |dot| *B* |beta| , (*i*, *j*\ )]
2. [*B* |rarr| |gamma| |dot| , (*j*, *k*\ )]

then the parser should add the new edge:

3. [*A* |rarr| |alpha| *B* |dot| |beta| , (*i*, *k*)]

A somewhat more intuitive version of the operation of the Fundamental
Rule can be given using chart diagrams. Thus, if we have a chart of
the form shown in Figure fr1_,

.. _fr1:
.. figure:: ../images/chart_fr1.png
   :scale: 30

then we can add a new complete edge to the chart as illustrated in Figure fr2_.

.. _fr2:
.. figure:: ../images/chart_fr2.png
   :scale: 30

.. [#] The Fundamental Rule corresponds to the Completer
   function in the Earley algorithm; cf. [Jurafsky2000SLP]_.

Bottom-Up Parsing
-----------------

As we saw in Chapter chap-parse_, bottom-up parsing starts from the
input string, and tries to find sequences of words and phrases that
correspond to the *right-hand* side of a grammar production. The
parser then replaces these with the left-hand side of the production,
until the whole sentence is reduced to an `S`:gc:.  Bottom-up chart
parsing is an extension of this approach in which hypotheses about
structure are recorded as edges on a chart. In terms of our earlier
terminology, bottom-up chart parsing can be seen as a parsing
strategy; in other words, bottom-up is a particular choice of
heuristics for adding new edges to a chart. 

The general procedure for chart parsing is
inductive: we start with a base case, and then show how we can move
from a given state of the chart to a new state. Since we are working
bottom-up, the base case for our induction will be determined by the
words in the input string, so we add new edges for each word.  Now,
for the induction step, suppose the chart contains an edge labeled
with constituent *A*. Since we are working bottom-up, we want to build
constituents which can have an *A* as a daughter. In other words, we
are going to look for rules of the form *B* |rarr| *A* |beta| and use
these to label new edges.

Let's look at the procedure a bit more formally.  To create a
bottom-up chart parser, we add to the Fundamental Rule two new rules:
the `Bottom-Up Initialization Rule`:dt:; and the `Bottom-Up Predict
Rule`:dt:.

The Bottom-Up Initialization Rule says to add all edges licensed by
the input.  In particular, it states that for every word w\ :subscript:`i`, the
parser should add the edge:


#. [`w`:subscript:`i` |rarr|  |dot| , (*i*, *i*\ +1)]

.. _buinit:
.. figure:: ../images/chart_bu_init.png
   :scale: 30


|nopar| Notice that the dot on the righthand side of these productions
is telling us that we have complete edges for the lexical items. Bby
including this information, we
can give a uniform statement of how the Fundamental Rule operates in
Bottom-Up parsing, as we will shortly see.

Let's look at a concrete example. Figure buex1_ shows the bottom-up
initialization for the input `Lee likes coffee`:lx:.

.. _buex1:
.. figure:: ../images/chart_bu_ex1.png
   :scale: 30

Next, suppose the chart contains a complete edge *e* whose
lefthand category is *A*. Then the Bottom-Up Predict Rule requires the
parser to add a self-loop edge at the left boundary of *e*
for each grammar production whose right-hand side begins with category
*A*.  In other words, it states that if
the chart contains the complete edge:

1. [*A* |rarr| |alpha| |dot| , (*i*, *j*\ )]

|nopar| and the grammar contains the production

2. *B* |rarr| *A* |beta|

|nopar| then the parser should add the self-loop edge:

3. [*B* |rarr|  |dot| *A* |beta| , (*i*, *i*\ )]

Graphically, if the chart looks as in Figure bupredict1_, 

.. _bupredict1:
.. figure:: ../images/chart_bu_predict1.png
   :scale: 30

|nopar| then the Bottom-Up Predict Rule tells the parser to augment the chart
as shown in Figure bupredict2_.

.. _bupredict2:
.. figure:: ../images/chart_bu_predict2.png
   :scale: 30

To continue our earlier example, let's suppose that our grammar
contains the following lexical productions:

.. ex::
   .. parsed-literal::

     `np`:gc: |rarr| `Lee`:lx: | `coffee`:lx:
     `v`:gc: |rarr| `likes`:lx:

|nopar| This allows us to add three self-loop edges to the chart, as
shown in Figure buex2_.

.. _buex2:
.. figure:: ../images/chart_bu_ex2.png
   :scale: 30

Once we have a chart like Figure bupredict2_, we can use the
Fundamental Rule to add an edge where we have "moved the dot" one
position to the right.

.. _bufr:
.. figure:: ../images/chart_bu_fr.png
   :scale: 30
        
Figure buex3_ illustrates this for our running example (we have
omitted the self-loop edges for simplicity.)

.. _buex3:
.. figure:: ../images/chart_bu_ex3.png
   :scale: 30

|nopar| We will now be able to add new self-loop edges such as 
[`s`:gc: |rarr|  |dot| `np`:gc: `vp`:gc:, (0, 0)] and
[`vp`:gc: |rarr|  |dot| `vp`:gc: `np`:gc:, (1, 1)], and use these to
build more complete edges.

Using the three rules just presented, we can parse a sentence as follows:

1. Create an empty chart spanning the sentence. 

#. Apply the Bottom-Up Initialization Rule to each word. 
#. Until no more edges are added: 

   a) Apply the Bottom-Up Predict Rule everywhere it applies. 
   #) Apply the Fundamental Rule everywhere it applies. 

#. Return all of the parse trees corresponding to the parse edges in the chart. 

|NLTK| provides a useful interactive tool for visualizing the way in which charts
are built; it can be invoked as follows:

.. doctest-ignore::
    >>> from nltk_lite.draw.chart import demo
    >>> demo()


The tool comes with a pre-defined input string and grammar, but both
of these can be readily modified with options inside the *Edit* menu.
Figure budemo1_ illustrates a window after the grammar has been updated:

.. _budemo1:       
.. figure:: ../images/chart_demo1.png

   Modifing the `demo()` grammar

.. Note:: To get the symbol |DoubleRightArrow| illustrated in Figure
   budemo1_. you just have to type the keyboard characters `'->'`.


Figure budemo2_ illustrates the tool interface. In order to invoke a
rule, you simply click one of the green buttons at the bottom of the window.
We show the state of the chart on the input `Lee
likes coffee`:lx: after three applications of the Bottom-Up
Initialiation Rule, followed by successive applications of the Bottom-Up Predict
Rule and the Fundamental Rule. 

.. _budemo2:       
.. figure:: ../images/chart_demo2.png
   :scale: 70

   Incomplete chart for `Lee likes coffee`:lx: 

Notice that in the topmost pane of the window, there is a partial tree
showing that we have constructed an `s`:gc: with an `np`:gc: subject
in the expectation that we will be able to find a `vp`:gc:.

Top-Down Parsing
----------------

Top-down chart parsing works in a similar way to the recursive descent
parser discussed in Chapter chap-parse_, in that it starts off with
the top-level goal of finding an `s`:gc:. This goal is then broken into
the subgoals of trying to find constituents such as `np`:gc: and
`vp`:gc: which can be immediatedly dominated by  `s`:gc:.
To create a top-down chart parser, we use the Fundamental Rule as before plus
three other rules: the `Top-Down Initialization Rule`:dt:, the `Top-Down
Expand Rule`:dt:, and the `Top-Down Match Rule`:dt:.

The Top-Down Initialization rule captures the fact that the root of any
parse must be the start symbol `s`:gc.  It states that for every grammar
production of the form

1. `s`:gc: |rarr| |alpha|

the parser should add the self-loop edge: 

2. [`s`:gc: |rarr|  |dot| |alpha|\ , (0, 0)]

.. _tdinit:
.. figure:: ../images/chart_td_init.png
   :scale: 30

Let's reuse our earlier example, but this time working top-down. So
our initial chart might look like Figure tdex1_.

.. _tdex1:
.. figure:: ../images/chart_td_ex1.png
   :scale: 30

As we mentioned before, the dot on the righthand side of a production
can be interpreted as a record of
how far our goals have been satisfied. So in the case of Figure
tdex1_, we are predicting that we will be able to find an `np`:gc: and a
`vp`:gc:, but have not yet satisfied these subgoals. So how do we
pursue them? In order to find an  `np`:gc:, for instance, we need to
invoke a production which has `np`:gc: on its lefthand side. The step
of adding the required edge to the chart is accomplished with the
Top-Down Expand rule. This tells us that if our chart contains an incomplete
edge whose dot is followed by a nonterminal *B*, then the parser
should add any self-loop edges licensed by the grammar whose left-hand
side is *B*.  In particular, if the chart contains the incomplete
edge

1. [*A* |rarr| |alpha| |dot| *B* |beta| , (*i*, *j*)]

|nopar| then for each grammar production

2. *B* |rarr| |gamma|

|nopar| the parser should add the edge

3. [*B* |rarr|  |dot| |gamma| , (*j*, *j*\ )]

|nopar| Thus, given a chart that looks like Figure tdexp1_, 

.. _tdexp1:
.. figure:: ../images/chart_td_expand1.png
   :scale: 30

|nopar| the Top-Down
Expand rule augments it with the edge shown in Figure tdexp2_.


.. _tdexp2:
.. figure:: ../images/chart_td_expand2.png
   :scale: 30

In terms of our example, therefore, we can augment our chart to Figure
tdex2_.

.. _tdex2:
.. figure:: ../images/chart_td_ex2.png
   :scale: 30

The Top-Down Match rule allows the predictions of the grammar to be
matched against the input string. Thus, if the chart contains an incomplete
edge whose dot is followed by a terminal *w*, then the parser should
add an edge if the terminal corresponds to the current input symbol.  In particular,
if the chart contains the incomplete edge

1. [*A* |rarr| |alpha| |dot| w\ :subscript:`j` |beta|\ , (*i*, *j*\ )]

then the parser should add a complete edge:

2. [`w`:subscript:`j` |rarr| |dot| , (*j*, *j*\ +1)]

|nopar| Graphically, the Top-Down Match rule takes us from Figure tdmatch1_, 

.. _tdmatch1:
.. figure:: ../images/chart_td_match1.png
   :scale: 30

|nopar| to Figure tdmatch2_.


.. _tdmatch2:
.. figure:: ../images/chart_td_match2.png
   :scale: 30

Figure tdex3_ illustrates how our example chart after applying the
Top-Down Match rule.

.. _tdex3:
.. figure:: ../images/chart_td_ex3.png
   :scale: 30

What rule is relevant now? The Fundamental Rule. If we remove the
self-loop edges from Figure tdex3_  for simplicity, the Fundamental
Rule gives us Figure tdex4_.

.. _tdex4:
.. figure:: ../images/chart_td_ex4.png
   :scale: 30

Using these four rules, we can parse a sentence top-down as follows:

1. Create an empty chart spanning the sentence. 
#. Apply the Top-Down Initialization Rule to each word. 
#. Until no more edges are added: 

   a) Apply the Top-Down Expand Rule everywhere it applies. 
   #) Apply the Top-Down Match Rule everywhere it applies. 
   #) Apply the Fundamental Rule everywhere it applies. 

#. Return all of the parse trees corresponding to the parse edges in the chart. 

We encourage you to experiment with the  |NLTK| chart parser demo,
as before, in order to test out the top-down strategy yourself.


Chart Parsing in NLTK-Lite
--------------------------


``nltk_lite.parse.chart`` defines a simple yet flexible chart parser,
``ChartParse``.  A new chart parser is constructed from a
grammar and a list of chart rules (also known as a *strategy*).  These
rules will be applied, in order, until no new edges are added to the
chart.  In particular, ``ChartParse`` uses the following algorithm:

.. ex::
   .. parsed-literal::   
  
    Until no new edges are added:
      For each chart rule *R*:
	Apply *R* to any applicable edges in the chart. 
    Return any complete parses in the chart. 
        
``nltk_lite.parse.chart`` defines two ready-made strategies:
``TD_STRATEGY``, a basic top-down strategy; and ``BU_STRATEGY``, a
basic bottom-up strategy.  When constructing a chart parser, you
can use either of these strategies, or create your own.

The following example illustrates the use of the chart parser.
We start by defining a simple grammar:

    >>> grammar = cfg.parse_grammar('''
    ...   S -> NP VP
    ...   VP -> V NP | VP PP
    ...   V -> "saw" | "ate"
    ...   NP -> "John" | "Mary" | "Bob" | Det N | NP PP
    ...   Det -> "a" | "an" | "the" | "my"
    ...   N -> "dog" | "cat" | "cookie"
    ...   PP -> P NP
    ...   P -> "on" | "by" | "with"
    ...   ''')

Next, we tokenize a sentence.  We make sure it is a list (not an iterator), since
we wish to use the same tokenized sentence several times.

    >>> from nltk_lite import tokenize
    >>> from nltk_lite.parse import *
    >>> sent = list(tokenize.whitespace('John saw a cat with my cookie'))
    >>> parser = ChartParse(grammar, BU_STRATEGY)
    >>> for tree in parser.get_parse_list(sent):
    ...     print tree
    (S:
      (NP: 'John')
      (VP:
        (VP: (V: 'saw') (NP: (Det: 'a') (N: 'cat')))
        (PP: (P: 'with') (NP: (Det: 'my') (N: 'cookie')))))
    (S:
      (NP: 'John')
      (VP:
        (V: 'saw')
        (NP:
          (NP: (Det: 'a') (N: 'cat'))
          (PP: (P: 'with') (NP: (Det: 'my') (N: 'cookie'))))))

The ``trace`` parameter can be specified when creating a parser, to
turn on tracing (higher trace levels produce more verbose output).
Example butrace_ shows the trace output for parsing a
sentence with the bottom-up strategy:

.. pylisting:: butrace

  # Parse the sentence, bottom-up, with tracing turned on.
  >>> parser = ChartParse(grammar, BU_STRATEGY, trace=2)
  >>> parser.get_parse(sent)
  |. John. saw .  a  . cat . with.  my .cooki.|
  Bottom Up Init Rule:
  |[-----]     .     .     .     .     .     .| [0:1] 'John' 
  |.     [-----]     .     .     .     .     .| [1:2] 'saw' 
  |.     .     [-----]     .     .     .     .| [2:3] 'a' 
  |.     .     .     [-----]     .     .     .| [3:4] 'cat' 
  |.     .     .     .     [-----]     .     .| [4:5] 'with' 
  |.     .     .     .     .     [-----]     .| [5:6] 'my' 
  |.     .     .     .     .     .     [-----]| [6:7] 'cookie' 
  Bottom Up Predict Rule:
  |>     .     .     .     .     .     .     .| [0:0] NP -> * 'John' 
  |.     >     .     .     .     .     .     .| [1:1] V  -> * 'saw' 
  |.     .     >     .     .     .     .     .| [2:2] Det -> * 'a' 
  |.     .     .     >     .     .     .     .| [3:3] N  -> * 'cat' 
  |.     .     .     .     >     .     .     .| [4:4] P  -> * 'with' 
  |.     .     .     .     .     >     .     .| [5:5] Det -> * 'my' 
  |.     .     .     .     .     .     >     .| [6:6] N  -> * 'cookie' 
  Fundamental Rule:
  |[-----]     .     .     .     .     .     .| [0:1] NP -> 'John' * 
  |.     [-----]     .     .     .     .     .| [1:2] V  -> 'saw' * 
  |.     .     [-----]     .     .     .     .| [2:3] Det -> 'a' * 
  |.     .     .     [-----]     .     .     .| [3:4] N  -> 'cat' * 
  |.     .     .     .     [-----]     .     .| [4:5] P  -> 'with' * 
  |.     .     .     .     .     [-----]     .| [5:6] Det -> 'my' * 
  |.     .     .     .     .     .     [-----]| [6:7] N  -> 'cookie' * 
  Bottom Up Predict Rule:
  |>     .     .     .     .     .     .     .| [0:0] S  -> * NP VP 
  |>     .     .     .     .     .     .     .| [0:0] NP -> * NP PP 
  |.     >     .     .     .     .     .     .| [1:1] VP -> * V NP 
  |.     .     >     .     .     .     .     .| [2:2] NP -> * Det N 
  |.     .     .     .     >     .     .     .| [4:4] PP -> * P NP 
  |.     .     .     .     .     >     .     .| [5:5] NP -> * Det N 
  Fundamental Rule:
  |[----->     .     .     .     .     .     .| [0:1] S  -> NP * VP 
  |[----->     .     .     .     .     .     .| [0:1] NP -> NP * PP 
  |.     [----->     .     .     .     .     .| [1:2] VP -> V * NP 
  |.     .     [----->     .     .     .     .| [2:3] NP -> Det * N 
  |.     .     [-----------]     .     .     .| [2:4] NP -> Det N * 
  |.     .     .     .     [----->     .     .| [4:5] PP -> P * NP 
  |.     .     .     .     .     [----->     .| [5:6] NP -> Det * N 
  |.     .     .     .     .     [-----------]| [5:7] NP -> Det N * 
  |.     [-----------------]     .     .     .| [1:4] VP -> V NP * 
  |.     .     .     .     [-----------------]| [4:7] PP -> P NP * 
  |[-----------------------]     .     .     .| [0:4] S  -> NP VP * 
  Bottom Up Predict Rule:
  |.     .     >     .     .     .     .     .| [2:2] S  -> * NP VP 
  |.     .     >     .     .     .     .     .| [2:2] NP -> * NP PP 
  |.     .     .     .     .     >     .     .| [5:5] S  -> * NP VP 
  |.     .     .     .     .     >     .     .| [5:5] NP -> * NP PP 
  |.     >     .     .     .     .     .     .| [1:1] VP -> * VP PP 
  Fundamental Rule:
  |.     .     [----------->     .     .     .| [2:4] S  -> NP * VP 
  |.     .     [----------->     .     .     .| [2:4] NP -> NP * PP 
  |.     .     .     .     .     [----------->| [5:7] S  -> NP * VP 
  |.     .     .     .     .     [----------->| [5:7] NP -> NP * PP 
  |.     [----------------->     .     .     .| [1:4] VP -> VP * PP 
  |.     .     [-----------------------------]| [2:7] NP -> NP PP * 
  |.     [-----------------------------------]| [1:7] VP -> VP PP * 
  |.     .     [----------------------------->| [2:7] S  -> NP * VP 
  |.     .     [----------------------------->| [2:7] NP -> NP * PP 
  |.     [----------------------------------->| [1:7] VP -> VP * PP 
  |.     [-----------------------------------]| [1:7] VP -> V NP * 
  |[=========================================]| [0:7] S  -> NP VP * 
  |[=========================================]| [0:7] S  -> NP VP * 
  |.     [----------------------------------->| [1:7] VP -> VP * PP 
  (S: (NP: 'John') (VP: (VP: (V: 'saw') (NP: (Det: 'a') (N: 'cat'))) 
  (PP: (P: 'with') (NP: (Det: 'my') (N: 'cookie')))))

|nopar| Notice that in this output, ``'[-----]'`` indicates a complete edge,
``'>'`` indicates a self-loop edge, and ``'[----->'`` indicates an
incomplete edge.

Exercises
---------

1. Use the graphical chart-parser interface to experiment with
   different rule invocation strategies. Come up with your own strategy
   which you can execute manually using the graphical interface. Describe
   the steps, and report any efficiency improvements it has (e.g. in terms
   of the size of the resulting chart). Do these improvements depend on
   the structure of the grammar? What do you think of the prospects for
   significant performance boosts from cleverer rule invocation
   strategies?

---------------------
Probabilistic Parsing
---------------------

.. TODO: mention interest in having weights is because they can be learned.
   Without this it is mysterious why we would want to bother.
   Technical aspects follow, but this is important motivation (Steven)

.. TODO: expand opening discussion to remind reader why we're doing this
   (we had moved stuff to front of chapter) (Steven)

As we pointed out in the introduction to this
chapter, dealing with ambiguity is a key challenge to broad coverage
parsers. We have shown how chart parsing can help improve the
efficiency of computing multiple parses of the same sentences. But
the sheer number of parses can be just overwhelming. We will show how
probabilistic parsing helps to manage a large space of parses.  
However, before we deal with these parsing issues,
we must first back up and introduce weighted grammars.

Weighted Grammars
-----------------
 
We begin by considering the verb `give`:lx:.  This verb requires both
a direct object (the thing being given) and an indirect object (the
recipient).  These complements can be given in either order, as
illustrated in example dative_.  In the "prepositional dative" form,
the indirect object appears last, and inside a prepositional phrase,
while in the "double object" form, the indirect object comes first:

.. _dative:
.. ex::
   .. ex::
      Kim gave a bone to the dog
   .. ex::
      Kim gave the dog a bone

Using the Penn Treebank sample, we can examine all instances of
prepositional dative and double object constructions involving
`give`:lx:, as shown below:

    >>> from nltk_lite.corpora import treebank
    >>> from string import join
    >>> give = lambda t: t.node == 'VP' and len(t) > 2 and t[1].node == 'NP'\
    ...        and (t[2].node == 'PP-DTV' or t[2].node == 'NP')\
    ...        and ('give' in t[0].leaves() or 'gave' in t[0].leaves())
    >>> for tree in treebank.parsed():
    ...     for t in tree.subtrees(give):
    ...         print "%s [%s: %s] [%s: %s]" %\
    ...               (join(t[0].leaves()),
    ...                t[1].node, join(t[1].leaves()),
    ...                t[2].node, join(t[2].leaves()))
    gave [NP: the chefs] [NP: a standing ovation]
    give [NP: advertisers] [NP: discounts for * maintaining or increasing ad spending]
    give [NP: it] [PP-DTV: to the politicians]
    gave [NP: them] [NP: similar help]
    give [NP: them] [NP: *T*-1]
    give [NP: only French history questions] [PP-DTV: to students in a European history class]
    give [NP: federal judges] [NP: a raise]
    give [NP: consumers] [NP: the straight scoop on the U.S. waste crisis]
    gave [NP: Mitsui] [NP: access to a high-tech medical product]
    give [NP: Mitsubishi] [NP: a window on the U.S. glass industry]
    give [NP: much thought] [PP-DTV: to the rates 0 she was receiving *T*-2 , nor to the fees 0
      she was paying *T*-3]
    give [NP: your Foster Savings Institution] [NP: the gift of hope and freedom from the federal
      regulators who *T*-206 want *-1 to close its doors -- for good]
    give [NP: market operators] [NP: the authority * to suspend trading in futures at any time]
    gave [NP: quick approval] [PP-DTV: to $ 3.18 billion *U* in supplemental appropriations for
      law enforcement and anti-drug programs in fiscal 1990]
    give [NP: the Transportation Department] [NP: up to 50 days 0 * to review any purchase of
      15 % or more of the stock in an airline *T*-1]
    give [NP: the president] [NP: such power]
    give [NP: me] [NP: the heebie-jeebies]
    give [NP: holders] [NP: the right *RNR*-1 , but not the obligation *RNR*-1 , * to buy -LRB-
      a call -RRB- or sell -LRB- a put -RRB- a specified amount of an underlying investment by
      a certain date at a preset price , known * as the strike price]
    gave [NP: Mr. Thomas] [NP: only a `` qualified '' rating , rather than `` well qualified]
    give [NP: the president] [NP: line-item veto power]

We can observe a strong tendency for the shortest complement to appear
first.  However, this does not account for a form like
``give [NP: federal judges] [NP: a raise]``, where animacy may be
playing a role.  In fact there turn out to be a large number of contributing
factors, as surveyed by Bresnan and Hay (2006).

How can such tendencies be expressed in a conventional context free
grammar?  It turns out that they cannot.  However, we can address the
problem by adding weights, or probabilities, to the productions of a
grammar.

A `probabilistic context free grammar`:dt: (or *PCFG*) is a context free
grammar that associates a probability with each of its productions.
It generates the same set of parses for a text that the corresponding
context free grammar does, and assigns a probability to each parse.
The probability of a parse generated by a PCFG is simply the product
of the probabilities of the productions used to generate it.

Probabilistic context free grammars are implemented by the
``nltk_lite.parse.pcfg.WeightedGrammar`` class.  Like CFGs, each PCFG consists
of a start state and a list of productions.  But the productions are
represented by ``pcfg.WeightedProduction``, a subclass of ``cfg.Production``
that associates a probability with a context free grammar
production.

PCFG Productions
----------------
      
Each PCFG production specifies that a nonterminal (the *left-hand
side*) can be expanded to a sequence of terminals and nonterminals
(the *right-hand side*).  In addition, each production has a
probability associated with it.  Productions are created using the
``nltk_lite.parse.pcfg.WeightedProduction`` constructor, which takes a
probability, a nonterminal left-hand side, and zero or more terminals
and nonterminals for the right-hand side.

  >>> from nltk_lite.parse import cfg
  >>> S, VP, V, NP = cfg.nonterminals('S, VP, V, NP')

  >>> from nltk_lite.parse import pcfg
  >>> prod1 = pcfg.WeightedProduction(VP, [V, NP], prob=0.23)
  >>> prod1
  VP -> V NP (p=0.23)

  >>> prod2 = pcfg.WeightedProduction(V, ['saw'], prob=0.12)
  >>> prod2
  V -> 'saw' (p=0.12)

  >>> prod3 = pcfg.WeightedProduction(NP, ['cookie'], prob=0.04)
  >>> prod3
  NP -> 'cookie' (p=0.04)

The probability associated with a production is returned by the
``prob`` method:

  >>> print prod1.prob(), prod2.prob(), prod3.prob() 
  0.23 0.12 0.04

As with CFG productions, the left-hand side of a PCFG production is
returned by the ``lhs`` method; and the right-hand side is returned by
the ``rhs`` method:

  >>> prod1.lhs() 
  <VP>
  >>> prod1.rhs() 
  (<V>, <NP>)


PCFGs
-----

PCFGs are created using the ``pcfg.WeightedGrammar`` constructor, which takes
a start symbol and a list of productions:

  >>> prods = [pcfg.WeightedProduction(S, [NP, VP], prob=1.0),
  ...          pcfg.WeightedProduction(VP, ['saw', NP], prob=0.4),
  ...          pcfg.WeightedProduction(VP, ['ate'], prob=0.3),
  ...          pcfg.WeightedProduction(VP, ['gave', NP, NP], prob=0.3),
  ...          pcfg.WeightedProduction(NP, ['the', 'cookie'], prob=0.8),
  ...          pcfg.WeightedProduction(NP, ['Jack'], prob=0.2)]

  >>> grammar = pcfg.WeightedGrammar(S, prods) 
  >>> print grammar
  Grammar with 6 productions (start state = S)
      S -> NP VP (p=1.0)
      VP -> 'saw' NP (p=0.4)
      VP -> 'ate' (p=0.3)
      VP -> 'gave' NP NP (p=0.3)
      NP -> 'the' 'cookie' (p=0.8)
      NP -> 'Jack' (p=0.2)

In order to ensure that the trees generated by the grammar form a
proper probability distribution, PCFG grammars impose the constraint
that all productions with a given left-hand side must have
probabilities that sum to one:

  for all *lhs*:
    |Sigma|\ :subscript:`rhs` P(*lhs* |rarr| *rhs*) = 1

The example grammar given above obeys this constraint: for ``S``,
there is only one production, with a probability of 1.0; for ``VP``,
0.4+0.3+0.3=1.0; and for ``NP``, 0.8+0.2=1.0.

As with CFGs, the start state of a PCFG is returned by the ``start``
method; and the productions are returned by the ``productions``
method:

  >>> grammar.start()
  <S>
  >>> from pprint import pprint
  >>> pprint(grammar.productions())
  [S -> NP VP (p=1.0),
   VP -> 'saw' NP (p=0.4),
   VP -> 'ate' (p=0.3),
   VP -> 'gave' NP NP (p=0.3),
   NP -> 'the' 'cookie' (p=0.8),
   NP -> 'Jack' (p=0.2)]

---------------------
Probabilistic Parsers
---------------------

The Probabilistic Parser Interface
----------------------------------

The parse trees returned by ``parse`` and ``get_parse_list`` include
probabilities:

  >>> from nltk_lite.parse import ViterbiParse
  >>> from nltk_lite import tokenize
  >>> viterbi_parser = ViterbiParse(grammar)
  >>> sent = list(tokenize.whitespace('Jack saw the cookie'))
  >>> viterbi_parser.get_parse(sent)
  (S: (NP: 'Jack') (p=0.2) (VP: 'saw' (NP: 'the' 'cookie') (p=0.8)) (p=0.32)) (p=0.064)

  >>> viterbi_parser.get_parse_list(sent)
  [(S: (NP: 'Jack') (p=0.2) (VP: 'saw' (NP: 'the' 'cookie') (p=0.8)) (p=0.32)) (p=0.064)]


Probabilistic Parser Implementations
------------------------------------

The next two sections introduce two probabilistic parsing algorithms
for PCFGs.  The first is a Viterbi-style algorithm that uses dynamic
programming to find the single most likely parse for a given text.
Whenever it finds multiple possible parses for a subtree, it discards
all but the most likely parse.  The second is a bottom-up chart parser
that maintains a queue of edges, and adds them to the chart one at a
time.  The ordering of this queue is based on the probabilities
associated with the edges, allowing the parser to expand more likely
edges before less likely ones.  Different queue orderings are used to
implement a variety of different search strategies.  These algorithms
are implemented in the ``nltk_lite.parse.viterbi`` and
``nltk_lite.parse.pchart`` modules.

A Viterbi-Style PCFG Parser
---------------------------

The ``ViterbiParse`` PCFG parser is a bottom-up parser that uses
dynamic programming to find the single most likely parse for a text.
It parses texts by iteratively filling in a *most likely constituents
table*.  This table records the most likely tree structure for each
span and node value.  In particular, it has an entry for every start
index, end index, and node value, recording the most likely subtree
that spans from the start index to the end index, and has the given
node value.  For example, after parsing the sentence "I saw John with
my cookie" with a simple grammar, the most likely constituents table
might be as follows:

===== ==== ==================================================================  =======
Most Likely Constituents Table
--------------------------------------------------------------------------------------
Span  Node Tree                                                                Prob
===== ==== ==================================================================  =======
[0:1] NP   (NP: I)                                                             0.3
[2:3] NP   (NP: John)                                                          0.3
[4:6] NP   (NP: my cookie)                                                     0.2
[3:6] PP   (PP: with (NP: my cookie))                                          0.1
[2:6] NP   (NP: (NP: John) (PP: with (NP: my cookie)))                         0.01
[1:3] VP   (VP: saw (NP: John)))                                               0.03
[1:6] VP   (VP: saw (NP: (NP: John) (PP: with (NP: my cookie))))               0.001
[0:6] S    (S: (NP: I) (VP: saw (NP: (NP: John) (PP: with (NP: my cookie)))))  0.0001
===== ==== ==================================================================  =======

Once the table has been completely filled in, the parser simply
returns the entry for the most likely constituent that spans the
entire text, and whose node value is the start symbol.  For this
example, it would return the entry with a span of [0:6] and a node
value of "S".

Note that we only record the *most likely* constituent for any given
span and node value.  For example, in the table above, there are
actually two possible constituents that cover the span [1:6] and have
"VP" node values.
    
1. "saw John, who has my cookie":

  (VP: saw
     (NP: (NP: John)
          (PP: with (NP: my cookie))))

.. the above example is nonsense when we use a proper name;
   it only works when we use a common noun,
   e.g. "saw the man with my cookie"


2. "used my cookie to see John":

  (VP: saw
     (NP: John)
     (PP: with (NP: my cookie)))

Since the grammar we are using to parse the text indicates that the
first of these tree structures has a higher probability, the parser
discards the second one.

**Filling in the Most Likely Constituents Table:**
Because the grammar used by ``ViterbiParse`` is a PCFG, the
probability of each constituent can be calculated from the
probabilities of its children.  Since a constituent's children can
never cover a larger span than the constituent itself, each entry of
the most likely constituents table depends only on entries for
constituents with *shorter* spans (or equal spans, in the case of
unary and epsilon productions).

``ViterbiParse`` takes advantage of this fact, and fills in the most
likely constituent table incrementally.  It starts by filling in the
entries for all constituents that span a single element of text.
After it has filled in all the table entries for constituents that
span one element of text, it fills in the entries for constituents
that span two elements of text.  It continues filling in the entries
for constituents spanning larger and larger portions of the text,
until the entire table has been filled.

To find the most likely constituent with a given span and node value,
``ViterbiParse`` considers all productions that could produce that
node value.  For each production, it checks the most likely
constituents table for sequences of children that collectively cover
the span and that have the node values specified by the production's
right hand side.  If the tree formed by applying the production to the
children has a higher probability than the current table entry, then
it updates the most likely constituents table with the new tree.

**Handling Unary Productions and Epsilon Productions:**
A minor difficulty is introduced by unary productions and epsilon
productions: an entry of the most likely constituents table might
depend on another entry with the same span.  For example, if the
grammar contains the production ``V`` |rarr| ``VP``, then the table
entries for ``VP`` depend on the entries for ``V`` with the same span.
This can be a problem if the constituents are checked in the wrong
order.  For example, if the parser tries to find the most likely
constituent for a ``VP`` spanning [1:3] before it finds the most
likely constituents for ``V`` spanning [1:3], then it can't apply the
``V`` |rarr| ``VP`` production.
      
To solve this problem, ``ViterbiParse`` repeatedly checks each span
until it finds no new table entries.  Note that cyclic grammar
productions (e.g. ``V`` |rarr| ``V``) will *not* cause this procedure
to enter an infinite loop.  Since all production probabilities are
less than or equal to 1, any constituent generated by a cycle in the
grammar will have a probability that is less than or equal to the
original constituent; so ``ViterbiParse`` will discard it.

Using ``ViterbiParse``
----------------------

Viterbi parsers are created using the ``ViterbiParse`` constructor:

  >>> from nltk_lite.parse.viterbi import *
  >>> ViterbiParse(grammar)
  <ViterbiParser for <Grammar with 6 productions>>

Note that since ``ViterbiParse`` only finds the single most likely
parse, that ``get_parse_list`` will never return more than one parse.

  >>> viterbi_parser1 = ViterbiParse(pcfg.toy1)
  >>> sent1 = list(tokenize.whitespace('I saw John with my cookie'))
  >>> tree1 = viterbi_parser1.parse(sent1)
  >>> print tree1
  (S:
    (NP: 'I')
    (VP:
      (V: 'saw')
      (NP:
        (NP: 'John')
        (PP: (P: 'with') (NP: (Det: 'my') (N: 'cookie')))))) (p=5.2040625e-05)

      
  >>> viterbi_parser2 = ViterbiParse(pcfg.toy2)
  >>> sent2 = list(tokenize.whitespace('the boy saw Jack with Bob under the table with a telescope'))
  >>> trees = viterbi_parser2.get_parse_list(sent2)
  >>> for tree in trees:
  ...     print tree
  (S:
    (NP: (Det: 'the') (N: 'boy'))
    (VP:
      (V: 'saw')
      (NP:
        (NP: (Name: 'Jack'))
        (PP:
          (P: 'with')
          (NP:
            (NP:
              (NP: (Name: 'Bob'))
              (PP:
                (P: 'under')
                (NP: (Det: 'the') (N: 'table'))))
            (PP:
              (P: 'with')
              (NP: (Det: 'a') (N: 'telescope')))))))) (p=7.53678903935e-11)

The ``trace`` method can be used to set the level of tracing output
that is generated when parsing a text.  Trace output displays the
constituents that are considered, and indicates which ones are added
to the most likely constituent table.  It also indicates the
likelihood for each constituent.

  >>> viterbi_parser1.trace(3)
  >>> tree = viterbi_parser1.parse(sent1)
  Inserting tokens into the most likely constituents table...
     Insert: |=.....| I
     Insert: |.=....| saw
     Insert: |..=...| John
     Insert: |...=..| with
     Insert: |....=.| my
     Insert: |.....=| cookie
  Finding the most likely constituents spanning 1 text elements...
     Insert: |=.....| NP -> 'I' (p=0.15)              0.1500000000 
     Insert: |.=....| V -> 'saw' (p=0.65)             0.6500000000 
     Insert: |.=....| VP -> V (p=0.2)                 0.1300000000 
     Insert: |..=...| NP -> 'John' (p=0.1)            0.1000000000 
     Insert: |...=..| P -> 'with' (p=0.61)            0.6100000000 
     Insert: |....=.| Det -> 'my' (p=0.2)             0.2000000000 
     Insert: |.....=| N -> 'cookie' (p=0.5)           0.5000000000 
  Finding the most likely constituents spanning 2 text elements...
     Insert: |==....| S -> NP VP (p=1.0)              0.0195000000 
     Insert: |.==...| VP -> V NP (p=0.7)              0.0455000000 
     Insert: |....==| NP -> Det N (p=0.5)             0.0500000000 
  Finding the most likely constituents spanning 3 text elements...
     Insert: |===...| S -> NP VP (p=1.0)              0.0068250000 
     Insert: |...===| PP -> P NP (p=1.0)              0.0305000000 
  Finding the most likely constituents spanning 4 text elements...
     Insert: |..====| NP -> NP PP (p=0.25)            0.0007625000 
  Finding the most likely constituents spanning 5 text elements...
     Insert: |.=====| VP -> VP PP (p=0.1)             0.0001387750 
     Insert: |.=====| VP -> V NP (p=0.7)              0.0003469375 
    Discard: |.=====| VP -> VP PP (p=0.1)             0.0001387750 
  Finding the most likely constituents spanning 6 text elements...
     Insert: |======| S -> NP VP (p=1.0)              0.0000520406 

The level of tracing output can also be set with an optional argument to
the ``ViterbiParse`` constructor.  By default, no tracing output is generated.
Tracing output can be turned off by calling ``trace`` with a value of ``0``.

-----------------------------
A Bottom-Up PCFG Chart Parser
-----------------------------

.. TODO: cull implementation discussion (Steven)

Introduction
------------

The Viterbi-style algorithm described in the previous section finds
the single most likely parse for a given text.  But for many
applications, it is useful to produce several alternative parses.
This is often the case when probabilistic parsers are combined with
other probabilistic systems.  In particular, the most probable parse
may be assigned a low probability by other systems; and a parse that
is given a low probability by the parser might have a better overall
probability.

For example, a probabilistic parser might decide that the most likely
parse for "I saw John with the cookie" is is the structure with the
interpretation "I used my cookie to see John"; but that parse would be
assigned a low probability by a semantic system.  Combining the
probability estimates from the parser and the semantic system, the
parse with the interpretation "I saw John, who had my cookie" would be
given a higher overall probability.

This section describes ``BottomUpChartParser``, a parser for PCFGs
that can find multiple parses for a text.  It assumes that you have
already read the chart parsing tutorial, and are familiar with the
data structures and productions used for chart parsing.

The Basic Algorithm
-------------------

``BottomUpChartParser`` is a bottom-up parser for PCFGs that uses
a ``Chart`` to record partial results.  It maintains a queue of
edges, and adds them to the chart one at a time.  The ordering
of this queue is based on the probabilities associated with the edges,
allowing the parser to insert more likely edges before exploring less
likely ones.  For each edge that the parser adds to the chart, it may
become possible to insert new edges into the chart; these are added to
the queue.  ``BottomUpChartParser`` continues adding the edges in the
queue to the chart until enough complete parses have been found, or
until the edge queue is empty.

Probabilistic Edges
-------------------

An ``Edge`` associates a dotted production and a location with a
(partial) parse tree.  A *probabilistic edge* can be formed by using a
``ProbabilisticTree`` to encode an edge's parse tree.  The probability
of this tree is the product of the probability of the production that
generated it and the probabilities of its children.  For example, the
probability associated with an edge ``[Edge: S`` |rarr| ``NP`` |dot|
``VP, (0:2]`` is the probability of its NP child times the probability
of the PCFG production ``S`` |rarr| ``NP VP``.  Note that an edge's
tree only includes children for elements to the left of the
edge's dot.  Thus, the edge's probability does *not* include any
probabilities for the elements to the right of the edge's dot.

The Edge Queue
--------------

The edge queue is a sorted list of edges that can be added to the
chart.  It is initialized with a single edge for each token in the
text.  These *token edges* have the form [Edge: token |rarr| |dot|]
where *token* is the word.

As each edge from the queue is added to the chart, it may become
possible to insert new edges into the chart; these new edges are added
to the queue.  There are two ways that it can become possible to
insert new edges into the chart:

1. The *bottom-up initialization production* can be used to add a self-loop edge
   whenever an edge whose dot is in position 0 is added to the chart.

2. The *fundamental production* can be used to combine a new edge with edges already
   present in the chart.

The edge queue is implemented using a ``list``.  For efficiency
reasons, ``BottomUpChartParser`` uses ``pop`` to remove edges from the
queue.  Thus, the front of the queue is the *end* of the list.  This
needs to be kept in mind when implementing sorting orders for the
queue: edges that should be tried first should be placed at the end of
the list.

Sorting The Edge Queue
----------------------

By changing the sorting order used by the queue, we can control the
strategy that the parser uses to search for parses of a text.  Since
there are a wide variety of reasonable search strategies,
``BottomUpChartParser`` does not define the sorting order for the
queue.  Instead, ``BottomUpPCFGChartParser`` is defined as an abstract
class; and subclasses are used to implement a variety of different
queue orderings.  Each subclass is required to define the
``sort_queue`` method, which sorts a given queue.  The remainder of
this section describes four different subclasses of
``BottomUpChartParser`` that are defined in the
``nltk_lite.parse.pchart`` module.

**InsideParse:**

.. We should either explain "inside probabilities" or rename this parser (to
        ``LowestCostFirstParser``?). 

The simplest way to order the queue is to sort the edges by the
probabilities of their trees.  This ordering concentrates the
efforts of the parser on edges that are more likely to be correct
descriptions of the texts that they span.  This approach is
implemented by the ``InsideParse`` class.

The probability of an edge's tree provides an upper bound on the
probability of any parse produced using that edge.  The probabilistic
"cost" of using an edge to form a parse is one minus its tree's
probability.  Thus, inserting the edges with the most likely trees
first results in a *lowest-cost-first* search strategy.
Lowest-cost-first search is an *optimal* search strategy: the first
solution it finds is guaranteed to be the best solution.

However, lowest-cost-first search can be rather inefficient.  Since a
tree's probability is the product of the probabilities of all the
productions used to generate it, smaller trees tend to have higher
probabilities than larger ones.  Thus, lowest-cost-first search tends
to insert edges with small trees before moving on to edges with larger
ones.  But any complete parse of the text will necessarily have a
large tree; so complete parses will tend to be inserted after nearly
all other edges.

The basic problem with lowest-cost-first search is that it ignores the
probability that an edge's tree is part of a complete parse.  It will
try parses that are locally coherent, even if they are unlikely to
form part of a complete parse.  Unfortunately, it can be quite
difficult to calculate the probability that a tree is part of a
complete parse.  However, we can use a variety of techniques to
approximate that probability.

Since ``InsideParse`` is a subclass of ``BottomUpChartParse``, it only
needs to define a ``sort_queue`` method.  Thus, the implementation of
``InsideParse`` class is quite simple::

  class InsideParse(BottomUpChartParse):
    def sort_queue(self, queue, chart):
      # Sort the edges by the probabilities of their trees.
      queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))

**LongestParse:**
``LongestParse`` sorts its queue in descending order of the edges'
lengths.  These lengths (properly normalized) provide a crude
approximations to the probabilities that trees are part of complete
parses.  Thus, ``LongestParse`` employs a *best-first* search
strategy, where it inserts the edges that are closest to producing
complete parses before trying any other edges.  Best-first search is
*not* an optimal search strategy: the first solution it finds is not
guaranteed to be the best solution.  However, it will usually find a
complete parse much more quickly than lowest-cost-first search.

Since ``LongestParse`` is a subclass of ``BottomUpChartParse``, its implementation
simply defines a ``sort_queue`` method::

  class LongestParse(BottomUpChartParse):
    def sort_queue(self, queue, chart):
      # Sort the edges by the lengths of their trees.
      queue.sort(lambda e1,e2: cmp(len(e1.loc()), len(e2.loc())))

**BeamParse:**
When large grammars are used to parse a text, the edge queue can grow
quite long.  The edges at the end of a large well-sorted queue are
unlikely to be used.  Therefore, it is reasonable to remove (or
*prune*) these edges from the queue.

``BeamParse`` provides a simple implementation of a pruning PCFG
parser.  It uses the same sorting order as ``InsideParse``.  But
whenever the edge queue grows beyond a pre-defined maximum length,
``BeamParse`` truncates it.  The resulting search strategy,
lowest-cost-first search with pruning, is a type of beam search.  (A
*beam search* is a search strategy that only keeps the best partial
results.)  The queue's predefined maximum length is called the *beam
size* (or simply the *beam*).  The parser's beam size is set by the
first argument to its constructor.

Beam search reduces the space requirements for lowest-cost-first
search, by discarding edges that are not likely to be used.  But beam
search also loses many of lowest-cost-first search's more useful
properties.  Beam search is not optimal: it is not guaranteed to find
the best parse first.  In fact, since it might prune a necessary edge,
beam search is not even *complete*: it is not guaranteed to return a
parse if one exists.

The implementation for ``BeamParse`` defines two methods.  First, it
overrides the constructor, since it needs to record the beam size.
And second, it defines the ``sort_queue`` method, which sorts the
queue and discards any excess edges::
        
  class BeamParse(BottomUpChartParse):
    def __init__(self, beam_size, grammar, trace=0):
      BottomUpChartParse.__init__(self, grammar, trace)
      self._beam_size = beam_size

    def sort_queue(self, queue, chart):
      # Sort the queue.
      queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))
      # Truncate the queue, if necessary.
      if len(queue) > self._beam_size:
        queue[:] = queue[len(queue)-self._beam_size:]

Note that when truncating the queue, ``sort_queue`` uses the
expression ``queue[:]`` to change the *contents* of the ``queue``
variable.  In particular, compare it to the following code, which
reassigns the local variable ``queue``, but does not modify the
contents of the given list::

  # WRONG: This does not change the contents of the edge queue. 
  if len(queue) > self._beam_size:
    queue = queue[len(queue) - self._beam_size:]

  # WRONG: The sort method returns None.
  return queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))

      
.. We plan to add an inside/outside parser; when we
   do, we'll add a description of it to this section. 

Using ``BottomUpChartParser``
-----------------------------

These parsers are created using the ``BottomUpChartParse``
subclasses's constructors.  These include: ``InsideParse``,
``LongestParse``, ``BeamParser``, and ``RandomParse``.

See the reference documentation for the ``BottomUpChartParse`` module
for a complete list of subclasses.  Unless a subclass overrides the
constructor, it takes a single PCFG:

  >>> from nltk_lite.parse.pchart import *
  >>> inside_parser = InsideParse(pcfg.toy1)
  >>> longest_parser = LongestParse(pcfg.toy1)
  >>> beam_parser = BeamParse(20, pcfg.toy1)

  >>> print inside_parser.parse(sent1)
  (S:
    (NP: 'I')
    (VP:
      (V: 'saw')
      (NP:
        (NP: 'John')
        (PP: (P: 'with') (NP: (Det: 'my') (N: 'cookie')))))) (p=5.2040625e-05)
      
  >>> for tree in inside_parser.get_parse_list(sent1):
  ...     print tree
  (S:
    (NP: 'I')
    (VP:
      (V: 'saw')
      (NP:
        (NP: 'John')
        (PP: (P: 'with') (NP: (Det: 'my') (N: 'cookie')))))) (p=5.2040625e-05)
  (S:
    (NP: 'I')
    (VP:
      (VP: (V: 'saw') (NP: 'John'))
      (PP: (P: 'with') (NP: (Det: 'my') (N: 'cookie'))))) (p=2.081625e-05)

.. Warning:: ``BottomUpChartParse`` is an abstract class; you should not directly
   instantiate it.  If you try to use it to parse a text, it will raise an exception,
   since ``sort_queue`` will be undefined.

The ``trace`` method can be used to set the level of tracing output
that is generated when parsing a text.  Trace output displays edges as
they are added to the chart, and shows the probability for each edges'
tree.

  >>> inside_parser.trace(3)
  >>> trees = inside_parser.get_parse_list(sent1)
    |. . . . . [-]| [5:6] 'cookie'                     prob=1.0
    |. . . . [-] .| [4:5] 'my'                         prob=1.0
    |. . . [-] . .| [3:4] 'with'                       prob=1.0
    |. . [-] . . .| [2:3] 'John'                       prob=1.0
    |. [-] . . . .| [1:2] 'saw'                        prob=1.0
    |[-] . . . . .| [0:1] 'I'                          prob=1.0
    |. [-] . . . .| [1:2] V  -> 'saw' *                prob=0.65
    |. > . . . . .| [1:1] VP -> * V NP                 prob=0.7
    |. > . . . . .| [1:1] V  -> * 'saw'                prob=0.65
    |. . . [-] . .| [3:4] P  -> 'with' *               prob=0.61
    |. . . > . . .| [3:3] PP -> * P NP                 prob=1.0
    |. . . [-> . .| [3:4] PP -> P * NP                 prob=0.61
    |. . . > . . .| [3:3] P  -> * 'with'               prob=0.61
    |. . . . . [-]| [5:6] N  -> 'cookie' *             prob=0.5
    |. . . . . > .| [5:5] N  -> * 'cookie'             prob=0.5
    |. [-> . . . .| [1:2] VP -> V * NP                 prob=0.455
    |. > . . . . .| [1:1] VP -> * V                    prob=0.2
    |. . . . [-] .| [4:5] Det -> 'my' *                prob=0.2
    |. . . . > . .| [4:4] NP -> * Det N                prob=0.5
    |. . . . > . .| [4:4] Det -> * 'my'                prob=0.2
    |[-] . . . . .| [0:1] NP -> 'I' *                  prob=0.15
    |> . . . . . .| [0:0] S  -> * NP VP                prob=1.0
    |> . . . . . .| [0:0] NP -> * NP PP                prob=0.25
    |[-> . . . . .| [0:1] S  -> NP * VP                prob=0.15
    |> . . . . . .| [0:0] NP -> * 'I'                  prob=0.15
    |. [-] . . . .| [1:2] VP -> V *                    prob=0.13
    |. > . . . . .| [1:1] VP -> * VP PP                prob=0.1
    |. . . . [-> .| [4:5] NP -> Det * N                prob=0.1
    |. . [-] . . .| [2:3] NP -> 'John' *               prob=0.1
    |. . > . . . .| [2:2] S  -> * NP VP                prob=1.0
    |. . > . . . .| [2:2] NP -> * NP PP                prob=0.25
    |. . [-> . . .| [2:3] S  -> NP * VP                prob=0.1
    |. . > . . . .| [2:2] NP -> * 'John'               prob=0.1
    |. . . . [---]| [4:6] NP -> Det N *                prob=0.05
    |. . . . > . .| [4:4] S  -> * NP VP                prob=1.0
    |. . . . > . .| [4:4] NP -> * NP PP                prob=0.25
    |. . . . [--->| [4:6] S  -> NP * VP                prob=0.05
    |. [---] . . .| [1:3] VP -> V NP *                 prob=0.0455
    |[-> . . . . .| [0:1] NP -> NP * PP                prob=0.0375
    |. . . [-----]| [3:6] PP -> P NP *                 prob=0.0305
    |. . [-> . . .| [2:3] NP -> NP * PP                prob=0.025
    |[---] . . . .| [0:2] S  -> NP VP *                prob=0.0195
    |. [-> . . . .| [1:2] VP -> VP * PP                prob=0.013
    |. . . . [--->| [4:6] NP -> NP * PP                prob=0.0125
    |[-----] . . .| [0:3] S  -> NP VP *                prob=0.006825
    |. [---> . . .| [1:3] VP -> VP * PP                prob=0.00455
    |. . [-------]| [2:6] NP -> NP PP *                prob=0.0007625
    |. . [------->| [2:6] S  -> NP * VP                prob=0.0007625
    |. [---------]| [1:6] VP -> V NP *                 prob=0.0003469375
    |. . [------->| [2:6] NP -> NP * PP                prob=0.000190625
    |. [---------]| [1:6] VP -> VP PP *                prob=0.000138775
    |[===========]| [0:6] S  -> NP VP *                prob=5.2040625e-05
    |. [--------->| [1:6] VP -> VP * PP                prob=3.469375e-05
    |[===========]| [0:6] S  -> NP VP *                prob=2.081625e-05
    |. [--------->| [1:6] VP -> VP * PP                prob=1.38775e-05


-----------------
Grammar Induction
-----------------

As we have seen, PCFG productions are just like CFG productions,
adorned with probabilities.  So far, we have simply specified these
probabilities in the grammar.  However, it is more usual to *estimate*
these probabilities from training data, namely a collection of parse
trees or *treebank*.

The simplest method uses *Maximum Likelihood Estimation*, so called
because probabilities are chosen in order to maximize the likelihood
of the training data.  The probability of a production
``VP`` |rarr| ``V NP PP`` is *p(V,NP,PP | VP)*.  We calculate this as
follows::

                        count(VP -> V NP PP)
      P(V,NP,PP | VP) = --------------------
                        count(VP -> ...)

Here is a simple program that induces a grammar from the first
three parse trees in the Penn Treebank corpus:

    >>> from nltk_lite.corpora import treebank
    >>> from itertools import islice
    >>> productions = []
    >>> for tree in islice(treebank.parsed(),3):
    ...      productions += tree.productions()
    >>> grammar = pcfg.induce(S, productions)
    >>> for production in grammar.productions()[:10]:
    ...      print production
    PP -> IN NP (p=1.0)
    NNP -> 'Nov.' (p=0.0714285714286)
    NNP -> 'Agnew' (p=0.0714285714286)
    JJ -> 'industrial' (p=0.142857142857)
    NP -> CD NNS (p=0.133333333333)
    , -> ',' (p=1.0)
    CC -> 'and' (p=1.0)
    NNP -> 'Pierre' (p=0.0714285714286)
    NP -> NNP NNP NNP NNP (p=0.0666666666667)
    NNP -> 'Rudolph' (p=0.0714285714286)

.. Note:: Grammar induction usually involves normalizing the grammar
   in various ways.  The ``nltk_lite.parse.treetransforms`` module
   supports binarization (Chomsky Normal Form), parent annotation,
   Markov order-N smoothing, and unary collapsing.  This information
   can be accessed by importing ``treetransforms`` from
   ``nltk_lite.parse``, then calling ``help(treetransforms)``.


---------------
Further Reading
---------------

.. [Abney1996] 
	Steven Abney (1996). 
	'Statistical Methods and Linguistics.'
	In: Judith Klavans and Philip Resnik (eds.),
	*The Balancing Act: Combining Symbolic and Statistical
	Approaches to Language*.
	MIT Press.
	`<http://www.vinartus.net/spa/95c.pdf>`_

.. [ChurchPatil1982]
        Kenneth Church and Ramesh Patil (1982).
	Coping with Syntactic Ambiguity or How to Put the Block in the Box on the Table
	American Journal of Computational Linguistics, Vol 8, No.\
        3--4, pp 139--149



.. [BresnanHay2006]
	Joan Bresnan and Jennifer Hay (2006). 
	'Gradient Grammar: An Effect of Animacy on the Syntax of `give`:lx: in Varieties of English.'
	`<http://www-lfg.stanford.edu/bresnan/anim-spokensyntax-final.pdf>`_

.. [JurafskyMartin]
        Daniel Jurafsky and James H. Martin
        *Speech and Language Processing*
        Prentice-Hall
        2nd Edition

.. [ManningSchutze1999]
	Christopher Manning and Hinrich Schutze (1999).  
	*Foundations of Statistical Natural Language Processing*.  
	MIT Press.  (esp chapter 12).

.. include:: footer.txt
