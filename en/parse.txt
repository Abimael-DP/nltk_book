.. -*- mode: rst -*-
.. include:: ../definitions.txt

=======================
7. Grammars and Parsing
=======================

------------
Introduction
------------

Early experiences with the kind of grammar taught in school are often
confusing.  Written work may be graded by a teacher who red-lines all
the grammar errors they won't put up with.  Like the dangling
preposition in the last sentence, or sentences like this one which
lack a main verb.  Learning English as a second language, it may be
difficult to discover which of these errors needs to be fixed.
Correct punctuation is something of an obsession for many writers and
editors (as our own students have observed).  Of course, it is all in
the name of effective communication.  In the following example, the
interpretation of a relative clause as restrictive or non-restrictive
depends on the presence of commas alone:

.. ex::
  .. _rest:
  .. ex:: The presidential candidate, who was extremely popular,
          smiled broadly.
  .. _nonrest:
  .. ex:: The presidential candidate who was extremely popular smiled broadly.

In rest_, we assume there is just one presidential candidate, and say
two things about her: that she was popular and that she smiled. In
nonrest_, on the other hand, we use the description `who was extremely
popular`:lx: as a means of identifying for the hearer which of several
candidates we are referring to. 

It is clear that some of these rules are important.  Others seem to be
vestiges of antiquated style that preoccupies only the most crusty
curmudgeons.  As an example, consider the injunction that
:lx:`however` -- when used to mean *nevertheless* -- must not appear
at the start of a sentence.  Pullum argues that Strunk and White were
merely insisting that English usage should conform to "an utterly
unimportant minor statistical detail of style concerning adverb
placement in the literature they knew" [languagelog.org].

When reading, we sometimes find that we have to stop and re-read a
sentence in order to resolve an ambiguity.  Curiously, it seems possible
to combine *unambiguous* words to create an ambiguous sentences:

.. ex::
  .. ex:: Fighting animals could be dangerous.
  .. ex:: Visiting relatives can be tiresome.

Perhaps another kind of syntactic variation, word order, is easier to
understand.  We know that the two sentences `Kim likes Sandy`:lx: and
`Sandy likes Kim`:lx: have different meanings, and that `likes Sandy
Kim`:lx: is simply ungrammatical.  Similarly, we know that the
following two sentences are equivalent:

.. ex::
  .. ex:: The farmer *loaded* the cart with sand
  .. ex:: The farmer *loaded* sand into the cart

However, consider the semantically similar verbs `filled`:lx: and `dumped`:lx:.
Now the word order cannot be altered (ungrammatical sentences are
prefixed with an asterisk.)

.. ex::
  .. ex:: The farmer *filled* the cart with sand
  .. ex:: \*The farmer *filled* sand into the cart
  .. ex:: \*The farmer *dumped* the cart with sand
  .. ex:: The farmer *dumped* sand into the cart

A further curious fact is that we are able to access the meaning of
sentences we have not encountered.  It is not difficult to concoct an
entirely novel sentence, one that has probably never been used before
in the history of the language, and yet all speakers of the language
will agree about its meaning.  In fact, the set of possible sentences would
seem to be infinite, given that there is no upper bound on length.
Consider the following passage from a children's story, containing a
rather impressive sentence:

   You can imagine Piglet's joy when at last the ship came in sight of
   him. In after-years he liked to think that he had been in Very
   Great Danger during the Terrible Flood, but the only danger he had
   really been in was the last half-hour of his imprisonment, when
   Owl, who had just flown up, sat on a branch of his tree to comfort
   him, and told him a very long story about an aunt who had once laid
   a seagull's egg by mistake, and the story went on and on, rather
   like this sentence, until Piglet who was listening out of his
   window without much hope, went to sleep quietly and naturally,
   slipping slowly out of the window towards the water until he was
   only hanging on by his toes, at which moment, luckily, a sudden
   loud squawk from Owl, which was really part of the story, being
   what his aunt said, woke the Piglet up and just gave him time to
   jerk himself back into safety and say, "How interesting, and did
   she?"  when -- well, you can imagine his joy when at last he saw
   the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear)
   coming over the sea to rescue him...  (from A.A. Milne *In which
   Piglet is Entirely Surrounded by Water*)

Our ability to produce and understand entirely new sentences, of
arbitrary length, demonstrates that the set of well-formed sentences in
English (and any other language) is infinite.

This chapter presents grammars and parsing, as the formal and
computational methods for investigating and modelling the linguistic
phenomena we have been touching on (or tripping over).
As we shall see, patterns of well-formedness and ill-formedness in a
sequence of words can be understood with respect to the internal
:dt:`phrase structure` of the sentences.  We can develop formal
models of these structures using grammars and parsers.
As before, the motivation is natural language *understanding*.  How
much more of the meaning of a text can we access when we can reliably
recognize the linguistic structures it contains?  Having read in a
text, can a program 'understand' it enough to be able to answer simple
questions about "what happened?" or "who did what to whom?"  Also as
before, we will develop simple programs to process annotated corpora
and perform useful tasks.

-------------------------
What's the Use of Syntax?
-------------------------

In previous chapters, we have focussed mainly on words: how to
identify them, how to analyse their morphology, and how to assign them
to classes via part-of-speech tags. We have also seen that we can
identify recurring sequences of words in the guise of
n-grams. Nevertheless, there seem to be linguistic regularities which
cannot readily be described in terms of n-grams. In this section, we
will look a little more deeply into the reasons why it is useful to
have some kind of syntactic representation of sentences. In
particular, we will see that there are systematic aspects of meaning
which are much easier to capture once we have established a level of
syntactic structure.


Syntactic Ambiguity
-------------------

We have seen that sentences can be ambiguous.  If we overheard someone
say :lx:`I went to the bank`, we wouldn't know whether it was
a river bank or a financial institution.  This ambiguity concerns
the meaning of the word :lx:`bank`, and is a kind of :dt:`lexical
ambiguity`.

However, other kinds of ambiguity cannot straightforwardly be
explained in terms of ambiguity of specific words. We can construct
simple examples of syntactic ambiguity involving coordinating
conjunctions like `and`:lx: and `or`:lx:. Consider for example the
following sentence:

.. ex:: Kim left or Dana arrived and everyone cheered.

It should be obvious to you that there are two distinct
interpretations of this sentence. How should we account for the
difference? If you are familiar with propositional logic, you will not
be surprised at the idea of using brackets to represent semantic
structure:

.. ex::
  .. ex::  Kim arrived or (Dana left and everyone cheered)
  .. ex::  (Kim arrived or Dana left) and everyone cheered

We can describe this ambiguity in terms of the semantic `scope`:dt: of
`or`:lx: and `and`:lx:\ : in the
reading represented by (6a), the operator `or`:lx:  takes
the conjoined sentence `Dana arrived and everyone cheered`:lx: as one
of its arguments, and therefore is said to have wider scope than
`and`:lx:. Conversely, in (6b), the operator `and`:lx: has wider scope than
`or`:lx:.
One convenient way of representing this scope difference at a structural
level is by means of a `tree diagram`:dt:. 

.. ex::
  .. ex::
    .. tree:: (S <S Kim arrived>
                 (conj or)
                 (S <S Dana left> (conj and) <S everyone cheered>))
  .. ex::
    .. tree:: (S (S <S Kim arrived> (conj or) <S Dana left>)
                 (conj and)
                 <S everyone cheered>)

Note that linguistic trees grow upside down: the node labeled `S`:gc:
is the `root`:dt: of the tree, while the `leaves`:dt: of the tree are
labeled with the words.

In NLTK-Lite, you can easily produce trees like this yourself with the
following commands:
 
.. doctest-ignore:
    >>> from nltk_lite.parse import bracket_parse
    >>> sent = '(S (S Kim arrived) (conj or) (S Dana left))'
    >>> tree = bracket_parse(sent)
    >>> tree.draw()

Conveniently, the resulting tree object supports Python's standard
array operations for accessing its children:

    >>> tree[0]
    (S: 'Kim' 'arrived')

A second example of scope ambiguity involves adjectives: 

.. ex:: old men and women

Does `old`:lx: have wider scope than `and`:lx:, or is it the other way
round? In fact, both interpretations are possible. As an exercise, you can try
modifying the code above to produce two trees, one for each scope reading. 

For our third illustration of ambiguity, we look at
prepositional phrases.
Consider a sentence like: :lx:`I saw the man with a telescope`.  Who
has the telescope?  To clarify what is going on here, consider the
following pair of sentences:

.. ex::
  .. ex:: The policeman saw a burglar *with a gun*.
         (not some other burglar)
  .. ex:: The policeman saw a burglar *with a telescope*.
         (not with his naked eye)

In both cases, there is a prepositional phrase introduced by
:lx:`with`.  In the first case this phrase modifies the noun
:lx:`burglar`, and in the second case it modifies the verb :lx:`saw`.
We could again think of this in terms of scope: does the prepositional
phrase (`PP`:gc:) just have scope over the `NP`:gc:
`a burglar`:lx:, or does it have scope over
the whole verb phrase? Again, we can represent the difference in terms
of tree structure:

.. ex::
  .. ex::
    .. tree:: (S (NP the policeman)
                 (VP (V saw)
                     (NP (NP the burglar)
                         (PP with a gun))))
  .. ex::
    .. tree:: (S (NP the policeman)
                 (VP (V saw)
                     (NP the burglar)
                     (PP with a telescope)))

We can generate these trees in Python as follows:

    >>> sent1 = '(S (NP the policeman) (VP (V saw) (NP (NP the burglar) (PP with a gun))))'
    >>> sent2 = '(S (NP the policeman) (VP (V saw) (NP the burglar) (PP with a telescope)))'
    >>> tree1 = bracket_parse(sent1)
    >>> tree2 = bracket_parse(sent2)
   
We can see that they are trees over the same sequence of words (that
is, the two trees have the same `leaves`:dt:):

    >>> tree1.leaves() == tree2.leaves
    True

On the other hand, they have different `heights`:dt: (given by the
number of nodes in the longest branch of the tree, starting at `S`:gc:
and descending to the words):

    >>> tree1.height() == tree2.height()
    False

In general, how can we determine whether a prepositional phrase
modifies the preceding noun or verb? This problem is often described
with the label `PP attachment`:dt:.  The :dt:`Prepositional Phrase
Attachment Corpus`, included with NLTK-Lite as `ppattach`, facilitates
systematic study of this question.  The corpus is derived from the
IBM-Lancaster Treebank of Computer Manuals and from the University of
Pennsylvania WSJ Treebank, and distills out only the essential
information about `PP`:gc: attachment. Consider for example the
following sentence from the WSJ:

.. ex:: Four of the five surviving workers have asbestos-related diseases,
      including three with recently diagnosed cancer.

The corresponding line in the  `ppattach` corpus is this::

   16 including three with cancer N

That is, it includes an identifier for the original sentence, the
head of the relevant verb phrase (i.e., `including`:lx:), the head of
the verb's `NP`:gc: object (`three`:lx:), the preposition
(`with`:lx:), and the head noun within the prepositional phrase
(`cancer`:lx:). Finally, it contains an 'attachment' feature (``N`` or
``V``) to indicate whether the prepositional phrase attaches to
(modifies) the noun phrase or the verb phrase. 
Here are some further examples::

  47830 allow visits between families N
  47830 allow visits on peninsula V
  42457 acquired interest in firm N
  42457 acquired interest in 1986 V

The attachments in the above examples
can also be made explicit by using phrase groupings in the following manner:

  | allow (NP visits (PP between families))
  | allow (NP visits) (PP on peninsula)
  | acquired (NP interest (PP in firm))
  | acquired (NP interest) (PP in 1986)

Observe in each case that the argument of the verb is either a single
complex expression ``(visits (between families))`` or a pair of
simpler expressions ``(visits) (on peninsula)``.
We can access this corpus from NLTK-Lite as follows:

    >>> from nltk_lite.corpora import ppattach, extract
    >>> from pprint import pprint
    >>> item = extract(16, ppattach.dictionary('devset'))
    >>> pprint(item)
    {'attachment': 'N',
     'noun1': 'three',
     'noun2': 'cancer',
     'prep': 'with',
     'sent': '16',
     'verb': 'including'}

If we go back to our first examples of `PP`:gc: attachment ambiguity,
it appears as though it is the `PP`:gc: itself (e.g., `with a gun`:lx:
versus `with a telescope`:lx:) that determines the attachment. However,
we can use this corpus to find examples where other factors come in to play.
The following program uses ``MinimalSet`` to find pairs of entries in the
corpus which have different attachments based on the *verb* only.

    >>> from nltk_lite.utilities import MinimalSet
    >>> ms = MinimalSet()
    >>> for entry in ppattach.dictionary('training'):
    ...     target  = entry['attachment']
    ...     context = (entry['noun1'], entry['prep'], entry['noun2'])
    ...     display = (target, entry['verb'])
    ...     ms.add(context, target, display)
    >>> for context in ms.contexts():
    ...     print context, ms.display_all(context)

Here is one of the pairs found by the program.

    | received (NP offer) (PP from group)
    | rejected (NP offer (PP from group))

This finding gives us clues to a structural difference: the verb
:lx:`receive` usually comes with two following arguments; we receive
something *from* someone.  In contrast, the verb :lx:`reject` only
needs a single following argument; we can reject something without
needing to say where it originated from. We expect that if you look at
the data, you will be able to come up with further ideas about the
factors that influence `PP`:gc: attachment.

Constituency
------------

We claimed earlier that one of the motivations for building syntactic
structure was to help make explicit how a sentence says "who did what
to whom". Let's just focus for a while on the "who" part of this
story: in other words, how can syntax tell us what the subject of a
sentence is? At first, you might think this task is rather simple
|mdash| so simple indeed that we don't need to bother with syntax. In
a sentence such as

.. ex:: The fierce dog bit the man.

we know that it is the dog that is doing the biting. So we could
say that the noun phrase immediately preceding the verb is the
subject of the sentence. And we might try to make this more explicit
in terms of sequences part-of-speech tags.  Let's try to come up with a simple
definition of `noun phrase`:gc:; we might start off with something
like this::

      DT JJ* NN

We're using regular expression notation here in the form of
`JJ*`:gc: to indicate a sequence of zero or more `JJ`:gc:\s. So this
is intended to say that a noun phrase can consist of a
determiner, possibly followed by some adjectives, followed by a
noun. Then we can go on to say that if we can find a sequence of
tagged words like this that precedes a word tagged as a verb, then
we've identified the subject. But now think about this sentence:

.. ex:: The child with a fierce dog bit the man.

This time, it's the child that is doing the biting. But the tag
sequence preceding the verb is::

  DT NN IN DT JJ NN

and our previous attempt at identifying the subject would have
incorrectly come up
with `the fierce dog`:lx: as the subject.

So our next hypothesis would have to be a bit more complex. For
example, we might say that the subject can be identified as any string
matching the following pattern before the verb::

  DT JJ* NN (IN DT JJ* NN)*

In other words, we need to find a noun phrase followed by zero or more
sequences consisting of a preposition followed by a noun phrase. Now
there are two unpleasant aspects to this proposed solution. The first
is aesthetic: we are forced into repeating the sequence of tags (`DT
JJ* NN`:gc:) that constituted our initial notion of noun phrase, and
our initial notion was in any case a drastic simplification. More
worrying, this approach still doesn't work! For consider the following
example:

.. ex:: The seagull that attacked the child with the fierce dog bit the man.

This time the seagull is the culprit, but it won't be detected as subject by our
attempt to match sequences of tags. So it seems that we need a
richer account of how words are *grouped* together into patterns, and
a way of referring to these groupings at different points in the
sentence structure. This idea of grouping is often called
syntactic `constituency`:dt:.

As we have just seen, a well-formed sentence of a language is more
than an arbitrary sequence of words from the language.  Certain kinds
of words usually go together.  For instance, determiners like `the`:lx:
are typically followed by adjectives or nouns, but not by verbs.
Groups of words form intermediate structures called phrases or
:dt:`constituents`.  These constituents can be identified using
standard syntactic tests, such as substitution, movement and
coordination.  For example, if a sequence of words can be replaced
with a pronoun, then that sequence is likely to be a constituent.
According to this test, we can infer that the italicised string in the
following example is a constituent, since it can be replaced by
`they`:lx:\:

.. ex::
  .. ex:: *Ordinary daily multivitamin and mineral supplements* could 
         help adults with diabetes fight off some minor infections.
  .. ex:: *They* could help adults with diabetes fight off some minor
         infections.


In order to identify whether a phrase is the subject of a sentence, we
can use the construction called `Subject-Auxiliary Inversion`:dt: in
English. This construction allows us to form so-called Yes-No
Questions. That is, corresponding to the statement in (14a), we have
the question in (14b):

.. ex::
  .. ex:: All the cakes have been eaten.
  .. ex:: Have *all the cakes* been eaten?

Roughly speaking, if a sentence already contains an auxiliary verb,
such as `has`:lx: 
in (14a), then we can turn it into a Yes-No Question by moving the
auxiliary verb 'over' the subject noun phrase to the front of the
sentence. If there is no auxiliary in the statement, then we insert
the appropriate form of `do`:lx: as the fronted auxiliary and replace
the tensed main verb by its base form:

.. ex::
  .. ex:: The fierce dog bit the man.
  .. ex:: Did *the fierce dog* bite the man?

As we would hope, this test also confirms our earlier claim about the
subject constituent of (12):

.. ex:: Did *the seagull that attacked the child with the fierce dog* bite
       the man?

To sum up then, we have seen that the notion of constituent brings a
number of benefits. By having a constituent labeled `noun phrase`:gc:,
we can provide a unified statement of the classes of word that
constitute that phrase, and reuse this statement in describing noun
phrases wherever they occur in the sentence. Second, we can use the
notion of a noun phrase in defining the subject of sentence, which in
turn is a crucial ingredient in determining the "who does what to
whom" aspect of meaning.

More on NLTK's Trees
--------------------

We have been discussing structural differences between sentences, and
we have been probing these structures by substituting words and
phrases.  We have informally shown how sentence structures can be
represented using syntactic trees, and we will now look at these
structures in a bit more detail.

Consider the following example:

.. ex::
  .. tree:: (S (NP Lee) (VP (V saw) (NP the dog)))

**Terminology:**
The tree is a set of connected nodes, each of which is labeled with a
category.  It also common to use a family metaphor to talk about the
relationships of nodes in a tree: for example, `S`:gc: is the
`parent`:dt: of `VP`:gc:; conversely `VP`:gc: is a `daughter`:dt: (or
`child`:dt:) of `S`:gc:.  Also, since `NP`:gc: and `VP`:gc: are both
daughters of `S`:gc:, they are also `sisters`:dt:. 

Although it is helpful to represent trees in a graphical format, for
computational purposes we usually need a more text-oriented
representation. One standard method is to use a combination of bracket
and labels to indicate the structure, as shown here:

.. ex::
  .. parsed-literal::

      (S 
         (NP  'Lee')
         (VP 
               (V 'saw')
               (NP 
                     (Det 'the')
                     (N  'dog'))))

The conventions for displaying trees in NLTK are similar::

  (S: (NP: 'Lee') (VP: (V: 'saw') (NP: 'the' 'dog')))

In such trees, the node value is a string containing the tree's
constituent type (e.g., `NP`:gc: or `VP`:gc:), while the children encode
the hierarchical contents of the tree [#]_.

.. [#] Although the ``Tree`` class is usually used for encoding
   syntax trees, it can be used to encode *any* homogeneous hierarchical
   structure that spans a text (such as morphological structure or
   discourse structure).  In the general case, leaves and node values do
   not have to be strings.

``Trees`` are created with the ``Tree constructor``, which takes a
node value and a list of zero or more children.  Here's an example of
a simple NLTK-Lite tree with a single child node, where the latter is
a token:

    >>> from nltk_lite.parse.tree import Tree
    >>> tree1 = Tree('NP', ['John'])
    >>> tree1
    (NP: 'John')

Here is an example with two children:

    >>> tree2 = Tree('NP', ['the', 'man'])
    >>> tree2
    (NP: 'the' 'man')

Finally, here is a more complex example, where one of the
children is itself a tree:

    >>> tree3 = Tree('VP', ['saw', tree2])
    >>> tree3
    (VP: 'saw' (NP: 'the' 'man'))

A tree's root node value is accessed with the ``node`` property, and
its leaves are accessed with the ``leaves()`` method:

    >>> tree3.node
    'VP'
    >>> tree3.leaves()
    ['saw', 'the', 'man']

One common way of defining the subject of a sentence `S`:gc: in
English is as *the noun phrase that is the daughter of `S`:gc: and the
sister of `VP`:gc:*. Although we do not have an explicit method for
accessing subjects in this sense, in practice we can get something
similar by using `tree positions`:dt:. Consider ``tree4`` defined as
follows:

    >>> tree4 = Tree('S', [tree1, tree3])
    >>> tree4
    (S: (NP: 'John') (VP: 'saw' (NP: 'the' 'man')))

As we have already noted, the daughters of the root node are specified
as a list, and as we already know, we can access items in a list using
indexing. In a similar way, we can use indexing to access the
daughters of a tree:

    >>> tree4[0]
    (NP: 'John')
    >>> tree4[1]
    (VP: 'saw' (NP: 'the' 'man'))

Since the value of ``tree4[1]`` is itself a tree, we can index into that as well:

    >>> tree4[1][0]
    'saw'
    >>> tree4[1][1]
    (NP: 'the' 'man')

The printed representation for complex trees can be difficult to read.
In these cases, the ``draw`` method can be very useful. 

.. doctest-ignore::
    >>> tree3.draw()

This method opens a new window, containing a graphical representation
of the tree:

.. image:: ../images/parse_draw.png
   :scale: 70

The tree display window allows you to zoom in and out;
to collapse and expand subtrees; and to print the graphical
representation to a postscript file. 

To compare multiple trees in a single window, use the ``draw_trees()``
method, from the ``nltk_lite.draw.tree`` module:

.. doctest-ignore::
    >>> from nltk_lite.draw.tree import draw_trees
    >>> draw_trees(tree1, tree2, tree3)

The ``Tree`` class implements a number of other useful methods.  See
the ``Tree`` reference documentation for more information about these
methods.

The ``nltk_lite.corpora`` module defines the ``treebank`` corpus,
which contains a collection of hand-annotated parse trees for English
text, derived from the Penn Treebank.

    >>> from nltk_lite.corpora import treebank, extract
    >>> print extract(0, treebank.parsed())
    (S:
      (NP-SBJ:
        (NP: (NNP: 'Pierre') (NNP: 'Vinken'))
        (,: ',')
        (ADJP: (NP: (CD: '61') (NNS: 'years')) (JJ: 'old'))
        (,: ','))
      (VP:
        (MD: 'will')
        (VP:
          (VB: 'join')
          (NP: (DT: 'the') (NN: 'board'))
          (PP-CLR:
            (IN: 'as')
            (NP: (DT: 'a') (JJ: 'nonexecutive') (NN: 'director')))
          (NP-TMP: (NNP: 'Nov.') (CD: '29'))))
      (.: '.'))


Exercises
---------

#. Using tree positions, list the subjects of the first 100
   sentences in the Penn treebank; to make the results easier to view,
   limit the extracted subjects to subtrees whose height is 2.

[more to be written]

--------------------
Context Free Grammar
--------------------

As we have seen, languages are infinite |mdash| there is no principled
upper-bound on the length of a sentence.  Nevertheless, we would like
to write programs that can process well-formed sentences.  It turns
out that we can characterize what we mean by well-formedness using a
grammar.  The way that finite grammars are able to describe an
infinite set uses `recursion`:dt:.  (We already came across this idea
when we looked at regular expressions: the finite expression ``a+`` is
able to describe the infinite set ``{a, aa, aaa, aaaa, ...}``).  Apart
from their compactness, grammars usually capture important structural
and distributional properties of the language, and can be used to map
between sequences of words and abstract representations of meaning.
Even if we were to impose an upper bound on sentence length to ensure
the language was finite, we would probably still want to come up with
a compact representation in the form of a grammar.

A `grammar`:dt: is a formal system which specifies which sequences of
words are well-formed in the language, and which provides one or more
phrase structures for well-formed sequences.  We will be looking at
:dt:`context-free grammar` (CFG), which is a collection of
`productions`:dt: of the form `S`:gc: |rarr| `NP VP`:gc:.  This says
that a constituent `S`:gc: can consist of sub-constituents `NP`:gc:
and `VP`:gc:. Similarly, the production `VB`:gc: |rarr| ``'help'``
means that the constituent `VB`:gc: can consist of the string
`help`:lx:.  For a phrase structure tree to be well-formed relative to
a grammar, each non-terminal node and its children must correspond to
a production in the grammar.


A Simple Grammar
----------------

Let's start off by looking at a simple context-free grammar:

_`G1`

 | S |rarr| NP VP
 | NP |rarr| Det N PP
 | NP |rarr| Det N
 | VP |rarr| V NP PP
 | VP |rarr| V NP
 | VP |rarr| V
 | PP |rarr| P NP
 |
 | Det |rarr| 'the'
 | Det |rarr| 'a'
 | N |rarr| 'man' | 'park' | 'dog' | 'telescope'
 | V |rarr| 'saw' | 'walked'
 | P |rarr| 'in' | 'with'

This grammar contains productions involving various syntactic categories,
as laid out in the following table:

.. table:: Syntactic Categories

  ======    ====================    =====================
  Symbol    Meaning                 Example
  ======    ====================    =====================
  S         sentence                *the man walked*
  NP        noun phrase             *a dog*
  VP        verb phrase             *saw a park*
  PP        prepositional phrase    *with a telescope*
  ..        ..                      ..
  Det       determiner              ..
  N         noun                    ..
  V         verb                    ..
  P         preposition             ..
  ======    ====================    =====================

**Terminology:**
The grammar consists of productions, where each production involves a
single `non-terminal`:dt: (e.g. `S`:gc:, `NP`:gc:), an arrow, and one
or more non-terminals and `terminals`:dt: (e.g. `walked`:lx:).
The productions are often divided into two main groups.
The `grammatical productions`:dt: are those without a terminal on
the right-hand side.  The `lexical productions`:dt: are those having
a terminal on the right-hand side.
A special case of non-terminals are the `pre-terminals`:dt:, which
appear on the left-hand side of lexical productions.

In order to get started with developing simple grammars of your own, you
will probably find it convenient to play with the Recursive Descent
Parsing Demo, which is invoked as follows:

.. doctest-ignore:
  >>> from nltk_lite.draw import rdparser
  >>> rdparser.demo()

The demo opens a window which displays a list of grammar rules in the
lefthand pane and the current parse diagram in the central pane:

.. image:: ../images/parse_rdparsewindow.png
   :scale: 70

The demo come with `G1`_ already loaded. We will discuss the parsing
algorithm in greater detail below, but for the time being you can get
a rough idea of how it works by using the *autostep* button.

If we parse the string `The dog saw a man in the park` using
`G1`_, we end up with two trees, as shown 
below:


.. table:: Structural Ambiguity

  =================================  =================================
  VP modification                    NP modification
  =================================  =================================
  |parse_tree2|                      |parse_tree3|
  =================================  =================================

.. |parse_tree2| image:: ../images/parse_tree2.png
.. |parse_tree3| image:: ../images/parse_tree3.png

Since our grammar assigns two distinct structures, the sentence is
said to be :dt:`structurally ambiguous`.  The ambiguity in question is called
a :dt:`PP attachment ambiguity`, as we saw earlier in this chapter.
As you may recall, it is an ambiguity about attachment since the
`PP`:gc: `in the park`:lx: needs to be attached to one of two places
in the tree: either as a daughter of `VP`:gc: or else as a daughter of
`NP`:gc:.

As we noted earlier, there is also a difference in interpretation:
where the `PP`:gc: is attached to `VP`:gc:, the intended interpretation is
that the event of seeing took place in the park, while if the `PP`:gc:
is attached to `NP`:gc:, being in the park is a property of the `NP`:gc:
referent; that is, the man was in the park, but the agent of the
seeing |mdash| the dog |mdash| might have been somewhere else (e.g.,
sitting on the balcony of an apartment overlooking the park).  As we
will see, dealing with ambiguity is a key challenge in parsing.

Exercises
---------

#. In the Recursive Descent Parser Demo, experiment with changing the
   sentence to be parsed by selecting *Edit Text* in the *Edit* menu.

#. Can `G1`_ be used to describe sentences which are more than 20
   words in length?

#. You can modify the grammar in the Recursive Descent Parser Demo
   by selecting *Edit Grammar*  in the *Edit* menu. Change
   the first expansion rule, namely ``NP -> Det N PP``, to ``NP -> NP
   PP``. Using the *Step* button, try to build a parse tree. What happens?



Recursion
---------

Observe that sentences can be nested within sentences, with no limit
to the depth:

.. ex::
  .. ex:: Jodie won the 100m freestyle
  .. ex:: 'The Age' reported that Jodie won the 100m freestyle
  .. ex:: Sandy said 'The Age' reported that Jodie won the 100m freestyle
  .. ex::  I think Sandy said 'The Age' reported that Jodie won the 100m freestyle

This nesting is explained in terms of :dt:`recursion`. A grammar is
said to be :dt:`recursive` if a category occurring on the lefthand
side of a production (such as `S`:gc: in this case) also appears on
the righthand side of a production. If this dual occurrence takes
place in *one and the same production*, then we have :dt:`direct
recursion`; otherwise we have :dt:`indirect recursion`. There is no
recursion in `G1`_. However, `G2`_ below illustrates both kinds of
recursive rule:

_`G2`
 | S  |rarr| NP VP
 | NP |rarr| Det Nom
 | NP |rarr| Det Nom PP
 | Nom |rarr| Adj Nom
 | Nom |rarr| N
 | VP |rarr| V NP PP
 | VP |rarr| V NP
 | VP |rarr| V S
 | VP |rarr| V
 | PP |rarr| P NP
 |
 | Det |rarr| 'the'
 | Det |rarr| 'a'
 | N |rarr| 'man' | 'woman' | 'park' | 'dog' | 'lead' | 'telescope' | 'butterfly'
 | Adj  |rarr| 'fierce' | 'black' |  'big' | 'European'
 | V |rarr| 'saw' | 'chased' | 'barked'  | 'disappeared' | 'said' | 'reported'' 
 | Aux |rarr| 'can' | 'will'
 | P |rarr| 'in' | 'with' 

That is, the production `Nom`:gc: |rarr| `Adj Nom`:gc: (where
`Nom`:gc: is the category of nominals) involves direct
recursion on the category `Nom`:gc:\, whereas indirect recursion on `S`:gc:
arises from the combination of two productions, namely `S`:gc: |rarr|
`NP VP`:gc: and `VP`:gc: |rarr| `V S`:gc:.  

To illustrate recursion in this grammar, we show first of all a tree
involving nested nominal phrases:

.. ex::
  .. tree::
   (S (NP (Det a) (Nom (Adj fierce)(Nom (Adj black) (N dog))))
       (VP (V chased)
          (NP (Det the) (Nom (Adj big)(Nom (Adj European) (N butterfly))))))

Next, we show how we can embed one `S`:gc:  constituent into another:

.. ex::
  .. tree::
    (S (NP (Det the) (N man))
       (VP (V said)
           (S (NP (Det the) (N woman))
              (VP (V reported)
                  (S (NP (Det the) (N dog))
                           (VP (V barked)))))))
   

If you did the exercises for the last section, you will have noticed
that the Recursive Descent Parser fails to deal properly with the
following rule:

.. ex::  
  .. parsed-literal:: 

   NP |rarr| NP PP

From a linguistic point of view, this rule is perfectly respectable,
and will allow us to derive trees like this:

.. ex::
  .. tree::
    (S (NP 
           (NP 
               (NP (Det the) (N man))
               (PP (P with) (NP  (Det a) (N dog))))
            (PP (P in  (NP  (Det the) (N park)))))
         (VP (V disappeared)))

More schematically, the trees will be of the following shape:

.. _leftrec:
.. ex::
  .. tree::
    <NP <NP <NP <NP Det N> PP> PP> PP>

leftrec_ is an example of a `left recursive`:dt: structure. Such
structures seem to occur rather frequently in analyses of English, and
the failure of Recursive Descent Parsers to deal adequately with left
recursion means that we will need to find alternative approaches, as
we will discuss later in this chapter.

Heads, Complements and Modifiers
--------------------------------

The grammar `G2`_ correctly generates examples like these,
corresponding to the four productions with `VP`:gc: on the lefthand side:

#. The woman gave the telescope to the dog.
#. The woman saw a man.
#. A man said that the woman disappeared.
#. The dog barked.

That is, `gave`:lx: can occur with a following `NP`:gc: and `PP`:gc:; 
`saw`:lx: can occur with a following `NP`:gc:; 
`said`:lx: can occur with a following `S`:gc:; 
and `barked`:lx: can occur with no following phrase.
In these cases, `NP`:gc:, `PP`:gc: and `S`:gc: are called :dt:`complements`
of the respective verbs, and the verbs themselves are called
:dt:`heads` of the verb phrase.

However, there are fairly strong constraints on what verbs can occur
with what complements. Thus, we would like our grammars to mark the
following examples as ungrammatical [#]_:
 
#. \*The woman disappeared the telescope to the dog.
#. \*The dog barked a man.
#. \*A man gave that the woman disappeared.
#. \*A man said.

.. [#] It should be borne in mind that it is possible to create
   examples which involve 'non-standard' but interpretable
   combinations of verbs and complements. Thus, we can, at a stretch,
   interpret *the man disappeared the dog* as meaning that the man
   made the dog disappear. We will ignore such examples here.

How can we ensure that our grammar correctly excludes the
ungrammatical examples above?  We need some way of constraining
grammar productions which expand `VP`:gc: so that verbs *only* cooccur
with their correct complements. We do this by dividing the class of
verbs into \'subcategories\', each of which is associated with a
different set of complements. For example, :dt:`transitive verbs` such
as `saw`:lx:, `kissed`:lx: and `hit`:lx: require a following `NP`:gc:
object complement. Let's introduce a new label for such verbs, namely
`V_tr`:gc:, and use it in the following productions:

| VP |rarr| V_tr NP
| V_tr |rarr| 'saw' | 'kissed' | 'hit'

Now `*the dog barked the man` is excluded since we haven't listed
`barked` as a `V_tr`, but `the woman saw a man` is still allowed.
The following table provides more examples of labels for verb subcategories.

======    ====================    ========================
Verb Subcategories
----------------------------------------------------------
Symbol    Meaning                 Example
======    ====================    ========================
V_itr     intransitive verb       *barked*
V_tr      transitive verb         *saw a man*
V_dat     dative verb             *gave a dog to a man*
V_sent    sentential verb         *said that a dog barked*
======    ====================    ========================

The revised grammar for `VP`:gc: will now look like this:

| VP |rarr| V_dat NP PP
| VP |rarr| V_tr NP
| VP |rarr| V_sent S
| VP |rarr| V_itr 
|
| V_dat |rarr| 'gave' | 'donated' | 'presented'
| V_tr |rarr| 'saw' | 'kissed' | 'hit' | 'sang'
| V_sent |rarr| 'said' | 'knew' | 'alleged'
| V_itr |rarr| 'barked' | 'disappeared' | 'elapsed' | 'sang'

Notice that with this approach, a given lexical item can belong to more
than one subcategory. For example, `sang`:lx: can occur both with and
without a following `NP`:gc: complement.


Lexical Acquisition
-------------------

We have seen increasingly detailed grammars, e.g., identifying
different kinds of verbs.  How are we to acquire this information in a
scalable way?  One method is to return to the chunking methods.  For
example, we saw in the `Chunking <chunk.html>`__ chapter that it is possible to collapse
chunks down to the chunk label, thus::

  gave NP
  gave up NP in NP
  gave NP up
  gave NP help
  gave NP to NP

We can use this as raw material to guide us as we manually construct
more grammar productions.

Context Free Grammars in NLTK-Lite
----------------------------------

Context free grammars are encoded by the ``Grammar`` class.  The easiest
way to construct a grammar object is from the standard string representation
of grammars:

    >>> from nltk_lite.parse import cfg
    >>> grammar2 = cfg.parse_grammar('''
    ...   S -> NP VP
    ...   VP -> V NP | V NP PP
    ...   V -> "saw" | "ate"
    ...   NP -> "John" | "Mary" | "Bob" | Det N | Det N PP
    ...   Det -> "a" | "an" | "the" | "my"
    ...   N -> "dog" | "cat" | "cookie"
    ...   PP -> P NP
    ...   P -> "on" | "by" | "with"
    ...   ''')
  
    >>> from nltk_lite import tokenize, parse
    >>> sent = list(tokenize.whitespace("Mary saw Bob"))
    >>> rd_parser = parse.RecursiveDescent(grammar2)
    >>> for p in rd_parser.get_parse_list(sent):
    ...      print p
    (S: (NP: 'Mary') (VP: (V: 'saw') (NP: 'Bob')))
  

Exercises
---------

1. a) Encode any of the trees presented in this chapter as a labeled
      bracketing and use the ``nltk_lite.parse`` module's
      ``bracket_parse()`` method to check that it is well-formed. Now
      use the ``draw()`` method from the ``Tree`` class in module
      ``nltk_lite.parse.tree`` to display the tree.

   b) Extend `G2`_ with productions which expand prepositions as
      intransitive, transitive and requiring a `PP`:gc:
      complement. Based on these productions, use the method of the
      preceding exercise to draw
      a tree for the sentence `Lee ran away home`:lx:\.

   c) As in (a) above, draw a tree for `The woman saw a man last Thursday`:lx:.

2. Pick some common verbs.

  a) Write a program to find those verbs in the PP Attachment Corpus
     included with NLTK-Lite.  Find any cases where the same verb
     exhibits two different attachments, but where the first noun,
     or second noun, or preposition, stay unchanged (as we saw in
     the PP Attachment Corpus example data above).

  b) Devise CFG grammar productions to cover some of these cases.

3. **Lexical Acquisition:**
   Identify some English verbs that are near-synonyms, such as the
   :lx:`dumped/filled/loaded` example from earlier in this chapter.
   Use the chunking method to study the complementation patterns of
   these verbs.  Create a grammar to cover these cases.  Can the verbs
   be freely substituted for each other, or are their constraints?
   Discuss your findings.


-------
Parsing
-------

A :dt:`parser` is a computational system which processes input sentences
according to the productions of a grammar, and builds one or more
constituent structures which conform to the grammar. While a
grammar is a declarative specification of well-formedness,  a
parser is a procedural interpretation of the grammar.  We can think of
the parser as searching through the space of possible trees licensed
by a grammar, to find one that has the required sentence along its fringe.
Following on from our description of context free grammars, we will now  describe some
simple parsers that work with them.

Parsing is important in linguistics and natural language processing
for a variety of reasons.  A parser permits a grammar to be evaluated
against a potentially large collection of test sentences, helping the
linguist to identify shortcomings in their analysis.  A parser can
also be used as a model of psycholinguistic processing, with the goal
of explaining the processing difficulties that humans have with
certain syntactic constructions (e.g., the so-called 'garden path'
sentences).  There are many NL applications which involve parsing at
some point; for example, we would expect the natural language questions submitted to
a question-answering system to undergo parsing as an initial step.

The Parser Interface 
--------------------

The ``parse`` module defines the ``ParseI`` interface, which in turn defines
the two methods which all parsers should support:


1. The ``parse`` method returns the single best parse for a given
   text.  The text is represented as a list of word tokens.  If no
   parses are found for the given text, then ``parse`` returns
   ``None``.

#. The ``get_parse_list`` method returns a list of the parses for the
   given text.

For example, here is what the recursive descent parser generates for a
simple sentence and grammar:

    >>> from nltk_lite import tokenize
    >>> sent = list(tokenize.whitespace('I saw a man in the park'))
    >>> from nltk_lite import parse
    >>> rd_parser = parse.RecursiveDescent(grammar)
  
    >>> for p in rd_parser.get_parse_list(sent):
    ...     print p
    (S:
      (NP: 'I')
      (VP:
        (V: 'saw')
        (NP:
          (Det: 'a')
          (N: 'man')
          (PP: (P: 'in') (NP: (Det: 'the') (N: 'park'))))))
    (S:
      (NP: 'I')
      (VP:
        (V: 'saw')
        (NP: (Det: 'a') (N: 'man'))
        (PP: (P: 'in') (NP: (Det: 'the') (N: 'park')))))


Recursive Descent Parsing 
-------------------------

The simplest kind of parser interprets the grammar as a specification
of how to break a high-level goal into several lower-level subgoals.
The top-level goal is to find an `S`:gc:.  The `S`:gc: |rarr| `NP VP`:gc:
production permits the parser to replace this goal with two subgoals:
find an `NP`:gc:, then find a `VP`:gc:.  Each of these subgoals can be
replaced in turn by sub-sub-goals, using productions that have `NP`:gc:
and `VP`:gc: on their left-hand side.  Eventually, this expansion
process leads to subgoals such as: find the word `telescope`:lx:.  Such
subgoals can be directly compared against the input string, and
succeed if the next word is matched.  If there is no match the parser
must back up and try a different alternative.

The recursive descent parser builds a parse tree during the above
process.  With the initial goal (find an `S`), the `S` root node
is created.  As the above process recursively expands its goals using
the productions of the grammar, the parse tree is extended downwards
(hence the name *recursive descent*).  We can see this in action using
the parser demonstration ``nltk_lite.draw.rdparser``.  To run this
demonstration, use the following commands:

  >>> from nltk_lite.draw import rdparser
  >>> rdparser.demo()

Six stages of the execution of this parser are shown below:

+----------------------------------------------------------------------------------+
|                        Six Stages of a Recursive Descent Parser                  |
+===========================+==========================+===========================+
| 1. Initial stage          | 2. After two productions | 3. After matching "the"   |
|                           |                          |                           |
| |rdparser1|               | |rdparser2|              | |rdparser3|               |
+---------------------------+--------------------------+---------------------------+
| 4. Failing to match "man" | 5. Completed parse       | 6. Backtracking           |
|                           |                          |                           |
| |rdparser4|               | |rdparser5|              | |rdparser6|               |
+---------------------------+--------------------------+---------------------------+

.. |rdparser1| image:: ../images/rdparser1.png
.. |rdparser2| image:: ../images/rdparser2.png
.. |rdparser3| image:: ../images/rdparser3.png
.. |rdparser4| image:: ../images/rdparser4.png
.. |rdparser5| image:: ../images/rdparser5.png
.. |rdparser6| image:: ../images/rdparser6.png

.. Discussion: choosing which of several possible productions to apply;
   backtracking; termination.

Problems with recursive descent parsing: considers structures and
words that are not attested; backtracking may discard parsed
constituents that need to be rebuilt; for example, backtracking over
`VP`:gc: |rarr| `V NP`:gc: will discard the structures created for the `V`:gc:
and `NP`:gc: non-terminals.  If the parser then proceeds with `VP`:gc:
|rarr| `V NP PP`:gc:, then the structures for the `V`:gc: and `NP`:gc: must be
created again.

Recursive descent parsing is a kind of *top-down parsing*.  These use
the grammar to *predict* what the input will be, before inspecting any
input.  However, since the input is available to the parser all along,
it would be more sensible to consider the input sentence from the very
beginning.  Such an approach is called *bottom-up parsing*, and is the
topic of the next section.

The Recursive Descent Parser in NLTK
------------------------------------

The ``nltk_lite.parse`` module defines ``RecursiveDescent``, a
simple recursive implementation of a top-down parser.  Unlike the
shift-reduce parser, this parser is guaranteed to find all parses for
a sentence.  But because it's a simple recursive top-down parser, it
can enter an infinite loop if the grammar contains a left-recursive
production.

Recursive descent parsers are created from ``Grammars`` by the
``RecursiveDescent`` constructor.

    >>> from nltk_lite.parse import *
    >>> rd_parser = RecursiveDescent(grammar)

The following example shows the trace output generated by
``rd_parser`` on a simple sentence:

    >>> sent = list(tokenize.whitespace('I saw a man'))
    >>> rd_parser.trace()
    >>> rd_parser.get_parse_list(sent)
    [(S: (NP: 'I') (VP: (V: 'saw') (NP: (Det: 'a') (N: 'man'))))]

.. note:: The constructor takes an optional parameter ``trace``.  If
          ``trace`` is greater than zero, then the parser will describe the
          steps that it takes as it parses a text.

Problems with the Recursive Descent Parser
------------------------------------------

(Repeated creation of subtrees; creation of trees that do not have any
relationship to the sentence being parsed; left-recursion problem)

Shift-Reduce Parsing 
--------------------

The simplest kind of bottom-up parsing is known as shift-reduce
parsing.  The parser repeatedly pushes the next input word onto a stack;
this is the `shift`:dt: operation.  If the top *n* items on the stack
match the *n* items on the right-hand side of some production, then
they are all popped off the stack, and the item on the left-hand side
of the production is pushed on the stack.  This replacement of the top
*n* items with a single item is the `reduce`:dt: operation.  The parser
finishes when all the input is consumed and there is only one item
remaining on the stack, a parse tree with an `S`:gc: node as its root.


.. 
 To do: add examples and motivate more - what are we doing with
 bottom up - find little pieces and expand...


.. note::
 Note that the reduce operation may only be applied to the top of the stack.
 Reducing items lower in the stack must be done before later items are pushed onto
 the stack.


The shift-reduce parser builds a parse tree during the above process.
If the top of stack holds the word `dog`:lx: and if the grammar has a
production `N`:gc: |rarr| `dog`:lx: then the reduce operation causes the word
to be replaced with the parse tree for this production.  For
convenience we will represent this tree as ``N(dog)``.  At a later
stage, if the top of the stack holds two items ``Det(the) N(dog)`` and
if the grammar has a production `NP`:gc: |rarr| `Det N`:gc: then the reduce
operation causes these two items to be replaced with ``NP(Det(the),
N(dog))``.  This process continues until a parse tree for the entire
sentence has been constructed.  We can see this in action using the
parser demonstration ``nltk_lite.draw.srparser``.  To run this
demonstration, use the following commands:

  from nltk_lite.draw import srparser
  srparser.demo()

Six stages of the execution of this parser are shown below:

.. 
   To do: use letter identifiers for subfigures.

+----------------------------------------------------+
| Six Stages of a Shift-Reduce Parser                |
+====================================================+
| .. image:: ../images/srparser1.png                 |
|                                                    |
| 1. Initial State                                   |
+----------------------------------------------------+
| .. image:: ../images/srparser2.png                 |
|                                                    |
| 2. After one shift                                 |
+----------------------------------------------------+
| .. image:: ../images/srparser3.png                 |
|                                                    |
| 3. After shift reduce shift                        |
+----------------------------------------------------+
| .. image:: ../images/srparser4.png                 |
|                                                    |
| 4. After recognizing the second NP                 |
+----------------------------------------------------+
| .. image:: ../images/srparser5.png                 |
|                                                    |
| 5. Complex NP                                      |
+----------------------------------------------------+
| .. image:: ../images/srparser6.png                 |
|                                                    |
| 6. Final Step                                      |
+----------------------------------------------------+

A shift-reduce parser may fail to parse the sentence, even though the
sentence is well-formed according to the grammar.  In such cases,
there are no remaining input words to shift, and there is no way to
reduce the remaining items on the stack, as exemplified in the left
example below.  The parser entered this blind alley at an earlier
stage shown in the middle example below, when it reduced instead of
shifted.  This situation is called a `shift-reduce conflict`:dt:.  At
another possible stage of processing shown in the right example below,
the parser must choose between two possible reductions, both matching
the top items on the stack: `V`:gc: |rarr| `V NP PP`:gc: or `NP`:gc: |rarr|
`NP PP`:gc:.  This situation is called a `reduce-reduce conflict`:dt:.

+----------------------------------------------------+
| Conflict in Shift-Reduce Parsing                   |
+====================================================+
| .. image:: ../images/srparser7.png                 |
+----------------------------------------------------+
| .. image:: ../images/srparser8.png                 |
+----------------------------------------------------+
| .. image:: ../images/srparser9.png                 |
+----------------------------------------------------+

.. To do: diagram showing search tree with success and failure.

Shift-reduce parsers may implement policies for resolving such
conflicts.  For example, they may address shift-reduce conflicts by
shifting only when no reductions are possible, and they may address
reduce-reduce conflicts by favouring the reduction operation that removes
the most items from the stack.  No such policies are failsafe however.

The advantages of shift-reduce parsers over recursive descent parsers
is that they only build structure that corresponds to the words in the
input.  Furthermore, they only build each substructure once,
e.g. ``NP(Det(the), N(man))`` is only built and pushed onto the stack
a single time, regardless of whether it will later be used by the `V`:gc:
|rarr| `V NP PP`:gc: reduction or the `NP`:gc: |rarr| `NP PP`:gc: reduction.

The Shift Reduce Parser in NLTK
-------------------------------

The ``nltk_lite.parse`` module defines ``ShiftReduce``, a simple
implementation of a bottom-up shift-reduce parser.
Since this parser does not implement any backtracking, it is not
guaranteed to find a parse for a text, even if one exists.
Furthermore, it will only find at most one parse, even if more
parses exist.

Shift reduce parsers are created from ``Grammars`` by the
``ShiftReduceParse`` constructor.  The constructor takes an optional
parameter ``trace``.  As with the recursive descent parser, this value
specifies how verbosely the parser should describe the steps that it
takes as it parses a text: 

    >>> sr_parse = parse.ShiftReduce(grammar)

The following example shows the trace output generated by
``sr_parser`` on a simple sentence: 

    >>> sent = list(tokenize.whitespace('I saw a man'))
    >>> sr_parse.trace()
    >>> sr_parse.parse(sent)
    Parsing 'I saw a man'
        [ * I saw a man]
      S [ 'I' * saw a man]
      R [ <NP> * saw a man]
      S [ <NP> 'saw' * a man]
      R [ <NP> <V> * a man]
      S [ <NP> <V> 'a' * man]
      R [ <NP> <V> <Det> * man]
      S [ <NP> <V> <Det> 'man' * ]
      R [ <NP> <V> <Det> <N> * ]
      R [ <NP> <V> <NP> * ]
      R [ <NP> <VP> * ]
      R [ <S> * ]
    (S: (NP: 'I') (VP: (V: 'saw') (NP: (Det: 'a') (N: 'man'))))

NLTK also defines a graphical demonstration tool for the
shift reduce parser:

.. doctest-ignore::
    >>> from nltk.draw.srparser import demo
    >>> demo()

Problems with Shift Reduce Parser
---------------------------------

(incomplete, building bottom-up sequences that have no bearing on the grammar)

The Left Corner Parser
----------------------

A left-corner parser is a top-down parser with bottom-up filtering.
Unlike an ordinary recursive descent parser, it does not get trapped
in left-recursive productions.  Suppose `X`:gc: is a non-terminal that
expands to some sequence `Y`:subscript:`1` |cdots| `Y`:subscript:`n`
of terminals and non-terminals.  We call `Y`:subscript:`1` the
`left-corner`:dt: of `X`.

Before starting its work, a left-corner parser preprocesses the
context-free grammar to build a table where each row contains two
cells, the first holding a non-terminal, and the second holding the
collection of possible left-corners of that non-terminal.

Each time a production is considered by the parser, it checks that the
next input word is compatible with at least one of the pre-terminal
categories in the left-corner table.

Exercises
---------

#. **Left-corner parser:** Develop a left-corner parser (inheriting from
   ``ParseI``), based on the recursive descent parser.

#. Compare the performance of the top-down, bottom-up, and left-corner
   parsers using the same grammar and three grammatical test
   sentences. Use ``time.time()`` to log the amount of time each
   parser takes on the same sentence.  Write a function which runs all
   three parsers on all three sentences, and prints a 3-by-3 grid of
   times, as well as row and column totals. Discuss your findings.

------------
More Grammar
------------

Lexical heads other than `V`:gc: can be subcategorized for particular
complements:

**Nouns**
   #. The rumour *that Kim was bald* circulated widely.
   #. \*The picture *that Kim was bald* circulated widely.
**Adjectives**
   #. Lee was afraid *to leave*.
   #. \*Lee was tall *to leave*.

It has also been suggested that 'ordinary' prepositions are
transitive, and that many so-called adverb are in fact intransitive
prepositions. For example, `towards`:lx: requires an `NP`:gc: complement,
while `home`:lx: and `forwards`:lx: forbid them.


.. example:: Lee ran towards the house.
.. example:: \*Lee ran towards.

.. example:: Sammy walked home.
.. example:: \*Sammy walked home the house.

.. example:: Brent stepped one pace forwards.
.. example:: \*Brent stepped one pace forwards the house.


Adopting this approach, we can also analyse certain prepositions as
allowing `PP`:gc: complements:

.. example:: Kim ran away *from the house*.
.. example:: Lee jumped down *into the boat*.

In general, the lexical categories `V`:gc:, `N`:gc:, `A`:gc: and `P`:gc: are
taken to be the heads of the respective phrases `VP`, `NP`,
`AP`:gc: and `PP`:gc:. Abstracting over the identity of these phrases, we
can say that a lexical category `X`:gc: is the head of its immediate
`XP`:gc: phrase, and moreover that the complements `C`:subscript:`1`
... `C`:subscript:`n` of
`X`:gc: will occur as sisters of `X`:gc: within that `XP`:gc:. This is
illustrated in the following schema:

.. ex::
  .. tree:: (XP (X) (*C_1*) ... (*C_n*))

We have argued that lexical categories need to be subdivided into
subcategories to account for the fact that different lexical items
select different sequences of following complements. That is, it is a
distinguishing property of complements that they co-occur with some
lexical items but not others. By contrast, :dt:`modifiers` can
occur with pretty much any instance of the relevant lexical class. For
example, consider the temporal adverbial *last Thursday*:

.. example:: The woman gave the telescope to the dog last Thursday.
.. example:: The woman saw a man last Thursday.
.. example:: The dog barked last Thursday.

Moreover, modifiers are always optional, whereas complements are at
least sometimes obligatory. We can use the phrase structure
geometry to draw a structural distinction between complements, which
occur as sisters of the lexical head, versus modifiers, which occur as
sisters of the phrase which encloses the head:

.. ex::
  .. tree:: (XP (XP (X) (*C_1*) ... (*C_n*)) (*Mod*))

Exercises
---------

#. Pick some of the syntactic constructions described in any
   introductory syntax text (e.g. Jurafsky and Martin, Chapter 9) and
   create a set of 15 sentences.  Five sentences should be unambiguous
   (have a unique parse), five should be ambiguous, and a further five
   should be ungrammatical.

  a) Develop a small grammar, consisting of about ten syntactic
     productions, to account for this data.  Refine your set of sentences
     as needed to test and demonstrate the grammar.  Write a function
     to demonstrate your grammar on three sentences: (i) a
     sentence having exactly one parse; (ii) a sentence having more than
     one parse; (iii) a sentence having no parses.  Discuss your
     observations using inline comments.

  b) Create a list ``words`` of all the words in your lexicon, and use
     ``random.choice(words)`` to generate sequences of 5-10 randomly
     selected words.  Does this generate any grammatical sentences which
     your grammar rejects, or any ungrammatical sentences which your
     grammar accepts?  Now use this information to help you improve your
     grammar.  Discuss your findings.


----------
Conclusion
----------


In this chapter, we only consider very small ('toy') context-free
grammars, in order to illustrate the key aspects of parsing such
grammars. But there is an obvious question as to whether the general
approach can be scaled up to cover large corpora of natural
languages. How hard would it be to construct such a set of productions by
hand? In general, the answer is: Very hard. Even if we allow ourselves
to use various formal devices that give much more succinct
representations of grammar productions (some of which will be
discussed in the next chapter), it is still extremely difficult to
keep control of the complex interactions between the many productions
required to cover the major constructions of a language. That is, it
is hard to modularize grammars, so that one portion can be worked on
independently of the other parts. This in turn means that it is
difficult to distribute the task of grammar writing across a team of
linguists. Another difficulty is that as the grammar expands to cover
a wider and wider range of constructions, there is a corresponding
increase in the number of analyses which are admitted for any one
sentence. In other words, ambiguity increases proportionally with
coverage.


Despite the problems just alluded to, there are a number of
large-scale collaborative projects ongoing which appear to have
achieved interesting and often impressive results in developing
rule-based grammars for several languages. Examples are the Lexical
Functional Grammar (LFG) Pargram project
(http://www2.parc.com/istl/groups/nltt/pargram/), the Head-Driven
Phrase Structure Grammar (HPSG) LinGO Matrix framework
(http://www.delph-in.net/matrix/), and the Lexicalized Tree Adjoining
Grammar XTAG Project (http://www.cis.upenn.edu/~xtag/).

---------------
Further Reading
---------------

McCawley (1998) *The Syntactic Phenomena of English*.
Chicago University Press.

Rodney D. Huddleston, Geoffrey K. Pullum (2002).
*The Cambridge Grammar of the English Language*.
Cambridge University Press.

.. include:: footer.txt






  
