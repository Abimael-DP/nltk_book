.. -*- mode: rst -*-
.. include:: ../definitions.rst
.. include:: regexp-defns.rst

.. standard global imports

    >>> import nltk, re, pprint

.. TODO: introduce the concept of search, backtracking
.. TODO: introduce diagnostic print statements
.. TODO: timeit()
.. TODO: cover yield statement, as promised in words chapter
.. TODO: exercise based on word squares http://itre.cis.upenn.edu/~myl/languagelog/archives/002679.html
.. TODO: docstrings, commenting practice
.. TODO: recipes for flattening a list of lists into a list, and for the reverse grouping a list into a list of lists
.. TODO: generating all combinations, e.g. map list of words to list of lists of stress patterns; concatenate to make phrase-level stress patterns in all possible ways
.. TODO: finding all pairs of nouns that occur in the same sentence more than n times in a text

.. _chap-structured-programming:

===================================
6. Structured Programming in Python
===================================

By now you will have a sense of the capabilities of the Python programming language
for processing natural language.  However, if you're new to Python or to programming, you may
still be wrestling with Python and not feel like you are in full control yet.  In this chapter we'll
address the following questions:

1. how can we write well-structured, readable programs that you and others will be able to re-use easily?

2. how do the fundamental building blocks work, such as loops, functions and assignment?

3. what are some of the pitfalls with Python programming and how can we avoid them?

Along the way, you will consolidate your knowledge of fundamental programming
constructs, learn more about using features of the Python language in a natural
and concise way, and learn some useful techniques in visualizing natural language data.
As before, this chapter contains many examples and
exercises (and as before, some exercises introduce new material).
Readers new to programming should work through them carefully
and consult other introductions to programming if necessary;
experienced programmers can quickly skim this chapter.

.. note:: Remember that our program samples assume you
   begin your interactive session or your program with: ``import nltk, re, pprint``

.. _sec-back-to-the-basics:

------------------
Back to the Basics
------------------

Iteration
---------

Here is a familiar technique for iterating over the members
of a list by initializing an index ``i`` and then incrementing the
index each time we pass through the loop:

    >>> sent = ['I', 'am', 'the', 'Walrus']
    >>> i = 0
    >>> while i < len(sent):
    ...     print sent[i].lower(),
    ...     i += 1
    ...
    i am the walrus

Although this does the job, it is not idiomatic Python. By
contrast, Python's ``for`` statement allows us to achieve the same
effect much more succinctly:

    >>> for s in sent:
    ...     print s.lower(),
    ...
    i am the walrus

This program actually prints "``i am the walrus ``" with a trailing space character.
In the context of a longer program, we might not want the extra space character, and we
might want a newline character at the end (requiring an extra ``print`` statement). 
A third solution is more compact again, but possibly less readable thanks to the use
of ``' '.join()``.  The list comprehension should be very familiar by now.

    >>> print ' '.join(w.lower() for w in sent)
    i am the walrus

This doesn't produce an extra space character, and does produce the required newline.

Another case where loop variables seem to be necessary is for printing the value of
a counter with each line of output.  Instead, we can use ``enumerate()``:

    >>> fd = nltk.FreqDist(nltk.corpus.brown.words())
    >>> cumulative = 0.0
    >>> for rank, word in enumerate(fd):
    ...     cumulative += fd[word] * 100.0 / fd.N()
    ...     print "%3d %6.2f%% %s" % (rank+1, cumulative, word)
    ...     if cumulative > 25:
    ...         break
    ...
      1   5.40% the
      2  10.42% ,
      3  14.67% .
      4  17.78% of
      5  20.19% and
      6  22.40% to
      7  24.29% a
      8  25.97% in

You can also use list comprehensions for a kind of multiplication (or `cartesian product`:dt:).
Here we generate all combinations of two determiners, two adjectives, and two nouns.
The list comprehension is split across three lines for readability.

We have seen many examples of list comprehensions in previous chapters.  Here we show
an example with a triply-nested loop, to perform a task called `cartesian product`:dt:.

    >>> [(det,adj,noun) for det in ('two', 'three')
    ...                 for adj in ('old', 'blind')
    ...                 for noun in ('men', 'mice')]
    [('two', 'old', 'men'), ('two', 'old', 'mice'), ('two', 'blind', 'men'),
     ('two', 'blind', 'mice'), ('three', 'old', 'men'), ('three', 'old', 'mice'),
     ('three', 'blind', 'men'), ('three', 'blind', 'mice')]

Our use of list comprehensions has helped us avoid loop variables.  Here's
some cases where we still want to use loop variables in a list comprehension.

In the first example, we extract successive overlapping slices of size n
(a sliding window) from a list (pay particular attention to the range of the variable i).
    
    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> n = 3
    >>> [sent[i:i+n] for i in range(len(sent)-n+1)]
    [['The', 'dog', 'gave'],
     ['dog', 'gave', 'John'],
     ['gave', 'John', 'the'],
     ['John', 'the', 'newspaper']]

Since this is slightly cumbersome, and a common operation in |NLP|\ , |NLTK|
supports it with functions ``ngrams(text, n)``, ``bigrams(text)`` and ``trigrams(text)``.

Here's an example of how we can use loop variables in
building multidimensional structures.
For example, to build an array with *m* rows and *n* columns,
where each cell is a set, we would use a nested list comprehension,
as shown below.  Observe that the loop variables
``i`` and ``j`` are not used anywhere in the expressions preceding the
``for`` clauses.

    >>> m, n = 3, 7
    >>> array = [[set() for i in range(n)] for j in range(m)]
    >>> array[2][5].add('Alice')
    >>> pprint.pprint(array)
    [[set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set(['Alice']), set([])]]

Note that it would be incorrect to do this work using multiplication:

    >>> array = [[set()] * n] * m
    >>> array[2][5].add(7)
    >>> pprint.pprint(array)
    [[set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])],
     [set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])],
     [set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])]]

Here, the same object appears at every position in the array, so an operation
on one cell affects all cells.  We explore this issue further in the next section.

Assignment
----------

Python's assignment statement operates on `values`:em:.  But what is a value?
Consider the following code fragment:

    >>> word1 = 'Monty'
    >>> word2 = word1              # [_assignment1]
    >>> word1 = 'Python'           # [_assignment2]
    >>> word2
    'Monty'

This code shows that when we write ``word2 = word1`` in line assignment1_,
the value of ``word1`` (the string ``'Monty'``) is assigned to ``word2``.
That is, ``word2`` is a `copy`:dt: of ``word1``, so when we overwrite
``word1`` with a new string ``'Python'`` in line assignment2_, the value
of ``word2`` is not affected.

However, assignment statements do not always involve making copies in this way.
An important subtlety of Python is that the "value" of a structured object
(such as a list) is actually a `reference`:em: to the object.  In the
following example, line assignment3_
assigns the reference of ``list1`` to the new variable ``list2``.
When we modify something inside ``list1`` on line assignment4_, we can see
that the contents of ``list2`` have also been changed.

    >>> list1 = ['Monty', 'Python']
    >>> list2 = list1             # [_assignment3]
    >>> list1[1] = 'Bodkin'       # [_assignment4]
    >>> list2
    ['Monty', 'Bodkin']

.. _array-memory:
.. figure:: ../images/array-memory.png
   :scale: 20

   List Assignment and Computer Memory

|nopar|    
Thus line assignment3_ does not copy the contents of the
variable, only its "object reference".
To understand what is going on here, we need to
know how lists are stored in the computer's memory.
In Figure array-memory_, we see that a list ``sent1`` is
a reference to an object stored at location 3133 (which is
itself a series of pointers to other locations holding strings).
When we assign ``sent2 = sent1``, it is just the object reference
3133 that gets copied.

Sequences
---------

We have seen three kinds of sequence object: strings, lists, and tuples.
As sequences, they have some common properties: they can be indexed and they have
a length:

    >>> text = 'I turned off the spectroroute'
    >>> words = ['I', 'turned', 'off', 'the', 'spectroroute']
    >>> pair = (6, 'turned')
    >>> text[2], words[3], pair[1]
    ('t', 'the', 'turned')
    >>> len(text), len(words), len(pair)
    (29, 5, 2)

|nopar|
We can iterate over the items in a sequence ``s`` in a variety of useful ways,
as shown in Table python-sequence_.

.. table:: python-sequence

   +---------------------------------------------+------------------------------------------------+
   | Python Expression                           | Comment                                        |
   +=============================================+================================================+
   | ``for item in s``                           | iterate over the items of ``s``                |
   +---------------------------------------------+------------------------------------------------+
   | ``for item in sorted(s)``                   | iterate over the items of ``s`` in order       |
   +---------------------------------------------+------------------------------------------------+
   | ``for item in set(s)``                      | iterate over unique elements of ``s``          |
   +---------------------------------------------+------------------------------------------------+
   | ``for item in reversed(s)``                 | iterate over elements of ``s`` in reverse      |
   +---------------------------------------------+------------------------------------------------+
   | ``for item in set(s).difference(t)``        | iterate over elements of ``s`` not in ``t``    |
   +---------------------------------------------+------------------------------------------------+
   | ``for item in random.shuffle(s)``           | iterate over elements of ``s`` in random order |
   +---------------------------------------------+------------------------------------------------+

   Various ways to iterate over sequences


The sequence functions illustrated in Table python-sequence_ can be combined
in various ways; for example, to get unique elements of ``s`` sorted
in reverse, use ``reversed(sorted(set(s)))``.

We can convert between these sequence types.  For example,
``tuple(s)`` converts any kind of sequence into a tuple, and
``list(s)`` converts any kind of sequence into a list.
We can convert a list of strings to a single string using the
``join()`` function, e.g. ``':'.join(words)``.

Notice in the above code sample that we computed multiple values on a
single line, separated by commas.  These comma-separated expressions
are actually just tuples |mdash| Python allows us to omit the
parentheses around tuples if there is no ambiguity. When we print a
tuple, the parentheses are always displayed. By using tuples in this
way, we are implicitly aggregating items together.

In the next example, we use tuples to re-arrange the
contents of our list.  (We can omit the parentheses
because the comma has higher precedence than assignment.)

    >>> words[2], words[3], words[4] = words[3], words[4], words[2]
    >>> words    
    ['I', 'turned', 'the', 'spectroroute', 'off']

|nopar|
This is an idiomatic and readable way to move items inside a list.
It is equivalent to the following traditional way of doing such
tasks that does not use tuples (notice that this method needs a
temporary variable ``tmp``).

    >>> tmp = words[2]
    >>> words[2] = words[3]
    >>> words[3] = words[4]
    >>> words[4] = tmp

As we have seen, Python has sequence functions such as ``sorted()`` and ``reversed()``
that rearrange the items of a sequence.  There are also functions that
modify the `structure`:em: of a sequence and which can be handy for
language processing.  Thus, ``zip()`` takes
the items of two sequences and "zips" them together into a single list of pairs.
Given a sequence ``s``, ``enumerate(s)`` returns an iterator that
produces a pair of an index and the item at that index.

    >>> words = ['I', 'turned', 'off', 'the', 'spectroroute']
    >>> tags = ['NNP', 'VBD', 'IN', 'DT', 'NN']
    >>> zip(words, tags)
    [('I', 'NNP'), ('turned', 'VBD'), ('off', 'IN'),
    ('the', 'DT'), ('spectroroute', 'NN')]
    >>> list(enumerate(words))
    [(0, 'I'), (1, 'turned'), (2, 'off'), (3, 'the'), (4, 'spectroroute')]

Combining Different Sequence Types
----------------------------------

Let's combine our knowledge of these three sequence types, together with list
comprehensions, to perform the task of sorting the words in a string by
their length.

    >>> words = 'I turned off the spectroroute'.split()     # [_sequence1]
    >>> wordlens = [(len(word), word) for word in words]    # [_sequence2]
    >>> wordlens
    [(1, 'I'), (6, 'turned'), (3, 'off'), (3, 'the'), (12, 'spectroroute')]
    >>> wordlens.sort()                                     # [_sequence3]
    >>> ' '.join([word for (count, word) in wordlens])      # [_sequence4]
    'I off the turned spectroroute'

Each of the above lines of code contains a significant feature.
Line sequence1_ demonstrates that a simple string is actually
an object with methods defined on it, such as ``split()``.
Line sequence2_ shows the construction of a list of tuples,
where each tuple consists of a number (the word length) and the
word, e.g. ``(3, 'the')``.  Line sequence3_ sorts the list,
modifying the list in-place.  Finally, line sequence4_ discards
the length information then joins the words back into a single string.

We began by talking about the commonalities in these sequence types,
but the above code illustrates important differences in their
roles.  First, strings appear at the beginning and the end: this is
typical in the context where our program is reading in some text and
producing output for us to read.  Lists and tuples are used in the
middle, but for different purposes.  A list is typically a sequence of
objects all having the `same type`:em:, of `arbitrary length`:em:.  We often
use lists to hold sequences of words.  In contrast,
a tuple is typically a collection of objects of `different types`:em:, of
`fixed length`:em:.  We often use a tuple to hold a `record`:dt:,
a collection of different `fields`:dt: relating to some entity.
This distinction between the use of lists and tuples takes some
getting used to,
so here is another example:

    >>> lexicon = [
    ...     ('the', 'DT', ['Di:', 'D@']),
    ...     ('off', 'IN', ['Qf', 'O:f'])
    ... ]

|nopar| Here, a lexicon is represented as a list because it is a
collection of objects of a single type |mdash| lexical entries |mdash|
of no predetermined length.  An individual entry is represented as a
tuple because it is a collection of objects with different
interpretations, such as the orthographic form, the part of speech,
and the pronunciations represented in the
`SAMPA <http://www.phon.ucl.ac.uk/home/sampa/index.html>`_ computer
readable phonetic alphabet.  Note that these pronunciations are stored
using a list. (Why?)

The distinction between lists and tuples has been described in terms of
usage.  However, there is a more fundamental difference: in Python,
lists are `mutable`:dt:, while tuples are `immutable`:dt:.  In other
words, lists can be modified, while tuples cannot.  Here are some of
the operations on lists that do in-place modification of the list.
None of these operations is permitted on a tuple, a fact you should
confirm for yourself.

    >>> lexicon.sort()
    >>> lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3`nd'])
    >>> del lexicon[0]

Stacks and Queues
-----------------

Lists are a particularly versatile data type.  We can use lists to
implement higher-level data types such as stacks and queues.
A `stack`:dt: is a container that has a `last-in-first-out`:dt: policy
for adding and removing items (see Figure stack-queue_).

.. _stack-queue:
.. figure:: ../images/stack-queue.png
   :scale: 30

   Stacks and Queues

Stacks are used to keep track of the current context in
computer processing of natural languages (and programming languages too).
We will seldom have to deal with stacks explicitly, as the implementation
of |NLTK| parsers, treebank corpus readers, (and even Python functions),
all use stacks behind the scenes.
However, it is important to understand what stacks are and how they work.

.. pylisting:: check-parens
   :caption: Check parentheses are balanced

   def check_parens(tokens):
       stack = []
       for token in tokens:
           if token == '(':     # push
               stack.append(token)
           elif token == ')':   # pop
               stack.pop()
       return stack

   >>> phrase = "( the cat ) ( sat ( on ( the mat )"
   >>> print check_parens(phrase.split())
   ['(', '(']

In Python, we can treat a list as a stack by limiting ourselves to the three
operations defined on stacks: ``append(item)`` (to push ``item`` onto the stack),
``pop()`` to pop the item off the top of the stack, and ``[-1]`` to access the
item on the top of the stack.  The program in Figure check-parens_ processes a sentence with
phrase markers, and checks that the parentheses are balanced.
The loop pushes material onto the stack when it gets an open parenthesis,
and pops the stack when it gets a close parenthesis.
We see that two are left on the stack at the end;
i.e. the parentheses are not balanced.

Although the program in Figure check-parens_ is a useful illustration of stacks,
it is overkill because we could have done a direct count:
``phrase.count('(') == phrase.count(')')``.  However, we
can use stacks for more sophisticated processing of strings
containing nested structure, as shown in Figure convert-parens_.
Here we build a (potentially deeply-nested) list of lists.
Whenever a token other than a parenthesis is encountered,
we add it to a list at the appropriate level of nesting.
The stack cleverly keeps track of this level of nesting, exploiting
the fact that the item at the top of the stack is actually shared with a
more deeply nested item.  (Hint: add diagnostic print statements to
the function to help you see what it is doing.)

.. pylisting:: convert-parens
   :caption: Convert a nested phrase into a nested list using a stack

   def convert_parens(tokens):
       stack = [[]]
       for token in tokens:
           if token == '(':     # push
               sublist = []
               stack[-1].append(sublist)
               stack.append(sublist)
           elif token == ')':   # pop
               stack.pop()
           else:                # update top of stack
               stack[-1].append(token)
       return stack[0]

   >>> phrase = "( the cat ) ( sat ( on ( the mat ) ) )"
   >>> print convert_parens(phrase.split())
   [['the', 'cat'], ['sat', ['on', ['the', 'mat']]]]    

Lists can be used to represent another important data structure.
A `queue`:dt: is a container that has a `first-in-first-out`:dt: policy
for adding and removing items (see Figure stack-queue_).
Queues are used for scheduling activities or resources.
As with stacks, we will seldom have to deal with queues explicitly,
as the implementation of |NLTK| n-gram taggers (Section sec-n-gram-tagging_)
and chart parsers (Section chart-parsing_) use queues behind the scenes.
However, we will take a brief look at how queues are implemented using lists.

    >>> queue = ['the', 'cat', 'sat']
    >>> queue.append('on')
    >>> queue.append('the')
    >>> queue.append('mat')
    >>> queue.pop(0)
    'the'
    >>> queue.pop(0)
    'cat'
    >>> queue
    ['sat', 'on', 'the', 'mat']


Conditionals
------------

In the condition part of an ``if`` statement, a
nonempty string or list is evaluated as true, while an empty string or
list evaluates as false. 

    >>> mixed = ['cat', '', ['dog'], []]
    >>> for element in mixed:
    ...     if element: 
    ...         print element
    ... 
    cat
    ['dog']

That is, we *don't* need to say ``if len(element) > 0:`` in the
condition.

What's the difference between using ``if...elif`` as opposed to using
a couple of ``if`` statements in a row? Well, consider the following
situation:

    >>> animals = ['cat', 'dog']
    >>> if 'cat' in animals:
    ...     print 1
    ... elif 'dog' in animals:
    ...     print 2
    ... 
    1
    >>>

Since the ``if`` clause of the statement is satisfied, Python never
tries to evaluate the ``elif`` clause, so we never get to print out
``2``. By contrast, if we replaced the ``elif`` by an ``if``, then we
would print out both ``1`` and ``2``. So an ``elif`` clause
potentially gives us more information than a bare ``if`` clause; when
it evaluates to true, it tells us not only that the condition is
satisfied, but also that the condition of the main ``if`` clause was
*not* satisfied.


---------------------------------
Visualizing Language Data (DRAFT)
---------------------------------

[Section on pylab, scatter plots, bar charts, line graphs]

[Section on networkx and displaying network diagrams]

So far we have focused on textual presentation and the use of formatted print
statements to get output lined up in columns.  It is often very useful to display
numerical data in graphical form, since this often makes it easier to detect
patterns.  For example, in Figure modal-tabulate_ we saw a table of numbers
showing the frequency of particular modal verbs in the Brown Corpus, classified
by genre.  The program in Figure modal-plot_ presents the same information in graphical
format.  The output is shown in Figure modal-genre_ (a color figure in the online version).

.. Note:: The program in Figure modal-plot_ uses the PyLab package which supports sophisticated
   plotting functions with a MATLAB-style interface.  For more information about
   this package please see ``http://matplotlib.sourceforge.net/``.

.. pylisting:: modal-plot
   :caption: Frequency of Modals in Different Sections of the Brown Corpus

   colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black
   def bar_chart(categories, words, counts):
       "Plot a bar chart showing counts for each word by category"
       import pylab
       ind = pylab.arange(len(words))
       width = 1.0 / (len(categories) + 1)
       bar_groups = []
       for c in range(len(categories)):
           bars = pylab.bar(ind+c*width, counts[categories[c]], width, color=colors[c % len(colors)])
           bar_groups.append(bars)
       pylab.xticks(ind+width, words)
       pylab.legend([b[0] for b in bar_groups], categories, loc='upper left')
       pylab.ylabel('Frequency')
       pylab.title('Frequency of Six Modal Verbs by Genre')
       pylab.show()

   >>> genres = ['a', 'd', 'e', 'h', 'n']
   >>> cfdist = count_words_by_tag('MD', genres)
   >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
   >>> counts = {}
   >>> for genre in genres:
   ...     counts[genre] = [cfdist[genre][word] for word in modals]
   >>> bar_chart(genres, modals, counts)

.. _modal-genre:
.. figure:: ../images/modal_genre.png
   :scale: 25

   Bar Chart Showing Frequency of Modals in Different Sections of Brown Corpus

From the bar chart it is immediately obvious that `may`:lx: and `must`:lx: have
almost identical relative frequencies.  The same goes for `could`:lx: and `might`:lx:.

.. _sec-functions:

---------
Functions
---------

Once you have been programming for a while, you will find that you need
to perform a task that you have done in the past.  In fact, over time,
the number of completely novel things you have to do in creating a program
decreases significantly.  Half of the work may involve simple tasks that
you have done before.  Thus it is important for your code to be `re-usable`:em:.
One effective way to do this is to abstract commonly used sequences of steps
into a `function`:dt:.

For example, suppose we find that we often want to read text from an HTML file.
This involves several steps: opening the file, reading it in, normalizing
whitespace, and stripping HTML markup.  We can collect these steps into a
function, and give it a name such as ``get_text()``:

.. pylisting:: get_text
   :caption: Read text from a file

   import re
   def get_text(file):
       """Read text from a file, normalizing whitespace
       and stripping HTML markup."""
       text = open(file).read()
       text = re.sub('\s+', ' ', text)
       text = re.sub(r'<.*?>', ' ', text)
       return text

Now, any time we want to get cleaned-up text from an HTML file, we can just call
``get_text()`` with the name of the file as its only argument.  It will return
a string, and we can assign this to a variable, e.g.:
``contents = get_text("test.html")``.  Each time we want to use this series of
steps we only have to call the function.

Notice that a function definition consists of the keyword ``def`` (short for "define"), followed
by the function name, followed by a sequence of parameters enclosed in parentheses, then
a colon.  The following lines contain an indented block of code, the `function body`:dt:.

Using functions has the benefit of saving space in our program.  More
importantly, our choice of name for the function helps make the program *readable*.
In the case of the above example, whenever our program needs to read cleaned-up
text from a file we don't have to clutter the program with four lines of code, we
simply need to call ``get_text()``.  This naming helps to provide some "semantic
interpretation" |mdash| it helps a reader of our program to see what the program "means".

Notice that the above function definition contains a string.  The first string inside
a function definition is called a `docstring`:dt:.  Not only does it document the
purpose of the function to someone reading the code, it is accessible to a programmer
who has loaded the code from a file:

    >>> help(get_text)
    Help on function get_text:

    get_text(file)
        Read text from a file, normalizing whitespace
        and stripping HTML markup.

We have seen that functions help to make our work reusable and readable.  They
also help make it *reliable*.  When we re-use code that has already been developed
and tested, we can be more confident that it handles a variety of cases correctly.
We also remove the risk that we forget some important step, or introduce a bug.
The program that calls our function also has increased reliability.  The author
of that program is dealing with a shorter program, and its components behave
transparently.

* [More: overview of section]

Function Arguments
------------------

* multiple arguments
* named arguments
* default values

Python is a `dynamically typed`:dt: language.  It does not force us to
declare the type of a variable when we write a program.  This feature
is often useful, as it permits us to define functions that are flexible
about the type of their arguments.  For example, a tagger might expect
a sequence of words, but it wouldn't care whether this sequence is expressed
as a list, a tuple, or an iterator.

However, often we want to write programs for later use by others, and want
to program in a defensive style, providing useful warnings when functions
have not been invoked correctly.  Observe that the ``tag()``
function in Figure tag1_ behaves sensibly for string arguments,
but that it does not complain when it is passed a dictionary.

.. pylisting:: tag1
   :caption: A tagger that tags anything

   def tag(word):
       if word in ['a', 'the', 'all']:
           return 'DT'
       else:
           return 'NN'

   >>> tag('the')
   'DT'
   >>> tag('dog')
   'NN'
   >>> tag({'lexeme':'turned', 'pos':'VBD', 'pron':['t3:nd', 't3`nd']})
   'NN'

It would be helpful if the author of this function took some extra steps to
ensure that the ``word`` parameter of the ``tag()`` function is a string.
A naive approach would be to check the type of the argument and return
a diagnostic value, such as Python's special empty value, ``None``,
as shown in Figure tag2_.

.. pylisting:: tag2
   :caption: A tagger that only tags strings

   def tag(word):
       if not type(word) is str:
           return None
       if word in ['a', 'the', 'all']:
           return 'DT'
       else:
           return 'NN'

However, this approach is dangerous because the calling program
may not detect the error, and the diagnostic return value may be
propagated to later parts of the program with unpredictable consequences.
A better solution is shown in Figure tag3_.

.. pylisting:: tag3
   :caption: A tagger that generates an error message when not passed a string

   def tag(word):
       if not type(word) is str:
           raise ValueError, "argument to tag() must be a string"
       if word in ['a', 'the', 'all']:
           return 'DT'
       else:
           return 'NN'

This produces an error that cannot be ignored, since it halts program execution.
Additionally, the error message is easy to interpret.  (We will see an even
better approach, known as "duck typing" in Chapter chap-applied-programming_.)

Another aspect of defensive programming concerns the return statement of a function.
In order to be confident that all execution paths through a function lead to a
return statement, it is best to have a single return statement at the end of
the function definition.
This approach has a further benefit: it makes it more likely that the
function will only return a single type.
Thus, the following version of our ``tag()`` function is safer:

    >>> def tag(word):
    ...     result = 'NN'                       # default value, a string
    ...     if word in ['a', 'the', 'all']:     # in certain cases...
    ...         result = 'DT'                   #   overwrite the value
    ...     return result                       # all paths end here

A return statement can be used to pass multiple values back to the calling
program, by packing them into a tuple.
Here we define a function that returns a tuple
consisting of the average word length of a sentence, and the inventory
of letters used in the sentence.  It would have been clearer to write
two separate functions.

    >>> def proc_words(words):
    ...     avg_wordlen = sum(len(word) for word in words)/len(words)
    ...     chars_used = ''.join(sorted(set(''.join(words))))
    ...     return avg_wordlen, chars_used
    >>> proc_words(['Not', 'a', 'good', 'way', 'to', 'write', 'functions'])
    (3, 'Nacdefginorstuwy')

.. EK: wouldn't it be better to use an example which isn't then open to criticism?

Functions do not need to have a return statement at all.
Some functions do their work as a side effect, printing a result,
modifying a file, or updating the contents of a parameter to the function.
Consider the following three sort functions; the last approach is dangerous
because a programmer could use it without realizing that it had modified
its input.

    >>> def my_sort1(l):      # good: modifies its argument, no return value
    ...     l.sort()
    >>> def my_sort2(l):      # good: doesn't touch its argument, returns value
    ...     return sorted(l)
    >>> def my_sort3(l):      # bad: modifies its argument and also returns it
    ...     l.sort()
    ...     return l

An Important Subtlety
---------------------

Back in Section sec-back-to-the-basics_ you saw that in Python, assignment works on values,
but that the value of a structured object is a reference to that object.  The same
is true for functions.  Python interprets function parameters as values (this is
known as `call-by-value`:dt:).  Consider Figure call-by-value_.
Function ``set_up()`` has two parameters,
both of which are modified inside the function.  We begin by assigning an empty string
to ``w`` and an empty dictionary to ``p``.  After calling the function, ``w`` is unchanged,
while ``p`` is changed:

.. pylisting:: call-by-value

   def set_up(word, properties):
       word = 'cat'
       properties['pos'] = 'noun'

   >>> w = ''
   >>> p = {}
   >>> set_up(w, p)
   >>> w
   ''
   >>> p
   {'pos': 'noun'}

|nopar|
To understand why ``w`` was not changed, it is necessary to understand call-by-value.
When we called ``set_up(w, p)``, the value of ``w`` (an empty string) was assigned to
a new variable ``word``.  Inside the function, the value of ``word`` was modified.
However, that had no effect on the external value of ``w``.  This parameter passing is
identical to the following sequence of assignments:

    >>> w = ''
    >>> word = w
    >>> word = 'cat'
    >>> w
    ''

|nopar|
In the case of the structured object, matters are quite different.
When we called ``set_up(w, p)``, the value of ``p`` (an empty
dictionary) was assigned to a new local variable ``properties``.  Since the
value of ``p`` is an object reference, both variables now reference the same
memory location.  Modifying something inside ``properties`` will also
change ``p``, just as if we had done the following sequence of assignments:

    >>> p = {}
    >>> properties = p
    >>> properties['pos'] = 'noun'
    >>> p
    {'pos': 'noun'}

Thus, to understand Python's call-by-value parameter passing,
it is enough to understand Python's assignment operation.
We will address some closely related issues in our later discussion
of variable scope (Section sec-variable-scope_).

Functional Decomposition
------------------------

Well-structured programs usually make extensive use of functions.
When a block of program code grows longer than 10-20 lines, it is a
great help to readability if the code is broken up into one or more
functions, each one having a clear purpose.  This is analogous to
the way a good essay is divided into paragraphs, each expressing one main idea.

Functions provide an important kind of *abstraction*.
They allow us to group multiple actions into a single, complex action,
and associate a name with it.
(Compare this with the way we combine the actions of
`go`:lx: and `bring back`:lx: into a single more complex action `fetch`:lx:.)
When we use functions, the main program can be written at a higher level
of abstraction, making its structure transparent, e.g.

.. doctest-ignore::
    >>> data = load_corpus()
    >>> results = analyze(data)
    >>> present(results)

Appropriate use of functions makes programs more readable and maintainable.
Additionally, it becomes possible to reimplement a function
|mdash| replacing the function's body with more efficient code |mdash|
without having to be concerned with the rest of the program.

Consider the ``freq_words`` function in Figure freq-words1_.
It updates the contents of a frequency distribution that is
passed in as a parameter, and it also prints a list of the
`n`:math: most frequent words.

.. pylisting:: freq-words1

   def freq_words(url, freqdist, n):
       text = nltk.clean_url(url)
       for word in nltk.wordpunct_tokenize(text):
           freqdist.inc(word.lower())
       print freqdist.keys()[:n]

   >>> constitution = "http://www.archives.gov/national-archives-experience/charters/constitution_transcript.html"
   >>> fd = nltk.FreqDist()
   >>> freq_words(constitution, fd, 20)
   ['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',
   'declaration', 'impact', 'freedom', '-', 'making', 'independence']

This function has a number of problems.
The function has two side-effects: it modifies the contents of its second
parameter, and it prints a selection of the results it has computed.
The function would be easier to understand and to reuse elsewhere if we
initialize the ``FreqDist()`` object inside the function (in the same place
it is populated), and if we moved the selection and display of results to the
calling program.  In Figure freq-words2_ we `refactor`:dt: this function,
and simplify its interface by providing a single ``url`` parameter.

.. pylisting:: freq-words2

   def freq_words(url):
       freqdist = nltk.FreqDist()
       text = nltk.clean_url(url)
       for word in nltk.wordpunct_tokenize(text):
           freqdist.inc(word.lower())
       return freqdist

   >>> fd = freq_words(constitution)
   >>> print fd.keys()[:20]
   ['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',
   'declaration', 'impact', 'freedom', '-', 'making', 'independence']

Note that we have now simplified the work of ``freq_words``
to the point that we can do its work with three lines of code:

    >>> words = nltk.wordpunct_tokenize(nltk.clean_url(constitution))
    >>> fd = nltk.FreqDist(word.lower() for word in words)
    >>> fd.keys()[:20]
    ['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',
    'declaration', 'impact', 'freedom', '-', 'making', 'independence']

Modularity
----------

Variable scope

* local and global variables
* scope rules
* global variables introduce dependency on context and limits the reusability of a function
* importance of avoiding side-effects
* functions hide implementation details

Documentation (notes)
---------------------

* some guidelines for literate programming (e.g. variable and function naming)
* documenting functions (user-level and developer-level documentation)

NLTK's docstrings are (mostly) written using the "epytext" markup
language.  Using an explicit markup language allows us to generate
prettier online documentation, e.g. see:

   http://nltk.org/doc/api/nltk.tree.Tree-class.html

[Examples of function-level docstrings with epytext markup.]


Functions as Arguments
----------------------

So far the arguments we have passed into functions have been simple objects like
strings, or structured objects like lists.  These arguments allow us to parameterize
the behavior of a function.  As a result, functions are very flexible and powerful
abstractions, permitting us to repeatedly apply the `same operation`:em: on `different data`:em:.
Python also lets us pass a function as
an argument to another function.  Now we can abstract out the operation, and apply
a `different operation`:em: on the `same data`:em:.  As the following examples show,
we can pass the built-in function ``len()`` or a user-defined function ``last_letter()``
as parameters to another function:

    >>> def extract_property(prop):
    ...     words = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    ...     return [prop(word) for word in words]
    >>> extract_property(len)
    [3, 3, 4, 4, 3, 9]
    >>> def last_letter(word):
    ...     return word[-1]
    >>> extract_property(last_letter)
    ['e', 'g', 'e', 'n', 'e', 'r']

Surprisingly, ``len`` and ``last_letter`` are objects that can be
passed around like lists and dictionaries.  Notice that parentheses
are only used after a function name if we are invoking the function;
when we are simply passing the function around as an object these are
not used.

Python provides us with one more way to define functions as arguments
to other functions, so-called `lambda expressions`:dt:.  Supposing there
was no need to use the above ``last_letter()`` function in multiple places,
and thus no need to give it a name.  We can equivalently write the following:

    >>> extract_property(lambda w: w[-1])
    ['e', 'g', 'e', 'n', 'e', 'r']

Our next example illustrates passing a function to the ``sorted()`` function.
When we call the latter with a single argument (the list to be sorted),
it uses the built-in lexicographic comparison function ``cmp()``.
However, we can supply our own sort function, e.g. to sort by decreasing
length.

    >>> words = 'I turned off the spectroroute'.split()
    >>> sorted(words)
    ['I', 'off', 'spectroroute', 'the', 'turned']
    >>> sorted(words, cmp)
    ['I', 'off', 'spectroroute', 'the', 'turned']
    >>> sorted(words, lambda x, y: cmp(len(y), len(x)))
    ['spectroroute', 'turned', 'off', 'the', 'I']

In sec-back-to-the-basics_ we saw an example of filtering out
some items in a list comprehension, using an ``if`` test.
Similarly, we can restrict a list to just the lexical words, using
``[word for word in sent if is_lexical(word)]``.  This is a little cumbersome
as it mentions the ``word`` variable three times.  A more compact way to express
the same thing is as follows.

    >>> def is_lexical(word):
    ...     return word.lower() not in ('a', 'an', 'the', 'that', 'to')
    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> filter(is_lexical, sent)
    ['dog', 'gave', 'John', 'newspaper']

|nopar| The function ``is_lexical(word)`` returns ``True`` just in case
``word``, when normalized to lowercase, is not in the given list.
This function is itself used as an argument to ``filter()``.
The ``filter()`` function applies its first argument (a function) to each
item of its second (a sequence), only passing it through if the
function returns true for that item.  Thus ``filter(f, seq)`` is
equivalent to ``[item for item in seq if apply(f,item) == True]``.

Another helpful function, which like ``filter()`` applies a function
to a sequence, is ``map()``.  Here is a simple
way to find the average length of a sentence in a section of the Brown Corpus:

    >>> average(map(len, nltk.corpus.brown.sents(categories='news')))
    21.7508111616

|nopar|
Instead of ``len()``, we could have passed in any other function we liked:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> def is_vowel(letter):
    ...     return letter in "AEIOUaeiou"
    >>> def vowelcount(word):
    ...     return len(filter(is_vowel, word))
    >>> map(vowelcount, sent)
    [1, 1, 2, 1, 1, 3]

|nopar|
Instead of using ``filter()`` to call a named function ``is_vowel``, we can
define a lambda expression as follows:

    >>> map(lambda w: len(filter(lambda c: c in "AEIOUaeiou", w)), sent)
    [1, 1, 2, 1, 1, 3]

Named Arguments
---------------

One of the difficulties in re-using functions is remembering the order of arguments.
Consider the following function, that finds the ``n`` most frequent words that are
at least ``min_len`` characters long:

    >>> def freq_words(file, min, num):
    ...     text = open(file).read()
    ...     tokens = nltk.wordpunct_tokenize(text)
    ...     freqdist = nltk.FreqDist(t for t in tokens if len(t) >= min)
    ...     return freqdist.keys()[:num]
    >>> freq_words('programming.txt', 4, 10)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words',
    'very', 'using']

This function has three arguments.  It follows the convention of listing the most
basic and substantial argument first (the file).  However, it might be hard to remember
the order of the second and third arguments on subsequent use.  We can make this function
more readable by using `keyword arguments`:dt:.  These appear in the function's argument
list with an equals sign and a default value:

    >>> def freq_words(file, min=1, num=10):
    ...     text = open(file).read()
    ...     tokens = nltk.wordpunct_tokenize(text)
    ...     freqdist = nltk.FreqDist(t for t in tokens if len(t) >= min)
    ...     return freqdist.keys()[:num]

|nopar|
Now there are several equivalent ways to call this function:

    >>> freq_words('programming.txt', 4, 10)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']
    >>> freq_words('programming.txt', min=4, num=10)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']
    >>> freq_words('programming.txt', num=10, min=4)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']

|nopar|
When we use an integrated development environment such as IDLE,
simply typing the name of a function at the command prompt will
list the arguments.  Using named arguments helps someone to re-use the code...

A side-effect of having named arguments is that they permit optionality.  Thus we
can leave out any arguments where we are happy with the default value.

    >>> freq_words('programming.txt', min=4)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']
    >>> freq_words('programming.txt', 4)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']

Another common use of optional arguments is to permit a flag, e.g.:

    >>> def freq_words(file, min=1, num=10, trace=False):
    ...     freqdist = FreqDist()
    ...     if trace: print "Opening", file
    ...     text = open(file).read()
    ...     if trace: print "Read in %d characters" % len(file)
    ...     for word in nltk.wordpunct_tokenize(text):
    ...         if len(word) >= min:
    ...             freqdist.inc(word)
    ...             if trace and freqdist.N() % 100 == 0: print "."
    ...     if trace: print
    ...     return freqdist.keys()[:num]

---------
Iterators
---------

[itertools, bigrams vs ibigrams, efficiency, ...]

Accumulative Functions
----------------------

These functions start by initializing some storage, and iterate over
input to build it up, before returning some final object (a large structure
or aggregated result).  The standard way to do this is to initialize an
empty list, accumulate the material, then return the list, as shown
in function ``find_nouns1()`` in Listing find-nouns1_.

.. pylisting:: find-nouns1
   :caption: Accumulating Output into a List

   def find_nouns1(tagged_text):
       nouns = []
       for word, tag in tagged_text:
           if tag[:2] == 'NN':
               nouns.append(word)
       return nouns

   >>> tagged_text = [('the', 'DT'), ('cat', 'NN'), ('sat', 'VBD'),
   ...                ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]
   >>> find_nouns1(tagged_text)
   ['cat', 'mat']

A superior way to perform this operation is define the function to
be a `generator`:dt:, as shown in Listing find-nouns2_.
The first time this function is called, it gets as far as the ``yield``
statement and stops.  The calling program gets the first word and does
any necessary processing.  Once the calling program is ready for another
word, execution of the function is continued from where it stopped, until
the next time it encounters a ``yield`` statement.  This approach is
typically more efficient, as the function only generates the data as it is
required by the calling program, and does not need to allocate additional
memory to store the output.

.. pylisting:: find-nouns2
   :caption: Defining a Generator Function
   
   def find_nouns2(tagged_text):
       for word, tag in tagged_text:
           if tag[:2] == 'NN':
               yield word

   >>> tagged_text = [('the', 'DT'), ('cat', 'NN'), ('sat', 'VBD'),
   ...                ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]
   >>> find_nouns2(tagged_text)
   <generator object at 0x14b2f30>
   >>> for noun in find_nouns2(tagged_text):
   ...     print noun,
   cat mat
   >>> list(find_nouns2(tagged_text))
   ['cat', 'mat']

If we call the function directly we see that it returns a "generator object", which is
not very useful to us.  Instead, we can iterate over it directly, using ``for noun in find_nouns(tagged_text)``,
or convert it into a list, using ``list(find_nouns(tagged_text))``.

.. _sec-algorithm-design-strategies:

---------------------------
Algorithm Design Strategies
---------------------------

A major part of algorithmic problem solving is selecting or adapting
an appropriate algorithm for the problem at hand.  Whole books are written
on this topic (e.g. [Levitin2004]_) and we only have space to introduce
some key concepts and elaborate on the approaches that are most prevalent
in natural language processing.

The best known strategy is known as `divide-and-conquer`:dt:.
We attack a problem of size *n* by dividing it into two problems of size *n/2*,
solve these problems, and combine their results into a solution of the original problem.
Figure mergesort_ illustrates this approach for sorting a list of words.

.. _mergesort:
.. figure:: ../images/mergesort.png
   :scale: 35

   Sorting by Divide-and-Conquer (Mergesort)

Another strategy is `decrease-and-conquer`:dt:.  In this approach, a small amount
of work on a problem of size *n* permits us to reduce it to a problem of
size *n/2*.  Figure binary-search_ illustrates this approach for the problem
of finding the index of an item in a sorted list.

.. _binary-search:
.. figure:: ../images/binary-search.png
   :scale: 35

   Searching by Decrease-and-Conquer (Binary Search)

A third well-known strategy is `transform-and-conquer`:dt:.  We attack a problem
by transforming it into an instance of a problem we already know how to solve.
For example, in order to detect duplicates entries in a list, we can `pre-sort`:dt:
the list, then look for adjacent identical items, as shown in Figure presorting_.
Our approach to n-gram chunking in Section n-gram-chunking_ is another case of
transform and conquer (why?).

.. pylisting:: presorting
   :caption: Presorting a list for duplicate detection

   def duplicates(words):
       prev = None
       dup = [None]
       for word in sorted(words):
           if word == prev and word != dup[-1]:
               dup.append(word)
           else:
               prev = word
       return dup[1:]

   >>> duplicates(['cat', 'dog', 'cat', 'pig', 'dog', 'cat', 'ant', 'cat'])
   ['cat', 'dog']        

Recursion (notes)
-----------------

We first saw recursion in Chapter chap-words_, in a function that navigated
the hypernym hierarchy of WordNet...

Iterative solution:

    >>> def factorial(n):
    ...     result = 1
    ...     for i in range(n):
    ...         result *= (i+1)
    ...     return result

Recursive solution (base case, induction step)

    >>> def factorial(n):
    ...     if n == 1:
    ...         return n
    ...     else:
    ...         return n * factorial(n-1)

[Simple example of recursion on strings.]

Generating all permutations of words, to check which ones are
grammatical:

    >>> def perms(seq):
    ...     if len(seq) <= 1:
    ...         yield seq
    ...     else:
    ...         for perm in perms(seq[1:]):
    ...             for i in range(len(perm)+1):
    ...                 yield perm[:i] + seq[0:1] + perm[i:]
    >>> list(perms(['police', 'fish', 'cream']))
    [['police', 'fish', 'cream'], ['fish', 'police', 'cream'],
     ['fish', 'cream', 'police'], ['police', 'cream', 'fish'],
     ['cream', 'police', 'fish'], ['cream', 'fish', 'police']]

Deeply Nested Objects (notes)
-----------------------------

We can use recursive functions to build deeply-nested objects.
Building a letter trie, Figure trie_.

.. pylisting:: trie
   :caption: Building a Letter Trie

   def insert(trie, key, value):
       if key:
           first, rest = key[0], key[1:]
           if first not in trie:
               trie[first] = {}
           insert(trie[first], rest, value)
       else:
           trie['value'] = value

   >>> trie = {}
   >>> insert(trie, 'chat', 'cat')
   >>> insert(trie, 'chien', 'dog')
   >>> trie['c']['h']
   {'a': {'t': {'value': 'cat'}}, 'i': {'e': {'n': {'value': 'dog'}}}}
   >>> trie['c']['h']['a']['t']['value']
   'cat'
   >>> pprint.pprint(trie)
   {'c': {'h': {'a': {'t': {'value': 'cat'}},
                'i': {'e': {'n': {'value': 'dog'}}}}}}


Dynamic Programming
-------------------

Dynamic programming is a general technique for designing algorithms
which is widely used in natural language processing.  The term
'programming' is used in a different sense to what you might expect,
to mean planning or scheduling.  Dynamic programming is used when a
problem contains overlapping sub-problems.  Instead of computing
solutions to these sub-problems repeatedly, we simply store them in a
lookup table.
In the remainder of this section we will introduce dynamic programming,
but in a rather different context to syntactic parsing.

Pingala was an Indian author who lived around the 5th century B.C.,
and wrote a treatise on Sanskrit prosody called the *Chandas Shastra*.
Virahanka extended this work around the 6th century A.D., studying the
number of ways of combining short and long syllables to create a meter
of length *n*.  He found, for example, that there are five ways to
construct a meter of length 4: *V*\ :subscript:`4` = *{LL, SSL, SLS,
LSS, SSSS}*.  Observe that we can split *V*\ :subscript:`4` into two
subsets, those starting with *L* and those starting with
*S*, as shown in v4_.


.. _v4:
.. ex::
   .. parsed-literal::

    *V*\ :subscript:`4` =
      LL, LSS
        i.e. L prefixed to each item of *V*\ :subscript:`2` = {L, SS}
      SSL, SLS, SSSS
        i.e. S prefixed to each item of *V*\ :subscript:`3` = {SL, LS, SSS}

.. pylisting:: virahanka
   :caption: Three Ways to Compute Sanskrit Meter

   def virahanka1(n):
       if n == 0:
           return [""]
       elif n == 1:
           return ["S"]
       else:
           s = ["S" + prosody for prosody in virahanka1(n-1)]
           l = ["L" + prosody for prosody in virahanka1(n-2)]
           return s + l

   def virahanka2(n):
       lookup = [[""], ["S"]]
       for i in range(n-1):
           s = ["S" + prosody for prosody in lookup[i+1]]
           l = ["L" + prosody for prosody in lookup[i]]
           lookup.append(s + l)
       return lookup[n]

   def virahanka3(n, lookup={0:[""], 1:["S"]}):
       if n not in lookup:
           s = ["S" + prosody for prosody in virahanka3(n-1)]
           l = ["L" + prosody for prosody in virahanka3(n-2)]
           lookup[n] = s + l
       return lookup[n]

   from nltk import memoize
   @memoize
   def virahanka4(n):
       if n == 0:
           return [""]
       elif n == 1:
           return ["S"]
       else:
           s = ["S" + prosody for prosody in virahanka4(n-1)]
           l = ["L" + prosody for prosody in virahanka4(n-2)]
           return s + l

   >>> virahanka1(4)
   ['SSSS', 'SSL', 'SLS', 'LSS', 'LL']  
   >>> virahanka2(4)
   ['SSSS', 'SSL', 'SLS', 'LSS', 'LL']  
   >>> virahanka3(4)
   ['SSSS', 'SSL', 'SLS', 'LSS', 'LL']  
   >>> virahanka4(4)
   ['SSSS', 'SSL', 'SLS', 'LSS', 'LL']  

|nopar| 
With this observation, we can write a little recursive function called
``virahanka1()`` to compute these meters, shown in Figure virahanka_.
Notice that, in order to compute *V*\ :subscript:`4` we first compute
*V*\ :subscript:`3` and *V*\ :subscript:`2`.  But to compute *V*\ :subscript:`3`,
we need to first compute *V*\ :subscript:`2` and *V*\ :subscript:`1`.  This `call
structure`:dt: is depicted in call-structure_.

.. _call-structure:
.. ex:: 
   .. tree:: (V4 (V3 (V2 V1 V0) V1) (V2 V1 V0))

|nopar| 
As you can see, *V*\ :subscript:`2` is computed twice.
This might not seem like a significant problem, but 
it turns out to be rather wasteful as *n* gets large:
to compute *V*\ :subscript:`20` using this recursive technique, we
would compute *V*\ :subscript:`2` 4,181 times;
and for *V*\ :subscript:`40` we would compute *V*\ :subscript:`2` 63,245,986 times!
A much better alternative is to store the value of *V*\ :subscript:`2` in a table
and look it up whenever we need it.  The same goes for other values, such
as *V*\ :subscript:`3` and so on.  Function ``virahanka2()`` implements a
dynamic programming approach to the problem.  It works by filling up a
table (called ``lookup``) with solutions to *all* smaller instances of the
problem, stopping as soon as we reach the value we're interested in.
At this point we read off the value and return it.  Crucially, each
sub-problem is only ever solved once.

Notice that the approach taken in ``virahanka2()`` is to solve smaller
problems on the way to solving larger problems.  Accordingly, this is known as the
`bottom-up`:dt: approach to dynamic programming.  Unfortunately it turns out
to be quite wasteful for some applications, since it
may compute solutions to sub-problems that are never required for
solving the main problem.  This wasted computation can be avoided
using the `top-down`:dt: approach to dynamic programming, which is
illustrated in the function ``virahanka3()`` in Figure virahanka_.
Unlike the bottom-up approach, this approach is recursive.  It avoids
the huge wastage of ``virahanka1()`` by checking whether it has
previously stored the result.  If not, it computes the result
recursively and stores it in the table.  The last step is to return
the stored result.  The final method is to use a Python `decorator`:dt:
called ``memoize``, which takes care of the housekeeping work done
by ``virahanka3()`` without cluttering up the program.

This concludes our brief introduction to dynamic programming.
We will encounter it again in Chapter chap-advanced-parsing_.

.. note:: Dynamic programming is a kind of `memoization`:dt:.
   A memoized function stores results of previous calls to the
   function along with the supplied parameters.  If the function is
   subsequently called with those parameters, it returns the
   stored result instead of recalculating it.

Timing (notes)
--------------

We can easily test the efficiency gains made by the use of dynamic programming,
or any other putative performance enhancement, using the ``timeit`` module:

.. doctest-ignore::
    >>> from timeit import Timer
    >>> Timer("PYTHON CODE", "INITIALIZATION CODE").timeit()

[MORE]


---------------
Further Reading
---------------

[Harel2004]_

[Levitin2004]_

http://docs.python.org/lib/typesseq-strings.html

---------
Exercises
---------

#. |easy| Find out more about sequence objects using Python's help facility.
   In the interpreter, type ``help(str)``, ``help(list)``, and ``help(tuple)``.
   This will give you a full list of the functions supported by each type.
   Some functions have special names flanked with underscore; as the
   help documentation shows, each such function corresponds to something
   more familiar.  For example ``x.__getitem__(y)`` is just a long-winded
   way of saying ``x[y]``.

#. |easy| Identify three operations that can be performed on both tuples
   and lists.  Identify three list operations that cannot be performed on
   tuples.  Name a context where using a list instead of a tuple generates
   a Python error.

#. |easy| Find out how to create a tuple consisting of a single item.
   There are at least two ways to do this.

#. |easy| Create a list ``words = ['is', 'NLP', 'fun', '?']``.  Use
   a series of assignment statements (e.g. ``words[1] = words[2]``)
   and a temporary variable ``tmp`` to transform this list into the
   list ``['NLP', 'is', 'fun', '!']``.  Now do the same transformation
   using tuple assignment.

#. |easy| Does the method for creating a sliding window of n-grams
   behave correctly for the two limiting cases: `n`:math: = 1, and `n`:math: = ``len(sent)``?

#. |easy| Create two dictionaries, ``d1`` and ``d2``, and add some entries to
   each.  Now issue the command ``d1.update(d2)``.  What did this do?
   What might it be useful for?

#. |easy| We pointed out that when empty strings and empty lists occur
   in the condition part of an ``if`` clause, they evaluate to
   false. In this case, they are said to be occuring in a `Boolean
   context`:dt:.
   Experiment with different kind of non-Boolean expressions in Boolean
   contexts, and see whether they evaluate as true or false.

#. |soso| Create a list of words and store it in a variable ``sent1``.
   Now assign ``sent2 = sent1``.  Modify one of the items in ``sent1``
   and verify that ``sent2`` has changed.

   a) Now try the same exercise but instead assign ``sent2 = sent1[:]``.
      Modify ``sent1`` again and see what happens to ``sent2``.  Explain.
   b) Now define ``text1`` to be a list of lists of strings (e.g. to
      represent a text consisting of multiple sentences.  Now assign
      ``text2 = text1[:]``, assign a new value to one of the words,
      e.g. ``text1[1][1] = 'Monty'``.  Check what this did to ``text2``.
      Explain.
   c) Load Python's ``deepcopy()`` function (i.e. ``from copy import deepcopy``),
      consult its documentation, and test that it makes a fresh copy of any
      object.

#. |soso| Write code that starts with a string of words and
   results in a new string consisting of the same words, but where
   the first word swaps places with the second, and so on. For
   example, ``'the cat sat on the mat'`` will be converted into ``'cat
   the on sat mat the'``.

#. |soso| Initialize an *n*\ -by-*m* list of lists of empty strings using list
   multiplication, e.g. ``word_table = [[''] * n] * m``.  What happens
   when you set one of its values, e.g. ``word_table[1][2] = "hello"``?
   Explain why this happens.  Now write an expression using ``range()``
   to construct a list of lists, and show that it does not have this problem.

#. |soso| Write code to initialize a two-dimensional array of sets called
   ``word_vowels`` and process a list of words, adding each
   word to ``word_vowels[l][v]`` where ``l`` is the length of the word and ``v`` is
   the number of vowels it contains.

#. |soso| Write a function ``novel10(text)`` that prints any word that
   appeared in the last 10% of a text that had not been encountered earlier.

#. |soso| Write a program that takes a sentence expressed as a single string,
   splits it and counts up the words.  Get it to print out each word and the
   word's frequency, one per line, in alphabetical order.

#. |soso| Write code that builds a dictionary of dictionaries of sets.

#. |soso| Use ``sorted()`` and ``set()`` to get a sorted list of tags used in the Brown
   corpus, removing duplicates.

#. |soso| Write code to convert text into *hAck3r*, where characters are
   mapped according to the following table:

   +---------+---+---+---+----+---+--------+-----+
   | Input:  | e | i | o | l  | s | .      | ate |
   +---------+---+---+---+----+---+--------+-----+
   | Output: | 3 | 1 | 0 | \| | 5 | 5w33t! | 8   |
   +---------+---+---+---+----+---+--------+-----+

#. |soso| Read up on Gematria, a method for assigning numbers to words, and for
   mapping between words having the same number to discover the hidden meaning of
   texts (``http://en.wikipedia.org/wiki/Gematria``, ``http://essenes.net/gemcal.htm``).

   a) Write a function ``gematria()`` that sums the numerical values of
      the letters of a word, according to the letter values in ``letter_vals``:

      letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,
       'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,
       'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}

   b) Process a corpus (e.g. ``nltk.corpus.state_union``) and for each document, count how
      many of its words have the number 666.

   c) Write a function ``decode()`` to process a text, randomly replacing words with
      their Gematria equivalents, in order to discover the "hidden meaning" of the text.

#. |soso| Write a function ``shorten(text, n)`` to process a text, omitting the ``n``
   most frequently occurring words of the text.  How readable is it?

#. |easy| Write code that removes whitespace at the beginning and end of a
   string, and normalizes whitespace between words to be a single
   space character.

   #) do this task using ``split()`` and ``join()``

   #) do this task using regular expression substitutions

#. |easy| What happens when the formatting strings ``%6s`` and ``%-6s``
   are used to display strings that are longer than six characters?

#. |easy| We can use a dictionary to specify the values to be
   substituted into a formatting string.  Read Python's library
   documentation for formatting strings
   (`http://docs.python.org/lib/typesseq-strings.html
   <http://docs.python.org/lib/typesseq-strings.html>`_),
   and use this method to display today's date in two
   different formats.

#. |soso| Figure baseline-tagger_ in Chapter chap-tag_ plotted a curve showing
   change in the performance of a lookup tagger as the model size was increased.
   Plot the performance curve for a unigram tagger, as the amount of training
   data is varied.

#. |easy| Review the answers that you gave for the exercises in sec-back-to-the-basics_,
   and rewrite the code as one or more functions.

#. |soso| In this section we saw examples of some special functions such as ``filter()`` and
   ``map()``.  Other functions in this family are ``zip()`` and ``reduce()``.
   Find out what these do, and write some code to try them out.
   What uses might they have in language processing?

#. |soso| Write a function that takes a list of words (containing duplicates) and
   returns a list of words (with no duplicates) sorted by decreasing frequency.
   E.g. if the input list contained 10 instances of the word ``table`` and 9 instances
   of the word ``chair``, then ``table`` would appear before ``chair`` in the output
   list.

#. |soso| Write a function that takes a text and a vocabulary as its arguments
   and returns the set of words that appear in the text but not in the
   vocabulary.  Both arguments can be represented as lists of strings.
   Can you do this in a single line, using ``set.difference()``?

#. |soso| As you saw, ``zip()`` combines two lists into a single list
   of pairs. What happens when the lists are of unequal lengths?
   Define a function ``myzip()`` that does something different with
   unequal lists.

#. |soso| Import the ``itemgetter()`` function from the ``operator`` module in Python's
   standard library (i.e. ``from operator import itemgetter``).  Create a list
   ``words`` containing several words.  Now try calling:
   ``sorted(words, key=itemgetter(1))``, and ``sorted(words, key=itemgetter(-1))``.
   Explain what ``itemgetter()`` is doing.

#. |soso| Write a recursive function ``lookup(trie, key)`` that looks up a key in a trie,
   and returns the value it finds.  Extend the function to return a word when it is uniquely
   determined by its prefix (e.g. ``vanguard`` is the only word that starts with ``vang-``,
   so ``lookup(trie, 'vang')`` should return the same thing as ``lookup(trie, 'vanguard')``).

#. |soso| Read about string edit distance and the Levenshtein Algorithm.
   Try the implementation provided in ``nltk.edit_dist()``.
   How is this using dynamic programming?  Does it use the bottom-up or
   top-down approach?
   [See also ``http://norvig.com/spell-correct.html``]

#. |soso| The Catalan numbers arise in many applications of combinatorial mathematics,
   including the counting of parse trees (Chapter chap-advanced-parsing_).  The series
   can be defined as follows: C\ :subscript:`0` = 1, and
   C\ :subscript:`n+1` = |Sigma|\ :subscript:`0..n` (C\ :subscript:`i`\ C\ :subscript:`n-i`).

   a) Write a recursive function to compute `n`:math:\ th Catalan number C\ :subscript:`n`

   b) Now write another function that does this computation using dynamic programming

   c) Use the ``timeit`` module to compare the performance of these functions as `n`:math:
      increases.

#. |hard| **Authorship identification:**
   Reproduce some of the results of [Zhao07]_.

#. |hard| **Gender-specific lexical choice:**
   Reproduce some of the results of ``http://www.clintoneast.com/articles/words.php``


#. |hard| Write a recursive function that pretty prints a trie in alphabetically
   sorted order, as follows

   chat: 'cat'
   --ien: 'dog'
   -???: ???

#. |hard| Write a recursive function that processes text, locating the uniqueness point in
   each word, and discarding the remainder of each word.  How much compression does this
   give?  How readable is the resulting text?

.. include:: footer.rst

.. #. |hard| Extend the example in Figure compound-keys_ in the following ways:

   a) Define two sets ``verbs`` and ``preps``, and add each verb and preposition
      as they are encountered.  (Note that you can add an item to a set without
      bothering to check whether it is already present.)

   b) Create nested loops to display the results, iterating over verbs and
      prepositions in sorted order.  Generate one line of output per verb,
      listing prepositions and attachment ratios as follows:
      ``raised: about 0:3, at 1:0, by 9:0, for 3:6, from 5:0, in 5:5...``

   c) We used a tuple to represent a compound key consisting of two strings.
      However, we could have simply concatenated the strings, e.g.
      ``key = verb + ":" + prep``, resulting in a simple string key.
      Why is it better to use tuples for compound keys?

