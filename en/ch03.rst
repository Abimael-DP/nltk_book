.. -*- mode: rst -*-
.. include:: ../definitions.rst
.. include:: regexp-defns.rst

.. standard global imports

    >>> import nltk, re, pprint

.. TODO: more on regular expressions, including ()
.. TODO: talk about fact that English lexicon is open set (e.g. malware = malicious software)
.. TODO: add pointers to regexp toolkits (e.g. Kodos)
.. TODO: other issues
   - nltk.corpus.brown.items returns a tuple, not a list (cf discussion in ch 6)
   - invocation of pprint.pprint is a little clunky
   - regexp_tokenize() doesn't work when it is given a compiled pattern
.. TODO: add more graphical plots
.. TODO: map and reduce
.. FreqDist of CHARACTER BIGRAMS...
.. TODO: corpus of word frequencies, so we can do certain tasks on the n most frequent words
.. TODO: plain wsj corpus
.. TODO: cover tag soup when talking about HTML
.. TODO: type conversion using int(), list(), etc
.. TODO: vowel harmony example: extract vowel sequence using re.findall; extract bigrams from the
   vowel sequences; then build a conditional frequency distribution
.. TODO: spelling correction (a kind of normalization);
   Soundex -- indexing strings in a dictionary (key = soundex code; value = list of words)
.. TODO: indexing a text with the help of wordnet hypernyms

.. _chap-words:

======================
3. Processing Raw Text
======================

The most important source of texts is undoubtedly the Web.  It's convenient
to have existing text collections to explore, such as the corpora we saw
in the previous chapters.  However, you probably have your own text sources
in mind, and need to learn how to access them.

The goal of this chapter is to answer the following questions:

#. How can we write programs to access text from local files and
   from the web, in order to get hold of an unlimited range of
   language material?
#. How can we split documents up into individual words and
   punctuation symbols, so we can do the same kinds of
   analysis we did with text corpora in earlier chapters?
#. What features of the Python programming language are
   needed to do this?

In order to address these questions, we will be covering
key concepts in NLP, including tokenization and stemming.
Along the way you will consolidate your Python knowledge and
learn about strings, files, and regular expressions.  Since
so much text on the web is in |HTML| format, we will also
see how to dispense with markup.

.. Note:: From this chapter onwards, our program samples will assume you
   begin your interactive session or your program with the following import
   statment: ``import nltk, re, pprint`` 

.. _sec-accessing-text:

-----------------------------------------
Accessing Text from the Web and from Disk
-----------------------------------------

Electronic Books
----------------

A small sample of texts from Project Gutenberg appears in the |NLTK| corpus collection.
However, you may be interested in analyzing other texts from Project Gutenberg.
You can browse the catalog of 25,000 free online books at
``http://www.gutenberg.org/catalog/``, and obtain a URL to an ASCII text file.
Although 90% of the texts in Project Gutenberg are in English, it
includes material in over 50 other languages, including Catalan, Chinese, Dutch,
Finnish, French, German, Italian, Portuguese and Spanish (with more than
100 texts each).

Text number 2554 is an English translation of *Crime and Punishment*,
and we can access it as follows:

    >>> from urllib import urlopen
    >>> url = "http://www.gutenberg.org/files/2554/2554.txt"
    >>> raw = urlopen(url).read()
    >>> type(raw)
    <type 'str'>
    >>> len(raw)
    1176831
    >>> raw[:75]
    'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n'

.. note:: The ``read()`` process will take a few seconds as it downloads this large book.
   If you're using an internet proxy which is not correctly detected by Python,
   you may need to specify the proxy manually as follows:
   
       >>> proxies = {'http': 'http://www.someproxy.com:3128'}
       >>> raw = urllib.urlopen(url, proxies=proxies).read()
   
The variable ``raw`` contains a string with 1,176,831 characters.  This is the raw
content of the book, including many details we are not interested in such as
whitespace, line breaks and blank lines.  Instead, we want to break it up into
words and punctuation, as we saw in Chapter chap-introduction_.  This step is
called `tokenization`:dt:, and it produces our familiar structure, a list of words
and punctuation.  From now on we will call these `tokens`:dt:.

    >>> tokens = nltk.word_tokenize(raw)
    >>> type(tokens)
    <type 'list'>
    >>> len(tokens)
    255809
    >>> tokens[:10]
    ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']

If we now take the further step of creating an |NLTK| text from this
list, we can carry out all of the other linguistic processing we saw
in Chapter chap-introduction_, along with the regular list operations
like slicing:

    >>> text = nltk.Text(tokens)
    >>> type(text)
    <type 'nltk.text.Text'>
    >>> text[1020:1060]
    ['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',
    'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',
    'which', 'he', 'lodged', 'in', 'S', '.', 'Place', 'and', 'walked', 'slowly',
    ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K', '.', 'bridge', '.']
    >>> text.collocations()
    Katerina Ivanovna; Pulcheria Alexandrovna; Avdotya Romanovna; Pyotr
    Petrovitch; Project Gutenberg; Marfa Petrovna; Rodion Romanovitch;
    Sofya Semyonovna; Nikodim Fomitch; did not; Hay Market; Andrey
    Semyonovitch; old woman; Literary Archive; Dmitri Prokofitch; great
    deal; United States; Praskovya Pavlovna; Porfiry Petrovitch; ear rings

Notice that `Project Gutenberg`:lx: appears as a collocation.
This is because each text downloaded from Project Gutenberg contains a header with the
name of the text, the author, the names of people who scanned and
corrected the text, a license, and so on.  Sometimes this information
appears in a footer at the end of the file.  We cannot reliably detect
where the content begins and ends, and so have to resort to manual
inspection of the file, to discover unique strings that mark the beginning
and the end, before trimming ``raw`` to be just the content and nothing else:

    >>> raw.find("PART I")
    5303
    >>> raw.rfind("End of Project Gutenberg's Crime")
    1157681
    >>> raw = raw[5303:1157681]

The ``find()`` and ``rfind()`` ("reverse find") functions help us get
the right index values.  Now the raw text begins with "PART I", and
goes up to (but not including) the phrase that marks the end of the
content.

This was our first brush with reality: texts found on the web may contain
unwanted material, and there may not be an automatic way to remove it.
But with a small amount of extra work we can extract the material we need.

Dealing with HTML
-----------------

Much of the text on the web is in the form of |HTML| documents.
You can use a web browser to save a page as text to a local
file, then access this as described in the section on files below.
However, if you're going to do this a lot, its easiest to get Python
to do the work directly.  The first step is the same as before,
using ``urlopen``.  For fun we'll pick a BBC News story
called *Blondes to die out in 200 years*, an urban legend
reported as established scientific fact:

    >>> url = "http://news.bbc.co.uk/2/hi/health/2284783.stm"
    >>> html = urlopen(url).read()
    >>> html[:60]
    '<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN'

You can type ``print html`` to see the |HTML| content in all its glory,
including meta tags, an image map, JavaScript, forms, and tables.

Getting text out of |HTML| is a sufficiently common task that |NLTK| provides
a helper function ``nltk.clean_html()``, which takes an |HTML| string and
returns raw text.  We can then tokenize this to get our familiar text structure:

    >>> raw = nltk.clean_html(html)
    >>> tokens = nltk.word_tokenize(raw)
    >>> tokens
    ['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', "'", 'to', 'die', 'out', ...]

This still contains unwanted material concerning site navigation and related
stories.  With some trial and error you can find the start and end indexes of the
content and select the tokens of interest, and initialize a text as before.

    >>> tokens = tokens[96:399]
    >>> text = nltk.Text(tokens)
    >>> text.concordance('gene')
     they say too few people now carry the gene for blondes to last beyond the next tw
    t blonde hair is caused by a recessive gene . In order for a child to have blonde 
    to have blonde hair , it must have the gene on both sides of the family in the gra
    there is a disadvantage of having that gene or by chance . They don ' t disappear 
    ondes would disappear is if having the gene was a disadvantage and I do not think 

.. note::
   For more sophisticated processing of |HTML|, use the *Beautiful Soup* package,
   available from ``http://www.crummy.com/software/BeautifulSoup/``

Processing Search Engine Results
--------------------------------

The web can be thought of as a huge corpus of unannotated text.  Web
search engines provide an efficient means of searching this large
quantity of text for relevant linguistic examples.  The main advantage
or search engines is size: since you are searching such a large set of
documents, you are more likely to find any linguistic pattern you
are interested in.  Furthermore, you can make use of very specific
patterns, which would only match one or two examples on a smaller
example, but which might match tens of thousands of examples when run
on the web.  A second advantage of web search engines is that they are
very easy to use.  Thus, they provide a very convenient tool for
quickly checking a theory, to see if it is reasonable.

[Accessing a search engine programmatically: search results; counts;
Python code to produce the contents of Table absolutely_; mention
Google API and xref to discussion of this in Chapter chap-data_.]

.. table:: absolutely

   ================  ===========  ==========  ==========  ============
   Google hits       `adore`:lx:  `love`:lx:  `like`:lx:  `prefer`:lx:
   ================  ===========  ==========  ==========  ============
   `absolutely`:lx:      289,000     905,000      16,200           644
   `definitely`:lx:        1,460      51,000     158,000        62,600
   ratio                   198:1        18:1        1:10          1:97
   ================  ===========  ==========  ==========  ============

   `Absolutely`:lx: vs `Definitely`:lx: (Liberman 2005, LanguageLog.org)

Unfortunately, search engines have some significant shortcomings.
First, the allowable range of search patterns is severely restricted.
Unlike local corpora, where you write programs to search for
arbitrarily complex patterns, search engines generally
only allow you to search for individual words or strings of
words, sometimes with wildcards.  Second, search engines give
inconsistent results, and can give widely different figures when used
at different times or in different locations.  When content has been
duplicated across multiple sites, search results may be boosted.
Finally, the markup the result returned by a search engine may change unpredictably,
breaking any pattern-based method of locating particular content.

.. note:: |TRY|
   Search the web for ``"the of"`` (inside quotes).  Based on the large
   count, can we conclude that `the of`:lx: is a frequent collocation
   in English?

Processing RSS Feeds
--------------------

The blogosphere is an important source of text, in both formal and informal registers.
With the help of a third-party Python library called the *Universal Feed Parser*,
freely downloadable from ``http://feedparser.org/``, we can easily access the content
of a blog, as shown below:

    >>> import feedparser
    >>> llog = feedparser.parse("http://languagelog.ldc.upenn.edu/nll/?feed=atom")
    >>> llog['feed']['title']
    u'Language Log'
    >>> len(llog.entries)
    15
    >>> post = llog.entries[2]
    >>> post.title
    u"He's My BF"
    >>> content = post.content[0].value
    >>> content[:70]
    u'<p>Today I was chatting with three of our visiting graduate students f'
    >>> nltk.word_tokenize(nltk.html_clean(content))
    >>> nltk.word_tokenize(nltk.clean_html(llog.entries[2].content[0].value))
    [u'Today', u'I', u'was', u'chatting', u'with', u'three', u'of', u'our', u'visiting',
    u'graduate', u'students', u'from', u'the', u'PRC', u'.', u'Thinking', u'that', u'I',
    u'was', u'being', u'au', u'courant', u',', u'I', u'mentioned', u'the', u'expression',
    u'DUI4XIANG4', u'\u5c0d\u8c61', u'("', u'boy', u'/', u'girl', u'friend', u'"', ...]


Reading Local Files
-------------------

.. Monkey-patching to fake the file/web examples in this section:

    >>> from StringIO import StringIO
    >>> def fake_open(filename, mode=None):
    ...     return StringIO('Time flies like an arrow.\nFruit flies like a banana.\n')
    >>> def fake_urlopen(url):
    ...     return StringIO('<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN"')
    >>> open = fake_open
    >>> import urllib
    >>> urllib.urlopen = fake_urlopen

.. note:: |TRY|
   Create a file called ``document.txt`` using a text editor, and type in a few lines of text,
   and save it as plain text.
   If you are using |IDLE|, select the *New Window* command in the *File* menu, typing
   the required text into this window, and then saving the file as
   ``doc.txt`` inside the directory that |IDLE| offers in the pop-up dialogue box.
   Next, in the Python interpreter, open the file using ``f = open('doc.txt')``, then
   inspect its contents using ``print f.read()``.

Various things might have gone wrong when you tried this.
If the interpreter couldn't find your file, you would have seen an
error like this:

.. doctest-ignore::
    >>> f = open('document.txt')
    Traceback (most recent call last):
    File "<pyshell#7>", line 1, in -toplevel-
    f = open('document.txt')
    IOError: [Errno 2] No such file or directory: 'document.txt'

To check that the file that you are trying to open is really in the
right directory, use |IDLE|\ 's *Open* command in the *File* menu;
this will display a list of all the files in the directory where
|IDLE| is running. An alternative is to examine the current
directory from within Python:

.. doctest-ignore::
    >>> import os
    >>> os.listdir('.')

Another possible problem you might have encountered when accessing a text file
is the newline conventions, which are different for different operating systems. 
The built-in ``open()`` function has a second parameter for controlling how
the file is opened: ``open('document.txt', 'rU')`` |mdash|
``'r'`` means to open the file for reading (the default), and
``'U'`` stands for "Universal", which lets us ignore the different
conventions used for marking newlines.

Assuming that you can open the file, there are several methods for reading it.
The ``read()`` method creates a string with the contents of the entire file:

.. doctest-ignore::
    >>> f.read() 
    'Time flies like an arrow.\nFruit flies like a banana.\n'

Recall that the ``'\n'`` characters are `newlines`:dt:\ ; this
is equivalent to pressing *Enter* on a keyboard and starting a new line. 

We can also read a file one line at a time using a ``for`` loop:

    >>> f = open('document.txt', 'rU')
    >>> for line in f:
    ...     print line.strip()
    Time flies like an arrow.
    Fruit flies like a banana.

Here we use the ``strip()`` function to remove the newline character at the end of
the input line.

|NLTK|\ 's corpus files can also be accessed using these methods.  We simply
have to use ``nltk.data.find()`` to get the filename for any corpus item.
Then we can open it in the usual way:

    >>> path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')
    >>> raw = open(path, 'rU').read()

Extracting Text from PDF, MSWord and other Binary Formats
---------------------------------------------------------

ASCII text and |HTML| text are human readable formats.  Text often comes in binary
formats |mdash| like PDF and MSWord |mdash| that can only be opened using specialized
software.  Third-party libraries such as ``pypdf`` and ``pywin32`` can be used to access
these formats.  Extracting text from multi-column documents can be particularly
challenging.  For once-off conversion of a few documents,
it is simpler to open the document with a suitable application, then save it as text
to your local drive, and access it as described below.
If the document is already on the web, you can enter its URL in Google's search box.
The search result often includes a link to an |HTML| version of the document,
which you can save as text.  

Getting User Input
------------------

Another source of text is a user interacting with our program.  We can prompt the user
to type a line of input using the Python function ``raw_input()``.  We can save that to a variable and
manipulate it just as we have done for other strings.

    >>> s = raw_input("Enter some text: ")
    Enter some text: On an exceptionally hot evening early in July
    >>> print "You typed", len(nltk.word_tokenize(s)), "words."
    You typed 8 words.

Summary
-------

Figure fig-pipeline1_ summarizes what we have covered in this section, including the process
of building a vocabulary that we saw in Chapter chap-introduction_.  (One step, normalization,
will be discussed in section sec-normalizing-text_).

.. _fig-pipeline1:
.. figure:: ../images/pipeline1.png
   :scale: 30:120:30

   The Processing Pipeline

There's a lot going on in this pipeline.  To understand it properly, it helps to be
clear about the type of each variable that it mentions.  We find out the type
of any Python object ``x`` using ``type(x)``, e.g. ``type(1)`` is ``<int>``
since ``1`` is an integer.

When we load the contents of a URL or file, and when we strip out HTML markup,
we are dealing with strings, Python's ``<str>`` data type
(We will learn more about strings in section sec-strings_):

    >>> raw = open('document.txt').read()
    >>> type(raw)
    <type 'str'>

When we tokenize a string we produce a list (of words), and this is Python's ``<list>``
type.  Normalizing and sorting lists produces other lists:

    >>> tokens = nltk.word_tokenize(raw)
    >>> type(tokens)
    <type 'list'> 
    >>> words = [w.lower() for w in tokens]
    >>> type(words)
    <type 'list'>
    >>> vocab = sorted(set(words))
    >>> type(vocab)
    <type 'list'>

The type of an object determines what operations you can perform on it.
So, for example, we can append to a list but not to a string:

    >>> vocab.append('blog')
    >>> raw.append('blog')
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    AttributeError: 'str' object has no attribute 'append'

Similarly, we can concatenate strings with strings, and lists with
lists, but we cannot concatenate strings with lists:

    >>> query = 'Who knows?'
    >>> beatles = ['john', 'paul', 'george', 'ringo']
    >>> query + beatles
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: cannot concatenate 'str' and 'list' objects

You may also have noticed that our analogy between operations on
strings and numbers works for multiplication and addition, but not
subtraction or division:

    >>> 'very' - 'y'
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: unsupported operand type(s) for -: 'str' and 'str'
    >>> 'very' / 2
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: unsupported operand type(s) for /: 'str' and 'int'

These error messages are another example of Python telling us that we
have got our data types in a muddle. In the first case, we are told
that the operation of substraction (i.e., ``-``) cannot apply to
objects of type ``str`` (strings), while in the second, we are told that
division cannot take ``str`` and ``int`` as its two operands.

.. _sec-strings:

--------------------------------------------
Strings: Text Processing at the Lowest Level
--------------------------------------------

It's time to study a fundamental data type that we've been studiously avoiding
so far.  In earlier chapters we focussed on a text as a list of words.  We didn't
look too closely at words and how they are handled in the programming
language.  By using |NLTK|\ 's corpus interface we were able to ignore
the files that these texts had come from.  The contents of a word, and
of a file, are represented by programming languages as a fundamental
data type known as a `string`:dt:.  In this section we explore strings
in detail, and show the connection between strings, words, texts and files.

Basic Operations with Strings
-----------------------------

.. doctest-ignore::
    >>> monty = 'Monty Python'
    >>> monty
    'Monty Python'
    >>> circus = 'Monty Python's Flying Circus'
      File "<stdin>", line 1
        circus = 'Monty Python's Flying Circus'
                               ^
    SyntaxError: invalid syntax
    >>> circus = "Monty Python's Flying Circus"
    >>> circus
    "Monty Python's Flying Circus"
     
The ``+`` operation can be used with strings, and is known as `concatenation`:dt:.
It produces a new string that is a copy of the
two original strings pasted together end-to-end.  Notice that
concatenation doesn't do anything clever like insert a space between
the words.  The Python interpreter has no way of knowing that you want
a space; it does `exactly`:em: what it is told.  Given the example
of ``+``, you might be able guess what multiplication will do:

    >>> 'very' + 'very' + 'very'
    'veryveryvery'
    >>> 'very' * 3
    'veryveryvery'

.. caution:: Be careful to distinguish between the string ``' '``, which
   is a single whitespace character, and ``''``, which is the empty string.

Printing Strings
----------------

So far, when we have wanted to look at the contents of a variable or
see the result of a calculation, we have just typed the variable name
into the interpreter.  We can also see the contents of a variable
using the ``print`` statement:

    >>> print monty
    Monty Python

Notice that there are no quotation marks this time.  When we inspect
a variable by typing its name in the interpreter, the interpreter prints
the Python representation of its value.  Since it's a string,
the result is quoted.  However, when we tell the
interpreter to print the contents of the variable, we don't see
quotation characters since there are none inside the string.

The ``print`` statement allows us to display more than one item on a line
in various ways, as shown below:

    >>> grail = 'Holy Grail'
    >>> print monty + grail
    Monty PythonHoly Grail
    >>> print monty, grail
    Monty Python Holy Grail
    >>> print monty, "and the", grail
    Monty Python and the Holy Grail    
    
Accessing Individual Characters
-------------------------------

As we saw in Section sec-a-closer-look-at-python-texts-as-lists-of-words_ for lists, strings are indexed, starting from zero.
When we index a string, we get one of its characters (or letters):

    >>> monty[0]
    'M'
    >>> monty[3]
    't'
    >>> monty[5]
    ' '

As with lists, if we try to access an index that is outside of the string we get an error:

    >>> monty[20]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: string index out of range

Again as with lists, we can use negative indexes for strings,
where ``-1`` is the index of the last character.
Using positive and negative indexes, we have two ways to refer to
any position in a string.  In this case, when the string had a length of 12,
indexes ``5`` and ``-7`` both refer to the same character (a space), and:
``5 = len(monty) - 7``.

    >>> monty[-1]
    'n'
    >>> monty[-7]
    ' '

We can write ``for`` loops to iterate over the characters
in strings.  This ``print`` statement ends with a trailing comma, which is
how we tell Python not to print a newline at the end.

    >>> sent = 'colorless green ideas sleep furiously'
    >>> for char in sent:
    ...     print char,
    ... 
    c o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y

We can count individual characters as well.  We should ignore the case
distinction by normalizing everything to lowercase, and filter out non-alphabetic characters:

    >>> from nltk.corpus import gutenberg
    >>> raw = gutenberg.raw('melville-moby_dick.txt')
    >>> fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())
    >>> fdist.keys()
    ['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',
    'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']

This gives us the letters of the alphabet, with the most frequently occurring letters
listed first (this is quite complicated and we'll explain it more carefully below).
You might like to visualize the distribution using ``fdist.plot()``.
The relative character frequencies of a text can be used in automatically identifying
the language of the text.

Accessing Substrings
--------------------

A substring is any continuous section of a string that we want to pull out for
further processing.  We can easily access substrings using the same slice notation
we used for lists.
For example, the following code accesses the substring starting at index ``6``,
up to (but not including) index ``10``:

    >>> monty[6:10]
    'Pyth'

Here we see the characters are ``'P'``, ``'y'``, ``'t'``, and ``'h'`` which correspond
to ``monty[6]`` ... ``monty[9]`` but not ``monty[10]``. This is because
a slice `starts`:em: at the first index but finishes `one before`:em: the end index.

We can also slice with negative indices |mdash| the same basic rule of starting
from the start index and stopping one before the end index applies;
here we stop before the space character.

    >>> monty[0:-7]
    'Monty'

As with list slices, if we omit the first value, the substring begins at the start
of the string.  If we omit the second value, the substring continues to the end
of the string: 

    >>> monty[:5]
    'Monty'
    >>> monty[6:]
    'Python'

We can also find the position of a substring within a string, using ``find()``:

    >>> monty.find('Python')
    6

.. note:: |TRY|
   Make up a sentence and assign it to a variable, e.g. ``sent = 'my sentence...'``.
   Now write slice expressions to pull out individual words.  (This is obviously
   not a convenient way to process the words of a text!)

Analyzing Strings
-----------------

* character frequency plot, e.g get text in some language using ``language_x = nltk.corpus.udhr.raw(x)``,
  then construct its frequency distribution ``fdist = FreqDist(language_x)``, then
  view the distribution with ``fdist.keys()`` and ``fdist.plot()``.

* functions involving strings, e.g. determining past tense

* built-ins, ``find()``, ``rfind()``, ``index()``, ``rindex()``

* revisit string tests like ``endswith()`` from chapter 1

The Difference between Lists and Strings
----------------------------------------

Strings and lists are both kind of `sequence`:dt:.  We can pull them
apart by indexing and slicing them, and we can join them together
by concatenating them.  However, we cannot join strings and lists:

    >>> query = 'Who knows?'
    >>> beatles = ['John', 'Paul', 'George', 'Ringo']
    >>> query[2]
    'o'
    >>> beatles[2]
    'George'
    >>> query[:2]
    'Wh'
    >>> beatles[:2]
    ['John', 'Paul']
    >>> query + " I don't"
    "Who knows? I don't"
    >>> beatles + 'Brian'
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: can only concatenate list (not "str") to list
    >>> beatles + ['Brian']
    ['John', 'Paul', 'George', 'Ringo', 'Brian']

When we open a file
for reading into a Python program, we get a string
corresponding to the contents of the whole file. If we to use a ``for`` loop to
process the elements of this string, all we can pick out are the
individual characters |mdash| we don't get to choose the
granularity. By contrast, the elements of a list can be as big or
small as we like: for example, they could be paragraphs, sentence,
phrases, words, characters. So lists have the advantage that we
can be flexible about the elements they contain, and
correspondingly flexible about any downstream processing.
So one of the first things we are likely to do in a piece of |NLP|
code is tokenize a string into a list of strings (Section sec-tokenization_).
Conversely, when we want to write our results to a file, or to a terminal,
we will usually format them as a string (Section sec-formatting_).

Lists and strings do not have exactly the same functionality.
Lists have the added power that you can change their elements:

    >>> beatles[0] = "John Lennon"
    >>> del beatles[-1]
    >>> beatles
    ['John Lennon', 'Paul', 'George']

On the other hand if we try to do that with a *string*
|mdash| changing the 0th character in ``query`` to ``'F'`` |mdash| we get:

.. doctest-ignore::
    >>> query[0] = 'F'
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    TypeError: object does not support item assignment

|nopar| This is because strings are `immutable`:dt: |mdash| you can't change a
string once you have created it.  However, lists are `mutable`:dt:,
and their contents can be modified at any time.  As a result, lists
support operations that modify the original value rather than producing a new value.


.. _sec-regular-expressions-word-patterns:

-----------------------------------------------
Regular Expressions for Detecting Word Patterns
-----------------------------------------------

Many linguistic processing tasks involve pattern matching.
For example, we can find words ending with `ed`:lx: using
``endswith('ed')``.  We saw a variety of such "word tests"
in Figure word-tests_.
Regular expressions give us a more powerful and flexible
method for describing the character patterns we are interested in.

.. note::
   There are many other published introductions to regular expressions,
   organized around the syntax of regular expressions and applied to searching
   text files.  Instead of doing this again, we focus on the use of regular expressions
   at different stages of linguistic processing.  As usual, we'll adopt
   a problem-based approach and present new features only as they are
   needed to solve practical problems.  In our discussion we will mark
   regular expressions using chevrons like this: |patt|\ .

To use regular expressions in Python we need to import the ``re``
library using: ``import re``.  We also need a list of words to search;
we'll use the words corpus again (Section sec-lexical-resources_).  We
will preprocess it to remove any proper names.

    >>> import re
    >>> wordlist = [w for w in nltk.corpus.words.words() if w.islower()]

Using Basic Meta-Characters
---------------------------

Let's find words ending with `ed`:lx: using the regular expression |l|\ ``ed$``\ |r|.
We will use the ``re.search(p, s)`` function to check whether the pattern ``p`` can be found
somewhere inside the string ``s``. 
We need to specify the characters of interest, and use the dollar sign which has a
special behavior in the context of regular expressions in that it matches
the end of the word:

    >>> [w for w in wordlist if re.search('ed$', w)]
    ['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', ...]

The "``.``" `wildcard`:dt: symbol matches any single character.
Suppose we have room in a crossword puzzle for an 8-letter word
with `j`:lx: as its third letter and `t`:lx: as its sixth letter.
In place of each blank cell we use a period:

    >>> [w for w in wordlist if re.search('^..j..t..$', w)]
    ['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', ...]

.. note:: |TRY|
   The caret symbol ``^`` matches the start of the word, just like the ``$`` matches
   the end of the word.  What results to we get with the above example if we leave out
   both of these, and search for |l|\ ``..j..t..``\ |r|?

Finally, the "``?``" symbol specifies that the previous character is optional.
Thus |l|\ ``e-?mail``\ |r| will match both `email`:lx: and `e-mail`:lx:.
We could count the total number of occurrences of this word (in either spelling)
using ``len(w for w in text if re.search('e-?mail', w))``.

Ranges and Closures
-------------------

.. _fig-T9:
.. figure:: ../images/T9.png
   :scale: 20

   T9: Text on 9 Keys

The `T9`:dt: system is used for entering text on mobile phones.  Two or more words that
are entered using the same sequence of keystrokes are known as `textonyms`:dt:.
For example, both `hole`:lx: and `golf`:lx: are entered using 4653.  What other words
could be produced with the same sequence?  Here we use the regular expression
|l|\ ``^[ghi][mno][jlk][def]$``\ |r|:
 
    >>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]
    ['gold', 'golf', 'hold', 'hole']

The first part of the expression, |l|\ ``^[ghi]``\ |r|, matches the start of
a word followed by `g`:lx:, `h`:lx:, or `i`:lx:.  The next part of the expression,
|l|\ ``[mno]``\ |r|, constrains the second character to be
`m`:lx:, `n`:lx:, or `o`:lx:.  The third and fourth characters are also constrained.
Only six words satisfy all these constraints.
Note that the order of characters inside the square brackets is not significant, so we
could have written |l|\ ``^[hig][nom][ljk][fed]$``\ |r| and matched the same
words.

.. note:: |TRY|
   Look for some "finger-twisters", by searching for words that only use part
   of the number-pad.  For example |l|\ ``^[g-o]+$``\ |r| will match words
   that only use keys 4, 5, 6 in the center row, and |l|\ ``^[a-fj-o]+$``\ |r|
   will match words that use keys 2, 3, 5, 6 in the top-right corner.
   What do "``-``" and "``+``" mean?

Let's explore the "``+``" symbol a bit further.  Notice that it can be applied to
individual letters, or to bracketed sets of letters:

    >>> chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))
    >>> [w for w in chat_words if re.search('^m+i+n+e+$', w)]
    ['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine',
    'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']
    >>> [w for w in chat_words if re.search('^[ha]+$', w)]
    ['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh',
    'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa',
    'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', ...]

It should be clear that "``+``" simply means "one or more instances of the preceding item",
which could be an individual character like ``m``, a set like ``[fed]`` or a range like ``[d-f]``.
Now let's replace "``+``" with "``*``" which means "zero or more instances of the preceding item".
The regular expression |l|\ ``^m*i*n*e*$``\ |r| will match everything that we found using
|l|\ ``^m+i+n+e+$``\ |r|, but also words where some of the letters don't appear at all,
e.g. `me`:lx:, `min`:lx:, and `mmmmm`:lx:.
Note that the "``+``" and "``*``" symbols are sometimes referred to as `Kleene closures`:dt:,
or simply `closures`:dt:. 

The "``^``" operator has another function when it appears inside square brackets.  For
example |l|\ ``[^aeiouAEIOU]``\ |r| matches any character other than a vowel.
We can search the Chat corpus for words that are made up entirely of non-vowel
characters using |l|\ ``^[^aeiouAEIOU]+$``\ |r| to find items like these:
``:):):)``, ``grrr``, ``cyb3r`` and ``zzzzzzzz``.  Notice this includes non-alphabetic
characters.

.. note:: |TRY|
   Study the following examples and work out what the ``\``, ``{}`` and ``|`` notations mean:

      >>> wsj = sorted(set(nltk.corpus.treebank.words()))
      >>> [w for w in wsj if re.search('^[0-9]+\.[0-9]+$', w)]
      ['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5',
      '0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99',
      '1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', ...]
      >>> [w for w in wsj if re.search('^[A-Z]+\$$', w)]
      ['C$', 'US$']
      >>> [w for w in wsj if re.search('^[0-9]{4}$', w)]
      ['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ...]
      >>> [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]
      ['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', ...]
      >>> [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]
      ['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting',
      'savings-and-loan']
      >>> [w for w in wsj if re.search('(ed|ing)$', w)]
      ['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', ...] 

You probably worked out that a backslash means that the following character is
deprived of its special powers and must literally match a specific character in the
word.  Thus, while '``.``' is special, '``\.``' only matches a period.
The brace characters are used to specify the number of repeats of the previous item.

The meta-characters we have seen are summarized in Table regexp-meta-characters1_. 

.. table:: regexp-meta-characters1

   ===============  =======================================================================================
   Operator         Behavior
   ===============  =======================================================================================
   ``.``            Wildcard, matches any character
   ``^abc``         Matches some pattern `abc`:math: at the start of a string
   ``abc$``         Matches some pattern `abc`:math: at the end of a string
   ``[abc]``        Matches a set of characters
   ``[A-Z0-9]``     Matches a range of characters
   ``ed|ing|s``     Matches one of the specified strings (disjunction)
   ``*``            Zero or more of previous item, e.g. ``a*``, ``[a-z]*`` (also known as *Kleene Closure*)
   ``+``            One or more of previous item, e.g. ``a+``, ``[a-z]+``
   ``?``            Zero or one of the previous item (i.e. optional), e.g. ``a?``, ``[a-z]?``
   ``{n}``          Exactly `n`:math: repeats where n is a non-negative integer
   ``{m,n}``        At least `m`:math: and no more than `n`:math: repeats (`m`:math:, `n`:math: optional)
   ``(ab|c)+``      Parentheses that indicate the scope of the operators
   ===============  =======================================================================================

   Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures

.. _sec-useful-applications-of-regular-expressions:

------------------------------------------
Useful Applications of Regular Expressions
------------------------------------------

The above examples all involved searching for words `w`:math:
that match some regular expression `regexp`:lx: using ``re.search(regexp, w)``.
Apart from checking if a regular expression matches a word, we can use
regular expressions to extract material from words, or to modify words
in specific ways.

Extracting Word Pieces
----------------------

The `re.findall()`` ("find all") method finds all (non-overlapping)
matches of the given regular expression.  Let's find all the vowels in
a word, then count them:

    >>> word = 'supercalifragulisticexpialidocious'
    >>> re.findall('[aeiou]', word)
    ['u', 'e', 'a', 'i', 'a', 'u', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']
    >>> len(re.findall('[aeiou]', word))
    16

Let's look for all sequences of two or more vowels in some text,
and determine their relative frequency:

    >>> wsj = sorted(set(nltk.corpus.treebank.words()))
    >>> fd = nltk.FreqDist(vs for word in wsj
    ...                       for vs in re.findall('[aeiou]{2,}', word))
    >>> fd.items()
    [('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253),
    ('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95),
    ('ei', 86), ('oi', 65), ('oa', 59), ('eo', 39), ('iou', 27), ('eu', 18), ...]

.. note:: |TRY|
   In the W3C Date Time Format, dates are represented like this: 2009-12-31.
   Replace the ``?`` in the following Python code with a regular expression,
   in order to convert the string ``'2009-12-31'`` to a list of integers
   ``[2009, 12, 31]``.

   ``[int(n) for n in re.findall(?, '2009-12-31')]``

Doing More with Word Pieces
---------------------------

Once we can use ``re.findall()`` to extract material from words, there's
interesting things to do with the pieces, like glue them back together or
plot them.

It is sometimes noted that English text is highly redundant, and it is still
easy to read when word-internal vowels are left out.  For example,
`declaration`:lx: becomes `dclrtn`:lx:, and `inalienable`:lx: becomes `inlnble`:lx:,
retaining any initial or final vowel sequences.   This regular expression
matches initial vowel sequences, final vowel sequences, and all consonants;
everything else is ignored.  We use ``re.findall()`` to extract all the matching
pieces, and ``''.join()`` to join them together (see Section sec-formatting_ for
more about the join operation). 

    >>> regexp = '^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'
    >>> def compress(word):
    ...     pieces = re.findall(regexp, word)
    ...     return ''.join(pieces)
    ...
    >>> english_udhr = nltk.corpus.udhr.words('English-Latin1')
    >>> print nltk.tokenwrap(compress(w) for w in english_udhr[:75])
    Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and
    of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn
    of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn
    rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,
    and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and

Next, let's combine regular expressions with conditional frequency
distributions.  Here we will extract all consonant-vowel sequences
from the words of Rotokas, such as `ka`:lx: and `si`:lx:.  Since each of
these is a pair, it can be used to initialize a conditional frequency
distribution.  We then tabulate the frequency of each pair:

    >>> rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')
    >>> cvs = [cv for w in rotokas_words for cv in re.findall('[ptksvr][aeiou]', w)]
    >>> cfd = nltk.ConditionalFreqDist(cvs)
    >>> cfd.tabulate()
         a    e    i    o    u
    k  418  148   94  420  173
    p   83   31  105   34   51
    r  187   63   84   89   79
    s    0    0  100    2    1
    t   47    8    0  148   37
    v   93   27  105   48   49

Examining the rows for `s`:lx: and `t`:lx:, we see they are in partial
"complementary distribution", which is evidence that they are not
distinct phonemes in the language.  Thus, we could conceivably drop
`s`:lx: from the Rotokas alphabet and simply have a pronunciation rule
that the letter `t`:lx: is pronounced `s`:lx: when followed by
`i`:lx:.  (Note that single entry having *su*, namely *kasuari*
'cassowary' is a loanword).

If we want to be able to inspect the words behind the numbers in the above table,
it would be helpful to have an index, allowing us to quickly find the list of words
that contains a given consonant-vowel pair, e.g. ``cv_index['su']`` should give us
all words containing `su`:lx:.  Here's how we can do this:

    >>> cv_word_pairs = [(cv, w) for w in rotokas_words
    ...                          for cv in re.findall('[ptksvr][aeiou]', w)]
    >>> cv_index = nltk.Index(cv_word_pairs)
    >>> cv_index['su']
    ['kasuari']
    >>> cv_index['po']
    ['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa',
    'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', ...]

This program processes each word ``w`` in turn, and for each one, finds every
substring that matches the regular expression |l|\ ``[ptksvr][aeiou]``\ |r|.
In the case of the word `kasuari`:lx:, it finds `ka`:lx:, `su`:lx: and `ri`:lx:.
Therefore, the ``cv_word_pairs`` list will contain ``('ka', 'kasuari')``,
``('su', 'kasuari')`` and ``('ri', 'kasuari')``.  One further step, using
``nltk.Index()``, converts this into a useful index.

Finding Word Stems
------------------

When we use a web search engine, we usually don't mind (or even notice)
if the words in the document differ from our search terms in having
different endings.  A query for `laptops`:lx: finds documents
containing `laptop`:lx: and vice versa.
Indeed, `laptop`:lx: and `laptops`:lx: are just two forms of the *same* word.
For some language processing tasks we want to ignore word endings, and just
deal with word stems.

There are various ways we can pull out the stem of a word.  Here's a simple-minded
approach which just strips off anything that looks like a suffix:

    >>> def stem(word):
    ...     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:
    ...         if word.endswith(suffix):
    ...             return word[:-len(suffix)]
    ...     return word

Although we will ultimately use |NLTK|\ 's built-in stemmers, its interesting
to see how we can use regular expressions for this task.  Our first step is
to build up a disjunction of all the suffixes.  We need to enclose it in parentheses
in order to limit the scope of the disjunction.

    >>> re.findall('^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
    ['ing']

Here, ``re.findall()`` just gave us the suffix even though the regular expression
matched the entire word.  This is because the parentheses have a second function,
to select substrings to be extracted.  If we want to use the parentheses for
scoping the disjunction but not for selecting output, we have to add ``?:``
(just one of many arcane subtleties of regular expressions).
Here's the revised version.

    >>> re.findall('^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
    ['processing']

However, we'd actually like to split the word into stem and suffix.
Instead, we should just parenthesize both parts of the regular expression:

    >>> re.findall('^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
    [('process', 'ing')]

This looks promising, but still has a problem.  Let's look at a different
word, `processes`:lx::

    >>> re.findall('^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')
    [('processe', 's')]

The regular expression incorrectly found an `-s`:lx: suffix instead of
an `-es`:lx: suffix.  This demonstrates another subtlety: the star operator
is "greedy" and the ``.*`` part of the expression tries to consume as much of the input
as possible.  If we use the "non-greedy" version of the star operator, written ``*?``,
we get what we want:
 
    >>> re.findall('^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')
    [('process', 'es')]

This works even when we allow empty suffix, by making the content of the
second parentheses optional:

    >>> re.findall('^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')
    [('language', '')]

This approach still has many problems (can you spot them?) but we will move
on to define a stemming function and apply it to a whole text:

    >>> def stem(word):
    ...     regexp = '^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'
    ...     stem, suffix = re.findall(regexp, word)[0]
    ...     return stem
    ...
    >>> raw = """DENNIS: Listen, strange women lying in ponds distributing swords
    ... is no basis for a system of government.  Supreme executive power derives from
    ... a mandate from the masses, not from some farcical aquatic ceremony."""
    >>> tokens = nltk.word_tokenize(raw)
    >>> [stem(t) for t in tokens]
    ['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond',
    'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',
    '.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from',
    'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']

Notice that our regular expression removed the `s`:lx: from `ponds`:lx: but also from `is`:lx:
and `basis`:lx:.  It produced some non-words like `distribut`:lx: and `deriv`:lx:, but these
are acceptable stems.

Searching Tokenized Text
------------------------

You can use a special kind of regular expression for searching across multiple words
in a text (where a text is a list of tokens).

    >>> from nltk.corpus import gutenberg, nps_chat
    >>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))
    >>> moby.findall("<a>(<.*>)<man>")
    monied; nervous; dangerous; white; white; white; pious; queer; good;
    mature; white; Cape; great; wise; wise; butterless; white; fiendish;
    pale; furious; better; certain; complete; dismasted; younger; brave;
    brave; brave; brave
    >>> chat = nltk.Text(nps_chat.words())
    >>> chat.search("<.*><.*><bro>")
    you rule bro; telling you bro; u twizted bro
    >>> chat.search("<l.*>{3,}")
    lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la
    la la; lovely lol lol love; lol lol lol.; la la la; la la la

.. note:: |TRY|
   Consolidate your understanding of regular expression patterns and substitutions using
   ``nltk.re_show(p, s)`` which annotates the string ``s`` to show every place where
   pattern ``p`` was matched, and ``nltk.app.finding_nemo()`` which provides a graphical
   interface for exploring regular expressions.


.. TODO: Add code example for type-instance relations.

It is easy to build search patterns when the linguistic phenomenon we're
studying is tied to particular words.  In some cases, a little creativity
will go a long way.  For instance, searching a large text corpus for
expressions of the form `x and other ys`:lx: allows us to discover
examples of instances and their corresponding types:

    >>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))
    >>> hobbies_learned.findall("<\w*> <and> <other> <\w*s>")
    speed and other activities; water and other liquids; tomb and other
    landmarks; Statues and other monuments; pearls and other jewels;
    charts and other items; roads and other features; figures and other
    objects; military and other areas; demands and other factors;
    abstracts and other compilations; iron and other metals

With enough text, this approach would give us a useful store
of information about the taxonomy of objects, without the need for
any manual labor.  However, our search results will usually
contain false positives, i.e. cases that we would want to exclude.
For example, the result: `demands and other factors`:lx: suggests
that `demand`:lx: is an instance of the type `factor`:lx:, but this
sentence is actually about wage demands.  Nevertheless, we could
construct our own corpus of instances and types by manually correcting
the output of such searches.

.. note::
   This combination of automatic and manual processing is the most common
   way for new corpora to be constructed.  We will return to this in
   Chapter chap-data_.   

Searching corpora also suffers from the problem of false negatives,
i.e. omitting cases that we would want to include.  It is risky to
conclude that some linguistic phenomenon doesn't exist in a corpus
just because we couldn't find any instances of a search pattern.
Perhaps we just didn't think carefully enough about suitable patterns. 

.. note:: |TRY|
   Look for instances of the pattern `as x as y`:lx: to discover
   information about entities and their properties.

.. 
   searching for doubled final consonants: .*([bdgptk])\1ed
   transforming date strings
   HTML stripping
   spelling correction
   textonyms

.. _sec-normalizing-text:

----------------
Normalizing Text
----------------

In earlier program examples we have often converted text to lowercase before
doing anything with its words, e.g. ``set(w.lower() for w in text)``.
By using ``lower()``, we have `normalized`:dt: the text to lowercase so that
the distinction between `The`:lx: and `the`:lx: is ignored.  Often we want
to go further than this, and strip off any affixes, a task known as stemming.
A further step is to make sure that the resulting form is a known word in a dictionary,
a task known as lemmatization.  We discuss each of these in turn.

Stemmers
--------

|NLTK| includes several off-the-shelf stemmers, and if you ever need
a stemmer you should use one of these in preference to crafting your own
using regular expressions, since these handle a wide range of irregular cases.
The Porter Stemmer strips affixes and knows about some special cases,
e.g. that `lie`:lx: not `ly`:lx: is the stem of `lying`:lx:.

    >>> porter = nltk.PorterStemmer()
    >>> lancaster = nltk.LancasterStemmer()
    >>> [porter.stem(t) for t in tokens]
    ['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond',
    'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',
    '.', 'Suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from',
    'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']
    >>> [lancaster.stem(t) for t in tokens]
    ['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut',
    'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem',
    'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not',
    'from', 'som', 'farc', 'aqu', 'ceremony', '.']

Stemming is not a well-defined process, and we typically pick the stemmer that best
suits the application we have in mind.  The Porter Stemmer is a good choice if you
are indexing some texts and want to support search using alternative forms of
words (illustrated in Figure stemmer-indexing_, which uses *object oriented*
programming techniques that will be covered in Chapter REF, and string formatting
techniques to be covered in section sec-formatting_). 

.. pylisting:: stemmer-indexing
   :caption: Indexing a Text Using a Stemmer

    class IndexedText(object):

        def __init__(self, stemmer, text):
            self._text = text
            self._stemmer = stemmer
            self._index = nltk.Index((self._stem(word), i)
                                     for (i, word) in enumerate(text))

        def concordance(self, word, width=40):
            key = self._stem(word)
            wc = width/4                # words of context
            for i in self._index[key]:
                lcontext = ' '.join(self._text[i-wc:i])
                rcontext = ' '.join(self._text[i:i+wc])
                ldisplay = '%*s'  % (width, lcontext[-width:])
                rdisplay = '%-*s' % (width, rcontext[:width])
                print ldisplay, rdisplay

        def _stem(self, word):
            return self._stemmer.stem(word).lower()

    >>> porter = nltk.PorterStemmer()
    >>> grail = nltk.corpus.webtext.words('grail.txt')
    >>> text = IndexedText(porter, grail)
    >>> text.concordance('lie')
    r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no
     beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of
           Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   
    doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  
    ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which 
       you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --
    h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k
    not stop our fight ' til each one of you lies dead , and the Holy Grail returns t


Lemmatization
-------------

The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary
(and this additional checking process makes it slower).
It doesn't handle `lying`:lx:, but it converts `women`:lx: to `woman`:lx:.

    >>> wnl = nltk.WordNetLemmatizer()
    >>> [wnl.lemmatize(t) for t in tokens]
    ['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond',
    'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of',
    'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a',
    'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical',
    'aquatic', 'ceremony', '.']

The WordNet lemmatizer is a good choice if you want to compile the vocabulary
of some texts and want a list of valid lexical items.


Non-Standard Words
------------------

[Discuss the practice of mapping words such as numbers, abbreviations, dates to
a special vocabulary, based on Sproat et al 2001; new NLTK support planned...]


.. _sec-tokenization:

---------------------------------------
Regular Expressions for Tokenizing Text
---------------------------------------

Tokenization is the task of cutting a string into
identifiable linguistic units that constitute a piece of language data.
Although it is a fundamental task, we have been able to
delay it til now because many corpora are already tokenized,
and because |NLTK| includes some tokenizers.
Now that you are familiar with regular expressions,
you can learn how to use them to tokenize text, and to
have much more control over the process.

Simple Approaches to Tokenization
---------------------------------

The very simplest method for tokenizing text is to split on whitespace.
Consider the following text from *Alice's Adventures in Wonderland*:

    >>> raw = """'When I'M a Duchess,' she said to herself, (not in a very hopeful tone                                                  
    ... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very                                                           
    ... well without--Maybe it's always pepper that makes people hot-tempered,'..."""

We could split this raw text on whitespace using ``raw.split()``.
To do the same using a regular expression, we need to match any number
of spaces, tabs, or newlines.

    >>> re.split(r'[ \t\n]+', raw)
    ["'When", "I'M", 'a', "Duchess,'", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a',
    'very', 'hopeful', 'tone', 'though),', "'I", "won't", 'have', 'any', 'pepper', 'in',
    'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe',
    "it's", 'always', 'pepper', 'that', 'makes', 'people', "hot-tempered,'..."]

The regular expression |l|\ ``[ \t\n]+``\ |r| matches one or more space, tab (``\t``)
or newline (``\n``).  Other whitespace characters, such as carriage-return and
form-feed should really be included too.  Instead, we will can use a built-in
``re`` abbreviation, ``\s``, which means any whitespace character.  The above
statement can be rewritten as ``re.split(r'\s+', raw)``.

.. note::
   When using regular expressions that contain the backslash character, you should
   prefix the string with the letter ``r`` (meaning "raw"), which instructs the Python
   interpreter to treat them as literal backslashes.

Splitting on whitespace gives us tokens like ``'(not'`` and ``'herself,'``.
An alternative is to use the fact that Python provides us with a character class ``\w``
for word characters [define] and also the complement of this class ``\W``.  So, we
can split on anything *other* than a word character:

    >>> re.split(r'\W+', raw)
    ['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in',
    'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in',
    'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe',
    'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']

Observe that this gives us empty strings [explain why].  We get the same result using
``re.findall(r'\w+', raw)``, using a pattern that matches the words instead of the spaces.

    >>> re.findall(r'\w+|\S\w*', raw)
    ["'When", 'I', "'M", 'a', 'Duchess', ',', "'", 'she', 'said', 'to', 'herself', ',',
    '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', "'I", 'won', "'t",
    'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does',
    'very', 'well', 'without', '-', '-Maybe', 'it', "'s", 'always', 'pepper', 'that',
    'makes', 'people', 'hot', '-tempered', ',', "'", '.', '.', '.']

The regular expression |l|\ ``\w+|\S\w*``\ |r| will first try to match any sequence
of word characters.  If no match is found, it will try to match any
*non*\ -whitespace character (``\S`` is the complement of ``\s``) followed by
further word characters.  This means that punctuation is grouped with any following
letters (e.g. `'s`:lx:) but that sequences of two or more punctuation
characters are separated.  Let's generalize the ``\w+`` in the above expression
to permit word-internal hyphens and apostrophes: |l|\ ``\w+([-']\w+)*``\ |r|.
This expression means ``\w+`` followed by zero or more instances of ``[-']\w+``;
it would match `hot-tempered`:lx: and `it's`:lx:.
(We need to include ``?:`` in this expression for reasons discussed earlier.) 
We'll also add a pattern to match quote characters so these are kept separate
from the text they enclose.

    >>> print re.findall(r"\w+(?:[-']\w+)*|'|[-.(]+|\S\w*", raw)
    ["'", 'When', "I'M", 'a', 'Duchess', ',', "'", 'she', 'said', 'to', 'herself', ',',
    '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', "'", 'I', "won't",
    'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does',
    'very', 'well', 'without', '--', 'Maybe', "it's", 'always', 'pepper', 'that',
    'makes', 'people', 'hot-tempered', ',', "'", '...']

The above expression also included |l|\ ``[-.)]+``\ |r| which causes the double hyphen,
ellipsis, and open bracket to be tokenized separately.

Table tab-re-symbols_ lists the regular expression character class symbols we have
seen in this section.

.. table:: tab-re-symbols

   ===========  ===================================================================
   Symbol       Function
   ===========  ===================================================================
   ``\b``       Word boundary (zero width)
   ``\d``       Any decimal digit (equivalent to ``[0-9]``)
   ``\D``       Any non-digit character (equivalent to ``[^0-9]``)
   ``\s``       Any whitespace character (equivalent to ``[ \t\n\r\f\v]``
   ``\S``       Any non-whitespace character (equivalent to ``[^ \t\n\r\f\v]``)
   ``\w``       Any alphanumeric character (equivalent to ``[a-zA-Z0-9_]``)
   ``\W``       Any non-alphanumeric character (equivalent to ``[^a-zA-Z0-9_]``)
   ``\t``       The tab character
   ``\n``       The newline character
   ===========  ===================================================================

   Regular Expression Symbols


NLTK's Regular Expression Tokenizer
-----------------------------------

The function ``nltk.regexp_tokenize()`` is like ``re.findall``, except it
is more efficient and it avoids the need for special treatment of parentheses.
For readability we break up the regular expression over several lines
and add a comment about each line.  The special ``(?x)`` "verbose flag"
tells Python to strip out the embedded whitespace and comments.

    >>> text = 'That U.S.A. poster-print costs $12.40...'
    >>> pattern = r'''(?x)    # set flag to allow verbose regexps
    ...     ([A-Z]\.)+        # abbreviations, e.g. U.S.A. 
    ...   | \w+(-\w+)*        # words with optional internal hyphens
    ...   | \$?\d+(\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
    ...   | \.\.\.            # ellipsis
    ...   | [][.,;"'?():-_`]  # these are separate tokens
    ... '''
    >>> nltk.regexp_tokenize(text, pattern)
    ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']

The ``regexp_tokenize()`` function has an optional ``gaps`` parameter.
When set to ``True``, the regular expression is applied to the gaps
between tokens (cf ``re.split()``).

.. note::
   We can evaluate a tokenizer by comparing the resulting tokens with a
   wordlist, and reporting any tokens that don't appear in the wordlist,
   using ``set(tokens).difference(wordlist)``.  You'll probably want to
   lowercase all the tokens first.

Dealing with Contractions
-------------------------

A final issue for tokenization is the presence of contractions, such
as `didn't`:lx:.  If we are analyzing the meaning
of a sentence, it would probably be more useful to normalize this
form to two separate forms: `did`:lx: and `n't`:lx: (or `not`:lx:).
[MORE]

.. _sec-sentence-segmentation:

---------------------
Sentence Segmentation
---------------------

[Explain how sentence segmentation followed by word tokenization can give
different results to word tokenization on its own.]

Manipulating texts at the level of individual words often presupposes
the ability to divide a text into individual sentences.  As we have
seen, some corpora already provide access at the sentence level.  In
the following example, we compute the average number of words per
sentence in the Brown Corpus:

    >>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())
    20

In other cases, the text is only available as a stream of characters.  Before
tokenizing the text into words, we need to segment it into sentences.  NLTK facilitates
this by including the Punkt sentence segmenter [KissStrunk2006]_, along with
supporting data for English.  Here is an example of its use in segmenting
the text of a novel:

    >>> sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')
    >>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
    >>> sents = sent_tokenizer.tokenize(text)
    >>> pprint.pprint(sents[171:181])
    ['"Nonsense!',
     '" said Gregory, who was very rational when anyone else\nattempted paradox.',
     '"Why do all the clerks and navvies in the\nrailway trains look so sad and tired, so very sad and tired?',
     'I will\ntell you.',
     'It is because they know that the train is going right.',
     'It\nis because they know that whatever place they have taken a ticket\nfor that place they will reach.',
     'It is because after they have\npassed Sloane Square they know that the next station must be\nVictoria, and nothing but Victoria.',
     'Oh, their wild rapture!',
     'oh,\ntheir eyes like stars and their souls again in Eden, if the next\nstation were unaccountably Baker Street!'
     '"\n\n"It is you who are unpoetical," replied the poet Syme.']

Notice that this example is really a single sentence, reporting the speech of Mr Lucian Gregory.
However, the quoted speech contains several sentences, and these have been split into individual
strings.  This is reasonable behavior for most applications.



.. _sec-formatting:

---------------------------------
Formatting: From Lists to Strings
---------------------------------

Often we write a program to report a single data item, such as a particular element
in a corpus that meets some complicated criterion, or a single summary statistic
such as a word-count or the performance of a tagger.  More often, we write a program
to produce a structured result, such as a tabulation of numbers or linguistic forms,
or a reformatting of the original data.  When the results to be presented are linguistic,
textual output is usually the most natural choice.  However, when the results are numerical,
it may be preferable to produce graphical output.  In this section you will learn about
a variety of ways to present program output.

Converting Between Strings and Lists (notes)
--------------------------------------------

We specify the string to be used as the "glue", followed by a
period, followed by the ``join()`` function.

    >>> silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']
    >>> ' '.join(silly)
    'We called him Tortoise because he taught us .'
    >>> ';'.join(silly)
    'We;called;him;Tortoise;because;he;taught;us;.'

So ``' '.join(silly)`` means: take all the items in ``silly`` and
concatenate them as one big string, using ``' '`` as a spacer between
the items.  (Many people find the notation for ``join()`` rather unintuitive.)

Notice that ``join()`` only works on a list of strings (what we have been calling a text).

.. (Thus we could say this type is privileged in Python.)

Formatting Output
-----------------

The output of a program is usually structured to make the information
easily digestible by a reader.  Instead of running some code and then
manually inspecting the contents of a variable, we would like the code
to tabulate some output.
There are many ways we might want to format the output of a program.  For
instance, we might want to place the length value in parentheses `after`:em: the
word, and print all the output on a single line:

    >>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',
    ...           'more', 'is', 'said', 'than', 'done', '.']
    >>> for word in saying:
    ...     print word, '(' + str(len(word)) + '),',
    After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1),

However, this approach has some problems.  First, the ``print``
statement intermingles variables and punctuation, making it a little
difficult to read.  Second, the output has spaces around every item
that was printed.  Third, we have to convert the length of the word to
a string so that we can surround it with parentheses.  A cleaner way to produce structured output uses
Python's `string formatting expressions`:dt:. Before diving into
clever formatting tricks, however, let's look at a really simple example. We
are going to use a special symbol, ``%s``, as a placeholder in
strings. Once we have a string containing this placeholder, we follow
it with a single ``%``
and then a value ``v``. Python then returns a new string where
``v`` has been slotted in to replace ``%s``:

    >>> "I want a %s right now" % "coffee"
    'I want a coffee right now'

In fact, we can have a number of placeholders, but following the ``%`` operator
we need to specify a tuple with exactly the same number of values.

    >>> "%s wants a %s %s" % ("Lee", "sandwich", "for lunch")
    'Lee wants a sandwich for lunch'
    >>>

We can also provide the values for the placeholders indirectly. Here's
an example using a ``for`` loop:

    >>> menu = ['sandwich', 'spam fritter', 'pancake']
    >>> for snack in menu:
    ...     "Lee wants a %s right now" % snack
    ... 
    'Lee wants a sandwich right now'
    'Lee wants a spam fritter right now'
    'Lee wants a pancake right now'
    >>>

We oversimplified things when we said that placeholders were
of the form ``%s``; in fact, this is a complex object, called a
`conversion specifier`:dt:. This has to start with the ``%``
character, and ends with conversion character such as ``s`` or ``d``. The ``%s``
specifier tells Python that the corresponding variable is a string (or
should be converted into a string), while the ``%d`` specifier
indicates that the corresponding variable should be converted into a
decimal representation. The string containing conversion specifiers is
called a `format string`:dt:.

Picking up on the ``print`` example that we opened this section with,
here's how we can use two different kinds of conversion specifier:

    >>> for word in saying:
    ...     print "%s (%d)," % (word, len(word)),
    After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1),'

To summarize, string formatting is accomplished with a three-part
object having the syntax: `format`:ph: ``%`` `values`:ph:.  The
`format`:ph: section is a string containing format specifiers
such as ``%s`` and ``%d`` that Python will replace with the supplied
values.  The `values`:ph: section of a formatting string is a parenthesized
list containing exactly as many items as there are format specifiers in the
`format`:ph: section.  In the case that there is just one item, the
parentheses can be left out.

In the above example, we used a trailing comma to suppress the
printing of a newline. Suppose, on the other hand, that we want to
introduce some additional newlines in our output. We can accomplish
this by inserting the "special" character ``\n`` into the ``print`` string:

    >>> for i, word in enumerate(saying[:6]):
    ...	   print "Word = %s\nIndex = %s" % (word, i)
    ...
    Word = After
    Index = 0
    Word = all
    Index = 1
    Word = is
    Index = 2
    Word = said
    Index = 3
    Word = and
    Index = 4
    Word = done
    Index = 5


Strings and Formats
-------------------

We have seen that there are two ways to display the contents of an object:

    >>> word = 'cat'
    >>> sentence = """hello
    ... world"""
    >>> print word
    cat
    >>> print sentence
    hello
    world
    >>> word
    'cat'
    >>> sentence
    'hello\nworld'

The ``print`` command yields Python's attempt to produce the most human-readable form of an object.
The second method |mdash| naming the variable at a prompt |mdash| shows us a string
that can be used to recreate this object.  It is important to keep in mind that both of
these are just strings, displayed for the benefit of you, the user.  They do not give
us any clue as to the actual internal representation of the object.

There are many other useful ways to display an object as a string of
characters.  This may be for the benefit of a human reader, or because
we want to `export`:dt: our data to a particular file format for use
in an external program.

Formatted output typically contains a combination of variables and
pre-specified strings, e.g. given a dictionary ``wordcount``
consisting of words and their frequencies we could do:

    >>> wordcount = {'cat':3, 'dog':4, 'snake':1}
    >>> for word in sorted(wordcount):
    ...     print word, '->', wordcount[word], ';',
    cat -> 3 ; dog -> 4 ; snake -> 1 ;

Apart from the problem of unwanted whitespace, print statements
that contain alternating variables and constants can be difficult to read and
maintain.  A better solution is to use formatting strings:

    >>> for word in sorted(wordcount):
    ...    print '%s->%d;' % (word, wordcount[word]),
    cat->3; dog->4; snake->1;

.. TODO: describe textwrap package

Lining Things Up
----------------

So far our formatting strings have contained specifications of fixed width, such
as ``%6s``, a string that is padded to width 6 and right-justified.
We can include a minus sign to make it left-justified.
In case we don't know in advance how wide a displayed value should be,
the width value can be replaced with a star in the formatting string,
then specified using a variable:

    >>> '%6s' % 'dog'
    '   dog'
    >>> '%-6s' % 'dog'
    'dog   '
    >>> width = 6
    >>> '%-*s' % (width, 'dog')
    'dog   '

Other control characters are used for decimal integers and floating point numbers.
Since the percent character ``%`` has a special interpretation in formatting strings,
we have to precede it with another ``%`` to get it in the output:

    >>> "accuracy for %d words: %2.4f%%" % (9375, 100.0 * 3205/9375)
    'accuracy for 9375 words: 34.1867%'

An important use of formatting strings is for tabulating data.
Recall that in section sec-extracting-text-from-corpora_ we saw
data being tabulated from a conditional frequency distribution.
Let's perform the tabulation ourselves, exercising full control
of headings and column widths.
Note the clear separation between the language processing work,
and the tabulation of results.

.. pylisting:: modal-tabulate
   :caption: Frequency of Modals in Different Sections of the Brown Corpus

    def tabulate(cfdist, words, categories):
        print '%-16s' % 'Category',
        for word in words:                                  # column headings
            print '%6s' % word,
        print
        for category in categories:
            print '%-16s' % category,                       # row heading
            for word in words:                              # for each word
                print '%6d' % cfdist[category][word],       # print table cell
            print                                           # end the row

    >>> from nltk.corpus import brown
    >>> cfd = nltk.ConditionalFreqDist((g,w)
    ...                                for g in brown.categories()
    ...                                for w in brown.words(categories=g))
    >>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> tabulate(cfd, modals, genres)
    Category            can  could    may  might   must   will
    news                 93     86     66     38     50    389
    religion             82     59     78     12     54     71
    hobbies             268     58    131     22     83    264
    science_fiction      16     49      4     12      8     16
    romance              74    193     11     51     45     43
    humor                16     30      8      8      9     13

Recall from the listing in Figure stemmer-indexing_ that we used a formatting string
``"%*s"``.  This allows us to specify the width of a field using a variable.

    >>> '%*s' % (15, "Monty Python")
    '   Monty Python'

We could use this to automatically customise the width of a column to be the
smallest value required to fit all the words, using
``width = min(len(w) for w in words)``.  Remember that the comma at the end
of print statements adds an extra space, and this is sufficient to prevent
the column headings from running into each other.

Writing Results to a File
-------------------------

We have seen how to read text from files (Section sec-accessing-text_).
It is often useful to write output to files as well.  The following
code opens a file ``output.txt`` for writing, and saves the program
output to the file.

    >>> output_file = open('output.txt', 'w')
    >>> words = set(nltk.corpus.genesis.words('english-kjv.txt'))
    >>> for word in sorted(words):
    ...     output_file.write(word + "\n")

When we write non-text data to a file we must convert it to a string first.
We can do this conversion using formatting strings, as we saw above.
We can also do it using Python's backquote notation, which converts any
object into a string.  Let's write the total number of words to our
file, before closing it.

    >>> len(words)
    2789
    >>> `len(words)`
    '2789'
    >>> file.write(`len(words)` + "\n")
    >>> file.close()



----------
Conclusion
----------

In this chapter we saw that we can do a variety of interesting
language processing tasks that focus solely on words.  Tokenization
turns out to be far more difficult than expected.
No single solution works well across-the-board, and we
must decide what counts as a token depending on the application
domain.  We also looked at normalization (including lemmatization) and
saw how it collapses distinctions between tokens.  In the next chapter
we will look at word classes and automatic tagging.

-------
Summary
-------

* In this book we view a text as a list of words.  A "raw text" is a potentially
  long string containing words and whitespace formatting, and is how we
  typically store and visualize a text.
* A string is specified in Python using single or double quotes: ``'Monty Python'``, ``"Monty Python"``.
* The characters of a string are accessed using indexes, counting from zero:
  ``'Monty Python'[1]`` gives the value ``o``.  The length of a string is
  found using ``len()``.
* Substrings are accessed using slice notation: ``'Monty Python'[1:5]``
  gives the value ``onty``.  If the start index is omitted, the
  substring begins at the start of the string; if the end index is omitted,
  the slice continues to the end of the string.
* Strings can be split into lists: ``'Monty Python'.split()`` gives
  ``['Monty', 'Python']``.  Lists can be joined into strings:
  ``'/'.join(['Monty', 'Python'])`` gives ``'Monty/Python'``.
* we can read text from a file ``f`` using ``text = open(f).read()``
* we can read text from a URL ``u`` using ``text = urlopen(u).read()``
* texts found on the web may contain unwanted material (such as headers, footers, markup),
  that need to be removed before we do any linguistic processing.
* a word token is an individual occurrence of a word in a particular context
* a word type is the vocabulary item, independent of any particular use of that item
* tokenization is the segmentation of a text into basic units |mdash| or tokens |mdash|
  such as words and punctuation.
* tokenization based on whitespace is inadequate for many applications because it
  bundles punctuation together with words
* lemmatization is a process that maps the various forms of a word (such as `appeared`:lx:, `appears`:lx:)
  to the canonical or citation form of the word, also known as the lexeme or lemma (e.g. `appear`:lex:).
* Regular expressions are a powerful and flexible method of specifying
  patterns. Once we have imported the ``re`` module, we can use
  ``re.findall()`` to find all substrings in a string that match a pattern,
  and we can use ``re.sub()`` to replace substrings of one sort with another.
* If a regular expression string includes a backslash, you should tell Python not to
  preprocess the string, by using a raw string with an ``r`` prefix: ``r'regexp'``.
* Normalization of words collapses distinctions, and is useful when indexing texts.

-----------------------
Further Reading (NOTES)
-----------------------

To learn about Unicode, see app-unicode_.

Sources discussing the unreliability of google hits.

as x as y: http://acl.ldc.upenn.edu/P/P07/P07-1008.pdf

Richard Sproat, Alan Black, Stanley Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards.
"Normalization of non-standard words." Computer Speech and Language, 15(3), 287-333, 2001

A.M. Kuchling.
*Regular Expression HOWTO*,
http://www.amk.ca/python/howto/regex/

For more examples of processing words with |NLTK|, please see the
tokenization, stemming and corpus HOWTOs at |NLTK-HOWTO-URL|.
Chapters 2 and 3 of [JurafskyMartin2008]_ contain more advanced
material on regular expressions and morphology.

For languages with a non-Roman script, tokenizing text is even more
challenging.  For example, in Chinese text there is no visual representation of word
boundaries.  The three-character string: |ai4|\ |guo3|\ |ren2|
(ai4 "love" (verb), guo3 "country", ren2 "person") could
be tokenized as |ai4|\ |guo3| / |ren2|\ , "country-loving person"
or as |ai4| / |guo3|\ |ren2|\ , "love country-person."
The problem of tokenizing Chinese text is a major focus of SIGHAN,
the ACL Special Interest Group on Chinese Language Processing
``http://sighan.org/``.

Regular Expressions
-------------------

There are many references for regular expressions, both practical and
theoretical.  [Friedl2002MRE]_ is a comprehensive and detailed manual
in using regular expressions, covering their syntax in most major
programming languages, including Python.

For an introductory
tutorial to using regular expressions in Python with the ``re``
module, see A. M. Kuchling, *Regular Expression HOWTO*,
http://www.amk.ca/python/howto/regex/. 

Chapter 3 of [Mertz2003TPP]_ provides a more extended tutorial on
Python's facilities for text processing with regular expressions.

http://www.regular-expressions.info/ is a useful online resource,
providing a tutorial and references to tools and other sources of
information.

Unicode Regular Expressions:
http://www.unicode.org/reports/tr18/

Regex Library:
http://regexlib.com/

..
   John Hopkins Center for Language and Speech Processing, 1999
   Summer Workshop on Normalization of Non-Standard Words: Final Report
   http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf

---------
Exercises
---------

#. |easy| Try the examples in this section, then try the following.

   a) Create a variable called ``msg`` and put a message
      of your own in this variable.  Remember that strings need
      to be quoted, so you will need to type something like:
      ``msg = "I like NLP!"``
   b) Now print the contents of this variable in two ways, first
      by simply typing the variable name and pressing enter, then
      by using the ``print`` statement.
   c) Try various arithmetic expressions using this string, e.g.
      ``msg + msg``, and ``5 * msg``.
   d) Define a new string ``hello``, and then try ``hello + msg``.
      Change the ``hello`` string so that it ends with a space
      character, and then try ``hello + msg`` again.

#. |easy| Consider the following two expressions which have the same
   result.  Which one will typically be more relevant in |NLP|?  Why?

   a) ``"Monty Python"[6:12]``
   b) ``["Monty", "Python"][1]``

#. |easy| Define a string ``s = 'colorless'``.  Write a Python statement
   that changes this to "colourless" using only the slice and
   concatenation operations.

#. |easy| Try the slice examples from this section using the interactive
   interpreter.  Then try some more of your own.  Guess what the result
   will be before executing the command.

#. |easy| We can use the slice notation to remove morphological endings on
   words.  For example, ``'dogs'[:-1]`` removes the last character of
   ``dogs``, leaving ``dog``.  Use slice notation to remove the
   affixes from these words (we've inserted a hyphen to
   indicate the affix boundary, but omit this from your strings):
   ``dish-es``, ``run-ning``, ``nation-ality``, ``un-do``,
   ``pre-heat``.

#. |easy| We saw how we can generate an ``IndexError`` by indexing beyond the end
   of a string.  Is it possible to construct an index that goes too far to
   the left, before the start of the string?

#. |easy| We can also specify a "step" size for the slice. The following
   returns every second character within the slice: ``msg[6:11:2]``.
   It also works in the reverse direction: ``msg[10:5:-2]``
   Try these for yourself, then experiment with different step values.

#. |easy| What happens if you ask the interpreter to evaluate ``msg[::-1]``?
   Explain why this is a reasonable result.

#. |easy| Describe the class of strings matched by the following regular
   expressions.

   a) ``[a-zA-Z]+``
   #) ``[A-Z][a-z]*``
   #) ``p[aeiou]{,2}t``
   #) ``\d+(\.\d+)?``
   #) ``([^aeiou][aeiou][^aeiou])*``
   #) ``\w+|[^\w\s]+``

   Test your answers using ``re_show()``.

#. |easy| Write regular expressions to match the following classes of strings:

    a) A single determiner (assume that `a`:lx:, `an`:lx:, and `the`:lx:
       are the only determiners).
    #) An arithmetic expression using integers, addition, and
       multiplication, such as ``2*3+8``.

#. |easy| Write a utility function that takes a URL as its argument, and returns
   the contents of the URL, with all HTML markup removed.  Use ``urllib.urlopen``
   to access the contents of the URL, e.g.
   ``raw_contents = urllib.urlopen('http://nltk.org/').read()``.

#. |easy| 
   Save some text into a file ``corpus.txt``.  Define a function ``load(f)``
   that reads from the file named in its sole argument, and returns a string
   containing the text of the file.

   a) Use ``nltk.regexp_tokenize()`` to create a tokenizer that tokenizes
      the various kinds of punctuation in this text.  Use a single
      regular expression, with inline comments using the
      ``re.VERBOSE`` flag.
   b) Use ``nltk.regexp_tokenize()`` to create a tokenizer that tokenizes
      the following kinds of expression: monetary amounts; dates; names
      of people and companies.

#. |easy| Rewrite the following loop as a list comprehension:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> result = []
    >>> for word in sent:
    ...     word_len = (word, len(word))
    ...     result.append(word_len)
    >>> result
    [('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]

#. |easy| Split ``sent`` on some other character, such as ``'s'``.

#. |easy| We pointed out that when ``phrase`` is a list, ``phrase.reverse()``
   returns a modified version of ``phrase`` rather than a new list. On
   the other hand, we can use the slice trick mentioned in the
   exercises for the previous section, ``[::-1]`` to create a `new`:em: reversed list
   without changing ``phrase``. Show how you can confirm this
   difference in behavior.

#. |easy| We have seen how to represent a sentence as a list of words, where
   each word is a sequence of characters.  What does ``phrase1[2][2]`` do?
   Why?  Experiment with other index values.

#. |easy| Write a ``for`` loop to print out the characters of a string, one per line.

#. |easy| What is the difference between calling ``split`` on a string
   with no argument or with ``' '`` as the argument,
   e.g. ``sent.split()`` versus ``sent.split(' ')``?  What happens
   when the string being split contains tab characters, consecutive
   space characters, or a sequence of tabs and spaces?  (In IDLE you
   will need to use ``'\t'`` to enter a tab character.)

#. |easy| Create a variable ``words`` containing a list of words.
   Experiment with ``words.sort()`` and ``sorted(words)``.
   What is the difference?

#. |easy| Explore the difference between strings and integers by typing
   the following at a Python prompt: ``"3" * 7`` and ``3 * 7``.
   Try converting between strings and integers using
   ``int("3")`` and ``str(3)``.  

#. |easy| Earlier, we asked you to use a text editor to create a file
   called ``test.py``, containing the single line ``msg = 'Monty Python'``.
   If you haven't already done this (or can't find the file),
   go ahead and do it now. Next, start up a new session with the
   Python interpreter, and enter the expression ``msg`` at the prompt.
   You will get an error from the interpreter. Now, try the following
   (note that you have to leave off the ``.py`` part of the filename):

       >>> from test import msg
       >>> msg

   This time, Python should return with a value. You can also try
   ``import test``, in which case Python should be able to
   evaluate the expression ``test.msg`` at the prompt.  

#. |soso| Read in some text from a corpus, tokenize it, and print the list of
   all `wh`:lx:\ -word types that occur.  (`wh`:lx:\ -words in English
   are used in questions, relative clauses and exclamations:
   `who`:lx:, `which`:lx:, `what`:lx:, and so on.) Print
   them in order.  Are any words duplicated in this list, because of
   the presence of case distinctions or punctuation?

#. |soso| Create a file consisting of words and (made up) frequencies, where each
   line consists of a word, the space character, and a positive integer,
   e.g. ``fuzzy 53``.  Read the file into a Python list using ``open(filename).readlines()``.
   Next, break each line into its two fields using ``split()``, and
   convert the number into an integer using ``int()``.  The result should
   be a list of the form: ``[['fuzzy', 53], ...]``.

#. |soso| Write code to access a favorite webpage and extract some text from it.
   For example, access a weather site and extract the forecast top
   temperature for your town or city today.

#. |soso| Write a function ``unknown()`` that takes a URL as its argument,
   and returns a list of unknown words that occur on that webpage.
   In order to do this, extract all substrings consisting of lowercase letters
   (using ``re.findall()``) and remove any items from this set that occur
   in the words corpus (``nltk.corpus.words``).  Try to categorize these words
   manually and discuss your findings.

#. |soso| Examine the results of processing the URL
   ``http://news.bbc.co.uk/`` using the regular expressions suggested
   above. You will see that there is still a fair amount of
   non-textual data there, particularly Javascript commands. You may
   also find that sentence breaks have not been properly
   preserved. Define further regular expressions that improve the
   extraction of text from this web page.

#. |soso| Define a function ``ghits()`` that takes a word as its argument and
   builds a Google query string of the form ``http://www.google.com/search?q=word``.
   Strip the |HTML| markup and normalize whitespace.  Search for a substring
   of the form ``Results 1 - 10 of about``, followed by some number
   `n`:math:,  and extract `n`:math:.
   Convert this to an integer and return it.

#. |soso| The above example of extracting (name, domain) pairs from
   text does not work when there is more than one email address
   on a line, because the ``+`` operator is "greedy" and consumes
   too much of the input.

   a) Experiment with input text containing more than one email address
      per line, such as that shown below.  What happens?
   #) Using ``re.findall()``, write another regular expression
      to extract email addresses, replacing the period character
      with a range or negated range, such as ``[a-z]+`` or ``[^ >]+``.
   #) Now try to match email addresses by changing the regular
      expression ``.+`` to its "non-greedy" counterpart, ``.+?``

   >>> s = """
   ... austen-emma.txt:hart@vmd.cso.uiuc.edu  (internet)  hart@uiucvmd (bitnet)
   ... austen-emma.txt:Internet (72600.2026@compuserve.com); TEL: (212-254-5093)
   ... austen-persuasion.txt:Editing by Martin Ward (Martin.Ward@uk.ac.durham)
   ... blake-songs.txt:Prepared by David Price, email ccx074@coventry.ac.uk
   ... """

#. |soso| Are you able to write a regular expression to tokenize text in such
   a way that the word `don't`:lx: is tokenized into `do`:lx: and `n't`:lx:?
   Explain why this regular expression won't work: |l|\ ``n't|\w+``\ |r|.

#. |soso| Write code to convert text into *hAck3r* again, this time using regular expressions
   and substitution, where
   ``e`` |rarr| ``3``,
   ``i`` |rarr| ``1``,
   ``o`` |rarr| ``0``,
   ``l`` |rarr| ``|``,
   ``s`` |rarr| ``5``,
   ``.`` |rarr| ``5w33t!``,
   ``ate`` |rarr| ``8``.
   Normalize the text to lowercase before converting it.
   Add more substitutions of your own.  Now try to map
   ``s`` to two different values: ``$`` for word-initial ``s``,
   and ``5`` for word-internal ``s``.

#. |soso| *Pig Latin* is a simple transliteration of English.  Each word of the
   text is converted as follows: move any consonant (or consonant cluster)
   that appears at the start of the word to the end,
   then append `ay`:lx:, e.g. `string`:lx: |rarr| `ingstray`:lx:,
   `idle`:lx: |rarr| `idleay`:lx:.  ``http://en.wikipedia.org/wiki/Pig_Latin``

   a) Write a function to convert a word to Pig Latin.

   b) Write code that converts text, instead of individual words.

   c) Extend it further to preserve capitalization, to keep ``qu`` together
      (i.e. so that ``quiet`` becomes ``ietquay``), and to detect when ``y``
      is used as a consonant (e.g. ``yellow``) vs a vowel (e.g. ``style``).

#. |soso| Download some text from a language that has vowel harmony (e.g. Hungarian),
   extract the vowel sequences of words, and create a vowel bigram table.

#. |soso| Consider the numeric expressions in the following sentence from
   the MedLine corpus: `The corresponding free cortisol fractions in these
   sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively.`:lx:
   Should we say that the numeric expression `4.53 +/- 0.15%`:lx: is three
   words?  Or should we say that it's a single compound word?  Or should
   we say that it is actually *nine* words, since it's read "four point
   five three, plus or minus fifteen percent"?  Or should we say that
   it's not a "real" word at all, since it wouldn't appear in any dictionary?
   Discuss these different possibilities.  Can you think of application domains
   that motivate at least two of these answers?

#. |soso| Readability measures are used to score the reading difficulty of a
   text, for the purposes of selecting texts of appropriate difficulty
   for language learners.  Let us define
   |mu|\ :subscript:`w` to be the average number of letters per word, and
   |mu|\ :subscript:`s` to be the average number of words per sentence, in
   a given text.  The Automated Readability Index (ARI) of the text
   is defined to be:
   ``4.71 * `` |mu|\ :subscript:`w` ``+ 0.5 * `` |mu|\ :subscript:`s` ``- 21.43``.
   Compute the ARI score for various sections of the Brown Corpus, including
   section ``f`` (popular lore) and ``j`` (learned).  Make use of the fact that
   ``nltk.corpus.brown.words()`` produces a sequence of words, while
   ``nltk.corpus.brown.sents()`` produces a sequence of sentences.

#. |soso| Use the Porter Stemmer to normalize some tokenized text, calling
   the stemmer on each word.  Do the same thing with the Lancaster Stemmer
   and see if you observe any differences.

#. |soso| Process the list ``saying`` using a ``for`` loop, and store the
   result in a new list ``lengths``.  Hint: begin by assigning the
   empty list to ``lengths``, using ``lengths = []``. Then each time
   through the loop, use ``append()`` to add another length value to
   the list.

#. |soso| Define a variable ``silly`` to contain the string:
   ``'newly formed bland ideas are inexpressible in an infuriating
   way'``.  (This happens to be the legitimate interpretation that
   bilingual English-Spanish speakers can assign to Chomsky's
   famous nonsense phrase, `colorless green ideas sleep furiously`:lx:
   according to Wikipedia).  Now write code to perform the following tasks:

   a) Split ``silly`` into a list of strings, one per
      word, using Python's ``split()`` operation, and save
      this to a variable called ``bland``.
   b) Extract the second letter of each word in ``silly`` and join
      them into a string, to get ``'eoldrnnnna'``.
   c) Combine the words in ``bland`` back into a single string, using ``join()``.
      Make sure the words in the resulting string are separated with
      whitespace.
   d) Print the words of ``silly`` in alphabetical order, one per line.

#. |soso| The ``index()`` function can be used to look up items in sequences.
   For example, ``'inexpressible'.index('e')`` tells us the index of the
   first position of the letter ``e``.

   a) What happens when you look up a substring, e.g. ``'inexpressible'.index('re')``?
   b) Define a variable ``words`` containing a list of words.  Now use ``words.index()``
      to look up the position of an individual word.
   c) Define a variable ``silly`` as in the exercise above.
      Use the ``index()`` function in combination with list slicing to
      build a list ``phrase`` consisting of all the words up to (but not
      including) ``in`` in ``silly``.

#. |soso| Write code to abbreviate text by removing all the vowels.
   Define ``sentence`` to hold any string you like, then initialize
   a new string ``result`` to hold the empty string ``''``.  Now write
   a ``for`` loop to process the string, one character at a time,
   and append any non-vowel characters to the result string.

#. |soso| Write code to convert nationality adjectives like `Canadian`:lx: and
   `Australian`:lx: to their corresponding nouns `Canada`:lx: and `Australia`:lx:.
   (see ``http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names``)

#. |soso| Read the LanguageLog post on phrases of the form `as best as p can`:lx:
   and `as best p can`:lx:, where `p`:lx: is a pronoun.   Investigate this
   phenomenon with the help of a corpus and the ``findall()`` method
   for searching tokenized text described in
   Section sec-useful-applications-of-regular-expressions_.
   ``http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html``

#. |hard| An interesting challenge for tokenization is words that have been
   split across a line-break.  E.g. if *long-term* is split, then we
   have the string ``long-\nterm``.

   a) Write a regular expression that identifies words that are
      hyphenated at a line-break.  The expression will need to include the
      ``\n`` character.

   b) Use ``re.sub()`` to remove the ``\n`` character from these
      words.

#. |hard| Read the Wikipedia entry on *Soundex*.  Implement this
   algorithm in Python.

#. |hard| Define a function ``percent(word, text)`` that calculates
   how often a given word occurs in a text, and expresses the result
   as a percentage.

#. |hard| Obtain raw texts from two or more genres and compute their respective
   reading difficulty scores as in the previous exercise.
   E.g. compare ABC Rural News and ABC Science News (``nltk.corpus.abc``).
   Use Punkt to perform sentence segmentation.

#. |hard| Rewrite the following nested loop as a nested list comprehension:

    >>> words = ['attribution', 'confabulation', 'elocution',
    ...          'sequoia', 'tenacious', 'unidirectional']
    >>> vsequences = set()
    >>> for word in words:
    ...     vowels = []
    ...     for char in word:
    ...         if char in 'aeiou':
    ...             vowels.append(char)
    ...     vsequences.add(''.join(vowels))
    >>> sorted(vsequences)
    ['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']    

    .. sorted(set(''.join(c for c in word if c in 'aeiou') for word in words))

#. |hard| Write a program that processes a text and discovers
   cases where a word has been used with a novel sense.
   For each word, compute the wordnet similarity
   between all synsets of the word and all synsets of the
   words in its context.  (Note that this is a crude
   approach; doing it well is an open research problem.)


.. include:: footer.rst
