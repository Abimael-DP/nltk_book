.. -*- mode: rst -*-
.. include:: ../definitions.txt
.. include:: regexp-defns.txt
.. sectnum::

=================================
6. Advanced Programming in Python
=================================

------------
Introduction
------------

This chapter introduces concepts in algorithms, data structures,
program design, and advanced Python programming.
It contains many working program fragments which you should try yourself.

--------------
More on Python
--------------

.. range(n)


List Comprehensions
-------------------

Many language processing tasks involve applying the same operation to
every item in a list.  `List comprehensions`:dt: are a convenient Python
construct for doing this.  Here we lowercase each word:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> [word.lower() for word in sent]
    ['the', 'dog', 'gave', 'john', 'the', 'newspaper']

As another example, we could remove all determiners from a list of
words:

    >>> def is_lexical(word):
    ...     return word.lower() not in ('a', 'an', 'the', 'that', 'to')
    >>> [word for word in sent if is_lexical(word)]
    ['dog', 'gave', 'John', 'newspaper']

Or equivalently:

    >>> filter(is_lexical, sent)
    ['dog', 'gave', 'John', 'newspaper']

Combining the transformations...

    >>> [word.lower() for word in sent if is_lexical(word)]
    ['dog', 'gave', 'john', 'newspaper']


.. Another example: stemming each word:
..   >>> from nltk_lite import stem
..   >>> stemmer = stem.Regexp('ing$|s$|e$')

The following code builds a list of tuples, where each tuple consists
of a word and its length.

    >>> [(x, len(x)) for x in sent]
    [('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]

We can build a list of n-grams as follows:

    >>> n = 3
    >>> [sent[i:i+n] for i in range(len(sent)-n+1)]
    [['The', 'dog', 'gave'],
     ['dog', 'gave', 'John'],
     ['gave', 'John', 'the'],
     ['John', 'the', 'newspaper']]


Iterators
---------

Generating all permutations of words, to check which ones are
grammatical (broken):

    >>> def permute(words, level):
    ...     if level == 1:
    ...         yield words
    ...     else:
    ...         for i in range(level):
    ...             permute(words, level-1)
    ...             if level % 2 == 1:
    ...                 words[0], words[level-1] = words[level-1], words[0]
    ...             else:
    ...                 words[i], words[level-1] = words[level-1], words[i]
    ...
    >>> list(permute([1,2,3], 3))


Formatted Strings
-----------------

Formatted output typically contains a combination of variables and
pre-specified strings, e.g. given a dictionary ``wordcount``
consisting of words and their frequencies we could do:

    >>> wordcount = {'cat':3, 'dog':4, 'snake':1}
    >>> for word in wordcount:
    ...     print word, "->", wordcount[word], ";",
    dog -> 4 ; cat -> 3 ; snake -> 1

Apart from the problem of unwanted whitespace, such print statements
alternating variables and constants can be difficult to read and
maintain.  Instead:

    >>> for word in wordcount:
    ...    print "%s->%d;" % (word, wordcount[word]),
    dog->4; cat->3; snake->1
    
Exceptions
----------

to be written

Element Trees
-------------



---------
Functions
---------

Defining Functions
------------------

.. copied into chapter 2

It often happens that part of a program needs to be used several times over.
For example, suppose we were writing a program that needed to be able to form
the plural of a singular noun, and that this needed to be done at various
places during the program.  Rather than repeating the same code several times
over, it is more efficient (and reliable) to localize this work inside a *function*.
A function is a programming construct which takes one or more inputs, and produces
an output.  In this case, we will take the singular noun as input, and generate a
plural form as output:

    >>> def plural(word):
    ...     if word[-1] == 'y':
    ...         return word[:-1] + 'ies'
    ...     elif word[-1] in 'sx':
    ...         return word + 'es'
    ...     elif word[-2:] in ['sh', 'ch']:
    ...         return word + 'es'
    ...     elif word[-2:] == 'an':
    ...         return word[:-2] + 'en'
    ...     return word + 's'
    >>> plural('fairy')
    'fairies'
    >>> plural('woman')
    'women'



Predicting the Next Word (Revisited)
------------------------------------


    >>> from nltk_lite.corpora import genesis
    >>> from nltk_lite.probability import ConditionalFreqDist
    >>> cfdist = ConditionalFreqDist()

We then examine each token in the corpus, and increment the
appropriate sample's count.  We use the variable ``prev`` to record
the previous word.

    >>> prev = None
    >>> for word in genesis.raw():
    ...     cfdist[prev].inc(word)
    ...     prev = word

.. Note:: Sometimes the context for an experiment is unavailable, or
   does not exist.  For example, the first token in a text does not
   follow any word.  In these cases, we must decide what context to
   use.  For this example, we use ``None`` as the context for the
   first token.  Another option would be to discard the first token.

Once we have constructed a conditional frequency distribution for the
training corpus, we can use it to find the most likely word for any
given context. For example, taking the word `living`:lx: as our context,
we can inspect all the words that occurred in that context.

    >>> word = 'living'
    >>> cfdist[word].samples()
    ['creature,', 'substance', 'soul.', 'thing', 'thing,', 'creature']

We can set up a simple loop to generate text: we set an initial
context, picking the most likely token in that context as our next
word, and then using that word as our new context:

    >>> word = 'living'
    >>> for i in range(20):
    ...     print word,
    ...     word = cfdist[word].max()
    living creature that he said, I will not be a wife of the land
    of the land of the land



NOW: doing this with functions...

Recursive Functions
-------------------

Iterative solution:

    >>> def factorial(n):
    ...     result = 1
    ...     for i in range(n+1):
    ...         result *= i
    ...     return result

Recursive solution (base case, induction step)

    >>> def factorial(n):
    ...     if n == 1:
    ...         return n
    ...     else:
    ...         return n * factorial(n-1)




Functional Decomposition
------------------------

Well-structured programs often make extensive use of functions.  Often when a block
of program code grows longer than a screenful, it is a great help to readability if
it is decomposed into one or more functions.



-----
Trees
-----

In this part of the book we will be investigating the constituent
structure of sentences, and we will see how these can be represented
using syntactic trees.  Here we will look at tree structures in NLTK.

A tree is a set of connected nodes, each of which is labeled with a
category.  It common to use a 'family' metaphor to talk about the
relationships of nodes in a tree: for example, `S`:gc: is the
`parent`:dt: of `VP`:gc:; conversely `VP`:gc: is a `daughter`:dt: (or
`child`:dt:) of `S`:gc:.  Also, since `NP`:gc: and `VP`:gc: are both
daughters of `S`:gc:, they are also `sisters`:dt:. 
Here is an example of a tree:

.. ex::
  .. tree:: (S (NP Lee) (VP (V saw) (NP the dog)))

Although it is helpful to represent trees in a graphical format, for
computational purposes we usually need a more text-oriented
representation. One standard method is to use a combination of bracket
and labels to indicate the structure, as shown here:

.. doctest-ignore::
      (S 
         (NP  'Lee')
         (VP 
            (V 'saw')
            (NP 
               (Det 'the')
               (N  'dog'))))

The conventions for displaying trees in NLTK are similar:

.. doctest-ignore::
      (S: (NP: 'Lee') (VP: (V: 'saw') (NP: 'the' 'dog')))

In such trees, the node value is a string containing the tree's
constituent type (e.g., `NP`:gc: or `VP`:gc:), while the children encode
the hierarchical contents of the tree [#]_.

.. [#] Although the ``Tree`` class is usually used for encoding
   syntax trees, it can be used to encode *any* homogeneous hierarchical
   structure that spans a text (such as morphological structure or
   discourse structure).  In the general case, leaves and node values do
   not have to be strings.

Trees are created with the ``Tree`` constructor, which takes a
node value and a list of zero or more children.  Here's an example of
a simple NLTK tree with a single child node, where the latter is a token:

    >>> from nltk_lite.parse.tree import Tree
    >>> tree1 = Tree('NP', ['John'])
    >>> tree1
    (NP: 'John')

Here is an example with two children:

    >>> tree2 = Tree('NP', ['the', 'man'])
    >>> tree2
    (NP: 'the' 'man')

Finally, here is a more complex example, where one of the
children is itself a tree:

    >>> tree3 = Tree('VP', ['saw', tree2])
    >>> tree3
    (VP: 'saw' (NP: 'the' 'man'))

A tree's root node value is accessed with the ``node`` property, and
its leaves are accessed with the ``leaves()`` method:

    >>> tree3.node
    'VP'
    >>> tree3.leaves()
    ['saw', 'the', 'man']

One common way of defining the subject of a sentence `S`:gc: in
English is as *the noun phrase that is the daughter of* `S`:gc: *and
the sister of* `VP`:gc:. Although we cannot access subjects directly,
in practice we can get something similar by using `tree
positions`:dt:. Consider ``tree4`` defined as follows:

    >>> tree4 = Tree('S', [tree1, tree3])
    >>> tree4
    (S: (NP: 'John') (VP: 'saw' (NP: 'the' 'man')))

Now we can just use indexing to access the subtrees of this tree:

    >>> tree4[0]
    (NP: 'John')
    >>> tree4[1]
    (VP: 'saw' (NP: 'the' 'man'))

Since the value of ``tree4[1]`` is itself a tree, we can index into that as well:

    >>> tree4[1][0]
    'saw'
    >>> tree4[1][1]
    (NP: 'the' 'man')

The printed representation for complex trees can be difficult to read.
In these cases, the ``draw`` method can be very useful. 

.. doctest-ignore::
    >>> tree3.draw()

This method opens a new window, containing a graphical representation
of the tree:

.. image:: ../images/parse_draw.png
   :scale: 70

The tree display window allows you to zoom in and out;
to collapse and expand subtrees; and to print the graphical
representation to a postscript file (for inclusion in a document).

To compare multiple trees in a single window, we can use the
``draw_trees()`` method:

.. doctest-ignore::
    >>> from nltk_lite.draw.tree import draw_trees
    >>> draw_trees(tree1, tree2, tree3)

The ``Tree`` class implements a number of other useful methods.  See
the ``Tree`` reference documentation for more information about these
methods.

The ``nltk_lite.corpora`` module defines the ``treebank`` corpus,
which contains a collection of hand-annotated parse trees for English
text, derived from the Penn Treebank.

    >>> from nltk_lite.corpora import treebank, extract
    >>> print extract(0, treebank.parsed())
    (S:
      (NP-SBJ:
        (NP: (NNP: 'Pierre') (NNP: 'Vinken'))
        (,: ',')
        (ADJP: (NP: (CD: '61') (NNS: 'years')) (JJ: 'old'))
        (,: ','))
      (VP:
        (MD: 'will')
        (VP:
          (VB: 'join')
          (NP: (DT: 'the') (NN: 'board'))
          (PP-CLR:
            (IN: 'as')
            (NP: (DT: 'a') (JJ: 'nonexecutive') (NN: 'director')))
          (NP-TMP: (NNP: 'Nov.') (CD: '29'))))
      (.: '.'))


Recursion on Trees
------------------

1. recurse over tree to display in some useful way (e.g. whitespace formatting)

2. recurse over tree to look for coordinate constructions (cf 4th
   example in chapter 1.1)

3. generate new dependency tree from a phrase-structure tree

(possible extension: callback function for Tree.subtrees())


-------------------------------
Sets and Mathematical Functions
-------------------------------

Sets
----

Knowing a bit about sets will come in useful when you look at 
Chapter `12: Semantic Representation <semantics.html>`_.
A set is a collection of entities, called the `members`:dt: of the
set. Sets can be finite or infinite, or even empty.
In Python, we can define a set just by listing its members; the
notation is similar to specifying a list:

   >>> set1 = set(['a', 'b', 1, 2, 3])
   >>> set1
   set(['a', 1, 2, 'b', 3])

:raw-latex:`\noindent`
In mathematical notation, we would specify this set as:

.. ex:: {'a', 'b', 1, 2, 3}

Set membership is a relation |mdash| we can ask whether some entity
`x`:math: belongs to a set `A`:math: (in mathematical notation,
written `x`:math: |element| `A`:math:).

   >>> 'a' in set1
   True
   >>> 'c' in set1
   False

:raw-latex:`\noindent`
However, sets differ from lists in that they are `unordered`:em: collections.
Two sets are equal if and only if they have exactly the same members:

   >>> set2 = set([3, 2, 1, 'b', 'a'])
   >>> set1 == set2
   True

The `cardinality`:dt: of a set `A`:math: (written |bar|\ `A`:math:\
|bar|) is the number of members in `A`:math:. We can get this value
using the ``len()`` function:

   >>> len(set1)
   5

The argument to the ``set()`` constructor can be any sequence, including
a string, and just calling the constructor with no argument creates
the empty set (written |empty|).

   >>> set('123')
   set(['1', '3', '2'])
   >>> a = set()
   >>> b = set()
   >>> a == b
   True


We can construct new sets out of old ones. The `union`:dt: of two sets
`A`:math: and `B`:math: (written `A`:math: |union| `B`:math:) is the set
of elements which belong to `A`:math: or `B`:math:. Union is
represented in Python with ``|``:

   >>> odds = set('13579')
   >>> evens = set('02468')
   >>> numbers = odds | evens
   >>> nats
   set(['1', '0', '3', '2', '5', '4', '7', '6', '9', '8'])

The `intersection`:dt: of two sets `A`:math: and `B`:math: (written
`A`:math: |intersect| `B`:math:) is the set of elements which belong
to both `A`:math: and `B`:math:. Intersection is represented in Python
with ``&``. If the intersection of two sets is empty, they are said to
be `disjoint`:dt:.

   >>> ints
   set(['1', '0', '2', '-1', '-2'])
   >>> ints & nats
   set(['1', '0', '2'])
   >>> odds & evens
   set([])

The `(relative) complement`:dt: of two sets `A`:math: and `B`:math: (written 
`A`:math: |diff| `B`:math:)  is the set of
elements which belong to `A`:math: but not `B`:math:. Complement is represented
in Python with ``-``. 

   >>> nats - ints
   set(['3', '5', '4', '7', '6', '9', '8'])
   >>> odds == nats - evens
   True
   >>> odds == odds - set()
   True

So far, we have described how to define 'basic' sets and how to form
new sets out of those basis ones. All the basic sets have been
specified by listing all their members. Often we want to specify set
membership more succinctly:

.. _set1:
.. ex:: the set of positive integers less than 10

.. _set2:
.. ex:: the set of people in Melbourne with red hair

:raw-latex:`\noindent` 
We can informally write these sets using the
following `predicate notation`:dt:\ :

.. _set3:
.. ex:: {`x`:math:\ : `x`:math: is a positive integer less than 10}

.. _set4:
.. ex:: {`x`:math:\ : `x`:math: is a person in Melbourne with red hair}

In axiomatic set theory, the axiom schema of comprehension states that
given a one-place predicate `P`:math:,  there is set `A`:math:
such that any `x`:math: belongs to `A`:math: if and only if (written |iff|)
`P(x)`:math: is true:

.. _compax:
.. ex:: |exists|\ `A`:math:\ |forall|\ `x`:math:.(`x`:math: |element|
        `A`:math: |iff| `P(x)`:math:\ )

:raw-latex:`\noindent` 
From a computational point of view, compax_ is
problematic: we have to treat sets as finite objects in the computer,
but there is nothing to stop us defining infinite sets using
comprehension. Now, there is a variant of compax_, called the axiom of
restricted comprehension, which allows us to specify a set `A`:math:
with a predicate `P`:math: so long as we only consider `x`:math:\ s
which belong to some `already defined set`:em: `B`:math:\:

.. _comprax:
.. ex:: |forall|\ `B`:math: |exists|\ `x`:math:\ |forall|\ `x`:math:.
        (`x`:math: |element| `A`:math: |iff| `x`:math: |element|
	`B`:math: |wedge|  `P(x)`:math:\ )

:raw-latex:`\noindent` 
This is equivalent to the following set in predicate notation:

.. ex:: {`x`:math:\ : `x`:math: |element| `B`:math: |wedge|  `P(x)`:math:\ )

:raw-latex:`\noindent`
comprax_ corresponds pretty much to what we get with list
comprehension in Python: if you already have a list, then you can
define a new list in terms of the old one, using an ``if``
condition. In other words, listcomp_ is the Python counterpart of
comprax_.

.. _listcomp:
.. ex:: ``set([x for x in B if P(x)])``

To illustrate this further, the following list comprehension relies on
the existence of the previously defined set ``nats``:

   >>> nats = set(range(10))
   >>> from math import fmod
   >>> evens1 = set([n for n in nats if fmod(n,2) == 0])

When we defined ``evens`` before, what actually had was a set of
strings, rather than Python numbers. We can use ``int`` to coerce the
strings to be of the right type.

   >>> evens2 = set([int(n) for n in evens])
   >>> evens1 == evens2
   True

If every member of `A`:math: is also a member of `B`:math:, we say
that `A`:math: is a subset of `B`:math: (written `A`:math: |subset|
`B`:math:). The subset relation is represented
in Python with ``<=``. 

   >>> evens1 <= nats
   True
   >>> set() <= nats
   True
   >>> evens1 <= evens1
   True

As the above examples show, `B`:math: can contain more members than
`A`:math: for `A`:math: |subset|
`B`:math: to hold, but this need not be so. Every set is a subset of
itself. To exclude the case where a set is a subset of itself, we use
the relation `proper subset`:dt: (written `A`:math: |propsubset|
`B`:math:). In Python, this relation is represented as ``<``.

   >>> evens1 < nats
   True
   >>> evens1 < evens1
   False

Sets can contain other sets. For instance, the set `A`:math: =
{{`a`:math:}, {`b`:math:} } contains the two singleton sets
{`a`:math:} and {`b`:math:}. Note that {`a`:math:} |subset| `A`:math:
does not hold, since `a`:math: belongs to {`a`:math:} but not
to `A`:math:. In Python, it is a bit more awkward to specify sets
whose members are also sets; the latter have to be defined as
``frozenset``\ s, i.e. unmutable objects.

   >>> a = frozenset('a')
   >>> aplus = set([a])
   >>> aplus
   set([frozenset(['a'])])

We also need to be careful to distinguish between the empty set |empty| and
the set whose only member is the empty set: {|empty|}.

Exercises
---------

1. Write a specification by hand in predicate notation, and an
   implementation in Python using list comprehension. 

   a. {2, 4, 8, 16, 32, 64}

   b. {2, 3, 5, 7, 11, 13, 17}

   c. {0, 2, -2, 4, -4, 6, -6, 8, -8}

   

#. The `powerset`:dt: of a set `A`:math: (written |power|\ `A`:math:\ ) is
   the set of all subsets of `A`:math:, including the empty set. List
   the members of the following sets:

   a.  |power|\ {`a`:math:, `b`:math:, `c`:math:\ }:

   b.  |power|\ {`a`:math:\ }

   c.  |power|\ {|empty|}

   d.  |power|\ |empty|

#. Write a Python function to compute the powerset of an arbitrary
   set. Remember that you will have to use ``frozenset`` for this.


Relations
---------



-------------------
Program Development
-------------------

Programming is a skill which is acquired over several years of
experience with a variety of programming languages and tasks.  Key
high-level abilities are *algorithm design* and its manifestation in
*structured programming*.  Key low-level abilities include familiarity
with the syntactic constructs of the language, and knowledge of a
variety of diagnostic methods for trouble-shooting a program which
does not exhibit the expected behaviour.

Programming Style
-----------------

We have just seen how the same task can be performed in different
ways, with implications for efficiency.  Another factor influencing
program development is *programming style*.  Consider the following
program to compute the average length of words in the Brown Corpus:

    >>> from nltk_lite.corpora import brown
    >>> count = 0
    >>> total = 0
    >>> for sent in brown.raw('a'):
    ...     for token in sent:
    ...         count += 1
    ...         total += len(token)
    >>> print float(total) / count
    4.2765382469

In this program we use the variable ``count`` to keep track of the
number of tokens seen, and ``total`` to store the combined length of
all words.  This is a low-level style, not far removed from machine
code, the primitive operations performed by the computer's CPU.
The two variables are just like a CPU's registers, accumulating values
at many intermediate stages, values which are almost meaningless. 
We say that this program is written in a *procedural* style, dictating
the machine operations step by step.  Now consider the following
program which computes the same thing:

    >>> tokens = [token for sent in brown.raw('a') for token in sent]
    >>> total = sum(map(len, tokens))
    >>> print float(total)/len(tokens)
    4.2765382469

The first line uses a list comprehension to construct the sequence of
tokens.  The second line *maps* the ``len`` function to this sequence,
to create a list of length values, which are summed.  The third line
computes the average as before.  Notice here that each line of code
performs a complete, meaningful action.  Moreover, they do not dictate
how the computer will perform the computations; we state high level
relationships like "``total`` is the sum of the lengths of the tokens"
and leave the details to the Python interpreter.  Accordingly, we say
that this program is written in a *declarative* style.

Here is another example to illustrate the procedural/declarative
distinction.  Notice again that the procedural version
involves low-level steps and a variable having meaningless
intermediate values:

    >>> word_list = []
    >>> for sent in brown.raw('a'):
    ...     for token in sent:
    ...         if token not in word_list:
    ...             word_list.append(token)
    >>> word_list.sort()

The declarative version (given second) makes use of higher-level
built-in functions:

    >>> tokens = [word for sent in brown.raw('a') for word in sent]
    >>> word_list = list(set(tokens))
    >>> word_list.sort()

What do these programs compute?  Which version did you find easier to interpret?

Consider one further example, which sorts three-letter words by their
final letters.  The words come from the widely-used Unix word-list,
made available as an NLTK corpus called ``words``.  Two words ending
with the same letter will be sorted according to their second-last
letters.  The result of this sort method is that many rhyming words will be
contiguous.  Two programs are given; Which one is more declarative,
and which is more procedural?

As an aside, for readability we define a function for reversing
strings that will be used by both programs:

    >>> def reverse(word):
    ...     return word[::-1]

Here's the first program.  We define a helper function ``reverse_cmp``
which calls the built-in ``cmp`` comparison function on reversed
strings.  The ``cmp`` function returns ``-1``, ``0``, or ``1``,
depending on whether its first argument is less than, equal to, or
greater than its second argument.  We tell the list sort function to
use ``reverse_cmp`` instead of ``cmp`` (the default).

    >>> from nltk_lite.corpora import words
    >>> def reverse_cmp(x,y):
    ...     return cmp(reverse(x), reverse(y))
    >>> word_list = [word for word in words.raw('en') if len(word) == 3]
    >>> word_list.sort(reverse_cmp)
    >>> print word_list[-12:]
    ['toy', 'spy', 'cry', 'dry', 'fry', 'pry', 'try', 'buy', 'guy', 'ivy', 'Paz', 'Liz']

Here's the second program.  In the first loop it collects up all the
three-letter words in reversed form.  Next, it sorts the list of
reversed words.  Then, in the second loop, it iterates over each
position in the list using the variable ``i``, and replaces each item
with its reverse.  We have now re-reversed the words, and can print
them out.

    >>> word_list = []
    >>> for word in words.raw('en'):
    ...     if len(word) == 3:
    ...         word_list.append(reverse(word))
    >>> word_list.sort()
    >>> for i in range(len(word_list)):
    ...     word_list[i] = reverse(word_list[i])
    >>> print word_list[-12:]
    ['toy', 'spy', 'cry', 'dry', 'fry', 'pry', 'try', 'buy', 'guy', 'ivy', 'Paz', 'Liz']

Choosing between procedural and declarative styles is just that, a
question of style.  There are no hard boundaries, and it is possible
to mix the two.  Readers new to programming are encouraged to
experiment with both styles, and to make the extra effort required to
master higher-level constructs, such as list comprehensions, and
built-in functions like ``map`` and ``filter``.

Debugging
---------

  >>> import pdb
  >>> import mymodule
  >>> pdb.run('mymodule.test()')


Commands:

list [first [,last]]: list sourcecode for the current file

next: continue execution until the next line in the current function is reached

cont: continue execution until a breakpoint is reached (or the end of the program)

break: list the breakpoints

break n: insert a breakpoint at this line number in the current file

break file.py:n: insert a breakpoint at this line in the specified file

break function: insert a breakpoint at the first executable line of the function



Exercises
---------

1. Write a program to sort words by length.  Define a helper function
   ``cmp_len`` which uses the ``cmp`` comparison function on word
   lengths.

2. Consider the tokenized sentence
   ``['The', 'dog', 'gave', 'John', 'the', 'newspaper']``.
   Using the ``map()`` and ``len()`` functions, write a single line
   program to convert this list of tokens into a list of token
   lengths: ``[3, 3, 4, 4, 3, 9]``

-------------------------
Writing Complete Programs
-------------------------

Classifying Words Automatically
-------------------------------

A tagged corpus can be used to *train* a simple classifier, which can
then be used to guess the tag for untagged words.  For each word, we
can count the number of times it is tagged with each tag.  For
instance, the word ``deal`` is tagged 89 times as ``nn`` and 41 times
as ``vb``.  On this evidence, if we were asked to guess the tag for
``deal`` we would choose ``nn``, and we would be right over two-thirds
of the time.  The following program performs this tagging task, when
trained on the "g" section of the Brown Corpus (so-called *belles
lettres*, creative writing valued for its aesthetic content).

    >>> from nltk_lite.corpora import brown
    >>> cfdist = ConditionalFreqDist()
    >>> for sentence in brown.tagged('g'):
    ...     for token in sentence:
    ...         word = token[0]
    ...         tag = token[1]
    ...         cfdist[word].inc(tag)
    >>> for word in "John saw 3 polar bears".split():
    ...     print word, cfdist[word].max()
    John np
    saw vbd
    3 cd-tl
    polar jj
    bears vbz
    
Note that ``bears`` was incorrectly tagged as the 3rd person singular
form of a verb, since this word appears more frequently as a verb than
a noun in esthetic writing.

A problem with this approach is that it creates a huge model, with an
entry for every possible combination of word and tag.  For certain
tasks it is possible to construct reasonably good models which are
tiny in comparison.  For instance, let's try to guess whether a verb
is a noun or adjective from the last letter of the word alone.  We can
do this as follows:

    >>> tokens = []
    >>> for sent in brown.tagged('g'):
    ...     for (word,tag) in sent:
    ...         if tag in ['nn', 'jj'] and len(word) > 3:
    ...             char = word[-1]
    ...             tokens.append((char,tag))
    >>> split = len(tokens)*9/10
    >>> train, test = tokens[:split], tokens[split:]
    >>> cfdist = ConditionalFreqDist()
    >>> for (char,tag) in train:
    ...     cfdist[char].inc(tag)
    >>> correct = total = 0
    >>> for (char,tag) in test:
    ...     if tag == cfdist[char].max():
    ...         correct += 1
    ...     total += 1
    >>> print correct*100/total
    71

This result of 71% is marginally better than the result of 65% that we
get if we assign the ``nn`` tag to every word.  We can inspect the
model to see which tag is assigned to a word given its final letter.
Here we learn that words which end in ``c`` or ``l`` are more likely
to be adjectives than nouns::

    >>> print [(c, cfdist[c].max()) for c in cfdist.conditions()]
    [('%', 'nn'), ("'", None), ('-', 'jj'), ('2', 'nn'), ('5', 'nn'),
     ('A', 'nn'), ('D', 'nn'), ('O', 'nn'), ('S', 'nn'), ('a', 'nn'),
     ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'), ('g', 'nn'),
     ('f', 'nn'), ('i', 'nn'), ('h', 'nn'), ('k', 'nn'), ('m', 'nn'),
     ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ('s', 'nn'),
     ('r', 'nn'), ('u', 'nn'), ('t', 'nn'), ('w', 'nn'), ('y', 'nn'),
     ('x', 'nn'), ('z', 'nn')]

Exploring text genres
---------------------

Now that we can load a significant quantity of tagged text, we can
process it and extract items of interest.  The following code iterates
over the fifteen genres of the Brown Corpus (accessed using
``brown.items()``).  Each of these is tokenized in turn.  The next
step is to check if the token has the ``md`` tag.  For each of these
words we increment a count.  This uses the conditional frequency
distribution, where the condition is the current genre, and the event
is the modal.

    >>> cfdist = ConditionalFreqDist()
    >>> for genre in brown.items:                  # each genre
    ...     for sent in brown.tagged(genre):       # each sentence
    ...         for (word,tag) in sent:            # each tagged token
    ...             if tag == 'md':                # found a modal
    ...                  cfdist[genre].inc(word.lower())

The conditional frequency distribution is nothing more than a mapping
from each genre to the distribution of modals in that genre.  The
following code fragment identifies a small set of modals of interest,
and processes the data structure to output the required counts.

    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> print "%-18s" % 'Genre', ' '.join([("%6s" % m) for m in modals])
    Genre                 can  could    may  might   must   will
    >>> for genre in cfdist.conditions():    # generate rows
    ...     print "%-18s" % brown.item_name[genre],
    ...     for modal in modals:
    ...         print "%6d" % cfdist[genre].count(modal),
    ...     print
    press: reportage       94     86     66     36     50    387
    press: reviews         44     40     45     26     18     56
    press: editorial      122     56     74     37     53    225
    skill and hobbies     273     59    130     22     83    259
    religion               84     59     79     12     54     64
    belles-lettres        249    216    213    113    169    222
    popular lore          168    142    165     45     95    163
    miscellaneous: gov    115     37    152     13     99    237
    fiction: general       39    168      8     42     55     50
    learned               366    159    325    126    202    330
    fiction: science       16     49      4     12      8     16
    fiction: mystery       44    145     13     57     31     17
    fiction: adventure     48    154      6     58     27     48
    fiction: romance       79    195     11     51     46     43
    humor                  17     33      8      8      9     13

There are some interesting patterns in this table.  For instance,
compare the rows for government literature and adventure literature;
the former is dominated by the use of ``can, may, must, will`` while
the latter is characterised by the use of ``could`` and ``might``.
With some further work it might be possible to guess the genre of a
new text automatically, according to its distribution of modals.

Exercises
---------

#. **Classifying words automatically:**
   The program for classifying words as nouns or adjectives scored 71%.
   Try to come up with better conditions, to get the system to score 80% or better.

   a) Revise the condition to use a longer suffix of the word, such as
      the last two characters, or the last three characters.  What happens
      to the performance?  Which suffixes are diagnostic for adjectives?

   #) Explore other conditions, such as variable length prefixes of a
      word, or the length of a word, or the number of vowels in a word.

   #) Finally, combine multiple conditions into a tuple, and explore
      which combination of conditions gives the best result.

#. **Exploring text genres:**
   Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?

-------------------
Dynamic programming
-------------------

Dynamic programming is a general technique for designing algorithms
which is widely used in natural language processing.  The term
"programming" is used in a different sense to what you might expect,
to mean planning or scheduling.  Dynamic programming is used when a
problem contains overlapping sub-problems.  Instead of computing
solutions to the sub-problems repeatedly, we simply store them in a
lookup table.

Sanscrit Meter
--------------

Pingala was an Indian author who lived around the 5th century B.C.,
and wrote a treatise on Sanscrit prosody called the *Chandas Shastra*.
Virahanka extended this work around the 6th century A.D., studying
the number of ways of combining short and long syllables to create a
meter of length *n*.  He found, for example, that there are five ways
to construct a meter of length 4: *V*\ :subscript:`4` = *{LL, SSL, SLS, LSS, SSSS}*.
In general, we can split *V*\ :subscript:`n` into two subsets, those starting with
*L: {LL, LSS}*, and those starting with *S: {SSL, SLS, SSSS}*.  This
is the clue for decomposing the problem:

|  *V*\ :subscript:`4` =
|    LL, LSS; or L prefixed to each item of *V*\ :subscript:`2` = {L, SS}
|    SSL, SLS, SSSS; or S prefixed to each item of *V*\ :subscript:`3` = {SL, LS, SSS}

With this observation, we can write a little recursive function to compute these meters:

  >>> def virahanka1(n):
  ...     if n == 0:
  ...         return [""]
  ...     elif n == 1:
  ...         return ["S"]
  ...     else:
  ...         s = ["S" + prosody for prosody in virahanka1(n-1)]
  ...         l = ["L" + prosody for prosody in virahanka1(n-2)]
  ...        return s + l
  >>> virahanka1(4)
  ['SSSS', 'SSL', 'SLS', 'LSS', 'LL']  

Notice that, in order to compute *V*\ :subscript:`4` we first compute
*V*\ :subscript:`3` and *V*\ :subscript:`2`.  But to compute *V*\ :subscript:`3`,
we need to first compute *V*\ :subscript:`2` and *V*\ :subscript:`1`.  This call
structure is depicted in the following tree:

.. tree:: (V4 (V3 (V2 V1 V0) V1) (V2 V1 V0))

Observe that *V*\ :subscript:`2` is computed twice.
This might not seem like a significant problem, but 
it turns out to be rather wasteful as *n* gets large:
for *V*\ :subscript:`10` it computes *V*\ :subscript:`2` 34 times;
for *V*\ :subscript:`20` it computes *V*\ :subscript:`2` 4,181 times;
for *V*\ :subscript:`30` it computes *V*\ :subscript:`2` 514,229 times;
and
for *V*\ :subscript:`40` it computes *V*\ :subscript:`2` 63,245,986 times!
Instead, we can simply store the value of *V*\ :subscript:`2` in a table
and look it up whenever we need it.  The same goes for other values, such
as *V*\ :subscript:`3` and so on.  Here is a dynamic programming
approach which computes the same result as the earlier program, only
much more efficiently.  It uses some auxiliary storage, a table
called ``lookup``:

  >>> lookup = [None] * 100
  >>> lookup[0] = [""]
  >>> lookup[1] = ["S"]
  >>> def virahanka2(n):
  ...     for i in range(n-1):
  ...         s = ["S" + prosody for prosody in lookup[i]]
  ...         l = ["L" + prosody for prosody in lookup[i+1]]
  ...         lookup[i+2] = s + l
  ...     return lookup[n]

This is the classic *bottom-up* approach to dynamic programming, where
we fill up a table with solutions to all smaller sub-problems, then simply
read off the result we are interested in.  Notice that each sub-problem
is only ever solved once.  However, this method is still wasteful for some
applications, because it may compute solutions to sub-problems that are never
used in solving the main problem.  This wasted computation can be avoided using
the *top-down* approach to dynamic programming:

  >>> lookup = [None] * 100
  >>> lookup[0] = [""]
  >>> lookup[1] = ["S"]
  >>> def virahanka3(n):
  ...     if not lookup[n]:
  ...         s = ["S" + prosody for prosody in virahanka3(n-1)]
  ...         l = ["L" + prosody for prosody in virahanka3(n-2)]
  ...         lookup[n] = s + l
  ...     return lookup[n]

Unlike the bottom-up approach, this approach is recursive.  It avoids
the huge wastage of our first version by checking whether it has
previously stored the result.  If not, it computes the result
recursively and stores it in the table.  The last step is to return
the stored result.


Grammaticality Testing
----------------------

Define a simple grammar.

    >>> from nltk_lite.parse import cfg
    >>> grammar = cfg.parse_grammar("""
    ... S -> NP VP
    ... PP -> P NP
    ... NP -> Det N | NP PP
    ... VP -> V NP | V PP | VP PP
    ... Det -> 'a' | 'the'
    ... N -> 'dog' | 'cat' | 'mat'
    ... V -> 'chased' | 'sat'
    ... P -> 'on' | 'in'
    ... """)

We will be processing the input bottom-up.  Well-formed substring table::

  the cat sat on the mat
  Det N   V   P  Det N
  NP      V   P  NP
  NP      V   PP
  NP      VP
  S

Need a place to keep hypothesis that there is a non-terminal spanning a certain
sequence of terminals::

  0 the 1 cat 2 sat 3 on 4 the 5 mat 6
  0 Det 1 N   2 V   3 P  4 Det 5 N   6
  0 NP        2 V   3 PP             6
  0 S                                6

Store this in a table::

  -   0   1   2   3   4   5   6
  0       Det NP              S
  1           N
  2               V           VP
  3                   P       PP
  4                       Det NP
  5                           N
  6

  -   0   1   2   3   4   5   6
  0   
  1   Det
  2   NP  N
  3           V
  4               P
  5                   Det
  6   S       VP  PP  NP  N

There is a ``V`` from ``2`` to ``3``,
and a ``PP`` from ``3`` to ``6``.
Check for a production with ``V PP`` on the right hand side.
Found ``VP -> V PP``, so we have a ``VP`` from ``2`` to ``6``.
Add this to the table.

Equivalent graph -- use omnigraffle

Need to index the grammar.

    >>> index = {}
    >>> for prod in grammar.productions():
    ...     index[prod.rhs()] = prod.lhs()

Example of a space-time trade-off.
Reverse lookup on the grammar, instead of having to check through
entire list of productions each time we want to look up via the right-hand side.

Get a sentence, and set up an empty table:

    >>> from pprint import pprint
    >>> tokens = "the cat sat on the mat".split()
    >>> numwords = len(tokens)
    >>> table = []
    >>> for i in range(numwords+1):
    ...     table.append([''] * (numwords+1))
    >>> for i in range(numwords):
    ...     prod_rhs = grammar.productions(rhs=tokens[i])
    ...     table[i+1][i] = prod_rhs[0].lhs()
    >>> pprint(table)
    [['', '', '', '', '', '', ''],
     [<Det>, '', '', '', '', '', ''],
     ['', <N>, '', '', '', '', ''],
     ['', '', <V>, '', '', '', ''],
     ['', '', '', <P>, '', '', ''],
     ['', '', '', '', <Det>, '', ''],
     ['', '', '', '', '', <N>, '']]


Next, ...

    >>> for span in range(2, numwords+1):
    ...     for start in range(numwords+1-span):
    ...         end = start + span
    ...         for mid in range(start+1, end):
    ...             nt1 = table[mid][start]
    ...             nt2 = table[end][mid]
    ...             if (nt1,nt2) in index:
    ...                 table[end][start] = index[(nt1,nt2)]


Shows why parsing is essentially n^3 -- triply nested loop, size based
on sentence length.

    >>> pprint(table)
    [['', '', '', '', '', '', ''],
     [<Det>, '', '', '', '', '', ''],
     [<NP>, <N>, '', '', '', '', ''],
     ['', '', <V>, '', '', '', ''],
     ['', '', '', <P>, '', '', ''],
     ['', '', '', '', <Det>, '', ''],
     [<S>, '', <VP>, <PP>, <NP>, <N>, '']]


.. Asymptotic behaviour?


Exercises
---------

1. Read about string edit distance and the Levenshtein Algorithm.
   Try the implementation provided in ``nltk_lite.utilities.edit_dist``.
   How is this using dynamic programming?  Does it use the bottom-up or
   top-down approach?



---------------
Further Reading
---------------

.. include:: footer.txt
