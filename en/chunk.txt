.. -*- mode: rst -*-
.. include:: ../definitions.txt

.. standard global imports

    >>> from nltk.book import *

.. _chap-chunk:

================================
5. Robust Syntax (and Semantics)
================================

------------
Introduction
------------

In processing natural language, we are looking for structure and meaning.
Two of the most common methods are
`segmentation`:dt: and `labeling`:dt:.  Recall that in tokenization, we
`segment`:em: a sequence of characters into tokens, while in tagging we
`label`:em: each of these tokens.  Moreover, these two operations of
segmentation and labeling go hand in hand.  We break up a stream of
characters into linguistically meaningful segments (e.g., words) so
that we can classify those segments with their part-of-speech
categories.  The result of such classification is represented by
adding a label (e.g., part-of-speech tag) to the segment in question.

We will see that many tasks can be construed as a combination of
segmentation and labeling. However, this involves generalizing our
notion of segmentation to encompass `sequences`:em: of tokens. Suppose
that we are trying to recognize the names of people, locations and
organizations in a piece of text (a task that is usually called `Named
Entity Recognition`:dt:). Many of these names will involve more than
one token: `Cecil H. Green`:lx:, `Escondido Village`:lx:, `Stanford
University`:lx:; indeed, some names may have sub-parts which are also
names: `Cecil H. Green Library`:lx:, `Escondido Village Conference
Service Center`:lx:. In Named Entity Recognition, therefore, we need
to be able to identify the beginning and end of multi-token
sequences.  

Identifying the boundaries of specific types of word sequences is also
required when we want to recognize pieces of syntactic
structure. Suppose for example that as a preliminary to Named Entity
Recognition, we have decided that it would be useful to just pick out
noun phrases from a piece of text. To carry this out in a complete
way, we would probably want to use a proper syntactic parser. But
parsing can be quite challenging and computationally expensive |mdash|
is there an easier alternative? The answer is Yes: we can look for
sequences of part-of-speech tags in a tagged text, using one or more
patterns that capture the typical ingredients of a noun phrase. 

For example, here is some Wall Street Journal text with noun phrases
marked using brackets:

.. _wsj-nx:
.. ex::
  [ The/DT market/NN ] for/IN [ system-management/NN software/NN ]
  for/IN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ
  enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP
  Associates/NNPS ] should/MD do/VB well/RB there/RB ./.

From the point of view of theoretical linguistics, we seem to have
been rather unorthodox in our use of the term "noun phrase"; although
all the bracketed strings are noun phrases, not every noun phrase has
been captured. We will discuss this issue in more detail shortly. For
the moment, let's say that we are identifying noun "chunks" rather
than full noun phrases.


In chunking, we carry out segmentation and labeling of multi-token sequences,
as illustrated in Figure chunk-segmentation_.  The smaller boxes show
word-level segmentation and labeling, while the large boxes show 
higher-level segmentation and labeling.  It is these larger pieces
that we will call `chunks`:dt:, and the process of identifying them is called
`chunking`:dt:.

.. _chunk-segmentation:
.. figure:: ../images/chunk-segmentation.png
   :scale: 30

   Segmentation and Labeling at both the Token and Chunk Levels

Like tokenization, chunking can skip over material in the input.  Tokenization
omits white space and punctuation characters.  Chunking uses only a subset of
the tokens and leaves others out.

In this chapter, we will explore chunking in some depth, beginning
with the definition and representation of chunks.  We will see regular
expression and n-gram approaches to chunking, and will develop and
evaluate chunkers using the CoNLL-2000 chunking corpus.  Towards the
end of the chapter, we will look more briefly at Named Entity
Recognition and related tasks.

.. 
    Chunking is an efficient and robust method for identifying short
    phrases in text, or "chunks".  Chunks are `non-overlapping spans of
    text`:em:, usually consisting of a head word (such as a noun) and the
    adjacent modifiers and function words (such as adjectives and
    determiners).  

    There are other motivations for chunking: to locate information, and
    to ignore information.  In the former case, we may want to extract
    all noun phrases so they can be indexed.  A text retrieval system
    could use such an index to support efficient retrieval for queries
    involving terminological expressions.

    The reverse side of the coin is to *ignore* information.  Suppose that
    we want to study syntactic patterns, finding particular verbs in a
    corpus and displaying their arguments.  For instance, here are some
    uses of the verb `gave`:lx: in the Wall Street Journal (in the Penn
    Treebank corpus sample).  After `np`:gc:\ -chunking, the internal
    details of each noun phrase have been suppessed, allowing us to see
    some higher-level patterns::

      gave NP
      gave up NP in NP
      gave NP up
      gave NP NP
      gave NP to NP

    |nopar|
    In this way we can acquire information about the complementation
    patterns of a verb like `gave`:lx:, for use in the development of a
    grammar (see Chapter chap-parse_).



--------------------------------
Defining and Representing Chunks
--------------------------------


Chunking vs Parsing
-------------------

Chunking is akin to parsing in the sense that it can be used to build
hierarchical structure over text.  There are several important
differences, however.  First, as noted above, chunking is not
exhaustive, and typically omits items in the surface string.  Second,
where parsing constructs nested structures which are arbitrarily deep,
chunking creates structures of fixed depth (typically depth 2).  These
chunks often correspond to the lowest level of grouping identified in
the full parse tree. This is illustrated in parsing-chunking_ below,
which shows trees corresponding to the initial chunk structure of
wsj-nx_ and a more deeply parsede counterpart:

.. _parsing-chunking:
.. ex::
  .. ex::
     .. tree:: (S (NP (DT The) (NN market))  (IN for) 
                  (NP (NN system-management) (NN software))
                      (IN for) (NP (NNP Digital)) (NP (POS 's)  (NN hardware))
	                \.\.\.) 
  .. ex::
     .. tree:: (S (NP (DT The) (Nom (Nom (NN market)) (PP (IN for) 
                  (NP (Nom (Nom (NN system-management) (NN software))
                      (PP (IN for) 
                        (NP (NP (NNP Digital)) (NP (POS 's)  (NN
			hardware) ) ) ) ) ) ) ) )
	                \.\.\.)


A significant motivation for chunking is its robustness and efficiency
relative to parsing.  Parsing uses recursive phrase structure grammars
and arbitrary-depth trees.  Parsing has problems with robustness,
given the difficulty in gaining broad coverage while minimizing
ambiguity.  Parsing is also relatively inefficient: the time taken to
parse a sentence grows with the cube of the length of the sentence,
while the time taken to chunk a sentence only grows linearly.

Representing Chunks: Tags vs Trees
----------------------------------

As befits its intermediate status between tagging and parsing, chunk
structures can be represented using either tags or trees.  The most
widespread file representation uses so-called `IOB tags`:dt:.  In this
scheme, each token is tagged with one of three special chunk tags,
``I`` (inside), ``O`` (outside), or ``B`` (begin).  A token is tagged
as ``B`` if it marks the beginning of a chunk.  Subsequent tokens
within the chunk are tagged ``I``.  All other tokens are tagged ``O``.
The ``B`` and ``I`` tags are suffixed with the chunk type,
e.g. ``B-NP``, ``I-NP``.  Of course, it is not necessary to specify a
chunk type for tokens that appear outside a chunk, so these are just
labeled ``O``. An example of this scheme is shown in Figure
chunk-tagrep_.

.. _chunk-tagrep:
.. figure:: ../images/chunk-tagrep.png
   :scale: 30

   Tag Representation of Chunk Structures

IOB tags have become the standard way to represent chunk structures in
files, and we will also be using this format.  Here is an example of
the file representation of the information in Figure chunk-tagrep_::

  We PRP B-NP
  saw VBD O
  the DT B-NP
  little JJ I-NP
  yellow JJ I-NP
  dog NN I-NP

.. Give example with a verb group?

|nopar| In this representation, there is one token per line, each with
its part-of-speech tag and its chunk tag.  We will see later that this
format permits us to represent more than one chunk type, so long as
the chunks do not overlap.  

As we saw earlier, chunk structures can also be represented using
trees.  These have the benefit that each chunk is a constituent that
can be manipulated directly.  An example is shown in Figure
chunk-treerep_:
    
.. _chunk-treerep:
.. figure:: ../images/chunk-treerep.png
   :scale: 30

   Tree Representation of Chunk Structures

|nopar|
|NLTK| uses trees for its internal representation of chunks, and
provides methods for reading and writing such trees to the IOB
format.  By now you should understand what chunks are, and how they
are represented.  In the next section you will see how to build a
simple chunker.

--------
Chunking
--------

A `chunker`:dt: finds contiguous, non-overlapping spans of related
tokens and groups them together into chunks.  Chunkers often operate
on tagged texts, and use the tags to make chunking decisions.  In this
section we will see how to write a special type of regular expression
over part-of-speech tags, and then how to combine these into a chunk
grammar.  Then we will set up a chunker to chunk some tagged text
according to the grammar.

Chunking in |NLTK| begins with tagged tokens.

.. doctest-ignore::
    >>> tagged_tokens = [("the", "DT"), ("little", "JJ"), ("yellow", "JJ"), 
    ... ("dog", "NN"), ("snarled", "VBD"), ("at", "IN"),  ("the", "DT"), ("cat", "NN")]
 
Next, we write regular expressions over tag sequences.  The
following example identifies noun phrases that consist of an
optional determiner, followed by any number of adjectives, then a
noun.

    >>> cp = chunk.Regexp("NP: {<DT>?<JJ>*<NN>}")

|nopar| We create a chunker ``cp`` which can then be used
repeatedly to parse tagged input.  The result of chunking is 
a tree.

.. doctest-ignore::
    >>> cp.parse(tagged_tokens).draw()

.. tree:: (S (NP (DT the) (JJ little) (JJ yellow) (NN dog))
            (VBD snarled) (IN at) (NP (DT the) (NN cat)))



.. note:: Remember that our program samples assume you
   begin your interactive session or your program with: ``from nltk.book import *``

Tag Patterns
------------

A `tag pattern`:dt: is a sequence of part-of-speech tags delimited
using angle brackets, e.g. ``<DT><JJ><NN>``.  Tag patterns are
the same as the regular expression patterns we have already seen,
except for two differences which make them easier to use for chunking.
First, angle brackets group their contents into atomic
units, so "``<NN>+``" matches one or more repetitions of the tag
``NN``; and "``<NN|JJ>``" matches the ``NN`` or ``JJ``.  Second, the period wildcard operator is
constrained not to cross tag delimiters, so that "``<N.*>``" matches
any single tag starting with ``N``, e.g. ``NN``, ``NNS``.

Now, consider the following noun phrases from the Wall Street
Journal::

  another/DT sharp/JJ dive/NN
  trade/NN figures/NNS
  any/DT new/JJ policy/NN measures/NNS
  earlier/JJR stages/NNS
  Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP

.. todo: how are they to view this CoNLL data?

We can match these using a slight refinement of the first tag pattern
above: ``<DT>?<JJ.*>*<NN.*>+``.  This can be used to chunk any sequence
of tokens beginning with an optional determiner ``DT``, followed by
zero or more adjectives of any type ``JJ.*`` (including relative
adjectives like ``earlier/JJR``), followed by one or more nouns of any
type ``NN.*``.  It is easy to find many more difficult examples::

  his/PRP$ Mansion/NNP House/NNP speech/NN
  the/DT price/NN cutting/VBG
  3/CD %/NN to/TO 4/CD %/NN
  more/JJR than/IN 10/CD %/NN
  the/DT fastest/JJS developing/VBG trends/NNS
  's/POS skill/NN

Your challenge will be to come up with tag patterns to cover these and
other examples.

Chunking with Regular Expressions
---------------------------------

The chunker begins with a flat structure in which no tokens are
chunked.  Patterns are applied in turn, successively updating the
chunk structure.  Once all of the patterns have been applied, the
resulting chunk structure is returned.  Listing chunker1_ shows a
simple chunk grammar consisting of two patterns.  The first pattern
matches an optional determiner or possesssive pronoun (recall that
``|`` indicates disjunction), zero or more adjectives, then a
noun. The second rule matches one or more proper nouns.  We also
define some tagged tokens to be chunked, and run the chunker on this
input.

.. pylisting:: chunker1
   :caption: Simple Noun Phrase Chunker
   
    grammar = r"""
      NP: {<DT|PP\$>?<JJ>*<NN>}    # chunk determiners/possessive pronouns, adjectives and nouns
          {<NNP>+}                 # chunk sequences of proper nouns
      """
    cp = chunk.Regexp(grammar)
    tagged_tokens = [("her", "DT"), ("little", "JJ"), ("yellow", "JJ"), ("dog", "NN"),
         ("snarled", "VBD"), ("at", "IN"), ("Easy", "NNP")]

    >>> print cp.parse(tagged_tokens)
    (S
      ('her', 'PP\\$')
      (NP little/JJ yellow/JJ dog/NN)
      ('snarled', 'VBD')
      ('at', 'IN')
      (NP Easy/NNP))
 

If a tag pattern matches at overlapping locations, the first
match takes precedence.  For example, if we apply a rule that matches
two consecutive nouns to a text containing three consecutive nouns,
then only the first two nouns will be chunked:

    >>> nouns = [("money", "NN"), ("market", "NN"), ("fund", "NN")]
    >>> grammar = "NP: {<NN><NN>}  # Chunk two consecutive nouns"
    >>> cp = chunk.Regexp(grammar)
    >>> print cp.parse(nouns)
    (S (NP money/NN market/NN) fund/NN)

Once we have created the chunk for `money market`:lx:, we have
removed the context that would have permitted `fund`:lx: to be
included in a chunk.  This issue would have been avoided with
a more permissive chunk rule, e.g. ``NP: {<NN>+}``.

Developing Chunkers
-------------------

Creating a good chunker usually requires several rounds of
development and testing, during which existing rules are refined and
new rules are added.  In order to diagnose any problems, it often
helps to trace the execution of a chunker, using its ``trace`` argument.
The tracing output shows the rules that are
applied, and uses braces to show the chunks that are created at each
stage of processing.
In Listing chunker2_, two chunk patterns are applied to the input
sentence.  The first rule finds all sequences of three tokens whose
tags are ``DT``, ``JJ``, and ``NN``, and the second rule finds any
sequence of tokens whose tags are either ``DT`` or ``NN``.  We
set up two chunkers, one for each rule ordering, and test them on
the same input.

.. pylisting:: chunker2
   :Caption: Two Noun Phrase Chunkers Having Identical Rules in Different Orders
      
    cp1 = chunk.Regexp(r"""
      NP: {<DT><JJ><NN>}      # Chunk det+adj+noun
          {<DT|NN>+}          # Chunk sequences of NN and DT
      """)
    cp2 = chunk.Regexp(r"""
      NP: {<DT|NN>+}          # Chunk sequences of NN and DT
          {<DT><JJ><NN>}      # Chunk det+adj+noun
      """)
    
    >>> print cp1.parse(tagged_tokens, trace=1)
    # Input:
     <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
    # Chunk det+adj+noun:
    {<DT>  <JJ>  <NN>} <VBD>  <IN>  <DT>  <NN> 
    # Chunk sequences of NN and DT:
    {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
    (S
      (NP the/DT little/JJ cat/NN)
      sat/VBD
      on/IN
      (NP the/DT mat/NN))
    >>> print cp2.parse(tagged_tokens, trace=1)
    # Input:
     <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
    # Chunk sequences of NN and DT:
    {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
    # Chunk det+adj+noun:
    {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
    (S
      (NP the/DT)
      little/JJ
      (NP cat/NN)
      sat/VBD
      on/IN
      (NP the/DT mat/NN))

Observe that when we chunk material that is already partially chunked,
the chunker will only create chunks that do not partially overlap
existing chunks.  In the case of ``cp2``, the second rule
did not find any chunks, since all chunks that matched
its tag pattern overlapped with existing chunks.  As you can see,
you need to be careful to put chunk rules in the right order. 

You may have noted that we have added explanatory comments, preceded
by ``#``, to each of our tag rules. Although it is not strictly
necessary to do this, it's a helpful reminder of what a rule is meant
to do, and it is used as a header line for the output of a rule
application when tracing is on.


You might want to test out some of your rules on a corpus. One option
is to use the Brown corpus. However, you need to  remember that the
Brown tagset is different from the Penn Treebank tagset that we
have been using for our examples so far in this chapter; see Table
brown-tags_ in chap-tag_ for a refresher. Because the Brown tagset
uses ``NP`` for proper nouns, in this example we have followed Abney
in labeling noun chunks as ``NX``.
    >>> grammar = (r"""
    ...    NX: {<AT|AP|PP\$>?<JJ.*>?<NN.*>}      # Chunk article/numeral+adj+noun
    ...        {<NP>+}                      # Chunk one or more proper nouns                   ...          """)
    >>> cp = chunk.Regexp(grammar)
    >>> sent = corpus.brown.tagged('a')[112]
    >>> print cp.parse(sent)
    (S
      (NX His/PP$ contention/NN)
      ('was', 'BEDZ')
      ('denied', 'VBN')
      ('by', 'IN')
      (NX several/AP bankers/NNS)
      (',', ',')
      ('including', 'IN')
      (NX Scott/NP Hudson/NP)
      ('of', 'IN')
      (NX Sherman/NP)
      (',', ',')
      (NX Gaynor/NP B./NP Jones/NP)
      ('of', 'IN')
      (NX Houston/NP)
      (',', ',')
      (NX J./NP B./NP Brady/NP)
      ('of', 'IN')
      (NX Harlingen/NP)
      ('and', 'CC')
      (NX Howard/NP Cox/NP)
      ('of', 'IN')
      (NX Austin/NP)
      ('.', '.'))
 

Exercises
---------

#. |easy| **Chunking Demonstration:**
   Run the chunking demonstration: ``chunk.demo()``

#. |easy| **IOB Tags:**
   The IOB format categorizes tagged tokens as ``I``,
   ``O`` and ``B``.  Why are three tags necessary?  What
   problem would be caused if we used ``I`` and ``O`` tags
   exclusively?
        
#. |easy| Write a tag pattern to match noun phrases containing plural head nouns,
   e.g. "many/JJ researchers/NNS", "two/CD weeks/NNS", "both/DT new/JJ positions/NNS".
   Try to do this by generalizing the tag pattern that handled singular
   noun phrases.

#. |soso| Write a tag pattern to cover noun phrases that contain gerunds,
   e.g. "the/DT receiving/VBG end/NN", "assistant/NN managing/VBG editor/NN".
   Add these patterns to the grammar, one per line.  Test your work using
   some tagged sentences of your own devising.

#. |soso| Write one or more tag patterns to handle coordinated noun phrases,
   e.g. "July/NNP and/CC August/NNP",
   "all/DT your/PRP$ managers/NNS and/CC supervisors/NNS",
   "company/NN courts/NNS and/CC adjudicators/NNS".



----------
Scaling Up
----------


Now you have a taste of what chunking can do, but we have not
explained how to carry out a quantitative evaluation of chunkers.  For
this, we need to get access to a corpus that has been annotated not
only with parts-of-speech, but also with chunk information.  We will
begin by looking at the mechanics of converting IOB format into an
|NLTK| tree, then at how this is done on a larger scale using a
chunked corpus directly.  We will see how to use the corpus to score
the accuracy of a chunker, then look some more flexible ways to
manipulate chunks.  Our focus throughout will be on scaling up the
coverage of a chunker.

Reading IOB Format and the CoNLL 2000 Corpus
--------------------------------------------

Using the ``corpora`` module we can load Wall Street Journal
text that has been tagged, then chunked using the IOB notation.  The
chunk categories provided in this corpus are `np`:gc:, `vp`:gc: and `pp`:gc:.  As we
have seen, each sentence is represented using multiple lines, as shown
below::
    
  he PRP B-NP
  accepted VBD B-VP
  the DT B-NP
  position NN I-NP
  ...

|nopar| A conversion function ``chunk.conllstr2tree()`` builds a tree
representation from one of these multi-line strings.  Moreover, it
permits us to choose any subset of the three chunk types to use.  The
example below produces only `np`` chunks:
    
.. doctest-ignore::
    >>> text = '''
    ... he PRP B-NP
    ... accepted VBD B-VP
    ... the DT B-NP
    ... position NN I-NP
    ... of IN B-PP
    ... vice NN B-NP
    ... chairman NN I-NP
    ... of IN B-PP
    ... Carlyle NNP B-NP
    ... Group NNP I-NP
    ... , , O
    ... a DT B-NP
    ... merchant NN I-NP
    ... banking NN I-NP
    ... concern NN I-NP
    ... . . O
    ... '''
    >>> chunk.conllstr2tree(text, chunk_types=('NP',)).draw()

.. tree:: (S (NP (PRP he))
             (VBD accepted)
             (NP (DT the) (NN position))
             (IN of)
             (NP (NN vice) (NN chairman))
             (IN of)
             (NP (NNP Carlyle) (NNP Group))
             (, ,)
             (NP (DT a) (NN merchant) (NN banking) (NN concern))
             (. .))
   :scale: 80


We can use the NLTK corpus module to access a larger amount of chunked
text.  The CoNLL 2000 corpus contains 270k words of Wall Street
Journal text, divided into "train" and "test" portions, annotated with
part-of-speech tags and chunk tags in the IOB format.  We can access
the data using an NLTK corpus reader called ``conll2000``.  Here is an
example which reads the 100th sentence of the "train" portion of the corpus:

    >>> print corpus.conll2000.chunked('train')[99]
    (S
      (PP Over/IN)
      (NP a/DT cup/NN)
      (PP of/IN)
      (NP coffee/NN)
      (',', ',')
      (NP Mr./NNP Stone/NNP)
      (VP told/VBD)
      (NP his/PRP$ story/NN)
      ('.', '.'))


|nopar|
This  showed three chunk types, for `np`:gc:, `vp`:gc: and `pp`:gc:.
We can also select which chunk types to read:

    >>> print corpus.conll2000.chunked('train', chunk_types=('NP',))[99]
    (S
      ('Over', 'IN')
      (NP a/DT cup/NN)
      ('of', 'IN')
      (NP coffee/NN)
      (',', ',')
      (NP Mr./NNP Stone/NNP)
      ('told', 'VBD')
      (NP his/PRP$ story/NN)
      ('.', '.'))


Simple Evaluation and Baselines
-------------------------------

Armed with a corpus, it is now possible to carry out some simple evaluation.
We start off by establishing a baseline for the trivial chunk parser
``cp`` which creates no chunks:

    >>> cp = chunk.Regexp("")
    >>> print chunk.accuracy(cp, corpus.conll2000.chunked('train', chunk_types=('NP',)))
    0.440845995079

This indicates that more than a third of the words are tagged with
``O`` (i.e., not in an `np`:gc: chunk).  Now let's try a naive regular
expression chunker that looks for tags (.e.g., ``CD``, ``DT``, ``JJ``,
etc) beginning with letters that are typical of noun phrase tags:

    >>> grammar = r"NP: {<[CDJNP].*>+}"
    >>> cp = chunk.Regexp(grammar)
    >>> print chunk.accuracy(cp, corpus.conll2000.chunked('train', chunk_types=('NP',)))
    0.874479872666

We can extend this approach, and create a function ``chunked_tags()``
that takes some chunked data, and sets up a conditional frequency
distribution.  For each tag, it counts up the number of times the tag
occurs inside an `np`:gc: chunk (the ``True`` case, where ``chtag`` is
``B-NP`` or ``I-NP``), or outside a chunk (the ``False`` case, where
``chtag`` is ``O``).  It returns a list of those tags that occur
inside chunks more often than outside chunks.

.. pylisting:: chunker2
   :caption: Capturing the conditional frequency of NP Chunk Tags

    def chunked_tags(train):
        """Generate a list of tags that tend to appear inside chunks"""
        cfdist = nltk.ConditionalFreqDist()
        for t in train:
            for word, tag, chtag in chunk.tree2conlltags(t):
                if chtag == "O":
                    cfdist[tag].inc(False)
                else:
                    cfdist[tag].inc(True)
        return [tag for tag in cfdist.conditions() if cfdist[tag].max() == True]
    >>> print chunked_tags(corpus.conll2000.chunked('train', chunk_types=('NP',)))
    ['PRP$', 'WDT', 'JJ', 'WP', 'DT', '#', '$', 'NN', 'FW', 'POS',
        'PRP', 'NNS', 'NNP', 'PDT', 'RBS', 'EX', 'WP$', 'CD', 'NNPS',
        'JJS', 'JJR']

The next step is to convert this list of tags into a tag pattern.  To
do this we need to "escape" all non-word characters, by preceding them
with a backslash.  Then we need to join them into a disjunction.  This
process would convert a tag list ``['NN', 'NN$']`` into the tag pattern
``<NN|NN\$>``.  The following function does this work, and returns a
regular expression chunker:

.. pylisting:: chunker3
   :caption: Deriving a Regexp Chunker from Training Data

    def baseline_chunker(train):
        chunk_tags = [re.sub(r'(\W)', r'\\\1', tag)
                      for tag in chunked_tags(train)]
        grammar = 'NP: {<' + '|'.join(chunk_tags) + '>+}'
        return chunk.Regexp(grammar)

|nopar|
The final step is to train this chunker and test its accuracy (this
time on the "test" portion of the corpus, i.e., data not seen during training):

    >>> cp = baseline_chunker(corpus.conll2000.chunked('train', chunk_types=('NP',)))
    >>> print chunk.accuracy(cp, corpus.conll2000.chunked('test', chunk_types=('NP',)))
    0.914262194736


Splitting and Merging (incomplete)
----------------------------------

[Notes: the above approach creates chunks that are too large,
e.g. `the cat the dog chased`:lx: would be given a single `np`:gc: chunk
because it does not detect that determiners introduce new chunks.
For this we would need a rule to split an `np`:gc: chunk
prior to any determiner, using a pattern like:
``"NP: <.*>}{<DT>"``.  We can also merge chunks, e.g.
``"NP: <NN>{}<NN>"``.]

Chinking
--------
      
Sometimes it is easier to define what we *don't* want to include in a
chunk than it is to define what we *do* want to include.  In these
cases, it may be easier to build a chunker using a method called
`chinking`:dt:.


Following Abney, we define a `chink`:dt: as a sequence
of tokens that is not included in a chunk.
In the following example, ``sat/VBD on/IN`` is a chink::

  [ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]

Chinking is the process of removing a sequence of tokens from a
chunk.  If the sequence of tokens spans an entire chunk, then the
whole chunk is removed; if the sequence of tokens appears in the
middle of the chunk, these tokens are removed, leaving two chunks
where there was only one before.  If the sequence is at the beginning
or end of the chunk, these tokens are removed, and a smaller chunk
remains.  These three possibilities are illustrated in Table chinking_.

.. table:: chinking

   +-------------+---------------------+-------------------+------------------+
   |             | Entire chunk        | Middle of a chunk | End of a chunk   |
   +-------------+---------------------+-------------------+------------------+
   | *Input*     | [a/DT  big/JJ       | [a/DT  big/JJ     | [a/DT  big/JJ    | 
   |             | cat/NN]             | cat/NN]           | cat/NN]          |
   +-------------+---------------------+-------------------+------------------+
   | *Operation* | Chink "DT JJ NN"    | Chink "JJ"        | Chink "NN"       |
   +-------------+---------------------+-------------------+------------------+
   | *Pattern*   | "}DT JJ NN{"        | "}JJ{"            | "}NN{"           |
   +-------------+---------------------+-------------------+------------------+
   | *Output*    | a/DT  big/JJ        | [a/DT] big/JJ     | [a/DT  big/JJ]   |
   |             | cat/NN              | [cat/NN]          | cat/NN           |
   +-------------+---------------------+-------------------+------------------+

   Three chinking rules applied to the same chunk

In the following grammar, we put the entire sentence into a single
chunk, then excise the chink:

    >>> grammar = r"""
    ... NP:
    ...   {<.*>+}          # Chunk everything
    ...   }<VBD|IN>+{      # Chink sequences of VBD and IN
    ... """
    >>> cp = chunk.Regexp(grammar)
    >>> print cp.parse(tagged_tokens)
    (S
      (NP the/DT little/JJ cat/NN)
      sat/VBD
      on/IN
      (NP the/DT mat/NN))
    >>> print chunk.accuracy(cp, corpus.conll2000.chunked('test', chunk_types=('NP',)))
    0.581041433607

|nopar|
A chunk grammar can use any number of chunking and chinking patterns
in any order.

Multiple Chunk Types (incomplete)
---------------------------------

So far we have only developed `np`:gc: chunkers.  However, as we saw earlier
in the chapter, the CoNLL chunking data is also annotated for `pp`:gc: and
`vp`:gc: chunks.  Here is an example, to show the structure we get from the
corpus and the flattened version that will be used as input to the parser.

    >>> example = corpus.conll2000.chunked('train')[99]
    (S
      (PP Over/IN)
      (NP a/DT cup/NN)
      (PP of/IN)
      (NP coffee/NN)
      (',', ',')
      (NP Mr./NNP Stone/NNP)
      (VP told/VBD)
      (NP his/PRP$ story/NN)
      ('.', '.'))
    >>> print example.flatten()
    (S
      ('Over', 'IN')
      ('a', 'DT')
      ('cup', 'NN')
      ('of', 'IN')
      ('coffee', 'NN')
      (',', ',')
      ('Mr.', 'NNP')
      ('Stone', 'NNP')
      ('told', 'VBD')
      ('his', 'PRP$')
      ('story', 'NN')
      ('.', '.'))

Now we can set up a multi-stage chunk grammar, as shown in
Listing multistage-chunker_.  It has a stage for each of the chunk types.

.. pylisting:: multistage-chunker
   :caption:

    cp = chunk.Regexp(r"""
      NP: {<DT>?<JJ>*<NN.*>+} # noun phrase chunks
      VP: {<TO>?<VB.*>}       # verb phrase chunks
      PP: {<IN>}              # prepositional phrase chunks
      """)

    >>> example = corpus.conll2000.chunked('train')[99]
    >>> print cp.parse(example.flatten(), trace=1)
    # Input:
     <IN>  <DT>  <NN>  <IN>  <NN>  <,>  <NNP>  <NNP>  <VBD>  <PRP$>  <NN>  <.> 
    # noun phrase chunks:
     <IN> {<DT>  <NN>} <IN> {<NN>} <,> {<NNP>  <NNP>} <VBD>  <PRP$> {<NN>} <.> 
    # Input:
     <IN>  <NP>  <IN>  <NP>  <,>  <NP>  <VBD>  <PRP$>  <NP>  <.> 
    # verb phrase chunks:
     <IN>  <NP>  <IN>  <NP>  <,>  <NP> {<VBD>} <PRP$>  <NP>  <.> 
    # Input:
     <IN>  <NP>  <IN>  <NP>  <,>  <NP>  <VP>  <PRP$>  <NP>  <.> 
    # prepositional phrase chunks:
    {<IN>} <NP> {<IN>} <NP>  <,>  <NP>  <VP>  <PRP$>  <NP>  <.> 
    (S
      (PP Over/IN)
      (NP a/DT cup/NN)
      (PP of/IN)
      (NP coffee/NN)
      (',', ',')
      (NP Mr./NNP Stone/NNP)
      (VP told/VBD)
      ('his', 'PRP$')
      (NP story/NN)
      ('.', '.'))


Exercises
---------

#. |easy|
   Pick one of the three chunk types in the CoNLL corpus.
   Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences
   that make up this kind of chunk.  Develop a simple chunker using
   the regular expression chunker ``chunk.Regexp``.
   Discuss any tag sequences that are difficult to chunk reliably.

#. |easy|
   An early definition of *chunk* was the material that occurs between chinks.
   Develop a chunker which starts by putting the whole sentence in a single
   chunk, and then does the rest of its work solely by chinking.
   Determine which tags (or tag sequences) are most likely to make up chinks
   with the help of your own utility program.  Compare the performance and
   simplicity of this approach relative to a chunker based entirely on
   chunk rules.

#. |soso|
   Develop a chunker for one of the chunk types in the CoNLL corpus using a
   regular-expression based chunk grammar ``RegexpChunk``.  Use any
   combination of rules for chunking, chinking, merging or splitting.

#. |soso| Sometimes a word is incorrectly tagged, e.g. the head noun in
   "12/CD or/CC so/RB cases/VBZ".  Instead of requiring manual correction of
   tagger output, good chunkers are able to work with the erroneous
   output of taggers.  Look for other examples of correctly chunked
   noun phrases with incorrect tags.

#. |hard|
   We saw in the tagging chapter that it is possible to establish
   an upper limit to tagging performance by looking for ambiguous n-grams,
   n-grams that are tagged in more than one possible way in the training data.
   Apply the same method to determine an upper bound on the performance
   of an n-gram chunker.

#. |hard|
   Pick one of the three chunk types in the CoNLL corpus.  Write functions
   to do the following tasks for your chosen type:

   a) List all the tag sequences that occur with each instance of this chunk type.
   b) Count the frequency of each tag sequence, and produce a ranked list in
      order of decreasing frequency; each line should consist of an integer (the frequency)
      and the tag sequence.
   c) Inspect the high-frequency tag sequences.  Use these as the basis for
      developing a better chunker.

#. |hard|
   The baseline chunker presented in the evaluation section tends to
   create larger chunks than it should.  For example, the
   phrase:
   ``[every/DT time/NN] [she/PRP] sees/VBZ [a/DT newspaper/NN]``
   contains two consecutive chunks, and our baseline chunker will
   incorrectly combine the first two: ``[every/DT time/NN she/PRP]``.
   Write a program that finds which of these chunk-internal tags
   typically occur at the start of a chunk, then
   devise one or more rules that will split up these chunks.
   Combine these with the existing baseline chunker and
   re-evaluate it, to see if you have discovered an improved baseline.

#. |hard|
   Develop an `np`:gc: chunker which converts POS-tagged text into a list of
   tuples, where each tuple consists of a verb followed by a sequence of
   noun phrases and prepositions,
   e.g. ``the little cat sat on the mat`` becomes ``('sat', 'on', 'NP')``...
        
#. |hard|
   The Penn Treebank contains a section of tagged Wall Street Journal text
   which has been chunked into noun phrases.  The format uses square brackets,
   and we have encountered it several times during this chapter.
   The Treebank corpus can be accessed using:
   ``for sent in corpus.treebank.chunked(item)``.  These items are flat trees,
   just as we got using ``corpus.conll2000.chunked()``.

   a) Consult the documentation for the NLTK chunk package to find out how
      to generate Treebank and IOB strings from a tree.
      Write functions ``chunk2brackets()`` and ``chunk2iob()`` which take a single
      chunk tree as their sole argument, and return the required multi-line string
      representation.
   b) Write command-line conversion utilities ``bracket2iob.py`` and ``iob2bracket.py``
      that take a file in Treebank or CoNLL format (resp) and convert it to the other
      format.  (Obtain some raw Treebank or CoNLL data from the NLTK Corpora, save it
      to a file, and then use ``for line in open(filename)`` to access it from Python.)

.. _n-gram-chunking:

---------------
N-Gram Chunking
---------------

Our approach to chunking has been to try to detect structure based on
the part-of-speech tags.  We have seen that the IOB format represents
this extra structure using another kind of tag.  The question arises
as to whether we could use the same n-gram tagging methods we saw in
the last chapter, applied to a different vocabulary. In this case,
rather than trying to determine the correct part-of-speech tag, given
a word, we are trying to determine the correct chunk tag, given a
part-of-speech tag.

The first step is to get the ``word, tag, chunk`` triples from the
CoNLL 2000 corpus and map these to ``tag, chunk`` pairs:

    >>> chunk_data = [[(t,c) for w,t,c in chunk.tree2conlltags(chtree)]
    ...              for chtree in corpus.conll2000.chunked('train')]

A Unigram Chunker
-----------------

Now we can train and score a `unigram chunker`:dt: on this data, just as if
it was a tagger:

    >>> unigram_chunker = tag.Unigram()
    >>> unigram_chunker.train(chunk_data)
    >>> print tag.accuracy(unigram_chunker, chunk_data)
    0.781378851068

This chunker does reasonably well.  Let's look at the errors it makes.
Consider the opening phrase of the first sentence of the CONLL chunking
data, here shown with part-of-speech tags:

  Confidence/NN in/IN the/DT pound/NN is/VBZ widely/RB expected/VBN
  to/TO take/VB another/DT sharp/JJ dive/NN

We can try out the unigram chunker on this first sentence by creating
some "tokens" using ``[t for t,c in chunk_data[0]]``, then running
our chunker over them using ``list(unigram_chunker.tag(tokens))``.
The unigram chunker only looks at the tags, and tries to add chunk
tags.  Here is what it comes up with:

  NN/I-NP IN/B-PP DT/B-NP NN/I-NP VBZ/B-VP RB/O VBN/I-VP TO/B-PP
  VB/I-VP DT/B-NP JJ/I-NP NN/I-NP

Notice that it tags all instances of ``NN`` with ``I-NP``, because
nouns usually do not appear at the beginning of noun phrases in
the training data.  Thus, the first noun ``Confidence/NN`` is
tagged incorrectly.  However, ``pound/NN`` and ``dive`` are
correctly tagged as ``I-NP``; they are not in the initial position
that should be tagged ``B-NP``.  It incorrectly tags
``widely/RB`` as outside ``O``, and it incorrectly tags the
infinitival ``to/TO`` as ``B-PP``, as if it was a preposition starting a
prepositional phrase.

A Bigram Chunker (incomplete)
-----------------------------

[Why these problems might go away if we look at the previous chunk tag?]

Let's run a bigram chunker:

    >>> bigram_chunker = tag.Bigram(backoff=unigram_chunker)
    >>> bigram_chunker.train(chunk_data)
    >>> print tag.accuracy(bigram_chunker, chunk_data)
    0.89312652614

We can run the bigram chunker over the same sentence as before
using ``list(bigram_chunker.tag(tokens))``.
Here is what it comes up with:

  NN/B-NP IN/B-PP DT/B-NP NN/I-NP VBZ/B-VP RB/I-VP VBN/I-VP TO/I-VP
  VB/I-VP DT/B-NP JJ/I-NP NN/I-NP

This is 100% correct.

Exercises
---------

#. |soso|
   The bigram chunker scores about 90% accuracy.
   Study its errors and try to work out why it doesn't get 100% accuracy.

#. |soso|
   Experiment with trigram chunking.  Are you
   able to improve the performance any more?

#. |hard|
   An n-gram chunker can use information other than the current
   part-of-speech tag and the `n-1`:math: previous chunk tags.
   Investigate other models of the context, such as
   the `n-1`:math: previous part-of-speech tags, or some combination of
   previous chunk tags along with previous and following part-of-speech tags.

#. |hard|
   Consider the way an n-gram tagger uses recent tags to inform its tagging choice.
   Now observe how a chunker may re-use this sequence information.  For example,
   both tasks will make use of the information that nouns tend to follow adjectives
   (in English).  It would appear that the same information is being maintained in
   two places.  Is this likely to become a problem as the size of the rule sets grows?
   If so, speculate about any ways that this problem might be addressed.

-----------------
Cascaded Chunkers
-----------------

So far, our chunk structures have been relatively flat.  Trees consist
of tagged tokens, optionally grouped under a chunk node such as
``NP``.  However, it is possible to build chunk structures of
arbitrary depth, simply by creating a multi-stage chunk grammar.

So far, our chunk grammars have consisted of a single stage: a chunk
type followed by one or more patterns.  However, chunk grammars can
have two or more such stages.  These stages are processed in the order
that they appear.  The patterns in later stages can refer to a mixture
of part-of-speech tags and chunk types.  Listing cascaded-chunker_ has
patterns for noun phrases, prepositional phrases, verb phrases, and
sentences.
This is a four-stage chunk grammar, and can be used to create
structures having a depth of at most four.

.. pylisting:: cascaded-chunker
   :caption: A Chunker that Handles NP, PP, VP and S

    grammar = r"""
      NP: {<DT|JJ|NN.*>+}       # Chunk sequences of DT, JJ, NN
      PP: {<IN><NP>}            # Chunk prepositions followed by NP
      VP: {<VB.*><NP|PP|S>+$}   # Chunk rightmost verbs and arguments/adjuncts
      S:  {<NP><VP>}            # Chunk NP, VP
      """
    cp = chunk.Regexp(grammar)
    tagged_tokens = [("Mary", "NN"), ("saw", "VBD"), ("the", "DT"), ("cat", "NN"),
        ("sit", "VB"), ("on", "IN"), ("the", "DT"), ("mat", "NN")]

    >>> print cp.parse(tagged_tokens)
    (S
      (NP Mary/NN)
      saw/VBD
      (S
        (NP the/DT cat/NN)
        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))

Unfortunately this result misses the `vp`:gc: headed by `saw`:lx:.  It has
other shortcomings too.  Let's see what happens when we apply this
chunker to a sentence having deeper nesting.

    >>> tagged_tokens = [("John", "NNP"), ("thinks", "VBZ"), ("Mary", "NN"),
    ...     ("saw", "VBD"), ("the", "DT"), ("cat", "NN"), ("sit", "VB"),
    ...     ("on", "IN"), ("the", "DT"), ("mat", "NN")]
    >>> print cp.parse(tagged_tokens)
    (S
      (NP John/NNP)
      thinks/VBZ
      (NP Mary/NN)
      saw/VBD
      (S
        (NP the/DT cat/NN)
        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))

The solution to these problems is to get the chunker to loop over its
patterns: after trying all of them, it repeats the process.
We add an optional second argument ``loop`` to specify the number
of times the set of patterns should be run:

    >>> cp = chunk.Regexp(grammar, loop=2)
    >>> print cp.parse(tagged_tokens)
    (S
      (NP John/NNP)
      thinks/VBZ
      (S
        (NP Mary/NN)
        (VP
          saw/VBD
          (S
            (NP the/DT cat/NN)
            (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))

This cascading process enables us to create deep structures.  However,
creating and debugging a cascade is quite difficult, and there comes
a point where it is more effective to do full parsing (see Chapter chap-parse_).


----------------------
Information Extraction
----------------------

`Information Extraction`:dt: is the task of converting `unstructured
data`:dt: (e.g., unrestricted text) or `semi-structured data`:dt:
(e.g., web pages marked up with |HTML|) into `structured data`:dt:
(e.g., tables in a relational database). For example, let's suppose we
are given a text containing the fragment ie1_, and let's also suppose we
are trying to find pairs of entities *X* and *Y* that stand in the
relation 'organization *X* is located in location *Y*'.

.. _ie1:
.. ex:: ... said William Gale, an economist at the Brookings Institution,
        the research group in Washington. 

|nopar| As a result of processing this text, we should be able to add
the pair |langle|\ *Brookings Institution*, *Washington*\ |rangle| to
this relation. As we will see shortly, Information Extraction proceeds
on the assumption that we are only looking for specific sorts of
information, and these have been decided in advance. This limitation
has been a necessary concession to allow the robust processing of
unrestricted text.

Potential applications of Information Extraction are
many, and include business intelligence, resume harvesting, media
analysis, sentiment detection patent search, and email scanning. A
particularly important area of current research involves the attempt
to extract structured data out of electronically-available scientific
literature, most notably in the domain of biology and medicine. 

Information Extraction is usually broken down into at least two major
steps: `Named Entity Recognition`:dt: and `Relation
Extraction`:dt:. Named Entities (|NE| \s) are usually taken to be noun phrases
that denote specific types of individuals such as organizations,
persons, dates, and so on. Thus, we
might use the following |XML| annotations to mark-up the |NE|\ s in ie1_:

.. _ie2:
.. ex:: ... said <ne type='PERSON'>William Gale</ne>, an economist at
        the <ne type='ORGANIZATION'>Brookings Institution</ne>,
        the research group in <ne type='LOCATION'>Washington<ne>. 

How do we go about identifying |NE|\ s? Our first thought might be
that we could look up candidate expressions in an appropriate list of
names. For example, in the case of locations, we might  try using a
resource such as the `Alexandria Gazetteer
<http://www.alexandria.ucsb.edu/gazetteer/>`_. Depending on the nature
of our input data, this be adequate |mdash| such a gazetteer is likely
to have good coverage of international cities and many locations in
the U.S.A., but will probably be missing the names of obscure villages
in remote regions. However, a list of names for people or organization
will probably have poor coverage. New organizations, and new names for
them, are coming into existence every day, so if we are trying to deal
with contemporary newswire or blog entries, say, it is unlikely that
we will be able to recognize many of the |NE|\ s by using gazetteer
lookup. 

A second consideration is that many |NE| terms are ambiguous. Thus
`May`:lx: and `North`:lx: are likely to be parts of |NE|\ s for DATE
and LOCATION, respectively, but could both be part of a PERSON |NE|;
conversely `Christian Dior`:lx: looks like a PERSON |NE| but is more
likely to be of type ORGANIZATION. A terms like `Yankee`:lx: will be
ordinary modifier in some contexts, but will be marked as an |NE| of
type ORGANIZATION in the phrase `Yankee infielders`:lx:. To summarize,
we cannot reliably detect |NE|\ s by looking them up in a gazetteer,
and it is also hard to develop rules that will correctly recognize
ambiguous |NE|\ s on the basis of their context of
occurrence. Although lookup may contribute to a solution, most
contemporary approaches to Named Entity Recognition treat it as a
statistical classification task that requires training data for good
performance. This task is facilitated by adopting an appropriate data
representation, such as the IOB tags which we saw being deployed in
the |CoNLL| chunk data (Chapter chap-chunk_). For example, here are a
representative few lines from the CONLL 2002 (``conll2002``) Dutch
training data::

    Eddy N B-PER
    Bonte N I-PER
    is V O
    woordvoerder N O
    van Prep O
    diezelfde Pron O
    Hogeschool N B-ORG
    . Punc O

|nopar| As noted before, in this representation, there is one token
per line, each with its part-of-speech tag and its |NE| tag.  When
|NE|\ s have been identified in a text, we then want to extract
relations that hold between them. As indicated earlier, we will
typically be looking for relations between specified types of
|NE|. One way of approaching this task is to initially look for all
triples of the form *X*, |alpha|, *Y*, where *X* and *Y* are |NE| \s
of the required types, and |alpha| is the string of words that
intervenes between *X* and *Y*. We can then use regular expressions to
pull out just those instances of |alpha| that express the relation
that we are looking for. The following example searches for strings
that contain the word `in`:lx:. The special character expression
``(?!\b.+ing\b)`` is a negative lookahead condition which allows us to
disregard strings such as `success in supervising the transition
of`:lx:, where `in`:lx: is followed by a gerundive verb.

    >>> from nltk.contrib.ieer_rels import *
    >>> from itertools import islice
    >>> IN = re.compile(r'.*\bin\b(?!\b.+ing\b)')
    >>> ieer_trees = (d['text'] for d in ieer.dictionary())
    >>> for r in islice(ieer_trees, relextract('ORG', 'LOC', pattern = IN), 29, 39): 
    ...     print show_tuple(r)
    [ORG: Cooper-Hewitt Museum] in [LOC: New York]
    [ORG: Canadian Museum of Civilization] in [LOC: Hull]
    [ORG: Kramerbooks and Afterwords] , an independent store in [LOC: Washington]
    [ORG: Waterford Foundation] in historic [LOC: Waterford]
    [ORG: U.S. District Court] in [LOC: Manhattan]
    [ORG: House Transportation Committee] , secured the most money in the [LOC: New York]
    [ORG: Angels] in [LOC: Anaheim]
    [ORG: Bayona] , which is in the heart of the [LOC: French Quarter]
    [ORG: CBS Studio Center] in [LOC: Studio City]
    [ORG: Smithsonian Institution] in [LOC: Washington]

|nopar| As you will see, although searching for `in`:lx: does a reasonably
good job, there is some inaccuracy in the output which is hard to
avoid |mdash| there is unlikely to be simple string-based method of excluding
fillers such as `secured the most money in the`:lx:.

As shown above, the ``conll2002`` corpus contains not just |NE|
annotation but also part-of-speech tags. In principle, this allows us
to devise patterns which are sensitive to these tags.

    >>> vnv = """
    >>> (
    >>> is/V|
    >>> was/V|
    >>> werd/V|
    >>> wordt/V
    >>> )
    >>> .*
    >>> van/Prep
    >>> """
    >>> VAN = re.compile(vnv, re.VERBOSE)
    >>> for r in relextract('PER', 'ORG', corpus='conll2002-ned', pattern = VAN): 
    ...     print show_tuple(r)


----------
Conclusion
----------

In this chapter we have explored efficient and robust methods that can
identify linguistic structures in text.  Using only part-of-speech
information for words in the local context, a "chunker" can
successfully identify simple structures such as noun phrases and verb
groups.  We have seen how chunking methods extend the same lightweight
methods that were successful in tagging.  The resulting structured
information is useful in information extraction tasks and in the
description of the syntactic environments of words.  The latter will
be invaluable as we move to full parsing.

There are a surprising number of ways to chunk a sentence using
regular expressions.  The patterns can add, shift and remove chunks in
many ways, and the patterns can be sequentially ordered in many ways.
One can use a small number of very complex rules, or a long sequence
of much simpler rules.  One can hand-craft a collection of rules, and
one can write programs to analyze a chunked corpus to help in the
development of such rules.  The process is painstaking, but generates
very compact chunkers that perform well and that transparently encode
linguistic knowledge.

It is also possible to chunk a sentence using the techniques of n-gram
tagging.  Instead of assigning part-of-speech tags to words, we assign
IOB tags to the part-of-speech tags.  Bigram tagging turned out to be
particularly effective, as it could be sensitive to the chunk tag on
the previous word.  This statistical approach requires far less effort
than rule-based chunking, but creates large models and delivers few
linguistic insights.

Like tagging, chunking cannot be done perfectly.  For example, as
pointed out by [Abney1996PST]_, we cannot correctly analyze the structure
of the sentence *I turned off the spectroroute* without knowing the
meaning of *spectroroute*; is it a kind of road or a type of device?
Without knowing this, we cannot tell whether *off* is part of a
prepositional phrase indicating direction (tagged ``B-PP``), or
whether *off* is part of the verb-particle construction *turn off*
(tagged ``I-VP``).

A recurring theme of this chapter has been `diagnosis`:dt:.  The simplest
kind is manual, when we inspect the tracing output of a chunker and
observe some undesirable behavior that we would like to fix.
Sometimes we discover cases where we cannot hope to get the correct
answer because the part-of-speech tags are too impoverished and do not
give us sufficient information about the lexical item.  A second
approach is to write utility programs to analyze the training data,
such as counting the number of times a given part-of-speech tag occurs
inside and outside an `np`:gc: chunk.  A third approach is to evaluate the
system against some gold standard data to obtain an overall
performance score.  We can even use this to parameterize the system,
specifying which chunk rules are used on a given run, and tabulating
performance for different parameter combinations.  Careful use of
these diagnostic methods permits us to optimize the performance of our
system.  We will see this theme emerge again later in chapters dealing
with other topics in natural language processing.

---------------
Further Reading
---------------

The popularity of chunking is due in great part to pioneering work by
Abney e.g., [Abney1996PST]_. Abney's Cass chunker is available at
`<http://www.vinartus.net/spa/97a.pdf>`_

The word `chink`:dt: initially meant a sequence of stopwords,
according to a 1975 paper by Ross and Tukey [Abney1996PST]_.

The IOB format (or sometimes  `BIO Format`:dt:) was developed for
`np`:gc: chunking by [Ramshaw1995TCU]_, and was used for the shared `np`:gc:
bracketing task run by the *Conference on Natural Language Learning*
(|CoNLL|) in 1999.  The same format was
adopted by |CoNLL| 2000 for annotating a section of Wall Street
Journal text as part of a shared task on `np`:gc: chunking.




.. include:: footer.txt
