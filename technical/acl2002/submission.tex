\documentclass[11pt]{article}
\usepackage{acl02sub,url,alltt,epsfig}
\title{NLTK: The Natural Language Toolkit}
\author{
Edward Loper and Steven Bird\\
Department of Computer and Information Science and Linguistic Data Consortium\\
University of Pennsylvania, Philadelphia, PA 19104-6389, USA
}
\summary{%
NLTK, the Natural Language Toolkit, is a suite of program modules,
tutorials and problem sets, providing ready-to-use CL courseware.
NLTK covers symbolic and statistical
NLP, and is interfaced to annotated corpora.
Students augment and replace existing components, learning
structured programming by example, and manipulating sophisticated
models from the outset.}
\paperid{Pxxxx}
\keywords{teaching, courseware, Python, corpora}
\contact{Edward Loper}
\conference{this paper has not been submitted to any other conferences}
\date{\today}

% Note: the maximum paper length is 6 pages.

% Outline:
%   - abstract
%   - introduction
%   - choice of programming language
%   - design criteria
%   - python implementation (??)
%   - modules
%   - uses
%     - assignments/projects
%     - class demonstrations
%   - evaluation
%   - other approaches
%   - conclusion

% To do:
%   - FSA module description
%   - visualization modules description
%   - in-class demonstrations section
%   - Get rid of Python implementation section, or at least shorten it
%   - General clean-up

\newenvironment{sv}{\scriptsize\begin{alltt}}{\end{alltt}\normalsize}

\begin{document}
\makeidpage
\maketitle

\begin{abstract}
Students in computational linguistics courses must often learn a new
programming language.  In some cases, such as when courses are offered
in linguistics departments, the students may be learning to program
for the very time.  In order to do interesting projects, it is usually
necessary for students to do many low-level ``housekeeping'' tasks.
At the same time, teachers of computational linguistics courses
sometimes feel that they spend too much time teaching students to
program, and not enough time teaching the subject itself.  They may
even avoid programming assignments altogether.  However, we believe
that it is crucial for a first computational linguistics course to
include a strong practical component, in which students develop real
programs to solve real problems with real data.

Python is a new object-oriented scripting language which runs on all
platforms.  Python has been praised as ``executable pseudocode,'' since
programs are so easy to write.  Recently, we have been developing
NLTK, an open-source Natural Language Toolkit written in Python.
In this presentation, we will motivate, describe and demonstrate NLTK.

NLTK, the Natural Language Toolkit, is a suite of program modules,
tutorials and problem sets.  NLTK covers symbolic and statistical
natural language processing, and is interfaced to annotated corpora.
Students augment and replace existing NLTK components, learning
structured programming by example, and manipulating sophisticated
models from the outset.  Along with extensive documentation and
problem sets, NLTK provides self-contained, ready-to-use CL
courseware.
\end{abstract}

\section{Introduction}

Teachers of introductory courses on computational linguistics are
often faced with the challenge of setting up a practical programming
component for student assignments and projects.  This is difficult not
just because of the variety of data structures, but also because of
the diverse range of topics which may need to be included in the
syllabus.  A widespread practice is to teach multiple programming
languages, where each language provides native data structures and functions
that are a good fit for the task at hand (e.g. Prolog for parsing,
Perl for corpus processing, a finite-state toolkit for morphological
analysis).  By relying on the built-in features of various languages,
the teacher avoids having to develop a lot of software infrastructure.
An unfortunate consequence is that a significant part of the course
must be devoted to teaching these languages.  Further, many
interesting projects require the languages to be bridged, e.g. a
project that involved syntactic parsing of corpus data from a
morphologically rich language would involve all three languages
mentioned above (Perl for file I/O, format conversions and overall
program control, with calls out to a finite state toolkit for
morphological analysis and to a Prolog engine for parsing).  It is
clear that these considerable overheads and shortcomings warrant a
fresh approach.

Apart from the practical component, computational linguistics courses
may also depend on software for in-class
demonstrations.  This context calls for highly interactive graphical
user interfaces making it possible to view program state (e.g. the
chart of a chart parser), observe program execution step-by-step
(e.g. execution of a finite-state machine), and even make minor
modifications to programs in response to ``what if'' questions from
the class.  Because of these difficulties it is common to avoid live
demonstrations, and keep classes for theoretical presentations only.
Apart from being dull, this approach leaves students to solve
important practical problems on their own, or to deal with them less
efficiently in office hours.

In this paper we describe a new approach to the above challenges,
a streamlined and flexible way of organizing the practical component
of an introductory computational linguistics course.  We describe
NLTK, the Natural Language Toolkit, which we have developed in
conjunction with a course we have taught at the University of Pennsylvania.

OVERVIEW OF THE PAPER

All materials discussed here are available under an open
source license from \url{nltk.sf.net}.
NLTK runs on most platforms, including Windows, OS X, Linux, and
UNIX.

\section{Choice of programming language}

The most basic step in setting up a practical component is choosing a
suitable programming language.  Here we list the desiderata that influence
our choice.

First, the language must have a shallow learning curve, so that novice
programmers get immediate rewards for their efforts.
Second, the language must support rapid prototyping and a short develop/test cycle;
an obligatory compilation step is a serious detraction.
Third, the code should be self-documenting,
with a transparent syntax and semantics.
Fourth, it should be easy to write structured programs, ideally object-oriented
but without the burden associated with languages like C++.
Finally, the language must have an easy-to-use graphics library to support
the development of simple graphical user interfaces.

In surveying the available languages, we believe that Python offers an especially good
fit to the above requirements.  Python is an object-oriented scripting language
developed by Guido van Rossum [REF] and available on all platforms
\url{www.python.org}.
Python offers a shallow learning curve by virtue of the
fact that it was designed to be easily learnt by children [REF].
As an interpreted language, Python is suitable for rapid prototyping.
Python code is exceptionally readable, and it has been praised as
``executable pseudocode.''
Python is an object-oriented language, but not punitively so, and it
is easy to encapsulate data and methods inside Python classes.
Finally, Python has an interface to the ``tk'' graphics toolkit, and
writing graphical interfaces is straightforward.

\section{Design criteria}


Several criteria must be considered in the design and implementation
of the toolkit.  The design criteria are divided into primary and
secondary criteria, and within those sections are listed in the order
of their importance.  A set of explicit non-requirements is also
given.  The toolkit is not expected to satisfy any of these
properties. 

\subsection{Primary Design Criteria}

\noindent\textbf{Ease of Use.} The primary purpose of the toolkit is
to allow students to concentrate on building NLP systems.  The more
time students must spend learning to use the toolkit, the less useful
it is.

\noindent\textbf{Consistency.} The toolkit should use consistent data
structures and interfaces.

\noindent\textbf{Extensibility.} The toolkit should easily
accommodate new components, whether those components replicate or
extend the toolkit's existing functionality.  Thus, the toolkit's
design should be modular, with simple and well-defined interfaces
between modules.  The toolkit should also be structured in such a way
that it is obvious where new extensions would fit into the toolkit's
infrastructure.

\noindent\textbf{Documentation.} The toolkit, its data structures,
and its implementation all need to be documented.  Three documentation
types are necessary: technical reports, to explain the toolkit's
design and implementation; tutorials, to teach students how to use the
toolkit to perform specific tasks; and reference documentation, to
describe every module, interface, class and method.  All nomenclature
must be carefully chosen and consistently used.

\subsection{Secondary Design Criteria}

\noindent\textbf{Simplicity.} The toolkit should structure the
complexities of building NLP systems, not hide them.  Therefore, each
class defined by the toolkit should be simple enough that a student
could implement it by the time they finish the course.

\noindent\textbf{Modularity.} The interaction between different
components of the toolkit should be kept to a minimum.  In particular,
it should be possible to complete individual projects using small
parts of the toolkit, without worrying about how they interact with
the rest of the toolkit.  This will allow students to learn to use the
toolkit incrementally throughout the course.  Modularity also makes it
easier to change or extend the toolkit.

\subsection{Non-Requirements}

\noindent\textbf{Comprehensiveness.} The toolkit is not intended to
provide a comprehensive set of tools.  Indeed, there should be a wide
variety of ways in which students can extend the toolkit.

\noindent\textbf{Efficiency.} The toolkit does not need to be highly
optimized for runtime performance.  However, it should be efficient
enough that students can use their NLP systems to perform real tasks.

\noindent\textbf{Cleverness.} Clear designs and implementations are
far preferable to ingenious yet indecipherable ones.

\section{Python Implementation}

The structure of the Python language directly affects the design and
implementation of the toolkit.  This section discusses how the design
of the toolkit is affected by the Python language.

% [EL] This is the only section that I think we might want to keep
% from python impl.  But I'm not sure we even need to keep this one.
\noindent\textbf{Interfaces.}
Python does not directly implement interfaces.  The toolkit
therefore implements interfaces as simple classes, all of whose methods 
raise \texttt{AssertionError} exceptions.  A description of the
functionality provided by the interface is contained in the
class's documentation string.  Each method's documentation string
specifies the behavior of that method.
In order to implement an interface, a class should include that
interface as one of its bases, and should override every
(non-optional) method defined by the interface.
Classes that define interfaces are named with a trailing ``I,'' such
as \texttt{TokenizerI} or \texttt{EventI}.

% [EL] Mention that Python doesn't provide native type checking?  Or
% ditch this section? :)
\noindent\textbf{Typing.}
Type checking is an important safety device, allowing programmers to
quickly locate errors in their code.  This is especially important for
novice programmers, who have less experience with debugging.  Every
function and method defined by the toolkit will check the types of its
its arguments.  Given the impact on performance, the level of type checking
will be adjustable.

%\noindent\textbf{Advanced Language Features.}
%The use of advanced language features and programming styles could
%potentially make the toolkit much more difficult to use or to
%understand.  The fewer language features that a student must learn in
%order to use the toolkit, the faster they can start developing NLP
%systems.  The language features used by
%the toolkit are summarized by figure \ref{fig:feature}.

%\begin{figure}
%\begin{centering}
%\begin{tabular}{|l|l|}
%\hline
%\noindent\textbf{Language Feature} & \textbf{Use} \\
%\hline
%Classes & used \\
%Exceptions & error conditions only \\
%Operator Overloading & used \\
%Python 2.0 Features & used \\
%Python 2.1 Features & not used \\
%Function Arguments & minimal use \\
%Lambda Functions & minimal use \\
%Mapping and Filtering & not used \\
%List Comprehensions & used \\
%Tkinter & used \\
%\hline
%\end{tabular}\\
%\end{centering}
%\caption{Advanced language features used by NLTK.}
%\label{fig:feature}
%\end{figure}

\section{Modules}
% [EL]: I think that this section works better as a textual
% description, rather than a long list.  Also, we're going to be short
% on space.. :)
% What order should these be in?  Put more impressive stuff at the
% beginning and end?  Maybe
% parsing;tagging;visualization;classification?  or swap viz and
% classification?  

The toolkit is implemented as a collection of independant
\emph{modules}, each of which defines a specific data structure or
task.  

A set of core modules defines basic data types and processing tasks
that are used througout the toolkit.  The \texttt{token} module
defines basic classes for processing individual elements of text, such
as words or sentences.  The \texttt{tree} module provides data
structures for representing tree strucutres over text, such as syntax
trees and morphological trees.  The \texttt{probability} module
defines classes that encode frequency distributions and probability
distributions, including a variety of smoothing techniques.

The remaining modules define data structures and interfaces for
performing specific NLP tasks.

\subsection*{Parsing Modules}

The \texttt{parser} module defines a high-level interface for
producing trees that represent the structure of texts.  The
\texttt{chunkparser} module defines a subinterface for parsers that
identify non-overlapping linguistic groups (such as noun phrases) in
unrestricted text.  

Three modules provide implementations for these abstract interfaces.
The \texttt{srparser} module implements a simple shift-reduce parser.
The \texttt{chartparser} module provides a flexible parser that uses a
\emph{chart} to record hypotheses about syntactic constituents.  And
the \texttt{rechunkparser} module defines a transformational
regular-expression based implementation of the chunk parser interface.

\subsection*{Tagging Modules}

The \texttt{tagger} module defines a standard interface for augmenting
each token of a text with supplementary information, such as its part
of speech or its WordNet synset tag; and provides several
different implementations for that interface.

\subsection*{Finite State Automata}

\texttt{fsa}.  Planning to add HMMs??

\subsection*{Visualization}

\texttt{draw.tree}
\texttt{draw.tree\_edit}
\texttt{draw.plot\_graph}
\texttt{draw.chart}
\texttt{draw.fsa}

\subsection*{Text Classification}

The \texttt{classifier} module defines a standard interface for
classifying texts into categories.  This interface is currently
implemented by two modules: the \texttt{classifier.naivebayes} module
defines a text classifier based on the Naive Bayes assumption; and the
\texttt{classifier.maxent} module defines the maximum entropy model
for text classification, and includes two algorithms for training the
model: Generalized Iterative Scaling and Improved Iterative Scaling.

The \texttt{classifier.feature} module provides a standard encoding
for the information that is used to make decisions for a particular
classification task.  This standard encoding allows students to
experiment with the differences between different text classification
algorithms, using identical feature sets.

The \texttt{classifier.featureselection} module defines a standard
interface for choosing which features are relevant for a particular
classification task.  Good feature selection can signifigantly improve
classification performance.




%NLTK currently includes the following modules.

%\subsection*{Basics}

%\begin{description}
%\item[token:] Basic classes for processing individual
%      elements of text, such as words or sentences. 
%\item[tree:] Classes for representing tree structures
%      over text (such as syntax trees and morphological trees).
%\item[probability:] Classes that encode frequency
%      distributions and probability distributions.
%\end{description}

%\subsection*{Tagging}

%\begin{description}
%\item[tagger:] A standard interface to tag each token of
%      a text with supplementary information, such as its part of
%      speech; and several implementations of that interface.
%\end{description}

%\subsection*{Parsing}

%\begin{description}
%\item[parser:] A standard interface to produce trees
%      representing the structure of texts.
%\item[chartparser:] A flexible parser implementation
%      that uses a \emph{chart} to record hypotheses about
%      syntactic constituents.
%\item[srparser\_template:] A partial implementation of a
%      shift-reduce parser; completing the implementation was a
%      student exercise.
%\item[chunkparser:] A standard interface for robust
%      parsers used to identify non-overlapping linguistic groups
%      (such as noun phrases) in unrestricted text.
%\item[rechunkparser:] A regular-expression based
%      implementation of the chunk parser interface.
%\end{description}

%\subsection*{Text Classification}

%\begin{description}
%\item[classifier:] A standard interface for classifying
%      texts into categories.
%\item[classifier.feature:] A standard way of encoding
%      the information used to make classification decisions.
%\item[classifier.naivebayes:] A text classifier
%      implementation based on the Naive Bayes assumption.
%\item[classifier.maxent:] An implementation of the
%      maximum entropy model for text classification; and
%      implementations of the GIS and IIS algorithms for training
%      the classifier.
%\item[classifier.featureselection:] A standard interface
%      for choosing which features are relevant for making
%      classification decisions.
%\end{description}

%\subsection*{Visualization}

%\begin{description}
%\item[draw.tree:] A graphical representation for tree
%      structures, such as syntax trees and morphological trees. 
%\item[draw.tree\_edit:] A graphical interface used to
%      build and modify tree structures.
%\item[draw.plot\_graph:] A graphical tool to graph
%      arbitrary functions.
%\item[draw.chart:] An interactive graphical tool used to
%      experiment with chart parsers.
%\end{description}

\section{Uses of NLTK}

\subsection{Assigments and projects}

NLTK can be used for simple and advanced projects, as well as
everything in between.  In the simplest assignments, students tweak an
existing module to alter its behavior.  The variety of existing
modules makes it easy to create simple assignments.  A more
challenging task is to develop a new module.  Here, NLTK provides some
useful starting points: predefined interfaces and data structures, and
existing modules that implement the same interface.  Advanced projects
typically involve the development of entirely new functionality for a
previously unsupported NLP task, or the development of a complete
system out of existing and new modules.

Here we illustrate the use of NLTK in an assignment of moderate
difficulty, for the case of chunk parsing.  This is a kind of parsing
where we only identify the main constituents of a phrase, a technique
with diverse applications.  The NLTK \texttt{chunk} module offers
a variety of rule formats.  For example,
\texttt{ChunkRule('<NN.*>')} builds chunks over consecutive noun tags;
\texttt{ChinkRule('<VB.>')} excises verbs from existing chunks;
\texttt{SplitRule('<NN>', '<DT>')} splits into two pieces any existing
chunk that contains a noun followed by a determiner; and
\texttt{MergeRule('<JJ>', '<JJ>')} combines two adjacent chunks
where the first chunk ends and the second chunk starts with \texttt{JJ}.
The goal of the assignment is to come up with a cascade of these rules,
and to create a set of chunks that are as similar as possible to the test set.

The chunking tutorial provides all the necessary code except for the
chunk rules.  The provided code is responsible for loading the
chunked, tagged text using an existing tokenizer, creating an
unchunked version of the text, applying the chunk rules to the
unchunked text, and scoring the result.  Students focus on the
NLP task only -- providing a ruleset with the best coverage.

In the remainder of this section we reproduce some of the cascades
created by the students.  The first example illustrates a combination
of several rule types:

\begin{sv}
cascade = [
  ChunkRule('<DT><NN.*><VB.><NN.*>'),
  ChunkRule('<DT><VB.><NN.*>'),
  ChunkRule('<.*>'),
  UnChunkRule('<IN|VB.*|CC|MD|RB.*>'),
  UnChunkRule("<,|{\textbackslash}.|``|''>"),
  MergeRule('<NN.*|DT|JJ.*|CD>', '<NN.*|DT|JJ.*|CD>'),
  SplitRule('<NN.*>', '<DT|JJ>')
]
\end{sv}

This next example illustrates a brute-force approach.  All tags most
commonly found inside of chunks are chunked, while other tags are excluded.

\begin{sv}
cascade = [
  ChunkRule('<{\textbackslash}\$|CD|DT|EX|PDT
             |PRP.*|WP.*|{\textbackslash}\#|FW
             |JJ.*|NN.*|POS|RBS|WDT>*')
]
\end{sv}

The third example involves putting everything in a single chunk,
then excising elements that do not belong.

\begin{sv}
cascade = [
  ChunkRule('<.*>+')
  ChinkRule('<VB.*|IN|CC|R.*|MD|WRB|TO|.|,>+')
]
\end{sv}

% [EL] I'm not sure this is necessary:
The main program which surrounds the student's work is moderately
simple as well.  The line replaced by students is underlined.
% [EL] I find the undeline hard to read; change to italics or bold or
% vbar in the margin?  Or maybe a \Rightarrow of some sort in the margin?
\begin{sv}
from nltk.token import *
from nltk.chunkparser import *
from nltk.rechunkparser import *
import os

path = "/cis530/data/wsj/tagged/"

sent_tok = RETokenizer('(=======+)?{\textbackslash}{\textbackslash}n{\textbackslash}{\textbackslash}n+', 0)
sentences = []
files = os.listdir(path)
for file in files:
    print 'Loading \%s...' \% file
    text = open(path+file).read()
    sentences += sent_tok.tokenize(text, source=file, unit='s')

chunkscore = ChunkScore()

\underline{cascade = [ ??? ]}
chunkparser = REChunkParser(cascade)

ctt = ChunkedTaggedTokenizer()
for s in sentences:
    correct = ctt.tokenize(s.type(), source=s.loc())
    unchunked = unchunk(correct)
    guess = chunkparser.parse(unchunked)
    chunkscore.score(correct, guess)

print chunkscore
\end{sv}

The score for each project is the precision, recall and F-measure.
Figure~\ref{contest} shows the results; students with the best two scores
were presented with prizes.

\begin{figure}
\centerline{\epsfig{figure=contest.ps,width=\linewidth}}
\caption{Precision/Recall Graph for Chunking Competition}\label{contest}
\vspace*{2ex}\hrule
\end{figure}

\subsection{Class demonstrations}

[chart parsing example, including screenshot]

\section{Evaluation}

About CIS-530: mixture of CS and linguistics students.
Approx 40 hours of classes.  All class materials (slides,
assignments) are posted on the course website
\url{http://www.cis.upenn.edu/~cis530/}.

The Language: Python was new to 95\% of students.  Some were initially
bothered about its block structuring, which is sensitive to white
space.  All quickly learnt the language, and reported that any initial
reservations were unfounded.  Many students said that they enjoyed
writing Python code.

CD-ROM: put NLTK and data on CD-ROM and gave it out at the start of class.

The toolkit: students liked being able to do interesting projects from
the outset, liked all the documentation, liked being able to run
everything on their machine at home, ...

Problems: available corpus data, ...

Other strengths / weaknesses

\section{Other approaches}

Java: \cite{Hammond02}
-- no obligation to cite this as it is unpublished
(and I haven't seen a copy).

\section{Conclusion}

Overview of the paper.

NOTES: Many CL textbooks provide good exercises at the end of each
chapter.  This isn't enough for many situations.  E.g. students spend
lots of time writing interface code (e.g. to process a corpus).
E.g. teachers spend lots of time creating do-able assignments...

NLTK is an open source project, and we welcome any contributions.  We
deliberately structured NLTK to facilitate parallel development.
Readers who are interested in contributing to NLTK, or who have any
suggestions for improvements, are encouraged to contact the authors.

\bibliographystyle{acl}
\bibliography{submission}

\end{document}
