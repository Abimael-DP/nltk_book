\documentclass[11pt]{article}
\usepackage{acl02sub,url,alltt,epsfig}
\title{NLTK: The Natural Language Toolkit}
\author{
Edward Loper and Steven Bird\\
Department of Computer and Information Science and Linguistic Data Consortium\\
University of Pennsylvania, Philadelphia, PA 19104-6389, USA
}
\summary{%
NLTK, the Natural Language Toolkit, is a suite of program modules,
tutorials and problem sets, providing ready-to-use computational
linguistics courseware.  NLTK covers symbolic and statistical natural
language processing, and is interfaced to annotated corpora.  Students
augment and replace existing components, learning structured
programming by example, and manipulating sophisticated models from the
outset.}

\paperid{Pxxxx}
\keywords{teaching, courseware, Python, corpora}
\contact{Edward Loper}
\conference{this paper has not been submitted to any other conferences}
\date{\today}

% Note: the maximum paper length is 6 pages.

% To do:
%   - Write up evaluation
%   - Write up other approaches
%   - Write up conclusion
%   - General clean-up
%   - ispell
%   - Should URLs include http:// or not (we should be consistant)?

% Outline:
%   - abstract (summary, above.)
%   - introduction
%   - choice of programming language
%   - design criteria
%   - modules
%   - uses
%     - assignments/projects
%     - class demonstrations
%   - evaluation
%   - other approaches
%   - conclusion

\newenvironment{sv}{\scriptsize\begin{alltt}}{\end{alltt}\normalsize}

\begin{document}
\makeidpage
\maketitle

% ===================== Abstract =====================
% [EL] I think that this abstract should be left out, or shortened and
% merged with the \summary.  The text in the 1st paragraph of the intro
% seems much more direct and relevant to our project than the 1st
% paragraph of this abstract (in particular, the 1st sentence seems like
% a strange way to start). :) I don't think that the second paragraph
% needs to be included in the abstract -- it's not a central point of
% our paper; and the third paragraph is almost identical to the \summary
% (but not quite).  For now, I'm just commenting out the abstract.

%\begin{abstract}
%Students in computational linguistics courses must often learn a new
%programming language.  In some cases, such as when courses are offered
%in linguistics departments, the students may be learning to program
%for the very time.  In order to do interesting projects, it is usually
%necessary for students to do many low-level ``housekeeping'' tasks.
%At the same time, teachers of computational linguistics courses
%sometimes feel that they spend too much time teaching students to
%program, and not enough time teaching the subject itself.  They may
%even avoid programming assignments altogether.  However, we believe
%that it is crucial for a first computational linguistics course to
%include a strong practical component, in which students develop real
%programs to solve real problems with real data.
%
%Python is a new object-oriented scripting language which runs on all
%platforms.  Python has been praised as ``executable pseudocode,'' since
%programs are so easy to write.  Recently, we have been developing
%NLTK, an open-source Natural Language Toolkit written in Python.
%In this presentation, we will motivate, describe and demonstrate NLTK.
%
%NLTK, the Natural Language Toolkit, is a suite of program modules,
%tutorials and problem sets.  NLTK covers symbolic and statistical
%natural language processing, and is interfaced to annotated corpora.
%Students augment and replace existing NLTK components, learning
%structured programming by example, and manipulating sophisticated
%models from the outset.  Along with extensive documentation and
%problem sets, NLTK provides self-contained, ready-to-use CL
%courseware.
%\end{abstract}

% ===================== Introduction =====================
\section{Introduction}

Teachers of introductory courses on computational linguistics are
often faced with the challenge of setting up a practical programming
component for student assignments and projects.  This is a difficult
task because different comptational linguistics domains require a
variety of different data structures and functionality; and because a
diverse range of topics may need to be included in the syllabus.

A widespread practice is to employ multiple programming languages,
where each language provides native data structures and functions that
are a good fit for the task at hand.  For example, a course might use
Prolog for parsing, Perl for corpus processing, and a finite-state
toolkit such for morphological analysis.  By relying on the built-in
features of various languages, the teacher avoids having to develop a
lot of software infrastructure.

An unfortunate consequence is that a significant part of the course
must be devoted to teaching these languages.  Further, many
interesting projects span a variety of domains, and would require that
multiple languages be bridged.  For example, a student project that
involved syntactic parsing of corpus data from a morphologically rich
language might involve all three of the languages mentioned above:
Perl for string processing; a finite state toolkit for morphological
analysis; and Prolog for parsing.
% [EL] This was more precise, but harder to read/parse:
%    (Perl for file I/O, format conversions and
%    overall program control, with calls out to a finite state toolkit for
%    morphological analysis and to a Prolog engine for parsing).  
It is clear that these considerable overheads and shortcomings warrant
a fresh approach.

% [EL] ``difficulties'' doesn't seem like the right word here?
Apart from the practical component, computational linguistics courses
may also depend on software for in-class demonstrations.  This context
calls for highly interactive graphical user interfaces, making it
possible to view program state (e.g. the chart of a chart parser),
observe program execution step-by-step (e.g. execution of a
finite-state machine), and even make minor modifications to programs
in response to ``what if'' questions from the class.  Because of these
difficulties it is common to avoid live demonstrations, and keep
classes for theoretical presentations only.  Apart from being dull,
this approach leaves students to solve important practical problems on
their own, or to deal with them less efficiently in office hours.

% [EL] akward
In this paper we describe a new approach to the above challenges, a
streamlined and flexible way of organizing the practical component of
an introductory computational linguistics course.  We describe NLTK,
the Natural Language Toolkit, which we have developed in conjunction
with a course we have taught at the University of Pennsylvania.

% Overview
% [EL] Given that we're short on space, I decided that this wasn't
% really necessary for a 6-page paper.  If we do include it, it needs
% to be completed (I only went up through sec:uses).
%%Section \ref{sec:python} discusses why we decided to implement the
%%toolkit using Python.  Section \ref{sec:criteria} describes several
%%important considerations and desiderata that guided the design and
%%implementation of the toolkit.  Sections \ref{sec:modules} and
%%\ref{sec:documentation} describe the current contents of the toolkit.
%%Section \ref{sec:uses} explains how the toolkit can be used for both
%%assignments and class demonstrations.

All materials discussed here are available under an open source
license from \url{nltk.sf.net}.  NLTK runs on most platforms,
including Windows, OS X, Linux, and UNIX.

% ===================== Python =====================
\section{Choice of Programming Language}
\label{sec:python}

The most basic step in setting up a practical component is choosing a
suitable programming language.  A number of desiderata influenced our
choice.  First, the language must have a shallow learning curve, so
that novice programmers get immediate rewards for their efforts.
Second, the language must support rapid prototyping and a short
develop/test cycle; an obligatory compilation step is a serious
detraction.  Third, the code should be self-documenting, with a
transparent syntax and semantics.  Fourth, it should be easy to write
structured programs, ideally object-oriented but without the burden
associated with languages like C++.  Finally, the language must have
an easy-to-use graphics library to support the development of simple
graphical user interfaces.

% I moved www.python.org to the \cite.
In surveying the available languages, we believe that Python offers an
especially good fit to the above requirements.  Python is an
object-oriented scripting language developed by Guido van Rossum
and available on all platforms \cite{python}.  Python offers a
shallow learning curve; it was designed to be easily learnt by
children \cite{rossum99}.  As an interpreted language, Python is
suitable for rapid prototyping.  Python code is exceptionally
readable, and it has been praised as ``executable pseudocode.''
% Couldn't find a REF for ``executable pseudocode'' -- closest I got
% was http://www.thinkware.se/cgi-bin/thinki.cgi/PythonQuotes
Python is an object-oriented language, but not punitively so, and it
is easy to encapsulate data and methods inside Python classes.
Finally, Python has an interface to the ``Tk'' graphics toolkit
\cite{tkinter}, and writing graphical interfaces is straightforward.

% ===================== Design Criteria =====================
\section{Design Criteria}
\label{sec:criteria}

Several criteria were considered in the design and implementation of
the toolkit.  The design criteria are divided into primary and
secondary criteria, and listed in the order of their importance.  It
was also important to decide what goals the toolkit would \emph{not}
attempt to accomplish; we therefore include an explicit set of
non-requirements, which the toolkit is not expected to satisfy.

\subsection{Primary Design Criteria}

\paragraph{\textit{Ease of Use.}} The primary purpose of the toolkit is
to allow students to concentrate on building NLP systems.  The more
time students must spend learning to use the toolkit, the less useful
it is.

\paragraph{\textit{Consistency.}} The toolkit should use consistent data
structures and interfaces.

\paragraph{\textit{Extensibility.}} The toolkit should easily
accommodate new components, whether those components replicate or
extend the toolkit's existing functionality.  Thus, the toolkit's
design should be modular, with simple and well-defined interfaces
between modules.  The toolkit should also be structured in such a way
that it is obvious where new extensions would fit into the toolkit's
infrastructure.

\paragraph{\textit{Documentation.}} The toolkit, its data structures,
and its implementation all need to be carefully and thoroughly
documented.  Three documentation types are necessary: tutorials, to
teach students how to use the toolkit to perform specific tasks;
technical reports, to explain the toolkit's design and implementation;
and reference documentation, to describe every module, interface,
class and method.  All nomenclature must be carefully chosen and
consistently used.

\subsection{Secondary Design Criteria}

\paragraph{\textit{Simplicity.}} The toolkit should structure the
complexities of building NLP systems, not hide them.  Therefore, each
class defined by the toolkit should be simple enough that a student
could implement it by the time they finish an introductory course in
computational linguistics.

\paragraph{\textit{Modularity.}} The interaction between different
components of the toolkit should be kept to a minimum.  In particular,
it should be possible to complete individual projects using small
parts of the toolkit, without worrying about how they interact with
the rest of the toolkit.  This allows students to learn how to use the
toolkit incrementally throughout a course.  Modularity also makes it
easier to change and extend the toolkit.

\subsection{Non-Requirements}

\paragraph{\textit{Comprehensiveness.}} The toolkit is not intended to
provide a comprehensive set of tools.  Indeed, there should be a wide
variety of ways in which students can extend the toolkit.

\paragraph{\textit{Efficiency.}} The toolkit does not need to be highly
optimized for runtime performance.  However, it should be efficient
enough that students can use their NLP systems to perform real tasks.

\paragraph{\textit{Cleverness.}} Clear designs and implementations are
far preferable to ingenious yet indecipherable ones.

% ===================== Modules =====================
\section{Modules}
\label{sec:modules}
% What order should these be in?  Put more impressive stuff at the
% beginning and end?

The toolkit is implemented as a collection of independant
\emph{modules}, each of which defines a specific data structure or
task.  

A set of core modules defines basic data types and processing systems
that are used througout the toolkit.  The \texttt{token} module
defines basic classes for processing individual elements of text, such
as words or sentences.  The \texttt{tree} module provides data
structures for representing tree structures over text, such as syntax
trees and morphological trees.  The \texttt{probability} module
defines classes that encode frequency distributions and probability
distributions, including a variety of statistical smoothing
techniques.

The remaining modules define data structures and interfaces for
performing specific NLP tasks.  This list of modules will grow over
time, as we add new tasks and algorithms to the toolkit.

\subsection*{Parsing Modules}

The \texttt{parser} module defines a high-level interface for
producing trees that represent the structures of texts.  The
\texttt{chunkparser} module defines a subinterface for parsers that
identify non-overlapping linguistic groups (such as noun phrases) in
unrestricted text.

Three modules provide implementations for these abstract interfaces.
The \texttt{srparser} module implements a simple shift-reduce parser.
The \texttt{chartparser} module provides a flexible parser that uses a
\emph{chart} to record hypotheses about syntactic constituents.  And
the \texttt{rechunkparser} module defines a transformational
regular-expression based implementation of the chunk parser interface.

\subsection*{Tagging Modules}

The \texttt{tagger} module defines a standard interface for augmenting
each token of a text with supplementary information, such as its part
of speech or its WordNet synset tag; and provides several different
implementations for that interface.

\subsection*{Finite State Automata}

% [EL] Steven, do you want to add anything here?  Say anything about HMMS? 
% Will you have implemented more interfaces by the time of ACL?
The \texttt{fsa} module defines a data type for encoding finite state
automata; and an interface for creating automata from regular
expressions.

\subsection*{Visualization}

% jb: this is broken up oddly.
A set of visualization modules define graphical tools for viewing and
editing data structures, and for experimenting with NLP tasks.  The
\texttt{draw.tree} module provides a simple graphical interface for
displaying tree structures; and the
\texttt{draw.tree\_edit} module provides an interface for building and
modifying tree structures.  The \texttt{draw.plot\_graph} module can be
used to graph mathematical functions.  The \texttt{draw.fsa} module
provides a graphical tool for displaying and simulating finite state
automata.  The \texttt{draw.chart} module provides an interactive
graphical tool for experimenting with chart parsers.

\subsection*{Text Classification}

The \texttt{classifier} module defines a standard interface for
classifying texts into categories.  This interface is currently
implemented by two modules: the \texttt{classifier.naivebayes} module
defines a text classifier based on the Naive Bayes assumption; and the
\texttt{classifier.maxent} module defines the maximum entropy model
for text classification, and implements two algorithms for training
the model: Generalized Iterative Scaling and Improved Iterative
Scaling.

The \texttt{classifier.feature} module provides a standard encoding
for the information that is used to make decisions for a particular
classification task.  This standard encoding allows students to
experiment with the differences between different text classification
algorithms, using identical feature sets.

The \texttt{classifier.featureselection} module defines a standard
interface for choosing which features are relevant for a particular
classification task.  Good feature selection can signifigantly improve
classification performance.

% ===================== Documentation =====================
\section{Documentation}
\label{sec:documentation}

The toolkit is accompanied by extensive documentation that explains
the toolkit, and describes how to use and extend it.  This
documentation is divided into three primary categories:

\paragraph{\textit{Tutorials}} teach students how to use the toolkit,
in the context of performing specific tasks.  Each tutorial focuses on
a single domain, such as tagging, probablistic systems, or text
classification.  The tutorials include a high-level discussion that
explains and motivates the domain, followed by a detailed
walk-through that uses examples to show how NLTK can be used to
perform specific tasks.

\paragraph{\textit{Reference Documentation}} provides precise
definitions for every module, interface, class, method, function, and
variable in the toolkit.  It is automatically extracted from docstring
comments in the Python source code, using Epydoc \cite{epydoc}.

\paragraph{\textit{Technical Reports}} explain and justify the
toolkit's design and implementation.  They are used by the developers
of the toolkit to guide and document the toolkit's construction.
Students can also consult these reports if they would like further
information about how the toolkit is designed and why it is designed
that way.

% ===================== Uses =====================
\section{Uses of NLTK}
\label{sec:uses}

\subsection{Assigments and Projects}

NLTK can be used to create student assignments of varying difficulty
and scope.  
% Use a module
In the simplest assignments, students experiment with an existing
module.  The wide variety of existing modules provide many opportunies
for creating these simple assignments.
% Edit/extend a module.
Once students become more familiar with the toolkit, they can be asked
to make minor changes or extensions to an existing module.
% Develop a new module.
A more challenging task is to develop a new module.  Here, NLTK
provides some useful starting points: predefined interfaces and data
structures, and existing modules that implement the same interface.
% Advanced projects..
Advanced projects typically involve the development of entirely new
functionality for a previously unsupported NLP task, or the
development of a complete system out of existing and new modules.

\subsubsection*{Example: Chunk Parsing}

As an example of a moderately difficult assignment, we asked students
to construct a chunk parser that correctly identifies base noun phrase
chunks in a given text, by defining a cascade of transformational
chunking rules.  The NLTK \texttt{rechunkparser} module provides a
variety of rule types, which the students can instantiate to construct
complete rules.  For example, \texttt{ChunkRule('<NN.*>')} builds
chunks from sequences of consecutive nouns; \texttt{ChinkRule('<VB.>')}
excises verbs from existing chunks; \texttt{SplitRule('<NN>', '<DT>')}
splits any existing chunk that contains a singular noun followed by
determiner into two pieces; and \texttt{MergeRule('<JJ>', '<JJ>')}
combines two adjacent chunks where the first chunk ends and the second
chunk starts with adjectives.

The chunking tutorial motivates chunk parsing, describes each rule
type, and provides all the necessary code for the assignment.  The
provided code is responsible for loading the chunked, part-of-speech
tagged text using an existing tokenizer, creating an unchunked version
of the text, applying the chunk rules to the unchunked text, and
scoring the result.  Students focus on the NLP task only -- providing
a ruleset with the best coverage.

In the remainder of this section we reproduce some of the cascades
created by the students.  The first example illustrates a combination
of several rule types:

\begin{sv}
cascade = [
  ChunkRule('<DT><NN.*><VB.><NN.*>'),
  ChunkRule('<DT><VB.><NN.*>'),
  ChunkRule('<.*>'),
  UnChunkRule('<IN|VB.*|CC|MD|RB.*>'),
  UnChunkRule("<,|{\textbackslash}{\textbackslash}.|``|''>"),
  MergeRule('<NN.*|DT|JJ.*|CD>', '<NN.*|DT|JJ.*|CD>'),
  SplitRule('<NN.*>', '<DT|JJ>')
]
\end{sv}

The next example illustrates a brute-force statistical approach.  The
student calculated how often each part-of-speech tag was included in a
noun phrase.  They then constructed chunks from any sequence of tags
that occured in a noun phrase more than 50\% of the time.

\begin{sv}
cascade = [
  ChunkRule('<{\textbackslash}{\textbackslash}\$|CD|DT|EX|PDT
             |PRP.*|WP.*|{\textbackslash}{\textbackslash}\#|FW
             |JJ.*|NN.*|POS|RBS|WDT>*')
]
\end{sv}

In the third example, the student constructed a single chunk
containing the entire text, and then excised all elements that did not
belong.

\begin{sv}
cascade = [
  ChunkRule('<.*>+')
  ChinkRule('<VB.*|IN|CC|R.*|MD|WRB|TO|.|,>+')
]
\end{sv}

%% [EL] This makes it sound like we GRADED them using their precision,
%% recall, and f-measure.  Either say something about the contest, or
%% leave this out:
%Each student's project was scored using its precision, recall, and
%F-measure.

% [EL] Leave out the picture.
%Figure~\ref{fig:contest} shows the results; students with the best two
%scores were presented with prizes.
%
%\begin{figure}
%\centerline{\epsfig{figure=contest.ps,width=\linewidth}}
%\caption{Precision/Recall Graph for Chunking Competition}\label{fig:contest}
%\vspace*{2ex}\hrule
%\end{figure}

\subsection{Class demonstrations}

NLTK provides graphical tools that can be used to give in-class
demonstrations that help to explain basic NLP concepts and algorithms.
These interactive tools can be used to display relevant data
structures, and to show the step-by-step execution of algorithms.
Both data structures and control flow can be easily modified during
the demonstration, in response questions from the class.

Since these graphical tools are included with the toolkit, they can
also be used by students.  This allows students to experiment at home
with the algorithms that they have seen presented in class.

\subsubsection*{Example: The Chart Parsing Tool}

The chart parsing tool is an example of a graphical tool provided by
NLTK.  This tool can be used to explain the basic concepts behind
chart parsing, and to show how the algorithm works.  Chart parsing is
a flexible parsing algorithm that uses a data structure called a
\emph{chart} to record hypotheses about syntactic constituents.  Each
hypothesis is represented by a single \emph{edge} on the chart.  A set
of \emph{rules} determine when new edges can be added to the chart.
This set of rules controls the overall behavior of the parser (e.g.,
whether it parses top-down or bottom-up).

The chart parsing tool demonstrates the process of parsing a single
sentence, with a given grammar and lexicon.  Its display is divided
into three sections: the bottom section displays the chart; the middle
section displays the sentence; and the top section displays the
partial syntax tree corresponding to the selected edge.  Buttons along
the bottom of the window are used to control the execution of the
algorithm.  The main display window for the chart parsing tool is
shown in Figure~\ref{fig:chartparse}.   

This tool can be used to explain several different aspects of chart
parsing.  First, it can be used to explain the basic chart data
structure, and to show how edges can represent hypotheses about
syntactic constituents.  It can then be used to demonstrate and
explain the individual rules that the chart parser uses to create new
edges.  Finally, it can be used to show how these individual rules
combine to find a complete parse for a given sentence.

% Is ``user'' a good word here?  ``lecturer''?
To reduce the overhead of setting up demonstrations during lecture,
the user can define a list of preset charts.  The tool can then be
reset to any one of these charts at any time.

The chart parsing tool allows for flexible control of the parsing
algorithm.  At each step of the algorithm, the user can select which
rule or strategy they wish to apply.  This allows the user to
experiment with mixing different strategies (e.g., top-down and
bottom-up).  The user can exercise fine-grained control over the
algorithm by selecting which edge they wish to apply a rule to.  This
flexibility allows lecturers to use the tool to respond to a wide
variety of questions; and allows students to experiment with different
variations on the chart parsing algorithm.

\begin{figure}
\centerline{\epsfig{figure=chartparse.eps,width=\linewidth}}
\caption{Chart Parsing Tool}\label{fig:chartparse}
\vspace*{2ex}\hrule
\end{figure}

% ===================== Evaluation =====================
\section{Evaluation}
\label{sec:evaluation}

% Do we want to mention that there were 40h of lecture time?
% Should we be more explicit about who took the class?
We used NLTK as a basis for the assignments and student projects in
CIS-530, an introductory computational linguistics class taught at the
University of Pennsylvania.  CIS-530 is a graduate level class,
although some advanced undergraduates were also enrolled.  Most
students had either a computer science background or a linguistics
backgroud.  Students were required to complete five assignments, two
exams, and a final project.  All class materials are available from
the corse website \url{http://www.cis.upenn.edu/~cis530/}.

% (Leaving this out for now\ldots)
%The Language: Python was new to 95\% of students.  Some were initially
%bothered about its block structuring, which is sensitive to white
%space.  All quickly learnt the language, and reported that any initial
%reservations were unfounded.  Many students said that they enjoyed
%writing Python code.

% Where does this go??
%At the beginning of the semester, we distributed 2 CD-ROMs to the
%students, containing NLTK and all of the corpera that we would use for
%the class.  

The experience of using NLTK was very positive, both for us and for
the students.  The students liked the fact that they could do
interesting projects from the outset.  They also liked being able to
run everything on their computer at home.  The students found the
extensive documentation very helpful for learning to use the toolkit.
They found the interfaces defined by NLTK intuitive, and appreciated
the ease with which they could combine different components to create
complete NLP systems.

We did encounter a few difficulties during the semester.  One problem
is finding large, clean, open-source corpera that the students can use
for their assignments.  This became especially apparent when students
started planning their final projects, since many projects required
different types of corpera.  Another issue was the fact that we were
actively developing NLTK during the semester; some modules were only
completed one or two weeks before we used them.  As a result, students
who worked at home needed to download new versions of the toolkit
several times throughout the semester.  Luckily, Python has extensive
support for installation scripts, which made these upgrades much
simpler.  The students encountered a couple of bugs in the toolkit,
but none were serious, and all were quickly corrected.

%Problems: available corpus data, ...
%
%Other strengths / weaknesses

% ===================== Other Approaches =====================
\section{Other Approaches}
\label{sec:approaches}

\textit{This section will include a discussion of other approaches to
implementing computational linguistics courseware, including
\emph{Programming for Linguistics: Java Technology for Language
Researchers}\cite{Hammond02} and \emph{the OpenNLP project}\cite{opennlp}.}

% ===================== Conclusion =====================
\section{Conclusions and Future Work}
\label{sec:conclusion}

%Overview of the paper:
% [EL] Pick better 3 adjectives :)
NLTK provides a simple, extensible, uniform framework for student
assignments and class demonstrations.  It is well documented, easy to
learn, and simple to use.  We hope that NLTK will allow computational
linguistics classes to include more hands-on experience building NLP
components and systems.

% [EL] No room for this for now\ldots
%NOTES: Many CL textbooks provide good exercises at the end of each
%chapter.  This isn't enough for many situations.  E.g. students spend
%lots of time writing interface code (e.g. to process a corpus).
%E.g. teachers spend lots of time creating do-able assignments...

%Future work: 
We plan to continue extending the breadth of materials covered by the
toolkit.  We are currently working on NLTK modules for Hidden Markov
Models, probablistic parsing, and tree adjoining grammars.  We also
plan to increase the number of algorithms implemented by some existing
modules, such as the text classification module.  Finally, we are
putting together a standard collection of large, clean, open-source
corpera, including corpera appropriate for every module defined by the
toolkit.

NLTK is an open source project, and we welcome any contributions.
Readers who are interested in contributing to NLTK, or who have
suggestions for improvements, are encouraged to contact the authors.

% include URL (a second time; 1st was in the intro) in conclusion??

\bibliographystyle{acl}
\bibliography{submission}

\end{document}
