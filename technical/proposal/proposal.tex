%
% Natural Language Toolkit for Python
% Edward Loper
% 
% Created [03/02/01 12:02 AM]
% $Id$
%

\documentclass{article}
\usepackage{fullpage}
\usepackage{boxedminipage}

\begin{document}
\title{Proposal: A Natural Language Programming Toolkit for Python}
\author{Edward Loper, Steven Bird}
\maketitle

%################################################################
%#  OVERVIEW
%################################################################
\section{Overview}

CIS-530 is an introductory course in natural language processing
(NLP), which is intended to be accessible to both linguists and
computer scientists \cite{cis530}. In such a class, it is instructive
for students to build real NLP systems.  By building real systems,
students can gain an in-depth understanding of the issues involved in
particular aspects of NLP.

However, the students do not necessarily have any experience
programming or designing software systems.  Thus, when students
attempt to build NLP systems, they may spend a significant amount of
their time on programming tasks that are not directly related to the
course content.

In order to try to remove some of the time overhead associated with
these programming tasks, we propose to design and implement a toolkit
for building natural language systems using the Python
language \cite{python}. This toolkit will help students in a variety
of ways:

\begin{itemize}
  \item It will provide students with basic tools for manipulating
  data and performing tasks that are related to NLP.

  \item It will allow the students to experiment in building NLP
  systems at a variety of levels.  At one extreme, they can implement
  individual components for use with the toolkit.  At the other
  extreme, they can create large systems by combining toolkit
  components.

  \item It will provide the students with an infrastructure for
  building natural language systems, by providing consistent and
  well-defined interfaces for different components.  Thus, students
  will not have to spend their time thinking about system design
  issues.
\end{itemize}

%################################################################
%#  DESIGN GOALS/CRITERIA
%################################################################
\section{Design Criteria}

Several criteria must be considered in the design and implementation
of the toolkit.  The design criteria are divided into primary and
secondary criteria, and within those sections are listed in the order
of their importance.  A set of explicit non-requirements is also
given.  The toolkit is not expected to satisfy any of these
properties. 

\subsection{Primary Design Criteria}

\begin{description}

\item[Ease of Use] The primary purpose of the toolkit is to allow
students to concentrate on building NLP systems.  The more time
students must spend learning to use the toolkit, the less useful it
is.  

\item[Consistency] The toolkit should use consistent data structures
and interfaces.

\item[Extensibility] The toolkit should easily accommodate new
components, whether those components replicate or extend the toolkit's
existing functionality.  Thus, the toolkit's design should be modular,
with simple and well-defined interfaces between modules.  The toolkit
should also be structured in such a way that it is obvious where new
extensions would fit into the toolkit's infrastructure.

\item[Documentation] The toolkit, its data structures, and its
implementation all need to be carefully and thoroughly documented.

\end{description}

\subsection{Secondary Design Criteria}

\begin{description}
\item[Simplicity] The toolkit should structure the complexities of
building NLP systems, not hide them.  Therefore, each class defined by 
the toolkit should be simple enough that a student could implement it
by the time they finish the course.

\item[Modularity] The interaction between different components of the
toolkit should be kept to a minimum.  In particular, it should be
possible to use complete individual projects using small parts of the
toolkit, without worrying about how they interact with the rest of the
toolkit.  This will allow students to learn to use the toolkit
incrementally throughout the course.  Modularity also makes it easier
to change or extend the toolkit.
\end{description}

\subsection{Non-Requirements}

\begin{description}
\item[Comprehensiveness] The toolkit is not intended to provide a
comprehensive set of tools.  Indeed, there should be a wide variety of 
ways in which students can extend the toolkit.

\item[Efficiency] The toolkit does not need to be highly optimized for
runtime performance.  However, it should be efficient enough that
students can use their NLP systems to perform real tasks.

\item[Cleverness] Clear designs and implementations are far preferable 
to ingenious yet indecipherable ones.
\end{description}

%################################################################
%#  TOOLKIT DESIGN AND IMPLEMENTATION
%################################################################
\section{Toolkit Design and Implementation}

\subsection{Infrastructure Design}

The natural language processing toolkit will be implemented by a
collection of independent modules\footnote{These modules do not
necessarily correspond to Python \texttt{module}s.}.  These modules are
organized according to two primary axes:

\begin{itemize}
  \item \textbf{Task:} Toolkit modules are generally associated with
  one or more NLP tasks, and several different modules may be
  associated with the same task.

  \item \textbf{Orientation:} Toolkit modules are either
  \emph{data-oriented} or \emph{task-oriented}.

  \begin{itemize}
    \item Data-oriented modules are used to store different types of
    information that are relevant to natural language processing.
    \item Task-oriented modules are used to perform a variety of tasks 
    that are relevant to natural language processing.
  \end{itemize}
\end{itemize}

\subsubsection{Dependencies}

In general, data-oriented modules should be self-contained, and should
exhibit very little inter-dependency.  Data-oriented modules never
depend on task-oriented modules.

Task-oriented modules should also be relatively self-contained, with
very little inter-dependency.  However, each task-oriented modules
will usually depend on a variety of data-oriented modules.  When
possible, the number of data-oriented modules that a task-oriented
module depends on should be kept low.  This allows students to use
task-oriented modules without needing to learn how to use a large
number of other modules.

\subsection{Module Design}

Each module defines a small set of interfaces, which give
specifications for the types of classes that can be implemented by the
module.  Each module also contains classes that implement each
interface.  Often, a module will contain more than one implementation
for the same interface.

\subsubsection{Example Module: Tokens}

For the sake of concreteness, I will describe the design of a
data-driven module for handling tokens.  

The token module defines two interfaces:

\vspace{1.2mm}\noindent
\begin{tabular}{||p{.21\textwidth}p{.7\textwidth}}
  \textbf{TokenType} & A unit of text, such as a word or a
  punctuation mark. \\

  \textbf{TextLocation} & The location of an entity within a
  text.\\
\end{tabular}
\vspace{1mm}

\noindent
Each of these interfaces specifies a set of methods that classes must
implement if they are to be considered \texttt{TokenType}s or
\texttt{TextLocation}s.  If a class properly implements every method specified
by an interface, then it is said to \emph{implement} that interface.

The token module also defines one top-level class:

\vspace{1.2mm}\noindent
\begin{tabular}{||p{.21\textwidth}p{.7\textwidth}}
  \textbf{Token} & An occurance of a \texttt{TokenType}
  within a text.  A \texttt{token} consists of a \texttt{TokenType}
  and a \texttt{TextLocation}. \\
\end{tabular}
\vspace{1mm}

\noindent
This is somewhat unusual; most modules do not implement any top-level
classes.  However, when a data structure has only one reasonable
implementation, a top-level class may be used in place of an interface 
and interface specification.  This is the case for \texttt{Token}s,
which are basically just the result of combining a \texttt{TokenType} 
with a \texttt{TextLocation}.

The only method that \texttt{TokenType}s are required to implement is
the comparison operator.  This allows users of \texttt{TokenType}s to
decide whether two \texttt{TokenType}s represent the same unit of
text.  The token module currently defines two implementations for the
\texttt{TokenType} interface:

\vspace{1.2mm}\noindent
\begin{tabular}{||p{.21\textwidth}p{.7\textwidth}}
  \textbf{SimpleTokenType} & A \texttt{TokenType} that is
  represented by a single Python \texttt{string}.  Examples of
  \texttt{SimpleTokenType} values are ``dog'' and ``walking.'' \\

  \textbf{TaggedTokenType} & A \texttt{TokenType} that is
  represented by a \texttt{string} and a part-of-speech tag.  Examples 
  of \texttt{TaggedTokenType} values are ``dog/NN,'' ``bank/NN,'' and
  ``bank/VB.'' \\
\end{tabular}
\vspace{1mm}

Like \texttt{TokenType}s, the only method that \texttt{TextLocation}s
are required to implement is the comparison operator.  This allows
users of \texttt{TextLocation}s to decide whether two
\texttt{TextLocation}s represent the same location in a text.  The
token module currently defines one implementation for the
\texttt{TextLocation} interface:

\vspace{1.2mm}\noindent
\begin{tabular}{||p{.21\textwidth}p{.7\textwidth}}
  \textbf{IndexTextLocation} & The location of an
  entity within a text, using a numerical index. \\
\end{tabular}
\vspace{1mm}

\subsubsection{Content}

The decision of what modules to include in the toolkit will be
initially guided by the past content of CIS-530 \cite{cis530}.  Later,
the content of several Natural Language Processing text books may be
used to guide decisions about what modules to write \cite{allen1995},
\cite{gazdar1989}, \cite{jurafsky2000}, \cite{manning2000}.

\subsubsection{Nomenclature}

When learning to use the toolkit, students intuitions about each
entity in the toolkit will be guided by the entities names.  It is
therefore important to use consistent, correct, and intuitive
nomenclature when naming the entities defined by the toolkit.
The documentation should be careful to always use correct
nomenclature; in particular, it should never conflate two concepts
with the same name.

\subsection{Programming Concepts}

The structure of the Python language directly affects the design and
implementation of the toolkit.  This section discusses how the design
of the toolkit is affected by the Python language.

\subsubsection{Interfaces}

Python does not directly implement interfaces.  The toolkit will
therefore implement interfaces as simple classes, all of whose methods 
raise \texttt{AssertionError} exceptions.  A description of the
functionality provided by the interface will be contained in the
class's documentation string.  Each method's documentation string will 
contain a specification of the behavior for that method.

Occasionally, interfaces will define optional methods.  These are
methods that may be defined by classes that implement the interface,
but need not be.  Instead of raising \texttt{AssertionError}
exceptions, these methods will raise \texttt{NotImplementedError}
exceptions. 

In order to implement an interface, a class should include that
interface as one of its bases, and should override every
(non-optional) method defined by the interface.

Since Python supports multiple inheritance, this implementation of
interfaces will allow a single class to implement multiple
interfaces.  In general, interfaces should be listed after true bases
in the base list, to ensure proper inheritance of methods.

In principle, an interface hierarchy could be built by defining
interfaces whose bases are interfaces.  However, I do not currently
plan to use this ability.

Classes that define interfaces are named with a trailing ``I,'' such
as \texttt{TokenizerI} or \texttt{EventI}.

\subsubsection{Typing}

Type checking is an important safety device, that allows programmers to
quickly detect the locations of errors in their code.  This is
especially important for novice programmers, who have less experience
with debugging.  

Therefore, every function and method defined by the toolkit will
perform a complete type check on all of its arguments.  By default,
container types (lists, tuples, and dictionaries) will be checked in a 
``deep'' fashion: every element will be checked to ensure that it is
the correct type.  For example, if a procedure expects a list of
integers, it will check every element of the list to ensure that they
are integers before it begins executing.  

However, such ``deep'' type checking can have a serious performance
cost: every time a procedure is called, the type of every element of a
potentially very large data structure must be checked.  Therefore, the 
toolkit will provide a function which can adjust the level of type
checking performed by all toolkit functions.  This function can be
used to temporarily lower the type checking level when processing
large amounts of data.  However, it is recommended that the highest
level of type checking be used when possible.

\subsubsection{Language Features}

The use of advanced language features and programming styles could
potentially make the toolkit much more difficult to use or to
understand.  The fewer language features that a student must learn in
order to use the toolkit, the faster they can start developing NLP
systems.  This section discusses the advantages and disadvantages of
using a variety of language features.  The language features used by
the toolkit are summarized by figure \ref{fig:feature}.

\begin{figure}
\begin{centering}
\begin{tabular}{|l|l|}
\hline
\textbf{Language Feature} & \textbf{Use} \\
\hline
Classes & used \\
Exceptions & error conditions only \\
Operator Overloading & used \\
Python 2.0 Features & used \\
Function Arguments & minimal use \\
Lambda Functions & minimal use \\
Mapping and Filtering & not used \\
List Comprehensions & not used \\
\hline
\end{tabular}\\
\end{centering}
\caption{Summary of which advanced language features are used by the
toolkit.}
\label{fig:feature}
\end{figure}

\begin{description}

  \item[Classes] Classes are an integral part of the design of the
  toolkit; they will be used.

  \item[Exceptions] Exceptions are used by the core Python language to
  report error conditions.  Exceptions can also be used to report
  exceptional conditions.  The toolkit will only use exceptions to
  report error conditions.  As a result, students who are using the
  toolkit need not learn anything about raising or catching
  exceptions.  Students who are extending the toolkit should learn how
  to raise exceptions, but need not learn anything about catching
  exceptions.

  \item[Operator Overloading] Python allows programmers to redefine
  the behavior of operators when used with class instances.  The use
  of operator overloading can make classes easier to use, since they
  can behave more like the built-in classes.  However, extensive use
  of operator overloading can make classes difficult to understand and
  difficult to program.  Classes and interfaces defined by the toolkit
  will use operator overloading.

  \item[Python 2.0 Features] Version 2.0 of Python defines a number of
  useful features, such as augmented assignment; string methods; and
  containership operator overloading.  In general, these features will
  make the toolkit easier to use and understand.  However, if the
  toolkit uses these features, then it cannot be used with earlier
  versions of Python.  The toolkit will use Python 2.0 features, and
  thus will require Python 2.0.

  \item[Function Arguments] Allowing functions to act as arguments can
  be a very powerful tool.  For example, by passing a function to the
  built-in function \texttt{sort}, a programmer can sort a list
  according to any arbitrary ordering.  However, the use of functions
  as arguments can be difficult to understand.  The toolkit will
  therefore make minimal use of functions as arguments.  It should
  never be necessary to use function arguments to accomplish a task.

  \item[Lambda Functions] Lambda functions allow simple functions to
  be defined where they are used.  When used in conjunction with
  functional arguments, lambda functions can be a powerful tool.
  However, their use is not always intuitive.  The toolkit will
  therefore make minimal use of lambda functions.  It will only use
  them in the context of simple and well-defined idioms, such as event
  creation: \texttt{FuncEvent(lambda~x:x>3)}.
        
  \item[Mapping and Filtering] Mapping and filtering provide a
  concise way of expressing transformations on lists.  However, they
  are fairly difficult to learn, and generally involve the use of
  \texttt{lambda} functions, so the toolkit will not use them.

  \item[List Comprehensions] List comprehensions provide an even more
  concise way of expressing transformations on lists.  However, in
  order to reduce the number of new language features that students
  must learn to understand the toolkit, they will not be used.

\end{description}

\subsection{Statistical Processing and Visualization}

The Python standard library contains minimal support for statistical
processing and visualization.  A number of third-party Python
libraries provide basic statistical and visualization tools
\cite{numpy, statspy, scipy, vtk}.  These tools could be used, either
by the toolkit, or directly by students, to perform statistical
processing and to visualize the results.

However, most of these tools have limited capabilities, and may not be
flexible enough for use with the toolkit.  Another option is to use a
well-established system for statistical processing and visualization,
that can be accessed indirectly via Python.  One such tool is the R
language \cite{r}.  R is an open-source version of the S system for
statistical processing and visualization \cite{s}.  This system
provides the advanced statistical tools and well-designed
visualization systems.  The R language can be accesed from Python via
the R/S Python interface \cite{rspython}.

The decision of what third-party modules and tools to be used by the
toolkit will be made after a more thourough investigation of the
advantages and distadvantages of respective tools.  Furthermore,
third-party modules will only be incorperated into the toolkit once
the toolkit develops enough to warrent their use.  However, tools such
as R seem like very promising candidates for adding advanced features
to the toolkit.

%################################################################
%#  DOCUMENTATION
%################################################################
\section{Documentation}

The toolkit will be accompanied by documentation that explains the
toolkit, and describes how to use and extend it.  This documentation
will be divided into three primary categories:

\begin{itemize}
  \item \textbf{Technical Reports}, which explain and justify the
  toolkit's design and implementation.

  \item \textbf{Tutorials}, which teach students how to use the
  toolkit, in the context of performing specific tasks.

  \item \textbf{Reference Documentation}, which describes every
  module, interface, class, method, function, and variable in the
  toolkit.
\end{itemize}

Technical reports will be available as latex files, dvi files, and
postscript files.  Tutorials and reference documentation will be
available as HTML files, accessible over the web.

\subsection{Technical Reports}

Throughout the course of developing the toolkit, I will write a number
of technical reports, which are intended to explain and justify
specific aspects of the toolkit's design or implementation.  This
document is an example of a technical report.  Students should not
need to read the technical reports to use the toolkit.  However,
students can consult these reports if they would like further
information about how the toolkit is designed and why it is designed
that way.

\subsection{Tutorials}

Tutorials are the primary type of documentation that students will use
to learn how to use the toolkit.  Each tutorial will explain how to
use some aspect of the toolkit, using specific examples.  In general,
the tutorials will explain how the student can perform some class of
tasks, such as using tokenizers and taggers to tag text with
part-of-speech tags.  Some tutorials may be associated with specific
toolkit modules.

Different tutorials will assume different amounts of background
knowledge about the toolkit.  This assumed background knowledge will
be specified by listing ``prerequisite'' tutorials for each tutorial.
At least one tutorial will assume no background knowledge about the
toolkit.

\subsection{Reference Documentation}

Reference documentation gives definitions for each module, interface,
class, method, function, and variable defined by the toolkit.  These
definitions should be both precise and concise.  I currently plan to
produce the reference documentation using Epydoc \cite{epydoc}.
Epydoc is a tool for automatically converting python documentation
strings to HTML pages.  Epydoc will generate the following HTML pages:

\begin{itemize}
  \item A page for each python \texttt{module}.  This page includes an
  overview of the \texttt{module}; a list of the \texttt{class}es
  defined by the \texttt{module}; a list of the \texttt{function}s
  defined by the \texttt{module}, and a detailed description of each
  \texttt{function}; a list of the \texttt{variable}s defined by the
  \texttt{module}, and a detailed description of each
  \texttt{variable}.

  \item A page for each python \texttt{class}.  This page includes an
  overview of the \texttt{class}; a list of the \texttt{method}s
  defined by the \texttt{class}, and a detailed description of each
  \texttt{method}; a list of the instance \texttt{variable}s defined
  by the \texttt{class}, and a detailed description of each instance
  \texttt{variable}; a list of the \texttt{class} \texttt{variable}s
  defined by the \texttt{class}, and a detailed description of each
  \texttt{class} \texttt{variable}.

  \item A page containing the toolkit's complete \texttt{class}
  hierarchy.

  \item A page containing an index of every \texttt{module},
  \texttt{class}, \texttt{method}, \texttt{function}, and
  \texttt{variable} defined by the toolkit.

\end{itemize}

Epydoc is currently under development.  Contact Edward Loper for more
information about Epydoc.  For examples of the HTML pages generated by
Epydoc, see the reference documentation for the toolkit's prototype
modules at 
``\texttt{http://www.cis.upenn.edu/$\sim$edloper/nltk/prototype/nltk.html}''.

%################################################################
%#  PRELIMINARY DESIGN AND IMPLEMENTATION
%################################################################
\section{Preliminary Design and Implementation}

As part of the preliminary design process, I designed and implemented
six prototype modules.  These modules are summarized in figures
\ref{fig:mod-data} and \ref{fig:mod-task}, and described in the
following sections.  Most of these descriptions are copied directly from
the modules' reference documentation.  For more information, see the
reference documentation 
at ``\texttt{http://www.cis.upenn.edu/$\sim$edloper/nltk/nltk.html}''.

\begin{figure}
\noindent
\begin{centering}
\begin{boxedminipage}{.95\textwidth}
\begin{itemize}
  \item \textbf{Sets}: Encodes the mathematical notion of a ``finite set.''
  \begin{itemize}
    \item \textit{Set}: A finite set.
  \end{itemize}

  \item \textbf{Tokens}: Encodes units of text such as words.
  \begin{itemize}
    \item \textit{TokenTypeI}:
         A unit of text.
    \item \textit{TextLocationI}:
         A location within a text.
    \item \textit{Token}:
         An occurance of a unit of text.
         Consists of a TokenType and a TextLocation.
  \end{itemize}

  \item \textbf{Syntax Trees}: Encodes syntax trees.  Not fully designed 
        yet.

  \item \textbf{Probability}: Encodes data structures associated with
        the mathematical notion of probability, such as events and
        frequency distributions.
  \begin{itemize}
    \item Sample: Encodes the mathematical notion of a
          ``sample.''  This is actually not implemented as a class or
          an interface -- (almost) anything can be a Sample.
    \item \textit{EventI}:
         A (possibly infinite) set of samples.
    \item \textit{FreqDistI}:
          The frequency distribution of a collection of samples.
    \item \textit{ProbDistI}:
          A probability distribution, typically derived from a
          frequency distribution (e.g., using ELE).
  \end{itemize}
\end{itemize}
\end{boxedminipage}\\
\end{centering}
 \caption[Data-oriented module prototypes]{Data-oriented module
 prototypes.  Under each module, the interfaces and top-level classes
 defined by that module are listed.}
\label{fig:mod-data}
\end{figure}

\subsection{Sets}

The set module contains a single class, \texttt{Set}.

\subsubsection{\texttt{Set} Class}

    A \texttt{Set} is an unordered container class that contains no
    duplicate elements.  In particular, a set contains no elements
    $e_1$ and $e_2$ such that $e_1=e_2$.  Currently, the \texttt{Set}
    class is given a fairly minimal implementation.  However, more
    members may be defined in the future.

    Although the \texttt{Set} class attempts to ensure that it
    contains no duplicate elements, it can only do so under the
    following circumstances:
    
    \begin{itemize}
      \item For all elements $e_i$, $e_j$ added to the \texttt{Set},
           $e_i=e_j$ if and only if $e_j=e_i$.  This should always be the
           case as long as the elements in the \texttt{Set} use
           well-defined comparison functions.  An example where it
           would not be the case would be if $e_i$ defined
           \texttt{\_\_cmp\_\_()} to always return 0, and $e_j$
           defined \texttt{\_\_cmp\_\_()} to always return $-1$.
           
      \item Mutable elements inserted in the \texttt{Set} are not
           modified after they are inserted.
    \end{itemize}

    If these circumstances are not met, the \texttt{Set} will
    continue to function, but it will no longer guarantee that it
    contains no duplicate elements.
 
\subsection{Tokens}

A \texttt{Token} is an occurance of a single unit of text, such as a word or a
punctuation mark.  The token module defines the \texttt{TokenTypeI}
interface, the \texttt{TextLocationI} interface, and the \texttt{Token}
class.

\subsubsection{\texttt{TokenTypeI} Interface}

    A \texttt{TokenTypeI} is single unit of text, such as a word or a
    punctuation mark.  The precise definition of what counts as a token
    type will vary for different analyses.  For example, ``bank'' and
    ``run'' might be tokens for one analysis, while ``bank/N'' and
    ``bank/V'' might be tokens for another analysis.

    \texttt{TokenType}s can be compared for equality, and should be
    equal if and only if they refer to the same token type.

\vspace{2mm}\noindent
\textit{The \texttt{TokenTypeI} interface is currently implemented by
the classes \texttt{SimpleTokenType} and \texttt{TaggedTokenType}.}

\subsubsection{\texttt{TextLocationI} Interface}

    A \texttt{TextLocationI} is the location of an entity within a
    text.  Text locations can be compared for equality, and should be
    equal if and only if they refer to the same location.

\vspace{2mm}\noindent
\textit{The \texttt{TextLocationI} interface is currently implemented
by the \texttt{IndexTextLocation} class.}

\subsubsection{Token Class}

    A \texttt{Token} is an occurance of a single unit of text, such as
    a word or a punctuation mark.  A \texttt{Token} consists of a token type
    and a source.  The token type is the unit of text (e.g., a
    specific word).  The source is the position at which this token
    occurred in the text.

    The precise definition of what counts as a token type, or unit of
    text, will vary for different analyses.  For example, ``bank'' and 
    ``run'' might be tokens for one analysis, while ``bank/N'' and
    ``bank/V'' might be tokens for another analysis.

\subsection{Syntax Trees}

The syntax tree module defines data structures for storing syntax
trees.  It is still under development.  Currently, it defines one
interface: \texttt{SyntaxNodeI}

This module is still under development.  Several open questions
remain, such as whether \texttt{SyntaxTree}s should correspond to
individual occurances of syntax trees, or to ``types'' of syntax trees
(c.f. the distinction between \texttt{Token} and TokenType).

\subsubsection{\texttt{SyntaxNodeI} Interface}

\emph{This interface is still under development}

A syntax node is a single internal node of a syntax tree.  Each syntax
node is characterized by a tag and an ordered list of children.  The
tag specifies the type of the syntax Node (e.g., ``NP'' or ``V'').
Each child must either be a \texttt{Token}, or implement the
\texttt{SyntaxNodeI} interface.  Access to the children is implemented
by emulating a sequence type.  The \texttt{SyntaxNodeI} does
\emph{not} perform any semantic or syntactic checks on its arguments.
Figure \ref{fig:syntaxnode} shows how syntax nodes can be used.

\begin{figure}
\noindent
\begin{centering}
\begin{boxedminipage}{\textwidth}
\begin{tabbing}
\texttt{>>> cat = Token(\ldots)}\\
\texttt{>>> dog = Token(\ldots)}\\
\texttt{>>> the = Token(\ldots)}\\
\texttt{>>> chases = Token(\ldots)}\\
\\
\texttt{>>> cat\_np = SyntaxNode('NP', [the, cat])}\\
\texttt{>>> dog\_np = SyntaxNode('NP', [the, dog])}\\
\texttt{>>> vp = SyntaxTree('VP', [chases, dog\_np])}\\
\texttt{>>> s = SyntaxTree('S', [cat\_np, vp])}\\
\\
\texttt{>>> print s}\\
\texttt{\textit{[S [NP the dog] [VP chases [NP the dog]]]}}\\
\texttt{>>> print s[0]}\\
\texttt{\textit{[NP the dog]}}\\
\texttt{>>> print s[1].tag()}\\
\texttt{\textit{VP}}\\
\\
\texttt{>>> s[0] = dog\_np}\\
\texttt{>>> vp[:] = [cat, cat, cat]}\\
\end{tabbing}
\end{boxedminipage}\\
\end{centering}
 \caption[Example \texttt{SyntaxNode} use]
 {A session showing how \texttt{SyntaxNode}s can be used.}
\label{fig:syntaxnode}
\end{figure}

\subsection{Probability}

The probability module provides data structures associated with the
mathematical notion of probability.  The following mathematical terms
are useful for thinking about probability:

\begin{itemize}
  \item An \emph{experiment} is an occurance we observe whose result 
        is uncertain.
  \item An \emph{outcome} is some aspect of the result of an
        experiment which we wish to consider.
  \item A \emph{sample} is a possible outcome of an experiment.
  \item The \emph{sample space} of an experiment is the set of its
        possible outcomes.
  \item An \emph{event} is some subset of the sample space of an
        experiment. 
\end{itemize}

The toolkit does not currently encode the notion of a sample with an
interface; instead, it allows (almost) any object to function as a
sample.  Thus, for example, the sample space corresponding to a roll
of a die might be the set of integers from 1 to 6.  The only
requirement on samples is that they properly define the comparison
operator. 

The toolkit currently defines the \texttt{CFSample} class for the
specific purpose of acting as a sample.  \texttt{CFSample}s consist of 
a (\emph{context}, \emph{feature}) pair.

The toolkit currently defines three interfaces for handling the
mathematical notion of probability: the \texttt{EventI} interface; the
\texttt{FreqDistI} interface; and the \texttt{ProbDistI} interface.

\subsubsection{\texttt{EventI} Interface}

    An \texttt{Event} is a subset of some sample space.  Note that
    this subset need not be finite.  Events are typically written as
    the set of samples they contain, or using a function.  Examples
    are:

\begin{verbatim}
{1,2,3}
{x:x>0}
\end{verbatim}

    The only method that events are required to implement is
    \texttt{\_\_contains\_\_()}, which tests whether a sample is a
    contained by the event.  However, when possible, events should
    also define the following methods:
    \begin{itemize}
      \item \texttt{\_\_cmp\_\_()}, which tests whether this event is
           equal to another event.
      \item \texttt{subset()}, which tests whether this event is a
           subset of another event.
      \item \texttt{superset()}, which tests whether this event is
           a superset of another event.
      \item \texttt{union()}, which returns an event containing the
           union of this event's samples and another event's samples.
      \item \texttt{intersection()}, which returns an event
           containing the intersection of this event's samples and
           another event's samples.
      \item \texttt{samples()}, which returns a \texttt{Set}
           containing all of the samples that are contained by this
           event. 
      \item \texttt{\_\_len\_\_()}, which returns the number of samples 
           contained by this event.
    \end{itemize}
    
    Classes implementing the \texttt{EventI} interface may choose
    to only support certain classes of samples, or may choose to only
    support certain types of events as arguments to the optional
    methods (\texttt{\_\_cmp\_\_}, \texttt{subset}, etc.).  If a
    method is unable to return a correct result because it is given an 
    unsupported type of sample or event, it should raise a
    \texttt{NotImplementedError}.

\vspace{2mm}\noindent
\textit{The \texttt{EventI} interface is currently implemented by the
classes: \texttt{UniversalEvent}, \texttt{NullEvent},
\texttt{SampleEvent}, \texttt{SetEvent}, \texttt{FuncEvent},
\texttt{ContextEvent}.}

\subsubsection{\texttt{FreqDistI} Interface}

    A \texttt{FreqDist} represents the frequency distribution for the
    outcomes of an experiment.  A frequency distribution records the
    number of times each outcome of an experiment has occurred.  For
    example, a frequency distribution could be used to record the
    frequency of each token in a document.  Formally, a frequency
    distribution can be defined as a function mapping from samples to
    the number of times that sample occurred as an outcome.

    Frequency distributions are generally constructed by running a
    number of experiments, and incrementing the count for a sample
    every time it is an outcome of an experiment.  For example, the
    following code will produce a frequency distribution that encodes
    how often each word occurs in a text:
    
\begin{verbatim}
freqDist = SimpleFreqDist()
for word in document:
    freqDist.inc(word)
\end{verbatim}

    Classes implementing the \texttt{FreqDistI} interface may
    choose to only support certain classes of samples or events.  If a
    method is unable to return a correct result because it is given an
    unsupported type of sample or event, it should raise a
    \texttt{NotImplementedError}.

    Since several methods defined by \texttt{FreqDistI} can accept
    either events or samples, classes that implement the EventI
    interface should never be used as samples for a frequency
    distribution.

    Frequency distributions are required to implement the methods
    \texttt{inc()}, \texttt{N()}, \texttt{count()},
    \texttt{freq()}, \texttt{max()}, \texttt{cond\_freq()}, 
    and \texttt{cond\_max()}.  In the future, this list may be
    expanded, and optional methods may be added.

\vspace{2mm}\noindent
\textit{The \texttt{FreqDistI} interface is currently implemented by the
classes \texttt{SimpleFreqDistI} and \texttt{CFFreqDistI}.}

\subsubsection{\texttt{ProbDistI} Interface}

    A probability distribution for the outcomes of an experiment.  A
    probability distribution specifies how likely it is that an
    experiment will have any given outcome.  For example, a
    probability distribution could be used to predict the probability
    that a given word will appear in a given context.  Formally, a
    probability distribution can be defined as a function mapping from
    samples to nonnegative real numbers, such that the sum of every
    number in the function's range is 1.0.  \texttt{ProbDist}s are
    often used to model the probability distribution underlying a
    frequency distribution. 

    Classes implementing the \texttt{ProbDistI} interface may
    choose to only support certain classes of samples or events.  If a
    method is unable to return a correct result because it is given an
    unsupported type of sample or event, it should raise a
    NotImplementedError. 

    Since several methods defined by \texttt{ProbDistI} can accept
    either events or samples, classes that implement the EventI
    interface should never be used as samples for a probability
    distribution.

    Probability distributions are required to implement the methods
    \texttt{prob()} and \texttt{cond\_prob()},
    \texttt{max()}, and \texttt{cond\_max()}.  In the future,
    this list may be expanded, and optional methods may be added.

\vspace{2mm}\noindent
\textit{The \texttt{ProbDistI} interface is not currently implemented
any class.}

\begin{figure}
\noindent
\begin{centering}
\begin{boxedminipage}{.95\textwidth}
\begin{itemize}
  \item \textbf{Tokenizers}: Separate a string of text into a list of
       \texttt{Token}s. 
  \begin{itemize}
     \item \textit{TokenizerI}
  \end{itemize}

  \item \textbf{Taggers}: Assign tags to each \texttt{Token} in a list of \texttt{Token}s.
  \begin{itemize}
     \item \textit{TaggerI}
  \end{itemize}
\end{itemize}
\end{boxedminipage}\\
\end{centering}
 \caption[Task-oriented module prototypes]{Task-oriented module
 prototypes.  Under each module, the interfaces and top-level classes
 defined by that module are listed.}
\label{fig:mod-task}
\end{figure}

\subsection{Tokenizers}

The process of separating a string of text into \texttt{Token}s is called
\emph{tokenization}.  The tokenizer module defines the
\texttt{TokenizerI} interface, which should be implemented by classes
that tokenize text strings.

\subsubsection{\texttt{TokenizerI} Interface}

    A \texttt{Tokenizer} is a processing class responsible for
    separating a string of text into a list of \texttt{Token}s.  This
    process is also known as \emph{tokenizing} the string of text.
    Particular \texttt{Tokenizer}s may split the text at different
    points, or may produce \texttt{Token}s with different types of
    \texttt{TokenType}.

\vspace{2mm}\noindent
\textit{The \texttt{TokenizerI} interface is currently implemented by
the classes \texttt{SimpleTokenizer} and \texttt{TaggedTokenizer}.}

\subsection{Taggers}

The process of assigning a descriptive tag to each \texttt{Token} in a
list of \texttt{Token}s is called \emph{tagging}.  Often, tagging is
used to assign a part-of-speech tag to each \texttt{Token} in a list
of \texttt{Token}s.  Particular \texttt{Taggers} may attach different
types of tags to the \texttt{Token}s in the list, or may use different
algorithms for deciding which tag to attach.

\subsubsection{\texttt{TaggerI} Interface}

    A \texttt{Tagger} is a processing class for assigning a tag to
    each token in an ordered list of tokens.  Taggers are required to
    define one function, \texttt{tag}, which takes a list of
    \texttt{Token}s, and returns a list of \texttt{Token}s.
    Typically, the input tokens will be \texttt{SimpleToken}s, and the
    output tokens will be \texttt{TaggedToken}s.  However, taggers may
    also be written to map between other types of tokens, as long as
    they are still performing the same conceptual task.

    Classes implementing the \texttt{TaggerI} interface may choose to
    only support certain classes of tokens.  If a method is unable to
    return a correct result because it is given an unsupported class
    of token, then it should raise a \texttt{NotImplementedError}.

%################################################################
%#  BIBLIOGRAPHY
%################################################################
\begin{thebibliography}{1}

\bibitem{allen1995}
{Allen, James.  1995.  
 \emph{Natural Language Understanding}.
 Redwood, CA: Benjamin Cummings.}

\bibitem{cis530}{CIS 530 class homepage.  
                 University of Pennsylvania. \\ 
    \texttt{http://www.cis.upenn.edu/~cis530/}}

\bibitem{epydoc}{Epydoc: Edloper's Python Documentation Tool.
    Contact \texttt{ed@loper.org} for more information.}

\bibitem{gazdar1989}
{Gazdar, Gerald and Mellish, Chris.  1989.
 \emph{Natural Language Processing in LISP : An Introduction to
       Computational Linguistics}. 
 Reading, MA: Addison-Wesley Pub. Co.}

\bibitem{jurafsky2000}
{Jurafsky, Daniel and Martin, James H.  2000.
 \emph{Speech and Language Processing: An Introduction to Natural
       Language Processing, Computational Linguistics, and Speech
       Recognition}.
 Upper Saddle River, NJ: Prentice Hall.}

\bibitem{manning2000}
{Manning, Christopher D. and Schutze, Hinrich.  2000.  
 \emph{Foundations of Statistical Natural Language Processing}.
 Cambridge, MA: MIT Press.}

\bibitem{numpy}{Numerical Python \\
    \texttt{http://numpy.sourceforge.net/}}

\bibitem{python}{The Python programming language. \\
    \texttt{http://www.python.org/}}

\bibitem{r}{The R Project for Statistical Computing \\
    \texttt{http://www.r-project.org/}}

\bibitem{rspython}{The R/S-Plus Python Interface \\
    \texttt{http://www.omegahat.org/RSPython/}}

\bibitem{s}{The S System \\
    \texttt{http://cm.bell-labs.com/cm/ms/departments/sia/S/}}

\bibitem{scipy}{Scientific Python \\
    \texttt{http://starship.python.net/crew/hinsen/scientific.html}}

\bibitem{statspy}{stats.py: A Python Statistical Module \\
    \texttt{http://www.vex.net/parnassus/apyllo.py/684222876.912038798}}

\bibitem{vtk}{VTK: The Visualization Toolkit \\
    \texttt{http://www.kitware.com/vtk.html}}

\end{thebibliography}

\end{document}