%
% Natural Language Toolkit for Python
% Technical Report: Representation Types
% Edward Loper
% 
% Created [03/02/01 12:02 AM]
% $Id$
%

% Note: be careful to say ``x is a kind of y'' instead of ``x is a type 
% of y,'' since we're giving ``type'' special meaning.

\newcommand{\concept}[1]{\textsf{#1}}
\newcommand{\code}[1]{\texttt{#1}}

\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}

\begin{document}
\title{Basic Representation Types\\
\Large Natural Language Processing Toolkit: Technical Report}
\author{Edward Loper}
\maketitle

\section{Introduction}

  The natural language toolkit allows students to manipulate a number
  of conceptual objects, such as words, sentences, and syntax trees.
  Before we can build a consistant framework for manipulating such
  objects, we must decide how they should be represented.  This
  technical report explains basic types used by the toolkit to
  represent a number of conceptual objects.  Furthermore, it attempts
  to justify why this set of basic types was chosen.

  Section \ref{sec:concepts} describes the basic concepts which this
  technical report attempts to address.  Sections \ref{sec:locations}
  through \ref{sec:trees} describe the basic types that were chosen to
  encapsulate these concepts, and the rational behind choosing those
  types.  Section \ref{sec:summary} provides a summary of the basic
  types, and discusses how they interact.

\newpage
\section{Abstract Concepts}
\label{sec:concepts}

  The following list of abstract concepts is representative of the
  concepts which should be captured by the basic types defined in this
  techincal report.\footnote{The types used to represent many other
  abstract concepts (such as frequency distributions) are beyond the
  scope of this technical report.}  Where the meaning of the concepts
  are not obvious, I included explanations.

  \begin{itemize}
  
    \item \concept{Document}
  
    \item \concept{Sentence}
  
    \item \concept{Word Type}: A single word (as opposed to a
        \concept{Word Occurance}).
  
    \item \concept{Tagged Word Type}: A word that has been tagged with
        additional information, such as part-of-speech information.
  
    \item \concept{Stemmed Word Type}: A word that has been stripped of
        inflectional suffixes and prefixes.
  
    \item \concept{Case-Normalized Word Type}: A word whose case has been
        normalized (usually to either upper case or lower case).
  
    \item \concept{Word Occurance}: A single occurance of a word type.

    \item \concept{Tagged Word Occurance}
    \item \concept{Stemmed Word Occurance}
    \item \concept{Case-Normalized Word Occurance}
  
    \item \concept{Tokenized Document}: A document that has been divided
        into individual word occurances.
  
    \item \concept{Tokenized Sentence}: A sentence that has been divided
        into individual word occurances.
  
    \item \concept{Location}: The location of one unit of text within
        another unit of text.  A typical location might give the
        location of a word within a document or a sentence.
  
    \item \concept{Tagged Document} or \concept{Tagged Sentence}: A
        document or sentence that has been divided into individual word
        occurances, each of which has been tagged with additional
        information.
  
    \item \concept{Stemmed Document} or \concept{Stemmed Sentence}: A
        document or sentence that has been divided into individual word
        occurances, each of which has been stripped of inflectional
        affixes.
  
    \item \concept{Case-Normalized Document} or \concept{Case-Normalized
        Sentence}: A document or sentence that has been divided into
        individual word occurances, each of which has had its case
        normalized.
  
    \item \concept{Syntax Tree}: A hierarchical representation of the
        structure of a sentence.
  
    \item \concept{Syntax Node}: The annotation appearing at a node in a
        syntax tree.  Typically, this will be a tag, such as ``NP.''
  
    \item \concept{Syntax Tree Occurance}: A single occurance of a syntax
        tree.
  
  \end{itemize}

\section{Locations}
\label{sec:locations}
%  - location

  \subsection{Design}

  The location of an element of text is represented using a span-based
  data structure.  A \emph{span} consists of a \emph{start index} and
  an \emph{end index}, both of which are non-negative integers.  A
  span with start index $s$ and end index $e$ is written ``$[s,e)$,''
  and specifies the location of the text beginning at $s$, and
  including everything up to (but not including) the text at $e$.  It
  is always true that $s<e$.\footnote{Should this be changed to $s\leq
  e$?  If $s=e$, then the span would be empty -- does it make sense to
  have empty locations?}

  The start and end indices can be based on a variety of different
  \emph{units}, such as character number, word number, or sentence
  number.  However, for any two indeces $x$ and $y$, it should be true
  that $x<y$ if and only if $x$ indicates a location earlier in the
  text than $y$.  The location of an element of text may be explicitly
  tagged with information about what unit its indices use.  Unit
  labels take the form of case-insensitive \code{string}s.  Typical
  examples of unit labels are the strings \code{"character"} and
  \code{"word"}.

  The location of an element of text may also be tagged with its
  \emph{source}.  This is an object that gives an indication of where
  the text was derived from.  A typical example of a source would be a
  \code{string} containing the name of the file from which the element
  of text was read.  Sources may also be represented as locations; for
  example, the source for the location of a word might be the location
  of the sentence that contained it.  This allows locations to be
  specified in a hierarchical fashion.

  \subsection{Implementation}

  Locations are implemented using the \code{Location} class.
  \code{Location}s consist of:

  \begin{itemize}

    \item A \emph{start index} (accessed via the \code{start()} member
    function).  \\
    Type: \code{int}

    \item An \emph{end index} (accessed via the \code{end()} member
    function).  \\
    Type: \code{int}

    \item An optional \emph{unit} (accessed via the \code{unit()}
    member function).  \\
    Type: \code{string} \emph{(normalized to lower case)}
    

    \item An optional \emph{source} (accessed via the \code{source()}
    member function).  \\
    Type: \emph{(any)}

  \end{itemize}

  \noindent
  \code{Location}s are immutable objects.  \code{Location}s can be
  compared for equality and ordering.  If two \code{Location}s with
  different units or different sources are compared, then an exception
  will be raised.  When \code{Location} comparisons do not raise
  exceptions, they are defined as follows:
  \begin{align*}
     [s_1,e_1) = [s_2,e_2) &\iff s_1=s_2 \quad \land \quad e_1=e_2 \\
     [s_1,e_1) < [s_2,e_2) &\iff e_1 < s_2 \\
     [s_1,e_1) > [s_2,e_2) &\iff s_1 > e_2 \\
     [s_1,e_1) \leq [s_2,e_2) &\iff [s_1,e_1) < [s_2,e_2) \quad \lor \quad
                             [s_1,e_1) = [s_2,e_2) \\
     [s_1,e_1) \geq [s_2,e_2) &\iff [s_1,e_1) > [s_2,e_2) \quad \lor \quad
                             [s_1,e_1) = [s_2,e_2) 
  \end{align*}

  \noindent
  Note that if two \code{Location}s $l_1$ and $l_2$ overlap, then it
  will never be the case that $l_1<l_2$ or $l_1>l_2$.

  \subsection{Discussion}

  Originally, I proposed a general, user-extensible notion of
  location.  However, the span-based representation described here
  seems to be sufficiently powerful to represent almost any location.
  We decided that the additional power granted by making the notion of
  location user-extensible did not compensate for the added
  complexity.  Also, if the notion of location built-in, then other
  classes can rely on it for computation.  For example, syntax trees
  can compute their locations as a function of the locations of their
  leaves.  If locations were user-extensible objects, then there would
  be no general way to perform such computations.

  A simpler notion of location, based on a single index, was
  considered.  However, it was decided that the added power provided
  by representing locations as ranges was worthwhile, especially in
  the context of multi-word structures like syntax trees.

  The optional unit and source fields were added to help distinguish
  different sets of locations, and to help ensure that unrelated
  locations are not accidentally compared.  For example, it may be
  useful for a student to keep track of the location of the words in
  two different files; but comparing these locations would be
  meaningless.  We considered simply returning false when locations
  with non-matching units or sources are compared, but decided that
  raising exceptions would be more likely to help students find
  errors.  If students wish to compare two locations that may have
  different sources or units, then they must explicitly compare the
  location's sources or units, or catch and process the exceptions
  raised by the comparison methods.

  We decided to restrict location indices to non-negative integers
  because it simplifies the interface without signifigantly reducing
  its power.  Other alternatives we considered were floating point
  numbers and arbitrary comperable objects.  Floating point numbers
  were rejected because rounding errors can invalidate tests for
  equality and ordering.  If a floating-point index seems appropriate,
  it is recommended that users either multiply the floating point
  numbers by a fixed constant and round them to integers, or create a
  bidirectional mapping between floating-point indices and integer
  indices.  The use of arbitrary comperable objects as indices was
  rejected because it signifigantly complicates the location
  interfaces, without adding signifigant power to the system.  Also,
  arbitrary comperable objects may incur some of the difficulties that
  would be encountered with floating point numbers.

  A span $[s,e)$ includes the text at $s$ but not the text at $e$ in
  order to be consistant with Python slice notation.  Also, all
  indices are required to be non-negative for consistancy with Python
  slice notation, where negative indices have special meaning.

\section{Words}
\label{sec:words}
%  - (type)
%  - token

  \subsection{Design}

  The term ``word'' is used to refer to a wide variety of different
  abstract concepts, as can be seen by looking through the list of
  concepts from Section \ref{sec:concepts}.  One important distinction
  that is easy to overlook is the difference between a word and an
  individual occurance of a word.  For example, the sentence ``my dog
  likes his dog'' contains four words but five occurances of words
  (since the word ``dog'' appears twice).  In order to make this
  distinction clear, we will use the words \emph{type} and
  \emph{token} to refer to words and to occurances of words,
  respectively.  For example, in the sentence ``my dog likes his
  dog,'' we can say that the second and fifth words are different
  tokens, but that they have the same types.

  Individual tokens extracted from the same text can always be
  distinguished based upon their locations.  This fact provides us
  with a convenient mechanism for encoding tokens: a token is simply a
  pair consisting of a type and a location.  Two tokens are only equal
  if both their location and thier type are equal.

  The notions of token and type are actually more general than the
  notions of word and word occurance.  In the context of the toolkit,
  types can refer to any object derived from text, and tokens can
  refer to any instances of objects derived from text.  For example,
  syntax trees are actually special kinds of tokens (see Section
  \ref{sec:trees}).  

  The toolkit does not define a specific encoding for individual kinds
  of tokens or token types, like simple words, tagged words, or
  stemmed words.  Instead, it defines the general encoding for tokens,
  and allows students to use almost any object as a type.  For
  example, \code{string}s could be used to encode the types of simple
  words, stemmed words, and case-normalized words; and pairs of
  strings could be used to encode tagged word types.

  However, the toolkit does provide a number of simple classes that
  are intended to be used as types.  Currently, the only class
  provided by the toolkit is the \code{TaggedType} class, which can be
  used to encode the type of a tagged token.  A \code{TaggedType}
  simply consists of a base type and a tag, both of which are
  \code{string}s.

  \subsection{Implementation}

  \subsubsection{Types}
      A Type can be any immutable object that properly implements
      equality comparison.

  \subsubsection{Tokens}
      Tokens are implemented using the \code{Token} class.
      \code{Token}s consist of:
  
      \begin{itemize}
    
        \item A \emph{location} (accessed via the \code{loc()} member
        function).  \\
        Type: \code{Location}
    
        \item A \emph{type} (accessed via the \code{type()} member
        function).  \\
        Type: \emph{(any)}
    
      \end{itemize}
      
      \noindent
      \code{Token}s are immutable objects.

  \subsubsection{TaggedTypes}
      \vspace{3mm}\noindent
      The types of tagged tokens can be encoded using the
      \code{TaggedType} class.  A \code{TaggedType} consists of:
  
      \begin{itemize}
    
        \item A \emph{base type} (accessed via the \code{base()}
        member function).  \\
        Type: \code{string}
    
        \item A \emph{tag} (accessed via the \code{tag()} member
        function).  \\
        Type: \code{string}
    
      \end{itemize}
      
      \noindent
      \code{TaggedType}s are immutable objects.  Two tagged types are
      equal if their bases are equal and their tags are equal.  Note
      that equality of strings is defined to be case-sensitive.
    
  \subsection{Discussion}
  % - why not a class/interface for type?
  % - why location+type?
  % - why not classes to distinguish stemmed/case-normalized/etc?

  We considered implementing an interface or class hierarchy for
  types.  Using an interface, rather than allowing arbitrary Python
  objects, has a number of benefits:

  \begin{description}

    \item[Exception Semantics] If an interface is used to define what
    type objects must do, then types can exhibit more precise
    exception semantics.  In particular, it would be possible to
    ensure that comparing two type objects with different classes
    would always raise an exception.  In constrast, it is impossible
    to make such guarantees when comparing arbitrary Python objects.

    \item[Type-kind Distinctions] If each kind of type (e.g., stemmed
    types, case-normalized types, tagged types, etc.) is implemented
    with a separate class, then it is easier to maintain distinctions
    between different kinds of types.  But if built-in Python objects
    such as strings and tuples are used to represent types, then the
    distinctions between different kinds of type will become blurred.
    For example, since strings are likely to be used for simple words,
    stemmed words, and case-normalized words, it will be impossible to
    determine what kind of type a string contains by inspecting the
    string object.

  \end{description}

  However, requiring that all types implement an interface often
  introduces an extra layer of indirection, and complicates the system
  used to represent words.  Since words are fundamental to the
  toolkit, we decided that they should be kept as simple as is
  practical.  Some of the exception semantics can be regained at a
  different level.  For example, even though testing two types for
  equality may not raise an exception if they have different Python
  types, it would be possible to implement the \code{type\_eq()}
  method of tokens to perform this check.\footnote{The
  \code{type\_eq()} method is also included because it can be more
  efficient than extracting the tokens' types and comparing them,
  especially for some special kinds of tokens like trees (see Section
  \ref{sec:trees}).}  Also, although type-kind distinctions cannot be
  derived directly from the type object, these distinctions are
  usually implicit in the history of the object, and it should be rare
  to have a type object without knowing what kind of type it contains.
  Therefore, we decided that the benefits derived from maintaining a
  class hierarchy for types did not justify the increase in complexity
  for the toolkit's set of basic type.

\section{Collections of Words}
\label{sec:collections}
%  - string (sentence/document)
%  - list of token

  \subsection{Design}

  The toolkit does not implement any special encodings for collections
  of words.  Instead, it relies on Python's built-in types.  However,
  there are a number of standard ways to use Python's built-in types
  to encode collections of words:

  \begin{description}

    \item[Strings] Unprocessed text is usually represented as a single
    string.  For example, when data is read from a file, it is
    typically read into a string.  Tokenizers can be used to convert
    strings into lists of tokens.

    \item[List of Tokens] A piece of text can be represented as an
    ordered list of the tokens that it contains.

    \begin{itemize}

        \item A list of tokens $l$ is said to be ``properly ordered''
        if, for every $i<j$, $l[i].loc()<l[j].loc()$.

        \item A list of tokens $l$ is said to be ``exhaustive'' if:

        \begin{itemize}
          \item for every $i$, $l[i].end() = l[i+1].start()$; 
          \item $l[0].start()$ is the start of the text.
          \item $l[-1].end()$ is the end of the text.
        \end{itemize}

    \end{itemize}

    \item[List of Types] When the locations of individual tokens is
    unimportant, a text can be represented as an ordered list of its
    token's types.

    \item[Set of Tokens] When the order of the tokens in a text is
    unimportant, the text can be represented as a set of tokens.
    \footnote{Python does not (yet) provide a built-in set type;
    however, the toolkit defines a generic set class.}  Since each
    token's location serves to distinguish it from all other tokens,
    the set will not contain any duplicate elements.

    \item[Set of Types] When the order of tokens in a text and the
    presence of multiple tokens with the same type are unimportant,
    the text can be represented as a set of types.

    \item[Dictionary Mappings] Since both tokens and types are
    immutable objects, they can be used as keys in dictionaries.  A
    typical use of a dictionary would be to encode a multiset of types
    as a dictionary mapping types to counts.

  \end{description}

%  \subsection{Implementation}
%  There is no real implementation, is there??

  \subsection{Discussion}

  % Why not explicit encodings?  Advantage of explicit encodings:
  % check that everything in a list/set/whatever has the same pytype.

\section{Syntax Trees}
\label{sec:trees}
%  - (node)
%  - tree (type of token)
%      - (node pytype)
%      - (leaf pytype)
%  - treetype

  \subsection{Design}

  \subsection{Implementation}

  \subsection{Discussion}

\section{Summary}
\label{sec:summary}
% Dependancies:
%     token \to location
%     tree \to token
%     tree \to location
%     token \to (type)
%     tree \to (type)
%     tree \to (node)


%% Meta-section
\newpage
\section{Meta-Questions}
\begin{itemize}
  \item Are the terms used in this document correct?  Could better
  terms be chosen?  Especially: properly ordered, exhaustive,
  type-kind.
  \item Are the interfaces presented here reasonable?
  \item Do any decisions need more justification?
\end{itemize}

\end{document}

