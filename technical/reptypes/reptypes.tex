%
% Natural Language Toolkit for Python
% Technical Report: Representation Types
% Edward Loper
% 
% Created [03/02/01 12:02 AM]
% $Id$
%

\newcommand{\concept}[1]{\textsf{#1}}
\newcommand{\code}[1]{\texttt{#1}}

\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}

\begin{document}
\title{Basic Representation Types\\
\Large Natural Language Processing Toolkit: Technical Report}
\author{Edward Loper}
\maketitle

\section{Introduction}

  The natural language toolkit allows students to manipulate a number
  of conceptual objects, such as words, sentences, and syntax trees.
  Before we can build a consistant framework for manipulating such
  objects, we must decide how they should be represented.  This
  technical report explains basic types used by the toolkit to
  represent a number of conceptual objects.  Furthermore, it attempts
  to justify why this set of basic types was chosen.

  Section \ref{sec:concepts} describes the basic concepts which this
  technical report attempts to address.  Sections \ref{sec:locations}
  through \ref{sec:trees} describe the basic types used to encapsulate
  these concepts.  Section \ref{sec:summary} provides a summary of the
  basic types, and discusses how they interact.

\newpage
\section{Abstract Concepts}
\label{sec:concepts}

  The following list of abstract concepts is representative of the
  concepts which should be captured by the basic types defined in this
  techincal report.\footnote{The types used to represent many other
  abstract concepts (such as frequency distributions) are beyond the
  scope of this technical report.}  Where the meaning of the concepts
  are not obvious, I included explanations.

  \begin{itemize}
  
    \item \concept{Document}
  
    \item \concept{Sentence}
  
    \item \concept{Word Type}: A single word (as opposed to a
        \concept{Word Occurance}).
  
    \item \concept{Tagged Word Type}: A word that has been tagged with
        additional information, such as part-of-speech information.
  
    \item \concept{Stemmed Word Type}: A word that has been stripped of
        inflectional suffixes and prefixes.
  
    \item \concept{Case-Normalized Word Type}: A word whose case has been
        normalized (usually to either upper case or lower case).
  
    \item \concept{Word Occurance}: A single occurance of a word type.

    \item \concept{Tagged Word Occurance}
    \item \concept{Stemmed Word Occurance}
    \item \concept{Case-Normalized Word Occurance}
  
    \item \concept{Tokenized Document}: A document that has been divided
        into individual word occurances.
  
    \item \concept{Tokenized Sentence}: A sentence that has been divided
        into individual word occurances.
  
    \item \concept{Location}: The location of one unit of text within
        another unit of text.  A typical location might give the
        location of a word within a document or a sentence.
  
    \item \concept{Tagged Document} or \concept{Tagged Sentence}: A
        document or sentence that has been divided into individual word
        occurances, each of which has been tagged with additional
        information.
  
    \item \concept{Stemmed Document} or \concept{Stemmed Sentence}: A
        document or sentence that has been divided into individual word
        occurances, each of which has been stripped of inflectional
        affixes.
  
    \item \concept{Case-Normalized Document} or \concept{Case-Normalized
        Sentence}: A document or sentence that has been divided into
        individual word occurances, each of which has had its case
        normalized.
  
    \item \concept{Syntax Tree}: A hierarchical representation of the
        structure of a sentence.
  
    \item \concept{Syntax Node}: The annotation appearing at a node in a
        syntax tree.  Typically, this will be a tag, such as ``NP.''
  
    \item \concept{Syntax Tree Occurance}: A single occurance of a syntax
        tree.
  
  \end{itemize}

\section{Locations}
\label{sec:locations}
%  - location

  \subsection{Design}

  The location of an element of text is represented using a span-based
  data structure.  A \emph{span} consists of a \emph{start index} and
  an \emph{end index}, both of which are integers.  A span with start
  index $s$ and end index $e$ is written ``$[s,e)$,'' and specifies
  the location of the text beginning at $s$, and including everything
  up to (but not including) the text at $e$.  It is always true that
  $s<e$.\footnote{Should this be changed to $s\leq e$?}

  The start and end indices can be based on a variety of different
  \emph{units}, such as character number, word number, or sentence
  number.  However, for any two indeces $x$ and $y$, it should be true
  that $x<y$ if and only if $x$ indicates a location earlier in the
  text than $y$.  The location of an element of text may be explicitly
  tagged with information about what unit its indices use.  Unit
  labels take the form of case-insensitive \code{string}s.  Typical
  examples of unit labels are the strings \code{"character"} and
  \code{"word"}.

  The location of an element of text may also be tagged with its
  \emph{source}.  This is an object that gives an indication of where
  the text was derived from.  A typical example of a source would be a
  \code{string} containing the name of the file from which the element
  of text was read.  Sources may also be represented as locations; for
  example, the source for the location of a word might be the location
  of the sentence that contained it.  This allows locations to be
  specified in a hierarchical fashion.

  \subsection{Implementation}

  Locations are implemented using the \code{Location} class.
  \code{Location}s consist of:

  \begin{itemize}

    \item A \emph{start index} (accessed via the \code{start()} member
    function).  \\Type: \code{int}

    \item An \emph{end index} (accessed via the \code{end()} member
    function).  \\Type: \code{int}

    \item An optional \emph{unit} (accessed via the \code{unit()}
    member function).  \\Type: \code{string} \emph{(normalized to
    lower case)}

    \item An optional \emph{source} (accessed via the \code{source()}
    member function).  \\Type: \emph{(any)}

  \end{itemize}

  \noindent
  Locations are immutable objects.

  Locations can be compared for equality and ordering.  If two
  locations with different units or different sources are compared,
  then an exception will be raised.  When location comparisons do not
  raise exceptions, they are defined as follows:
  \begin{align*}
     [s_1,e_1) = [s_2,e_2) &\iff s_1=s_2 \quad \land \quad e_1=e_2 \\
     [s_1,e_1) < [s_2,e_2) &\iff e_1 < s_2 \\
     [s_1,e_1) > [s_2,e_2) &\iff s_1 > e_2 \\
     [s_1,e_1) \leq [s_2,e_2) &\iff [s_1,e_1) < [s_2,e_2) \quad \lor \quad
                             [s_1,e_1) = [s_2,e_2) \\
     [s_1,e_1) \geq [s_2,e_2) &\iff [s_1,e_1) > [s_2,e_2) \quad \lor \quad
                             [s_1,e_1) = [s_2,e_2) 
  \end{align*}

  \noindent
  Note that if two locations $l_1$ and $l_2$ overlap, then it will
  never be the case that $l_1<l_2$ or $l_1>l_2$.

  \subsection{Explanation and Comments}

  Originally, I proposed a general, user-extensible notion of
  location.  However, the span-based representation described here
  seems to be sufficiently powerful to represent almost any location.
  We decided that the additional power granted by making the notion of
  location user-extensible did not compensate for the added
  complexity.  Also, if the notion of location built-in, then other
  classes can rely on it for computation.  For example, syntax trees
  can compute their locations as a function of the locations of their
  leaves.  If locations were user-extensible objects, then there would
  be no general way to perform such computations.

  A simpler notion of location, based on a single index, was
  considered.  However, it was decided that the added power provided
  by representing locations as ranges was worthwhile, especially in
  the context of multi-word structures like syntax trees.

  The optional unit and source fields were added to help distinguish
  different sets of locations, and to help ensure that unrelated
  locations are not accidentally compared.  For example, it may be
  useful for a student to keep track of the location of the words in
  two different files; but comparing these locations would be
  meaningless.  We considered simply returning false when locations
  with non-matching units or sources are compared, but decided that
  raising exceptions would be more likely to help students find
  errors.  If students wish to compare two locations that may have
  different sources or units, then they must explicitly compare the
  location's sources or units, or catch and process the exceptions
  raised by the comparison methods.

  We decided to restrict location indices to integers because it
  simplifies the interface without signifigantly reducing its power.
  Other alternatives we considered were floating point numbers and
  arbitrary comperable objects.  Floating point numbers were rejected
  because rounding errors can invalidate tests for equality and
  ordering.  If a floating-point index seems appropriate, it is
  recommended that users either multiply the floating point numbers by
  a fixed constant and round them to integers, or create a
  bidirectional mapping between floating-point indices and integer
  indices.  The use of arbitrary comperable objects as indices was
  rejected because it signifigantly complicates the location
  interfaces, without adding signifigant power to the system.  Also,
  arbitrary comperable objects may incur some of the difficulties that
  would be encountered with floating point numbers.

\section{Words}
\label{sec:words}
%  - (type)
%  - token

\section{Collections of Words}
\label{sec:collections}
%  - string (sentence/document)
%  - list of token

\section{Syntax Trees}
\label{sec:trees}
%  - (node)
%  - tree (type of token)
%      - (node pytype)
%      - (leaf pytype)
%  - treetype

\section{Summary}
\label{sec:summary}
% Dependancies:
%     token \to location
%     tree \to token
%     tree \to location
%     token \to (type)
%     tree \to (type)
%     tree \to (node)




\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








The abstract concepts which 



This document catalogs some of the different conceptual entities which
the tooklit should deal with.  These conceptual entities sometimes
draw more distinctions than is convenient when actually working with
objects.  

An open question is how strict the toolkit should be about enforcing
the distinctions between conceptually distinct types of entities.  On
one hand, strictly enforcing these distinctions is conceptually
``simpler,'' and may promote clearer thinking.  On the other hand, it
is often cumbersome to make these distinctions -- in many
circumstances, they don't matter, so they get dropped in practice.
For example, most people will not distinguish ``tokens'' from ``token
types'' unless they need to.  In a sense, this question boils down to
whether it will be easier to learn/use a set of rules/conventions that
are simpler, but somewhat less intuitive; or more complex, but more
intuitive.  I think that one danger is that the more complex rules
will only be more intuitive some of the time.  And it may be dangerous
to be inconsistantly intuitive..

Another open question is whether the current names should be kept.  In
particular, names of conceptual entities should be both \emph{correct}
and \emph{easy to use/remember}.

Also, it may be possible that some of these conceptual entities could
be merged or done away with altogther.  Are all of these entities
serving useful purposes?



\section{Token Stuff}

{\it (Currently, many of these definitions
are a bit hand-wavey.  That partially stems from the fact that they
are not really as well-defined as we would like (esp the more
primitive ones).  But I'll try to clean them up eventually.)}

\begin{itemize}

  \item \textbf{Lexeme:} An identifier for a word-like thing.
  Normally, this would be a string.  But if you wanted to process text
  more efficiently, it could be an integer.  It might be stemmed, not
  stemmed, case-normalized, not case-normalized, etc.
 
  \item \textbf{Token Type:} A word-like unit of text.  Includes a
  lexeme, and may also include other info, like part of speech tag,
  etc.

  \item \textbf{Token:} An occurance of a token type

  \item \textbf{Location:} The location of something in text.  There
  are actually a number of ways of representing locations: as indexes,
  as spans, etc.  Also, indexes can refer to word number, character
  number, etc.

\end{itemize}

\section{Syntax Tree Stuff}

\begin{itemize}

  \item \textbf{SyntaxNode:} The annotation that appears at a node
  in a syntax tree.  E.g., this might hold a phrase type (``NP'' or
  ``V'').

  \item \textbf{SyntaxTree:} An occurance of a tree or subtree.
  Consists of a SyntaxNode and an ordered list of zero or more
  children.  Children must be SyntaxTrees or Tokens.

  \item \textbf{SyntaxTreeType:} The ``type'' of a syntax tree (as
  opposed to an occurance of a syntax tree).  Consists of a SyntaxNode
  and an ordered list of zero or more children.  Children must be
  SyntaxTreeTypes or TokenTypes.

\end{itemize}

\section{How much to enforce?}

So basically, the question of deciding how strictly to enforce these
distinctions comes down to two questions:

\begin{enumerate}
  \item Should any entities/distinctions be eliminated?
  \item Should it be possible to use entities in environments that
        are not \emph{strictly speaking} correct?
\end{enumerate}

I'll leave the first question for later, and concentrate on the
second.  There are a number of ways that objects could try to be more
friendly when you use them in ways that that are not strictly speaking
correct:

\begin{itemize}
  \item Equality tests: What happens when you compare a string to a
  token?  A string to a TokenType?  A Token to a TokenType?  etc.?  A
  related question is how to handle comparisons within one class
  hierarchy.  E.g., equality tests between a tagged token and a simple
  token.  The strict view would say that any of these comparisons
  should raise an exception.  However, if we decide not to be strict,
  there are a number of places where these equality tests could be
  well-defined.  Examples that come to mind are:
  \begin{itemize}
    \item equality between a string and a SimpleTokenType
    \item equality between a string and a Token (ignore location)
    \item equality between an integer and a Location
    \item equality between a list and a SyntaxTree(Type)
  \end{itemize}

  \item Arguments to methods: Should methods attempt to handle
  arguments that are not quite the expected type?  E.g., when Steven
  passed strings to a FreqDist, when it was really (sort of) expecting
  TokenTypes.

  \item Arguments to constructors: Should constructors automatically
  ``wrap'' objects if they're not the right type?  E.g., should
  Token(``cat'') automatically create a SimpleTokenType containing
  ``cat''?
\end{itemize}
