\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}


\title{NLTK: Text Classification}
\author{Edward Loper}

\begin{document}
\maketitle

\section{Introduction}

  The Natural Language Toolkit is a pedagogical package designed to
  help students get hands-on experience with using, designing, and
  implementing natural language processing systems.  It is organized
  as a suite of modules, each of which provides basic data types,
  tools, and interfaces for a particular natural language processing
  task.

  This report gives an overview of the design and implementation of
  the text classification module.  This module is used to implement
  systems that classify texts into categories.  It focuses on the
  problem of \emph{single-category text classification}, in which:

  \begin{itemize}
    \item The set of categories is known.
    \item The number of categories is finite.
    \item Each text belongs to exactly one category.
  \end{itemize}

  This report assumes that the reader is familiar with text
  classification, feature-based classification, and feature selection.

\section{Documentation}

  The text classification module is accompanied by extensive
  documentation.  

  \begin{itemize}

    \item The \emph{Text Classification Tutorial} provides a gentle
    introduction to the text classification module.  It uses concrete
    examples to explain and motivate the data types, tools, and
    interfaces that are used for text classification. 

    \texttt{http://nltk.sourceforge.net/tutorial/classifying/t1.html}

    \item The \emph{Text Classification Reference Documentation} gives
    precise and detailed descriptions of every sub-module, interface,
    class, method, and function defined by the text classification
    module.

    \texttt{http://nltk.sourceforge.net/ref/nltk.classifier.html}

  \end{itemize}

\section{Architecture}

  The text classification module currently consists of five
  sub-modules:

  \begin{itemize}

    \item \texttt{nltk.classifier} defines the basic data types and
    interfaces that are used to define the text classification task
    and the classifier training task.

    \item \texttt{nltk.classifier.feature} defines data types,
    classes, and interfaces used for feature-based classification.
    Features provide a standard way of encoding the information used
    to make classification decisions.  This standard encoding allows
    the same classifier to be used to solve many different problems.

    \item \texttt{nltk.classifier.featureselection} defines classes
    and interfaces used to select which features are relevant for a
    given classification task.

    \item \texttt{nltk.classifier.naivebayes} defines a text
    classifier model based on the Naive Bayes assumption; and a
    classifier trainer to build Naive Bayes classifiers from training
    data.

    \item \texttt{nltk.classifier.maxent} defines a text classifier
    model based on the maximum entropy modeling framework; and a
    classifier trainer to build maximum entropy classifiers from
    training data.

  \end{itemize}

  This report focuses on the text classification module's \emph{core
  architecture}, consisting of the first three modules.  For more
  information on \texttt{nltk.classifier.naivebayes} or on
  \texttt{nltk.classifier.maxent}, see the tutorial documentation and
  the reference documentation.

  \subsection{Basic Data Types}

    The classification module defines two basic data types:

    \begin{itemize}

      \item A \textit{Label} is a unique identifier for a category.

      \item A \textit{LabeledText} pairs a \textit{Label} with a
      \textit{TextType}.

    \end{itemize}

    \textit{Label} is an abstract data type: it is not directly
    encoded in Python.  The only restrictions on \textit{Labels} are
    that they must be immutable; and they must be comparable for
    equality and inequality.  For example, numbers, strings, tuples,
    and instances can all be used as labels.  Defining \textit{Labels}
    as abstract types provides the user with maximal flexibility in
    their use of the classification module.  The restriction that
    \textit{Labels} be immutable allows them to be used as dictionary
    keys; and ensures that \textit{LabeledTexts} can be used as
    \texttt{Token} types\footnote{\texttt{Token} types are required to
    be immutable.}.  The restriction that \textit{Labels} be
    comparable allows us to differentiate labels.

    The \textit{LabeledText} data type is implemented with the
    \texttt{LabeledText} class, which pairs a \textit{TextType} with
    a \textit{Label}.  The following list summarizes the methods
    provided by the \texttt{LabeledText} class:

    \begin{itemize}

      \item The \textit{LabeledText} constructor builds a
      \texttt{LabeledText} from a given \textit{TextType} and
      \textit{Label}.

      \item The \texttt{text} method returns the
      \texttt{LabeledText}'s \textit{TextType}.

      \item The \texttt{label} method returns the
      \texttt{LabeledText}'s \textit{Label}.

      \item Comparison operators test \texttt{LabeledText}s for
      equality and inequality.

      \item The hash operator allows \texttt{LabeledText}s to be used
      as dictionary keys.

      \item The string representation operator is used to print
      \texttt{LabeledText}s.

    \end{itemize}

    \noindent
    \textit{LabeledTexts} are a considered a sub-type of the abstract
    data type \textit{TextType}.

    We chose not to define a data type to represent categories
    themselves; instead, categories are identified via
    \textit{Labels}.  This decision was based on the fact that we
    could think of no common interface for categories that would
    provide more functionality than labels.  However, it remains
    possible to add a \textit{Category} data type, if it becomes
    apparent that such a data type would be useful.  Since the only
    restriction on \textit{Labels} is that they be immutable, it would
    be possible to use \textit{Labels} containing pointers to their
    categories; these could be used to navigate from a
    \textit{LabeledText} to its \textit{Category}.

  \subsection{Text Classification}

    The text classification module defines the \textit{Classifier}
    interface, which requires that text classifiers implement two
    methods:

    \begin{itemize}

      \item \texttt{labels} returns the list of category labels that
      can be assigned by the \textit{Classifier}.

      \item \texttt{classify} determines which label is most
      appropriate for a given text token, and return a
      \texttt{LabeledText} token constructed from the given text token
      and the chosen label.

    \end{itemize}

    \noindent
    \textit{Classifiers} are also encouraged to implement the
    following methods:

    \begin{itemize}

      \item \texttt{prob} takes a \texttt{LabeledText} token, and
      returns the probability that the text is assigned to the correct
      category.

      \item \texttt{distribution} takes an unlabeled token, and
      returns a probability distribution over \texttt{LabeledText}
      tokens, indicating the likelihood of each potential
      classification decision.

      \item \texttt{distribution\_dictionary} takes an unlabeled token,
      and returns a dictionary mapping from each category's label to
      the probability that the token is a member of that category.

      \item \texttt{distribution\_list} takes an unlabeled token, and
      returns a list specifying the likelihood that it is a member of
      each category.  The $i^{th}$ element of this list corresponds to
      the $i^{th}$ label returned by \texttt{labels}.
    \end{itemize}

    The two required methods were chosen because they represent the
    functionality which we believe that any \textit{Classifier} should be
    capable of providing.  The remaining methods are optional, since
    some classification algorithms cannot generate probability
    estimates.

    The methods \texttt{prob}, \texttt{distribution}, and
    \texttt{distribution\_dictionary} are provided ensure that users
    have convenient access to probability estimates.
    \texttt{distribution\_list} is included to allow efficient access
    to these estimates (e.g., for classifier training algorithms).

    Although the \textit{Classifer} interface defines a large number
    of methods, each \textit{Classifier} does not need to implement
    each method.  Instead, an abstract base class can be used to
    provide default definitions for most methods.  In particular, the
    methods \texttt{labels} and \texttt{distribution\_list} can be used
    to define default definitions for every other method.  These
    methods only need to be implemented directly when desired (e.g, to
    improve efficiency).

    We are currently considering adding another optional method, which
    would take an unlabeled token, and return a corresponding list of
    \texttt{LabeledText} tokens, sorted in descending order of
    likelihood.  This method would be useful for classifiers that can
    decide which categories are more likely than which other
    categories; but which cannot generate probability estimates.  An
    abstract base class could provide a default definition for this
    method, based on \texttt{distribution\_list}.

  \subsection{Classifier Training}

    Typically, \textit{Classifiers} encode specific classifier models;
    but do not include the algorithms for training the classifiers.
    Instead, \textit{ClassifierTrainers} are used to generate
    classifiers from training data.  The \textit{ClassifierTrainer}
    interface requires that classifier trainers implement a single
    method: \texttt{train}.  This method takes a list of
    \texttt{LabeledText} tokens, and returns a new
    \textit{Classifier}.  

    The \texttt{train} method should also accept an optional keyword
    argument ``\texttt{labels},'' which specifies the set of category
    labels that the classifier should use.  When this keyword argument
    is not used, the \textit{ClassifierTrainer} should use the set of
    labels that are attested in the given list of \texttt{LabeledText}
    tokens.  

    \textit{ClassifierTrainers} can use arguments to their
    constructors to provide additional information for the training
    algorithm.  For example, the constructor for the naive Bayes
    classifier trainer expects a feature detector list.
    \textit{ClassifierTrainers} can also define optional keyword
    arguments to the \texttt{train} method to control parameters of
    the training algorithm.  For example, the maximum entropy
    classifier trainers accept keyword arguments that determine how
    long the algorithm should iterate.

    We deliberately kept the \textit{ClassifierTrainer} interface
    simple, to ensure that it can be used to define a wide variety of
    classifier training algorithms.  

  \subsection{Evaluation Metrics}

    The text classification module defines two functions for
    evaluating the performance of a \textit{Classifier}:

    \begin{itemize}

      \item \texttt{accuracy} takes a \textit{Classifier} and a list
      of \texttt{LabeledText} tokens, and returns the percentage of
      the texts which the \textit{Classifier} correctly categorizes.

      \item \texttt{log\_likelihood} takes a \textit{Classifier} and a
      list of \texttt{LabeledText} tokens, and evaluates the per-token
      log likelihood of the list of tokens for the \textit{Classifer}.
      This value $LL(classifier, toks)$ is defined as:

      $$LL(classifier, toks) = \frac{1}{|toks|}\sum_{tok \in toks} 
        \log P_{classifier}(label_{tok}|type_{tok})$$
    \end{itemize}

    We may add more evaluation metrics in the future.

  \subsection{Feature-Based Classifiers}

    Most text classification algorithms do not depend on the specific
    details of the task being performed. By defining a standard
    encoding for the information used to make classification
    decisions, we can allow \textit{Classifiers} to abstract away from
    the details of particular tasks.  This allows us to use the same
    \textit{Classifier} to perform many different classification
    tasks; and to solve the same classification task with many
    different \textit{Classifiers}.

    \subsubsection{Features}

      There are many possible ways of encoding the information that is
      relevant to classification.  We decided to use ``features'' to
      encode this information, since they are simple, flexible, and
      fairly standard.  A \emph{feature} specifies some aspect of a
      \texttt{LabeledText} that is relevant to deciding how likely
      that \texttt{LabeledText} is to occur.  These features can be
      used by classification algorithms to examine how likely
      different labels are for a given text.  Features can be
      expressed as functions mapping from \texttt{LabeledTexts} to
      values.

    \subsubsection{Contexts}

      We considered several alternative encodings for the information
      that is relevant to classification.  One promising possibility
      is sometimes called ``contexts'' or ``aspects.''\footnote{To my
      knowledge, this encoding does not have a standard name.}  A
      \emph{context} specifies some aspect of a \textit{TextType} that
      is relevant to classification.  Contexts can be expressed as
      functions mapping from \textit{TextTypes} to values.

      For many problems, contexts can be expressed in terms of
      features.  In particular, if $c$ is a context function, then for
      some problems we can think of $c$ as shorthand for a set of 
      features $f_1, f_2, \ldots, f_L$ (where $L$ is the set of labels):

      \begin{equation}
        f_i(lt) = \bigg\{ \begin{array}{cl} 
                 c(lt_{text}) & \qquad \textrm{if } lt_{label}=L_i \\
                 0 & \qquad \textrm{otherwise} \end{array}
      \end{equation}

      We decided not to use contexts for our standard information
      encoding, since features are more powerful.  However, we leave
      open the possibility of introducing alternative encodings (such
      as contexts) to the classification module in the future.

    \subsubsection{Feature Data Types}

      The feature sub-module defines three data types:

      \begin{itemize}

        \item A \textit{FeatureID} is a unique identifier for a feature.

        \item A \textit{FeatureValue} is a value that can be produced
        by a feature.

        \item An \textit{Assignment} pairs a \textit{FeatureID} with a
        \textit{FeatureValue}.
      \end{itemize}

      \textit{FeatureIDs} are bounded non-negative integer
      identifiers.  These identifiers are used to indicate which
      feature a \textit{FeatureValue} or \textit{FeatureDetector}
      corresponds to.  The mapping from \textit{FeatureIDs} to
      \textit{FeatureValues} and \textit{FeatureDetectors} is
      specified using \textit{FeatureValueLists} and
      \textit{FeatureDetectorLists}, which are discussed below.

      \textit{FeatureValue} is an abstract data type.  The only
      restrictions that the text classification module places on
      \textit{FeatureValues} are that they must be immutable, and they
      must be comparable for equality and inequality.  However,
      feature-based \textit{Classifiers} usually put additional
      restrictions on the kinds of \textit{FeatueValues} that they
      support.  The most commonly supported kinds of
      \textit{FeatureValue} are booleans and integers.

      \textit{Assignments} pair \textit{FeatureIDs} with
      \textit{FeatureValues}.  They are used to efficiently process
      sparse \textit{FeatureValueLists}.  \textit{Assignments} are
      currently represented as two-element \texttt{tuples}, whose
      first element is a \textit{FeatureID}, and whose second element
      is a \textit{FeatureValue}.  However, we are considering
      implementing them as a class.

    \subsubsection{Feature Interfaces}

      The feature sub-module defines three interfaces:

      \begin{itemize}
        \item A \textit{FeatureDetector} encapsulates a feature.

        \item A \textit{FeatureDetectorList} provides an efficient way
        of grouping feature detectors together, and associating each
        detector with a \textit{FeatureID}.

        \item A \textit{FeatureValueList} provides an efficient way of
        grouping feature values together, and associating each value
        with a \textit{FeatureID}.
      \end{itemize}

      \paragraph{FeatureDetectors}

      \textit{FeatureDetectors} can be used to find the value of a
      feature for a given \texttt{LabeledText}.
      \textit{FeatureDetectors} are required to implement a single
      method: \texttt{detect}.  This method returns the
      \textit{FeatureValue} for a given \texttt{LabeledText}.

      Originally, \textit{FeatureDetectors} were simply called
      \textit{Features}.  However, the bare term ``feature'' can be
      confusing, since it is used in the literature to refer to both
      \textit{FeatureDetectors} and \textit{FeatureValues}.  We
      therefore decided to use the more explicit name
      \textit{FeatureDetector} instead.

      \paragraph{FeatureDetectorLists}

      \textit{FeatureDetectorLists} are data structures that represent
      a set of features.  Abstractly, a \textit{FeatureDetectorList}
      can be thought of as a \texttt{list} of
      \textit{FeatureDetectors}.  The $i^{th}$ element of this list is
      the \textit{FeatureDetector} corresponding to the
      \textit{FeatureID} $i$.  \textit{FeatureDetectorLists} are
      required to implement four methods:

      \begin{itemize}

        \item \texttt{detect} takes a \texttt{LabeledText},
        and returns a \textit{FeatureValueList} specifying the
        \textit{FeatureValue} for each feature.

        \item The length operator returns the number of features
        represented by the \textit{FeatureDetectorList}:

        \item The indexing operator allows
        \textit{FeatureDetectorLists} to be treated as \texttt{lists}
        of \textit{FeatureDetectors}.

        \item The addition operator merges the features defined by two
        \textit{FeatureDetectorLists}.

      \end{itemize}

      \noindent \textit{FeatureDetectorLists} serve three important
      functions:

      \begin{itemize}
        \item They provide a mechanism for grouping
        \textit{FeatureDetectors} together.

        \item They provide a means of associating \textit{FeatureIDs}
        with each \textit{FeatureDetector}.

        \item They allow for efficient implementations for sets of
        related \textit{FeatureDetectors}.
      \end{itemize}

      Although \textit{FeatureDetectorLists} can be abstractly thought
      of as lists of independent \textit{FeatureDetectors}, they are
      not usually implemented that way, for efficiency reasons.  Using
      a complex \textit{FeatureDetectorList} to find the
      \textit{FeatureValues} for a \texttt{LabeledText} allows us to
      examine all features in parallel.  For example, consider a
      bag-of-words feature set for a document.  Implementing this
      feature set with a list of independent \textit{FeatureDetectors}
      would require making one pass through the document for each
      feature.  But if we build a single complex
      \textit{FeatureDetectorList}, we can make one pass through the
      document, checking for all relevant words at the same time.

      In fact, the \textit{FeatureDetector} interface will almost
      never be used in practice; instead, we will always use
      \textit{FeatureDetectorLists}.  The \textit{FeatureDetector}
      interface is kept mainly for pedagogical purposes.

      \paragraph{FeatureValueLists}

      \textit{FeatureValueLists} are data structures that represent a
      set of features.  Abstractly, a \textit{FeatureValueList} can be
      thought of as a \texttt{list} of \textit{FeatureValues}.  The
      $i^{th}$ element of this list is the \textit{FeatureValue}
      corresponding to the \textit{FeatureID} $i$.

      For many classification tasks, \textit{FeatureValueLists} are
      very sparse; in other words, most of the \textit{FeatureValues}
      have some \emph{default value} (usually, zero).

      \textit{FeatureValueLists} are required to implement four
      methods:

      \begin{itemize}

        \item \texttt{assignments} returns a list of the
        \textit{Assignments} for each feature whose
        \textit{FeatureValue} is not the default value.  

        \item \texttt{default} returns the \textit{FeatureValueList}'s
        default value.

        \item The length operator returns the number of features
        represented by the \textit{FeatureValueList}:

        \item The indexing operator allows
        \textit{FeatureValueLists} to be treated as \texttt{lists}
        of \textit{FeatureValues}.

      \end{itemize}

      \noindent
      \textit{FeatureValueLists} serve three important functions:

      \begin{itemize}

        \item They provide a mechanism for grouping
        \textit{FeatureValues} together.

        \item They provide a means of associating \textit{FeatureIDs}
        with each \textit{FeatureValue}.

        \item They allow for efficient encoding of sparse sets of
        \textit{FeatureValues}.

      \end{itemize}

      It is possible to process \textit{FeatureValueLists} by
      accessing each feature individually.  However, for sparse
      \textit{FeatureValueLists}, it can be significantly more
      efficient to process the list of \textit{Assignments} returned
      by the \texttt{assignments} method.  This approach is used by
      both the Naive Bayes classifier and the maximum entropy
      classifier.

      Currently, the default value is an attribute of the
      \textit{FeatureValueList}, and not of the
      \textit{FeatureDetectorList}.  However, we are considering
      adding a default value attribute to
      \textit{FeatureDetectorLists}.  This would prevent a single
      \textit{FeatureDetectorList} from producing different
      \textit{FeatureValueLists} with different default values.  It
      would also allow \textit{Classifiers} and
      \textit{ClassifierTrainers} to perform compatibility checks on
      any \textit{FeatureDetectorLists} that they are given.

    \subsubsection{AbstractFeatureClassifier}

      The feature sub-module defines
      \texttt{AbstractFeatureClassifier}, an abstract base class for
      building feature-driven classifiers.
      \texttt{AbstractFeatureClassifier} provides default definitions
      for all of the \texttt{Classifier} methods.  The only method
      that subclasses need to implement is \texttt{fvlist\_likelihood},
      which returns a float indicating the likelihood of a given
      \textit{FeatureValueList}.  This base significantly simplifies
      the design of feature-based \textit{Classifiers}.  See the
      reference documentation or the tutorial documentation for more
      details.

  \subsection{Feature Selection}

    The feature selection sub-module defines the
    \textit{FeatureSelector} interface, which requires that feature
    selectors define one method: \texttt{select}.  This method takes a
    \textit{FeatureDetectorList}, and decides which features are
    relevant for classification. It returns a new
    \textit{FeatureDetectorList} that includes the
    \textit{FeatureDetectors} for the relevant features.

    \textit{FeatureSelectors} can use arguments to their constructors
    to provide additional information for feature selection, such as a
    set of training data.  \textit{FeatureSelectors} can also define
    optional keyword arguments to the \texttt{select} method, to
    control parameters of feature selection.

    The \texttt{SelectedFDList} class provides
    \textit{FeatureSelectors} with a convenient way of building
    \textit{FeatureDetectorLists} containing the
    \textit{FeatureDetectors} for the relevant features.  A
    \texttt{SelectedFDList} consists of a subset of the feature
    detectors defined by a base \textit{FeatureDetectorList}.  See the
    reference documentation or the tutorial documentation for more
    details.

\section{Classifier Issues}

  This section describes some open issues concerning the text
  classifiers currently implemented by the text classification module.

  \subsection{Smoothing}

    The Naive Bayes classifier (\texttt{NBClassifier}) is
    paramaterized by a single probability distribution, whose samples
    are \textit{FeatureValueLists}.  It uses this probability
    distribution to find the probabilities of individual
    \textit{Assignments}.

    This probability distribution is usually constructed by first
    creating a frequency distribution over \textit{FeatureValueLists};
    and then using that frequency distribution to create a probability
    distribution class, such as an \texttt{MLEProbDist} or a
    \texttt{LaplaceProbDist}.

    However, this does not produce the desired effect.  Most of the
    probability distribution classes are designed to perform smoothing
    over \emph{samples}.  But we really want to be using smoothing
    over \emph{events}.  For example, consider a
    \texttt{LaplaceProbDist}.  This probability distribution uses the
    following formula to smooth frequency estimates:

      $P(s) = \frac{count(s) + 1}{N+B}$

    \noindent Where $s$ is a sample, $N$ is the total number of
    samples, and $B$ is the number of possible outcomes.  Using this
    estimate to find the probability of an event gives:

      $P(e) = \frac{count(e) + |e|}{N+B}$

    \noindent Now, consider the case of a frequency distribution over
    \textit{FeatureValueLists}, with 1,000 binary features.  The
    number of possible outcomes is $2^{1000}$.  As a result, the
    estimated for each assignment will be almost identical.

    NLTK's probability framework needs to be redesigned to allow
    probability distributions to perform smoothing over events, and
    not just over samples.

  \subsection{Zero Probabilities}

    Some classification algorithms begin by calculating un-normalized
    estimates of $P(l|t)$ for each label; and then normalize them by
    dividing by $\sum_lP(l|t)$.  However, for some model settings and
    some inputs, the un-normalized estimates of $P(l|t)$ may be zero
    for each label.  In this case, it is not always clear what the
    \textit{Classifier} should do.  Some alternatives are:

    \begin{itemize}
      \item Return a uniform distribution.

      \item Return a distribution that assigns a probability of zero
      to every label.

      \item ``Cancel'' zeros until at least one un-normalized estimate
      is nonzero.  For example, if the un-normalized estimates are
      $0\times2\times3$, $0\times1.8\times4$, and $0\times0\times3$, then replace these estimates with
      $2\times3$, $1.8\times4$, and $0\times3$.
    \end{itemize}

    For some classifiers, it is apparent which choice is correct.  For
    example, the maximum entropy classifier should return a uniform
    distribution, since this maximizes entropy.  However, for other
    classifiers, the choice is not so clear.

\end{document}