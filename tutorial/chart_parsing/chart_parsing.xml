<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Chart Parsing</title>
    &copyright;
  </articleinfo>

<!--  <section> <title> Introduction </title>

    <para></para>

X.1 introduction: motivation, background, history
    - why is this an interesting problem for CL?

  </section>

    <section> <title> Linguistic Overview </title>

    <para></para>

X.2 linguistic overview (for non-linguist readers)
    - how have linguists addressed the problem?
    - what are the shortcomings of the non-computational approach?

    </section>

    <section> <title> Computational Approaches to Chart Parsing </title>

    <para></para>

X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)

    </section>

    <section> <title> Advanced Topics in Chart Parsing (optional) </title>

    <para></para>

X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions

    </section>

    <section> <title> Chart Parsing in NLTK </title>

X.5 implementation
    - how does NLTK do it?
    - simple problems and worked solutions
    - suggested projects (e.g. for your MSc students)
-->

<!--  <section> <title> Chart Parsing </title>-->
   <section id="intro"> <title> Introduction </title>

    <para>The simple parsers discussed in <ulink
    url="&tutdoc;/parsing">the parsing tutorial</ulink> suffer from
    some significant problems.  The bottom-up shift-reduce parser can
    only find one parse, and it often fails to find a parse even if
    one exits.  The top-down recursive-descent parser can be very
    inefficient, since it often builds and discards the same
    sub-structure many times over; and if the grammar contains
    left-recursive rules, it can enter into an infinite loop. </para>

    <para> These completeness and efficiency problems can be addressed
    by employing a technique called <glossterm>dynamic
    programming</glossterm>, which stores intermediate results, and
    re-uses them when appropriate.  For parsing </para>

    <para>In general, a parser hypothesizes constituents based on the
    grammar and on the current state of its knowledge about the
    constituents which are already attested.  Any given constituent
    may be proposed during the search of a blind alley; there is no
    way of knowing at the time whether the constituent will be used in
    any of the final parses which are reported.  Locally, what we know
    about a constituent is the production which licenses its node and
    immediate daughter nodes (i.e. the "local tree").  Also, in the
    case of chart parsing, we know the whole subtree of this node, how
    it connects to the tokens of the sentence being parsed, and its
    span within the sentence (i.e. location).  In a chart parser,
    these three things: rule, subtree and location, are stored in a
    special structure called an edge.</para>

    <section id="edges"> <title> Edges </title>

      <para> Consider a sequence of word tokens, e.g. <emphasis>the park</emphasis>.
      The space character between the two tokens is a shared boundary.  There are
      further token boundaries at the start and end of the sequence.  We can
      make these boundaries explicit like this:
      <emphasis>&bull; the &bull; park &bull; </emphasis>.
      Now think of these bullets as nodes in a graph.  Each word can be thought
      of as an edge connecting two nodes.  In NLTK we can create edges from tokens
      as follows:</para>

<programlisting>
    <emphasis># import chart parser module: </emphasis>
    &prompt;<command> from nltk.parser.chart import * </command>

    <emphasis># create some token edges: </emphasis>
    &prompt;<command> tokens = [Token('the', Location(0)), Token('park', Location(1))] </command>
    &prompt;<command> edge0 = TokenEdge(tokens[0]) </command>
    &prompt;<command> edge1 = TokenEdge(tokens[1]) </command>

    <emphasis># now create a container for the edges: </emphasis>
    &prompt;<command> chart = Chart(Location(0,2)) </command>
    &prompt;<command> chart.insert(edge0) </command>
    &prompt;<command> chart.insert(edge1) </command>
    &prompt;<command> print chart.pp() </command>
    |[--]  .| 'the'.
    |.  [--]| 'park'.
</programlisting>

      <para>The <literal>Chart.pp()</literal> method produces one line of output
      for each edge.  The <literal>[--]</literal> shows the span (or location) of
      the edge, while the rest of the line shows the token
      (e.g. <literal>'the'</literal>).
      </para>

      <para>Since each of these edges corresponds to exactly one token,
      we call them <literal>TokenEdge</literal>s.  These are the simplest
      type of edge.  However, we can generalize
      the notion of an edge to edges that span more than one token.  Consider again
      our words and boundary markers:
      <emphasis>&bull; the &bull; park &bull; </emphasis>.  In addition to
      the two token edges discussed above, we can have a third edge which connects
      the initial and final boundaries, spanning two tokens.  This edge could
      represent a noun phrase constituent.  The next example illustrates:</para>

<programlisting>
    <emphasis># create a treetoken using the tokens: </emphasis>
    &prompt;<command> tree = TreeToken('NP', tokens[0], tokens[1]) </command>
    ('NP': 'the' 'park')@[0:2]

    <emphasis># get the location of this treetoken: </emphasis>
    &prompt;<command> loc = tree.loc() </command>
    @[0:2]

    <emphasis># create the corresponding grammar production: </emphasis>
    &prompt;<command> prod = CFGProduction(Nonterminal('NP'), 'the', 'park') </command>
    NP -> 'the' 'park'

    <emphasis># now construct the new edge spanning both locations and insert: </emphasis>
    &prompt;<command> edge2 = ProductionEdge(prod, tree, loc, 2) </command>
    &prompt;<command> chart.insert(edge2) </command>
    &prompt;<command> print chart.pp() </command>
    |[--]  .| 'the'.
    |.  [--]| 'park'.
    |[=====]| NP -> 'the' 'park' 
</programlisting>

      <para>Here, the <literal>ProductionEdge</literal> spans both tokens.
      This edge corresponds to the <literal>NP</literal> non-terminal.  In fact,
      it even has a tree structure associated with it:</para>

<programlisting>
    &prompt;<command> edge2.structure() </command>
    ('NP': 'the' 'park')@[0:2]
</programlisting>

      <para>The purpose of an edge is to store a <emphasis>hypothesis</emphasis>
      about the syntactic structure of the sentence.  Such hypotheses may be
      either <emphasis>complete</emphasis> or <emphasis>partial</emphasis>.
      In a <glossterm>complete edge</glossterm>, each terminal or non-terminal
      on the right-hand side of the production has been satisfied; supporting
      evidence for each one exists in the chart.  The above example contains
      a complete edge, represented as <literal>[-----]</literal> or
      <literal>[=====]</literal> (the latter is for complete edges that span the
      entire chart).  The right-hand
      side of its production consists of two terminals, and for each of them there
      is a corresponding edge in the chart.</para>

      <para>In a <glossterm>partial edge</glossterm>, the right-hand
      side of the production is not yet fully satisfied.  An asterisk
      is used to indicate how much of the production has been
      satisfied.  For example, in the partially matched production
      <literal>NP -> 'the' * 'park'</literal>, only the material to
      the left of the asterisk, the word <literal>the</literal>, has
      been matched.  The symbol immediately to the right of the
      asterisk is the next symbol (terminal or non-terminal) to be
      processed for this edge.</para>

<programlisting>
    &prompt;<command> edge2 = ProductionEdge(prod, tree, tree[0].loc(), 1) </command>
    &prompt;<command> chart.insert(edge2) </command>
    &prompt;<command> print chart.pp() </command>
    |[--]  .| 'the'.
    |.  [--]| 'park'.
    |[-->  .| NP -> 'the' * 'park' 
</programlisting>

      <para>The span of the edge, depicted as <literal>[--></literal>
      only corresponds to the material which has been matched.  The
      greater-than symbol indicates that the edge is incomplete.  Note
      that an incomplete edge stores a complete tree corresponding
      to the production; this is the hypothesis it is attempting to verify.
      Observe that the last argument to the <literal>ProductionEdge</literal>
      constructor specifies the position of the asterisk.
      </para>

      <note><para>The reader might expect a complete edge to be displayed
      with the asterisk at the very end, e.g. <literal>NP -> 'the' 'park' *</literal>.
      By convention we omit this final asterisk.</para></note>

      <para> The only remaining logical
      possibility is for the asterisk to be at the start, e.g.
      <literal>NP -> * 'the' 'park'</literal>.  This is a special case of
      an incomplete edge having zero width (i.e. a self-loop which begins and
      ends at the same node).
      Zero-width edges represent the hypothesis that a syntactic constituent
      <emphasis>begins</emphasis> at this location, however no evidence
      for the right-hand side of the production has yet been found.</para>

<programlisting>
    &prompt;<command> edge2 = ProductionEdge(prod, tree, Location(0,0), 0) </command>
    &prompt;<command> chart.insert(edge2) </command>
    &prompt;<command> print chart.pp() </command>
    |[--]  .| 'the'.
    |.  [--]| 'park'.
    |>  .  .| NP -> * 'the' 'park' 
</programlisting>

      <note><para>Note that a <literal>TokenEdge</literal> is a complete edge.
      It has a trivial production, tree and location which are predictable from
      the token itself, hence the simpler constructor.</para></note>

      <para>In the process of chart parsing, edges are combined with other edges to
      form new edges which are then added to the chart.  Nothing is ever
      removed from the chart; nothing is modified once it entered in the chart.
      </para>

    </section> <!-- Edges -->

    <section id="charts"> <title> Charts </title>

      <para>A chart is little more than a set of edges.  The edge set represents
      the state of a chart parser during processing, and new edges can be inserted
      into the set at any time.  Once entered, an edge cannot be modified or
      removed.  The method for adding a new edge is <literal>insert()</literal>.
      Methods for accessing the edges are
      <literal>edgeset()</literal> (returns a <literal>Set</literal>),
      <literal>edges()</literal> (returns a <literal>list</literal>),
      <literal>complete_edges()</literal>,
      <literal>incomplete_edges()</literal>.  The <literal>Chart</literal>
      class supports the <literal>in</literal> operator and the
      <literal>len()</literal> function.</para>

      <para>The chart also stores a location.  This represents the
      combined span of the list of input tokens, and is supplied when
      the chart is initialized.  The chart uses this information in
      order to identify those edges which span the entire
      sentence.  When such an edge exists, and also has the required non-terminal,
      its associated tree is a parse tree for the sentence.  We can access
      all such parses with the <literal>parses()</literal> method:</para>

<programlisting>
    &prompt;<command> chart.parses(Nonterminal('S')) </command>
    []
    &prompt;<command> chart.parses(Nonterminal('NP')) </command>
    [('NP': 'the' 'park')@[0:2]]
</programlisting>

      <para>There are three ways in which edges are added to a chart during
      the parsing process.  The first is to add a <literal>TokenEdge</literal>
      for every token:</para>

<programlisting>
    &prompt;<command> for token in tokens: </command>
    &prompt2;<command>     chart.insert(TokenEdge(token)) </command>
</programlisting>

      <para>The second way in which edges are added requires a production
      and a chart node.  This function, called <literal>self_loop_edge()</literal>
      creates a zero-width edge at the specified location.  It also
      puts an asterisk at the start of the production's right-hand side.</para>

      <para>The third way of adding edges is more complicated.  It takes a
      pair of adjacent edges and creates a new edge spanning them both.
      It only does this if: (i) the left edge is incomplete; (ii) the right
      edge is complete; and (iii) the next
      symbol after the asterisk on the right-hand side of the left edge's production
      matches the symbol on the left-hand side of the right edge's production.
      The function which does this work is called <literal>fr_edge()</literal>.
      This operation is best illustrated by example:</para>

<programlisting>
    <emphasis># get some tokens and create a TokenEdge: </emphasis>
    &prompt;<command> tokens = [Token('the', Location(0)), Token('park', Location(1))] </command>
    &prompt;<command> edge1 = TokenEdge(tokens[1]) </command>

    <emphasis># create a chart and insert the TokenEdge</emphasis>
    &prompt;<command> chart = Chart(Location(0,2)) </command>
    &prompt;<command> chart.insert(edge1) </command>
    &prompt;<command> chart.pp() </command>
    |.  [--]| 'park'. 

    <emphasis># create an incomplete edge and insert</emphasis>
    &prompt;<command> tree = TreeToken('NP', tokens[0]) </command>
    &prompt;<command> prod = CFGProduction(Nonterminal('NP'), 'the', 'park') </command>
    &prompt;<command> edge2 = ProductionEdge(prod, tree, tree[0].loc(), 1) </command>
    &prompt;<command> chart.insert(edge2) </command>
    &prompt;<command> chart.pp() </command>
    |.  [--]| 'park'. 
    |[-->  .| NP -> 'the' * 'park'

    <emphasis># now use these two edges to create a new one:</emphasis>
    &prompt;<command> edge3 = fr_edge(edge2, edge1) </command>
    &prompt;<command> chart.insert(edge3) </command>
    &prompt;<command> chart.pp() </command>
    |.  [--]| 'park'. 
    |[-->  .| NP -> 'the' * 'park' 
    |[=====]| NP -> 'the' 'park' 
</programlisting>

      <para>Observe that the two original edges meet the three requirements:
      (i) the left edge (<literal>edge2</literal>) is incomplete;
      (ii) the right edge (<literal>edge1</literal>) is complete; and
      (iii) the next symbol after the asterisk on the left edge
      (<literal>park</literal>) matches the symbol on the left of
      the right edge's production.</para>

    </section> <!-- Charts -->

    <section id="rules"> <title> Chart Rules </title>

      <para> A chart parser is controlled by a rule-invocation strategy, which determines
      the order in which edges are added to the chart.  In this section we consider the
      various kinds of chart rules which make up these strategies.  Each rule examines
      the productions in the grammar and the current edges in the chart, and adds
      more edges.  All strategies begin by loading the tokens into the chart,
      creating a <literal>TokenEdge</literal> for every token (using the first
      edge-addition method described in the last section).  This work is done
      when the chart parser is created.</para>

    <section id="rules.bu"> <title> Bottom Up Rule </title>

      <para>A bottom up parser builds syntactic structure starting
      from the words in the input.  For each constituent constructed
      so far, it must consider which grammar rules could be applied.
      For each complete edge, it gets the symbol on the left-hand side
      of that edge's production, then finds all productions in the
      grammar where that symbol appears as the first symbol on the right.
      For example, if there is an existing complete edge with the
      production <literal>VP -> V NP</literal> then it identifies the
      left-hand side (<literal>VP</literal>) then searches for productions
      in the grammar of the form <literal>X -> VP ...</literal>.  All
      such productions are then inserted into the chart as zero-width
      edges.</para>

<programlisting>
    <emphasis># For each complete edge:</emphasis>
    &prompt;<command> for edge in chart.complete_edges():</command>

    <emphasis>        # For each production in the grammar: </emphasis>
    &prompt2;<command>     for production in grammar.productions(): </command>
    <emphasis>            # Does the edge LHS match the production RHS? </emphasis>
    &prompt2;<command>         if edge.lhs() == production.rhs()[0]: </command>
    &prompt2;<command>             loc = edge.loc().start_loc() </command>
    &prompt2;<command>             chart.insert(self_loop_edge(production, loc))</command>
</programlisting>

      <para>Now we consider the action of this rule in the context of
      a simple grammar.  We initialize the chart with the tokens then,
      for each <literal>TokenEdge</literal>, find a production in the grammar and add
      a zero-width <literal>ProductionEdge</literal>.</para>

<programlisting>
    &prompt;<command> from nltk.parser.chart import * </command>
    &prompt;<command> from nltk.tokenizer import * </command>
    &prompt;<command> from nltk.cfg import * </command>

    <emphasis># Define a grammar</emphasis>
    &prompt;<command> S, VP, NP, PP = nonterminals('S, VP, NP, PP') </command>
    &prompt;<command> V, N, P, Name, Det = nonterminals('V, N, P, Name, Det') </command>

    &prompt;<command> productions = [ </command>
    &prompt2;<command>     CFGProduction(NP, 'I'),    CFGProduction(V, 'saw'), </command>
    &prompt2;<command>     CFGProduction(Det, 'the'), CFGProduction(Det, 'my'), </command>
    &prompt2;<command>     CFGProduction(N, 'dog'),   CFGProduction(N, 'cookie'), </command>
    &prompt2;<command>     CFGProduction(P, 'with'), </command>
    &prompt2;<command>     CFGProduction(S, NP, VP),  CFGProduction(PP, P, NP), </command>
    &prompt2;<command>     CFGProduction(NP, Det, N), CFGProduction(NP, NP, PP), </command>
    &prompt2;<command>     CFGProduction(VP, VP, PP), CFGProduction(VP, V, NP), </command>
    &prompt2;<command>     CFGProduction(VP, V), </command>
    &prompt2;<command>     ] </command>
    &prompt;<command> grammar = CFG(S, productions) </command>

    <emphasis># Tokenize a sentence</emphasis>
    &prompt;<command> sentence = 'I saw the dog with my cookie' </command>
    &prompt;<command> from nltk.tokenizer import WSTokenizer </command>
    &prompt;<command> tokens = WSTokenizer().tokenize(sentence) </command>

    <emphasis># Create a chart parser which only has the bottom-up rule</emphasis>
    &prompt;<command> cp = ChartParser(grammar, [BottomUpRule()], trace=2) </command>
    &prompt;<command> parses = cp.parse_n(tokens) </command>
    Lexical Insertion    |[--]  .  .  .  .  .  .| 'I'. 
    Lexical Insertion    |.  [--]  .  .  .  .  .| 'saw'. 
    Lexical Insertion    |.  .  [--]  .  .  .  .| 'the'. 
    Lexical Insertion    |.  .  .  [--]  .  .  .| 'dog'. 
    Lexical Insertion    |.  .  .  .  [--]  .  .| 'with'. 
    Lexical Insertion    |.  .  .  .  .  [--]  .| 'my'. 
    Lexical Insertion    |.  .  .  .  .  .  [--]| 'cookie'. 
    Bottom Up Rule       |.  .  .  .  .  .  >  .| N -> * 'cookie' 
    Bottom Up Rule       |.  .  .  .  .  >  .  .| Det -> * 'my' 
    Bottom Up Rule       |.  .  .  .  >  .  .  .| P -> * 'with' 
    Bottom Up Rule       |.  .  .  >  .  .  .  .| N -> * 'dog' 
    Bottom Up Rule       |.  .  >  .  .  .  .  .| Det -> * 'the' 
    Bottom Up Rule       |.  >  .  .  .  .  .  .| V -> * 'saw' 
    Bottom Up Rule       |>  .  .  .  .  .  .  .| NP -> * 'I' 
    Found 0 parses with 14 edges
</programlisting>

    <para>Observe that the new zero-width edges have productions with
    an initial asterisk.  This is as far as the bottom-up rule can go
    without the help of the fundamental rule, discussed below.</para>

    </section> <!-- Bottom Up Rule -->

    <section id="rules.td"> <title> Top Down Initialization </title>

      <para>Top down parsing is initialized by creating zero-width
      edges at the leftmost position in the chart.
      For every grammar rule whose left-hand side is the base category
      of the grammar, we create the corresponding dotted rule with the
      dot position at the start of the right-hand side.</para>

<programlisting>
    <emphasis># for each production in the grammar: </emphasis>
    &prompt;<command> for production in grammar.productions(): </command>
    <emphasis>    # does the production expand the start-symbol of the grammar? </emphasis>
    &prompt2;<command>     if production.lhs() == grammar.start():</command>
    &prompt2;<command>         loc = chart.loc().start_loc()</command>
    &prompt2;<command>         chart.insert(self_loop_edge(production, loc))</command>
</programlisting>
      
    <para>We can apply this rule to the above example.  The start symbol
    of the grammar is <literal>S</literal> and there is only one production
    having <literal>S</literal> on its left-hand side, namely
    <literal>S -> NP VP</literal>.  The <literal>TopDownInitRule()</literal>
    inserts a zero-width edge with this production, at the very left of
    the chart:</para>

<programlisting>
    &prompt;<command>cp = ChartParser(grammar, [TopDownInitRule()], trace=2)</command>
    &prompt;<command>parses = cp.parse_n(tokens)</command>
    Lexical Insertion    |[--]  .  .  .  .  .  .| 'I'. 
    Lexical Insertion    |.  [--]  .  .  .  .  .| 'saw'. 
    Lexical Insertion    |.  .  [--]  .  .  .  .| 'the'. 
    Lexical Insertion    |.  .  .  [--]  .  .  .| 'dog'. 
    Lexical Insertion    |.  .  .  .  [--]  .  .| 'with'. 
    Lexical Insertion    |.  .  .  .  .  [--]  .| 'my'. 
    Lexical Insertion    |.  .  .  .  .  .  [--]| 'cookie'. 
    Top Down Init Rule   |>  .  .  .  .  .  .  .| S -> * NP VP 
    Found 0 parses with 8 edges
</programlisting>

    </section> <!-- Top Down Initialization -->

    <section id="rules.td2"> <title> Top Down Rule </title>

      <para>Whenever a chart contains an incomplete edge, with an incomplete
      rule having <replaceable>X</replaceable> as the next symbol (to the right
      of the asterisk), we know that
      the parser is expecting to find an <replaceable>X</replaceable>
      constituent immediately to the right.  The top-down 
      rule looks for all incomplete edges expecting an
      <replaceable>X</replaceable> and, for each production having
      <replaceable>X</replaceable> on its left-hand side, creates a zero-width
      edge containing this production on the right of the incomplete edge.
      This expresses the top-down prediction that the hypothesized constituent
      exists, and evidence for it should be sought immediately to the right.
      </para>

<programlisting>
    <emphasis># for each production in the grammar: </emphasis>
    &prompt;<command> for production in grammar.productions(): </command>
    <emphasis>    # for each incomplete edge in the chart: </emphasis>
    &prompt2;<command>     for edge in chart.incomplete_edges():</command>
    <emphasis>        # does the expected constituent match the production? </emphasis>
    &prompt2;<command>         if edge.next() == production.lhs():</command>
    &prompt2;<command>             loc = edge.loc().end_loc()</command>
    &prompt2;<command>             chart.insert(self_loop_edge(production, loc))</command>
</programlisting>

    <para>As before, we apply this rule to our running example:</para>

<programlisting>
    &prompt;<command>cp = ChartParser(grammar, [TopDownInitRule(),TopDownRule()], trace=2)</command>
    &prompt;<command>parses = cp.parse_n(tokens)</command>
    Lexical Insertion    |[--]  .  .  .  .  .  .| 'I'. 
    Lexical Insertion    |.  [--]  .  .  .  .  .| 'saw'. 
    Lexical Insertion    |.  .  [--]  .  .  .  .| 'the'. 
    Lexical Insertion    |.  .  .  [--]  .  .  .| 'dog'. 
    Lexical Insertion    |.  .  .  .  [--]  .  .| 'with'. 
    Lexical Insertion    |.  .  .  .  .  [--]  .| 'my'. 
    Lexical Insertion    |.  .  .  .  .  .  [--]| 'cookie'. 
    Top Down Init Rule   |>  .  .  .  .  .  .  .| S -> * NP VP 
    Top Down Rule        |>  .  .  .  .  .  .  .| NP -> * 'I' 
    Top Down Rule        |>  .  .  .  .  .  .  .| NP -> * Det N 
    Top Down Rule        |>  .  .  .  .  .  .  .| NP -> * NP PP 
    Top Down Rule        |>  .  .  .  .  .  .  .| Det -> * 'the' 
    Top Down Rule        |>  .  .  .  .  .  .  .| Det -> * 'my' 
    Found 0 parses with 13 edges
</programlisting>

    </section> <!-- The Top Down Edge-Triggered Rule -->

      <section id="rules.fr"> <title> Fundamental Rule </title>

      <para>In the last section we encountered a rule which combines the
      information from two edges and creates a new edge.
      This method of combining edges is known as the
      <emphasis>Fundamental Rule</emphasis>.  It is so important that we
      repeat the definition, using more formal notation.  Suppose that an edge
        <replaceable>e<subscript>1</subscript></replaceable> has a dotted
        rule whose next symbol is <replaceable>X</replaceable>.  Suppose that
        a second edge <replaceable>e<subscript>2</subscript></replaceable>,
        immediately to the right of
        <replaceable>e<subscript>1</subscript></replaceable>,
        represents a complete <replaceable>X</replaceable> constituent.
        Then the Fundamental Rule states that we must add a new edge
        <replaceable>e<subscript>3</subscript></replaceable> spanning both
        <replaceable>e<subscript>1</subscript></replaceable> and
        <replaceable>e<subscript>2</subscript></replaceable>, in which
        the dot is moved one position to the right.  In other words,
        <replaceable>e<subscript>1</subscript></replaceable> was looking for
        an <replaceable>X</replaceable> to its right, which it found on
        <replaceable>e<subscript>2</subscript></replaceable>, and we record
        this fact on <replaceable>e<subscript>3</subscript></replaceable>.
        This rule is applied as many times as possible -- to all pairs of
        adjacent edges which match the above criteria.
        </para>

    </section> <!-- Fundamental Rule -->

    </section> <!-- Chart Rules -->

    <section id="strategies"> <title> Chart Parser Strategies </title>

      <para>Chart parsers use the various rules described above in order
      to emulate top down, bottom up or hybrid parsers.  A policy about
      what chart rule should be applied under what circumstances is called a
      <emphasis>rule-invocation stratagy</emphasis>.  Here are the definitions
      of the two most common strategies:</para>

<programlisting>
    &prompt;<command> TD_STRATEGY = [TopDownRule(), FundamentalRule(), TopDownInitRule()]</command>
    &prompt;<command> BU_STRATEGY = [BottomUpRule(), FundamentalRule()]</command>
</programlisting>

      <para>A chart parser is initialized with a parsing strategy.
      During the parsing process, it repeatedly applies the rules of
      the strategy until no new edges are added.</para>

      <note><para>It may seem strange that we have created complex chart
      parsing machinery only to redefine the top-down and bottom-up parsers.
      However, these versions are much more efficient, since they cache
      intermediate parse results in the chart.</para></note>

    </section> <!-- Parsing Strategies -->

    <section id="using"> <title> Chart Parsers </title>

      <para>Given these complete strategies, we can now parse the
      input sentence and generate parse trees.  The parser finds
      two trees, owing to the syntactic ambiguity in the prepositional
      phrase attachment.</para>

<programlisting>
    &prompt;<command> ChartParser(grammar, TD_STRATEGY).parse_n(tokens) </command>
    [
        ('S':
            ('NP': 'I')
            ('VP':
                ('VP':
                    ('V': 'saw')
                    ('NP': ('Det': 'the') ('N': 'dog')))
                ('PP':
                    ('P': 'with')
                    ('NP': ('Det': 'my') ('N': 'cookie')))))@[0w:7w],
        ('S':
            ('NP': 'I')
            ('VP':
                ('V': 'saw')
                ('NP':
                    ('NP': ('Det': 'the') ('N': 'dog'))
                    ('PP': 
                        ('P': 'with')
                        ('NP': ('Det': 'my') ('N': 'cookie'))))))@[0w:7w]
    ]
</programlisting>

      <para>----OLD CONTENT FOLLOWS----</para>

      <para>To see the corresponding chart, use the function
      <literal>Chart.chart().draw()</literal>.</para>

      <note><para>Note that some edges
      are duplicated because they contain different trees.  In order
      for a chart parser to have polynomial complexity the trees
      corresponding to an edge must be stored in a distributed way...
      </para></note>

    </section> <!-- Creating and Using Chart Parsers -->

    <section id="demo"> <title> The Tk Interface </title>

      <note><para>To be written.</para></note>

      <para>Please see the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.draw.chart.html">nltk.draw.chart</ulink>
      module.</para>

    </section> <!-- The Tk Interface -->

</section> <!-- Intro -->
<!-- </section>  Chart Parsing in NLTK -->

  &index;
</article>
