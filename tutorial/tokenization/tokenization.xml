<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <title>Elementary Language Processing: Tokenizing Text and Classifying Words</title>
    &versiondate; &copyright;
  </articleinfo>

<!------------------------------------------------------------------------>

<section id="intro"><title>Introduction</title>

<para>
Texts are usually represented in a computer as files containing a
potentially long sequence of characters.  For most kinds of linguistic
processing, we need to identify and categorize the words of the text.
This turns out to be a non-trivial task.  In this chapter we introduce
<emphasis>tokens</emphasis> as the building blocks of text, and show
how texts can be <emphasis>tokenized</emphasis>.  Next we consider
the categorization of tokens according to their parts-of-speech, and
do some preliminary exploration of the Brown Corpus, a collection of
over a million words of tagged English text.  Along the way we take a
look at some interesting applications: generating random text,
classifying words automatically, and analyzing the modal verbs of
different genres.
</para>

</section> <!-- Introduction -->

<!------------------------------------------------------------------------>

<section id="token"><title>Tokens: the building blocks of text</title>

<para> How do we know that piece of text is a
<glossterm>word</glossterm>, and how do we represent words and
associated information in a machine?  It might seem needlessly picky
to ask what a word is. Can't we just say that
a word is a string of characters which has white space
before and after it?
However, it turns out that things are quite a bit
more complex. To get a flavour of the problems, consider the following
text from the Wall Street Journal:
<example id="wsj_0034">
<title>Paragraph 12 from <filename>wsj_0034</filename></title>
<literallayout><literal>
It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

"I'm very alarmed to see these rich valuations," says Smith Barney's
Mr. Porter.
</literal></literallayout>
</example>
</para>

<para>
Let's start with the string <literal>aren't</literal>. According to
our naive definition, it counts as only one word. But consider a
situation where we wanted to check whether all the words in our text
occurred in a dictionary, and our dictionary had entries for <literal
lang="en">are</literal> and <literal lang="en">not</literal>, but not
for <literal lang="en">aren't</literal>.  In this case, we would
probably be happy to say that <literal>aren't</literal> is a contraction
of two distinct words. 

<!--We can make a similar point about
<literal>1992's</literal>. We might want to run a small program over our
text to extract all words which express dates. In this case, we would
get achieve more generality by first stripping oexcept in this case, we
would not expect to find <literal>1992</literal> in a dictionary.--> 

If we take our naive definition of word literally (as we should, if we are
thinking of implementing it in code), then there are some other minor
but real problems. For example, assuming our file consists of a number
of separate lines, as indicated in <xref linkend="wsj_0034"/>, then all
the words which come at the beginning of a line will fail to be preceded
by whitespace (unless we treat the newline character as a
whitespace). Second, according to our criterion, punctuation symbols
will form part of words; that is, a string like
<literal>investors,</literal> will also count as a word, since there is
no whitespace between <literal>investors</literal> and the following
comma. Consequently, we run the risk of failing to recognise that 
<literal>investors,</literal> (with appended comma) is a token of the
same type as <literal>investors</literal> (without appended comma. More
importantly, we would like punctuation to be a <quote>first-class
citizen</quote> for tokenization and subsequent processing. For example,
we might want to implement a rule which says that a word followed by a period is likely to
be an abbreviation if the immediately following word has a lowercase
initial. However, to formulate such a rule, we must be able to identify
a period as a token in its own right.


</para>

<para>
A slightly different challenge is raised by examples such as the
following (drawn from the MedLine [ref] corpus):
<orderedlist>
<listitem>
<para>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.
</para>
</listitem>
<listitem>
<para>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.
</para>
</listitem>
</orderedlist>
In these cases, we encounter terms which are unlikely to be found in any
general purpose English lexicon. Moreover, we will have no success in
trying to syntactically analyse these strings using a standard grammar
of English. Now for some applications, we would like to <quote>bundle
up</quote> expressions such as
<literal>alpha-galactosyl-1,4-beta-galactosyl-specific adhesin</literal>
and <literal>4.53 +/- 0.15%</literal> so that they are presented as
unanalysable atoms to the parser. That is, we want to treat them as
single <quote>words</quote> for the purposes of subsequent processing.
The upshot is that, even if we confine our attention to English
text, the question of what we treat as word may depend a great deal on
what our purposes are.</para>

<note><para>
If we turn to languages other than English,
segmenting words can be even more of a challenge. For example, in
Chinese orthography, characters correspond to monosyllabic
morphemes. Many morphemes are words in their own right, but many words
contain more than one morpheme; most of them consist of two
morphemes. However, there is no visual representation of word boundaries
in Chinese text.
</para></note>

<para>
Let's look in more detail at the words in <xref linkend="wsj_0034"/>.
Suppose we use white space as the delimiter for words, and then list
all the words of the text in alphabetical order; we would expect to
get something like the following:
<informalexample>
<programlisting>
120, 1992, And, Barney, But, European, European, Fund, I, It, It, Japanese, Korea, 
Mr, Porter, Smith, South, Spain, a, a, a, ...  
</programlisting>
</informalexample>

<!--
Words according to the Unix <command>wc</command>:
<programlisting><![CDATA[
wc -w wsj_0034_pars12-13
    90 wsj_0034_pars12-13
]]></programlisting>
-->
Now, if we ask a program utility to tell us how many words there in the
text, it will probably return the answer: 90.
<indexterm><primary>word token</primary></indexterm>
<indexterm><primary>word type</primary></indexterm>
This calculation depends on treating each of the three occurrences of
<literal>a</literal> as a separate word. Yet what do we mean by saying
that is some object <literal>a</literal> which occurs three times? Are
there three words <literal>a</literal> or just one? We can in fact
answer "Both" if we draw a distinction between a word
<emphasis>token</emphasis> versus a word <emphasis>type</emphasis>.
A word type is
somewhat abstract; it's what we're talking about when we say that we
know the meaning of the word <literal>deprecate</literal>, or when we
say that the words <literal>barf</literal> and <literal>vomit</literal>
are synonyms. On the other hand, a word token is something which exists in
time and space. For example, we could talk about my uttering a token of
the word <literal>grunge</literal> in Edinburgh on July 14, 2003;
equally, we can say that the second word token in <xref
linkend="wsj_0034"/> is a token of the word type
<literal>probably</literal>, or that there are two tokens of the type
<literal>European</literal> in the text.
More generally, we want to say that there are 90 word tokens in <xref
linkend="wsj_0034"/>, but only 76 word types. 
</para>

<para> The terms <glossterm>token</glossterm> and
<glossterm>type</glossterm> can also be applied to other linguistic
entities.  For example, a <glossterm>sentence token</glossterm> is an
individual occurrence of a sentence; but a <glossterm>sentence
type</glossterm> is an abstract sentence, without context.  If someone
repeats a sentence twice, they have uttered two sentence tokens, but
only one sentence type.  When the kind of token or type is obvious from
context, we will simply use the terms <glossterm>token</glossterm> and
<glossterm>type</glossterm>.
  </para>

<!-- SUBSECTION: Strings -->
<section id="token.representing">
<title>Representing tokens</title>

<para> When written language is stored in a computer file it is normally
represented as a sequence or <glossterm>string</glossterm> of
characters.  That is, in a standard text file, individual words are
strings, sentences are strings, and indeed the whole text is one long
string. The characters in a string don't have to be just the ordinary
alphanumerics; strings can also include special characters which
represent space, tab and newline.</para>

<para> Most computational processing is performed above the level of
characters.  In compiling a programming language, for example, the
compiler expects its input to be a sequence of
<glossterm>tokens</glossterm> that it knows how to deal with; for
example, the classes of identifiers, string constants and numerals.
Analogously, a parser will expect its input to be a sequence of word
tokens rather than a sequence of individual characters.  At its
simplest, then, <glossterm>tokenization</glossterm> of a text involves
searching for locations in the string of characters containing
whitespace (space, tab, or newline) or certain punctuation symbols, and
breaking the string into word tokens at these points.
For example, suppose we have a file containing the following two lines:
</para>
<programlisting>
<![CDATA[
The cat climbed
the tree.
]]>
</programlisting>
<para>From the parser's point of view, this file is just a string of
characters:</para>
<programlisting>
'The&blank;cat&blank;climbed\n&blank;the&blank;tree.'
</programlisting>
<para>Note that we use single quotes to delimit strings,
"&blank;" to represent space and "\n" to represent newline.</para>

<para>
As we just pointed out, to tokenize this text for consumption by the
parser, we need to explicitly indicate which substrings are words. One
convenient way to do this in Python is to split the string into a
<emphasis>list</emphasis> of words, where each word is a string, such
as <literal>'dog'</literal>.<footnote>
<para>We say <quote>convenient</quote> because Python makes it easy to
iterate through a list, processing the items one by one.</para>
</footnote>

In Python, lists are printed as a series of
objects (in this case, strings), surrounded by square brackets and
separated by commas:</para>

<programlisting><![CDATA[
>>> words = ['the', 'cat', 'climbed', 'the', 'tree']
>>> words
['the', 'cat', 'climbed', 'the', 'tree']
]]></programlisting>

<para>
Notice that we have introduced a new variable
<literal>words</literal> which is bound to the list, and that we entered
the variable on a new line to check its value. In future, we shall skip
this last step in our example program listings, and simply give the
result of evaluation, as follows:
 </para>
<programlisting><![CDATA[
>>> words = ['the', 'cat', 'climbed', 'the', 'tree']
['the', 'cat', 'climbed', 'the', 'tree']
]]></programlisting>


<para>To summarize, we have just illustrated how, at its simplest,
tokenization of a text can be carried out by converting the single string
representing the text into a list of strings, each of which corresponds
to a word.</para>



<para>However, there
are a couple of problems with the naive view of treating a word token as
just a string of characters.  First, it makes it difficult to associate
other information (such as a part-of-speech tag) with a word
token. Second, it doesn't give us any way of distinguishing between word
types and word tokens.  We will now show how both of these problems can
be remedied.</para>

<para>
In NLTK, we usually represent word tokens as members of the 

<ulink url="&refdoc;/nltk.token.Token-class.html"><literal>Token</literal></ulink>
class.
<literal>Token</literal>s can be constructed from word strings
using the <ulink url="&refdoc;/nltk.token.Token-class.html#__init__">
<literal>Token</literal> constructor</ulink>, which is defined in the
<ulink url="&refdoc;/nltk.token-module.html"><literal>nltk.token</literal></ulink>
module.  Here is a simple example of the use of this constructor:
</para>

<programlisting><![CDATA[
>>> from nltk.token import * 
>>> my_word_type = 'dog' 
'dog'
>>> first_word_token = Token(TEXT=my_word_type) 
<dog>
>>> second_word_token = Token(TEXT='cat')
<cat>
]]></programlisting>

<para> For the linguistically-oriented reader who is familiar with
feature-based grammars, it may be helpful to think of a
<literal>Token</literal> as a kind of attribute-value structure. That
is, the expression <literal>Token(TEXT='cat')</literal> tells us that we
have built a <literal>Token</literal> object which has a
<literal>TEXT</literal> attribute, and that the value of this attribute
is the string <literal>'cat'</literal>. Intuitively, we think of the
<literal>TEXT</literal> attribute as specifying the type of the token.
For the computationally-oriented reader, a <literal>Token</literal> can
be thought of as a kind of Python dictionary; see [XREF]. The advantage of
treating tokens in this way is that we can easily associate a variety of
additional properties with a <literal>Token</literal> &mdash; the choice
of attributes will be determined by the needs of a particular
application. For example, we might want to specify attributes for a
part-of-speech tag or for a semantic reference:
</para>

<programlisting><![CDATA[
>>> my_word_token['TAG'] = 'Noun' 
'Noun'
>>> my_word_token['REFERENT'] = 'entity123' 
'entity123'
]]></programlisting>

<para>
However, none of this permits us to distinguish two instances of the
same word type.
For that, we use <emphasis>locations</emphasis>, which we turn to next.
</para>
</section><!--Representing Tokens-->

<section id="token.locations">
<title> Locations </title>

    <para> <glossterm>Index locations</glossterm> specify offsets into
    a text.  The location of the fourth word of a text would be
    constructed as follows:</para>

<programlisting><![CDATA[
>>> from nltk.token import * 
>>> WordIndexLocation(3) 
[3w]
]]></programlisting>

<para> <glossterm>Span locations</glossterm> specify regions of
      texts, using a <glossterm>start index</glossterm> and an
      <glossterm>end index</glossterm>.  A location in a stream of
      text starting at character
      <replaceable>s</replaceable> and ending at character
      <replaceable>e</replaceable> is written
      <literal>@[</literal><replaceable>s</replaceable><literal>:</literal><replaceable>e</replaceable><literal>c]</literal>.
      This location specifies the region of the text beginning at
      <replaceable>s</replaceable>, and including everything up to
      (but not including) <replaceable>e</replaceable>.
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html"
      ><literal>CharSpanLocations</literal></ulink> are created using the
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html#__init__"
      ><literal>CharSpanLocation</literal> constructor</ulink>, which is
      defined in the <ulink url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module:</para>

<programlisting><![CDATA[
>>> from nltk.token import * 
>>> my_loc = CharSpanLocation(1, 5) 
[1:5c]
]]></programlisting>

<para> Note that a text location does <emphasis>not</emphasis> include
the text at its end location.  This convention may seem unintuitive at
first, but it has a number of advantages.  First, it is consistent with
Python's slice notation (e.g., <literal>x[1:3]</literal> specifies
elements 1 and 2 of <literal>x</literal>).

<footnote><para>Unlike Python slices, text locations do
<emphasis>not</emphasis> support negative indexes.</para></footnote> 

Second, it allows text locations to specify points <emphasis>between</emphasis> tokens,
instead of just ranges.  For example, <literal>Location(3,3)</literal> specifies the
point just before the text at index 3.  Finally, it simplifies arithmetic on
indices; for example, the length of <literal>Location(5,10)</literal> is
<literal>10-5</literal>, and two locations are contiguous if the start
of one equals the end of the other. </para>

<para>A text location may be tagged with a
<glossterm>source</glossterm>, which gives an indication of where the
text was derived from.  By default, the location's source is
unspecified.  A typical example of a source would be the name of the
file from which the element of text was read.<footnote><para> The
location indexes are zero-based, so the first sentence starts at an
index of zero, not one.</para></footnote>
</para>

<programlisting><![CDATA[
>>> my_loc = CharSpanLocation(1, 5, 'foo.txt') 
[1:5c]
>>> my_loc.srource()
'foo.txt'
]]></programlisting>

<para> As discussed above, a text token represents a single
      occurrence of a piece of text.  In NLTK, a token is defined by a
      type, together with a location at which that type occurs.  A
      token with text <replaceable>t</replaceable> and location
      <literal>@[<replaceable>l</replaceable>]</literal> can be
      written as
      <literal>&lt;<replaceable>t</replaceable>>@[<replaceable>l</replaceable>]</literal>.
      This location information can be used to distinguish two instances of
      the same text.
      Tokens are constructed with the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal> constructor</ulink>: </para>

<programlisting><![CDATA[
>>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
<hello>@[0,5c]
>>> token2 = Token(TEXT='world', LOC=CharSpanLocation(6, 11)) 
<world>@[6,11c]
]]></programlisting>

<para> Two tokens are only equal if they are equal on all their attributes,
in this case, for the attributes <literal>text</literal> and <literal>loc</literal>.
</para>

<programlisting><![CDATA[
>>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
<hello>@[0,5c]
>>> token2 = Token(TEXT='hello', LOC=CharSpanLocation(6, 11)) 
<hello>@[6,11c]
>>> token1 == token2 
False
>>> token1['TEXT'] == token2['TEXT'] 
True
]]></programlisting>

<note>
<para> When a token's location is unknown or unimportant, it may be omitted.
However, the distinction between a word token and a word type is lost in this
case.
</para>
</note>

<note>
<para> A location's start index, end index, and source, can be accessed
  using the <literal>start()</literal>, <literal>end()</literal>, and
  <literal>source()</literal> member functions:
<programlisting><![CDATA[
>>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5, 'corpus.txt')) 
>>> loc1 = token1['LOC']
[0:5c]
>>> loc1.start()
0
>>> loc1.end()
5
>>> loc1.source()
'corpus.txt'
]]></programlisting>
</para>
</note>

</section> <!-- Locations -->

</section>  <!-- Tokens -->

<!------------------------------------------------------------------------>

<section> <title> Tokenization </title>

<para> Many natural language processing tasks involve analyzing
    texts of varying sizes, ranging from single sentences to very
    large corpora.  There are a number of ways to represent texts
    using NLTK.  The simplest is as a single
    <literal>string</literal>.  These strings can be loaded directly
    from files: </para>

<programlisting><![CDATA[
>>> text_str = open('corpus.txt').read() 
'Hello world.  This is a test file.\n'
]]></programlisting>

<para> However, as we noted in <xref linkend="token.representing"/>, it
is usually preferrable to represent a text as a
    <literal>list</literal> of <literal>Token</literal>s.  These
    <literal>list</literal>s are typically created using a
    <glossterm>tokenizer</glossterm>, such as <ulink
    url="&refdoc;/nltk.tokenizer.WhitespaceTokenizer-class.html"
    ><literal>WhitespaceTokenizer</literal></ulink> (which splits
    strings into words at 
    whitespaces): </para>

<programlisting><![CDATA[
>>> from nltk.tokenizer import *
>>> text_token = Token(TEXT='Hello world.  This is a test file.')
<Hello world.  This is a test file.>
>>> WhitespaceTokenizer(SUBTOKENS='WORDS').tokenize(text_token)
>>> print text_token 
<[<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]>
]]></programlisting>

<note>
<para>By <quote>whitespce</quote>, we mean not only interword space, but
also tab and line-end.</para>
</note>
<para> Observe that the <literal>text_token</literal> initially
contains a single token for the entire text.  We further tokenize this
into its component word tokens, and the result is stored as an
attribute of the original token.  That is, the original source token and the
resulting tokens are all stored together, as the following code
illustrates:<footnote><para>The default method for printing a token
containing subtokens is just to print the subtokens.</para></footnote>
</para>

<programlisting><![CDATA[
>>> print text_token['TEXT'] 
<Hello world.  This is a test file.>
>>> print text_token['WORDS'] 
[<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]
]]></programlisting>

<para> If we want all tokens to contain location data, then we
specify the <literal>add_locs</literal> option:</para>

<programlisting><![CDATA[
>>> WhitespaceTokenizer(SUBTOKENS='WORDS').tokenize(text_token, add_locs=True) 
>>> print text_token 
<[<Hello>@[0:5c], <world.>@[6:12c], <This>@[14:18c], <is>@[19:21c], <a>@[22:23c],
<test>@[24:28c], <file.>@[29:34c]]>
]]></programlisting>

<note>
<para> Tokenization may normalize the text, mapping all words to
lowercase, expanding contractions, and possibly even stemming the
words.  An example for stemming is shown below:
<programlisting><![CDATA[
>>> text_token = Token(TEXT='stemming can be fun and exciting') 
>>> WhitespaceTokenizer(SUBTOKENS='WORDS').tokenize(text_token) 
>>> from nltk.stemmer.porter import * 
>>> stemmer = PorterStemmer() 
>>> for word_token in text_token['WORDS']: 
...     stemmer.stem(word_token)
>>> print text_token 
<[<TEXT='stemming', STEM='stem'>, <TEXT='can', STEM='can'>, <TEXT='be', STEM='be'>,
<TEXT='fun', STEM='fun'>, <TEXT='and', STEM='and'>, <TEXT='exciting', STEM='excit'>]>
]]></programlisting>
</para>
</note>

<para>We noted in <xref linkend="intro"/>that tokenization based
on whitespace is too simplistic for most applications; for instance, it
fails to separate the last word of a phrase or sentence from punctuation
characters, such as comma, period, exclamation mark and question mark.
As its name suggests, <literal>RegexpTokenizer</literal> employs a
regular expression to determine how text should be split up.  This
regular expression specifies the characters that can be included in a
valid word.  To define a tokenizer that includes punctuation as separate
tokens, we could use:</para>

<programlisting><![CDATA[
>>> regexp = r'\w+|[^\w\s]+'
'\w+|[^\w\s]+'
>>> tokenizer = RegexpTokenizer(regexp, SUBTOKENS='WORDS')
>>> tokenizer.tokenize(text_token)
>>> print text_token
<[<Hello>, <.>, <Isn>, <'>, <t>, <this>, <fun>, <?>]>
]]></programlisting>
<tip>
<para>Recall that <literal>\w+|[^\w\s]+</literal> is a disjunction of
two subexpressions, namely <literal>w+</literal> and
<literal>[^\w\s]+</literal>. The first of these matches one or more
<quote>word</quote> characters; i.e., characters other than whitespace
or punctuation. The second pattern is a negated range expression; it
matches on or more characters which are not word characters (i.e., not a
match for
<literal>\w</literal>) and not a whitespace character  (i.e., not a
match for
<literal>\s</literal>).</para>
</tip>
<para> The regular expression in this example will match
a sequence consisting of one or more word characters <literal>\w+</literal>.
It will also match a sequence consisting of one or more punctuation
characters (or non-word, non-space characters <literal>[^\w\s]+</literal>).
</para>

<para> There are a number of ways we might want to improve this
      regular expression.  For example, it currently breaks the string
      "$22.50" into four tokens; but we might want it to include this
      as a single token.  One approach to making this change would be
      to add a new clause to the tokenizer's regular expression, which
      is specialized for handling strings of this form: </para>

<programlisting><![CDATA[
>>> text_token = Token(TEXT='That poster costs $22.40.') 
>>> regexp = r'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
>>> tokenizer = RegexpTokenizer(regexp, SUBTOKENS='WORDS')
>>> tokenizer.tokenize(text_token)
>>> print text_token
<[<That>, <poster>, <costs>, <$22.40>, <.>]>
]]></programlisting>

<para>It is sometimes more convenient to write a regular expression
matching the material that appears <emphasis>between</emphasis> tokens, such as whitespace
and punctuation.  The <literal>RegexpTokenizer()</literal> constructor permits
an optional parameter <literal>negative</literal> which inverts the meaning
of the regular expression.  For example, the following two tokenizers are
equivalent:
<literal>RegexpTokenizer(r'[^\s]+')</literal>,
<literal>RegexpTokenizer(r'\s+', negative=1)</literal>.
</para>

<para> 
The <literal>nltk.corpus</literal> module provides ready access to several
corpora included with NLTK, along with built-in tokenizers.  Here is an
example of its use:
<programlisting><![CDATA[
>>> from nltk.corpus import gutenberg 
>>> print gutenberg.read('milton-paradise.txt')
<[<**This>, <is>, <the>, <Project>, <Gutenberg>,
<Etext>, <of>, <Paradise>, <Lost(Raben)**> ... ]>
]]></programlisting>
In particular, <literal>gutenberg.read</literal> doesn't just read the
relevant text into a string, it also invokes a tokenizer
that is appropriate for texts in the Gutenberg corpus.
</para>

</section> <!-- Regexp tokenizer -->

<!------------------------------------------------------------------------>

<section id="frequency">
<title> Counting Tokens </title>

  <para>Perhaps the simplest thing to do once we have pulled tokens
    out of text is to count them.  We can do this as follows, to
    compare the lengths of the English and Finnish translations of
    the book of Genesis:
  </para>

<programlisting><![CDATA[
>>> from nltk.corpus import genesis
>>> corpus = genesis.read('english-kjv.txt')
>>> print len(corpus['WORDS'])
38240
>>> corpus = genesis.read('finnish.txt')
>>> print len(corpus['WORDS'])
26595
]]></programlisting>

    <para> We can do more sophisticated counting using
    a <glossterm>frequency distribution</glossterm>.  In general,
    a frequency distribution records the
    number of times each outcome of an experiment has occured.  For
    instance, a frequency distribution could be used to record the
    frequency of each word in a document.
    Frequency distributions are generally initialized by
    repeatedly running an experiment, and incrementing the count for
    a sample every time it is an outcome of the experiment.
    The following program produces a frequency
    distribution that records how often each word occurs in a
    text, and prints the most frequently occurring word:</para>

<programlisting><![CDATA[
>>> from nltk.probability import FreqDist
>>> fd = FreqDist()
>>> for token in corpus['WORDS']:
...     fd.inc(token['TEXT'])
>>> print fd.max()
'the'
]]></programlisting>

    <para> Once we construct a frequency distribution that records
      the outcomes of an experiment, we can use it to examine a number
      of interesting properties of the experiment.  These properties
      are summarized in <xref linkend="table.freqdist"/>.</para>

<table id="table.freqdist"> 
  <title>Frequency Distribution Module</title> 
  <tgroup cols="2">
    <colspec colwidth='2cm'/>
    <colspec colwidth='6cm'/>
    <colspec colwidth='6cm'/>
    <thead> 
      <row> 
        <entry>Name</entry> 
        <entry>Code Sample</entry> 
        <entry>Description</entry> 
      </row> 
    </thead> 

    <tbody>
      <row><entry>Count</entry><entry>
<programlisting><![CDATA[
>>> fd.count('the')
6
]]></programlisting>
</entry>
<entry><para>The number of times a given sample occurred
</para></entry></row>

      <row><entry>Frequency</entry><entry>
<programlisting><![CDATA[
>>> fd.freq('the')
0.012
]]></programlisting>
</entry>
<entry><para>The frequency of a given sample
</para></entry></row>

      <row><entry>N</entry><entry>
<programlisting><![CDATA[
>>> fd.N()
500
]]></programlisting>
</entry>
<entry><para>The number of samples
</para></entry></row>

      <row><entry>Samples</entry><entry>
<programlisting><![CDATA[
>>> fd.samples()
['happy', 'but', 'the',
'in', 'of', ...]
]]></programlisting>
</entry>
<entry><para>All the distinct samples that were recorded
</para></entry></row>

      <row><entry>Max</entry><entry>
<programlisting><![CDATA[
>>> fd.max()
'the'
]]></programlisting>
</entry>
<entry><para>The sample with the greatest number of outcomes
</para></entry></row>
</tbody>
</tgroup>
</table>

    <para> We can use a <literal>FreqDist</literal> to
      examine the distribution of word lengths in a corpus.
      We construct a frequency distribution whose
      samples are word lengths; and plot the results.
      To begin, we import the classes we'll be using, and
      load a corpus from a text file:</para>

<programlisting><![CDATA[
>>> from nltk.probability import FreqDist 
>>> from nltk.corpus import genesis
]]></programlisting>

      <para> Next, we define a function which takes the name of a
        text as its argument, and prints the distribution of
        wordlengths of the text.  For each word, we find its length,
        and increment the count for words of this length.

</para>

<programlisting><![CDATA[
>>> def length_dist(text):
...     fd = FreqDist()                        # initialize an empty frequency distribution
...     corpus = genesis.read(text)            # tokenize the text
...     for token in corpus['WORDS']:          # for each token
...         fd.inc(len(token['TEXT']))         # found another word with this length
...     for i in range(15):                    # for each length from 0 to 14
...         print "%2d" % int(100*fd.freq(i)), # print the percentage of words with this length
...     print
]]></programlisting>


<programlisting><![CDATA[
>>> length_dist('english-kjv.txt')
0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
>>> length_dist('finnish.txt')
0  0  9  6 11 16 16 12  9  6  3  2  2  1  0
]]></programlisting>

    <para> A <glossterm>condition</glossterm> specifies the context in
    which an experiment is performed.  Often, we are interested in the
    effect that conditions have on the outcome for an experiment.  For
    example, we might want to examine how the distribution of a word's
    length (the outcome) is affected by the word's initial letter (the
    condition).  Conditional frequency distributions provide a tool
    for exploring this type of question. </para>

    <para> A <glossterm>conditional frequency distribution</glossterm>
    is a collection of frequency distributions for the same
    experiment, run under different conditions.  The individual
    frequency distributions are indexed by the condition.</para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import genesis
>>> from nltk.draw.plot import Plot 

>>> cfdist = ConditionalFreqDist()

>>> for text in genesis.items():
...     corpus = genesis.read(text)
...     for token in corpus['WORDS']:
...         cfdist[text].inc(len(token['TEXT']))
]]></programlisting>

        <para> To plot the results, we
        construct a list of points, where the x coordinate is the
        word length, and the y coordinate is the frequency with which
        that word length is used: </para>

<programlisting><![CDATA[
>>> for cond in cfdist.conditions():
...     wordlens = cfdist[cond].samples()
...     wordlens.sort()
...     points = [(i, cfdist[cond].freq(i)) for i in wordlens]
...     Plot(points).mainloop()
]]></programlisting>



    <section id="frequency.predicting"> 
      <title> An Application: Predicting the Next Word </title>

      <para> Conditional frequency distributions are often used for
      prediction.  <glossterm>Prediction</glossterm> is the problem of
      deciding a likely outcome for a given run of an experiment.  The
      decision of which outcome to predict is usually based on the
      context in which the experiment is performed.  For example, we
      might try to predict a word's text (outcome), based on the text
      of the word that it follows (context). </para>

      <para> To predict the outcomes of an experiment, we first
      examine a representative <glossterm>training corpus</glossterm>,
      where the context and outcome for each run of the experiment are
      known.  When presented with a new run of the experiment, we
      simply choose the outcome that occured most frequently for the
      experiment's context. </para>

      <para> We can use a <literal>ConditionalFreqDist</literal> to
      find the most frequent occurence for each context.  First, we
      record each outcome in the training corpus, using the context
      that the experiment was run under as the condition.  Then, we
      can access the frequency distribution for a given context with
      the indexing operator, and use the <literal>max()</literal>
      method to find the most likely outcome. </para>

      <para> We will now use a
        <literal>ConditionalFreqDist</literal> to predict the most
        likely next word in a text.  To
        begin, we load a corpus from a text file, and create an empty
        <literal>ConditionalFreqDist</literal>:</para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import genesis
>>> corpus = genesis.read('english-kjv.txt')
>>> cfdist = ConditionalFreqDist()
]]></programlisting>

        <para> We then examine each token in the corpus, and increment
        the appropriate sample's count.  We use the variable
        <literal>prev</literal> to record the previous word.</para>

<programlisting><![CDATA[
>>> prev = None
>>> for token in corpus['WORDS']:
...     word = token['TEXT']
...     cfdist[prev].inc(word)
...     prev = word
]]></programlisting>

        <note> 
          <para> Sometimes the context for an experiment is
          unavailable, or does not exist.  For example, the first
          token in a text does not follow any word.  In these cases,
          we must decide what context to use.  For this example, we
          use <literal>None</literal> as the context for the first
          token.  Another option would be to discard the first
          token. </para>
        </note>

        <para> Once we have constructed a conditional frequency
        distribution for the training corpus, we can use it to find
        the most likely word for any given context. For example, taking
        the word <literal>living</literal> as our context, we can
        inspect all the words that occurred in that context.

<programlisting><![CDATA[
>>> word = 'living'
>>> cfdist['living'].samples()
['creature,', 'substance', 'soul.', 'thing', 'thing,', 'creature']
]]></programlisting>

        We can set up a simple loop to generate text: we set an
        initial context, picking the most likely token in that context
        as our
        next word, and then using that word as our new context: </para>

<programlisting><![CDATA[
>>> word = 'living'
>>> for i in range(20):
...     print word,
...     word = cfdist[word].max()
living creature that he said, I will not be a wife of the land of the land of the land
]]></programlisting>

        <para> This simple approach to text generation tends to
        get stuck in loops, as demonstrated by the text generated
        above.  A more advanced approach would be to randomly choose
        each word, with more frequent words chosen more often. </para>

    </section> <!-- Predicting -->
  </section> <!-- ConditionalFreqDist -->


<section id="pos">
  <title>Word Classes and Parts of Speech</title>

<para>In the preceding sections, we have pretty much treated all words
alike: things are either tokens or not. However, for many applications,
we want to distinguish between <emphasis>different kinds</emphasis> of
tokens. For example, we want to be able to make explicit that we have
recognized one string as an ordinary lexical item, another as numerical
expression, and yt another as a punctuation character. Morever, we want
to distinguish different kinds of lexical items.  In fact, 
    there is a long tradition within linguistics of classifying words
into categories called <firstterm>parts of speech</firstterm>. These
      are sometimes also called  word classes or <firstterm>lexical
	categories</firstterm>.
    Familiar examples are <type>noun</type>, <type>verb</type>,
    <type>preposition</type>, <type>adjective</type> and
    <type>adverb</type>.  In this section we present the standard
    criteria for categorising words in this way, then discuss the main
    classes of words in English.
  </para>

<section id="pos.categorise">
  <title>Categorising Words</title>

  <para>
    How do we decide what category a word should belong to? In general,
    linguists invoke three kinds of criteria for making the decision:
    formal; syntactic (or distributional); notional (or semantic).
    A <firstterm>formal</firstterm> criterion is one which looks at
    the internal structure of a word. For example,
    <literal>-ness</literal> is a suffix which combines with an
    adjective to produce a noun. Examples are <literal>happy</literal>
    &gt; <literal>happiness</literal>, <literal>ill</literal> &gt;
    <literal>illness</literal>.<footnote>
     <para>We use <markup>&gt;</markup> to mean 'is derived from'.</para>
	</footnote>
So if we encounter a word which ends
    in <literal>-ness</literal>, this is very likely to be a noun.
</para>

  <para>
    A <firstterm>syntactic</firstterm> criterion refers to the
    syntactic contexts in which a word can occur. For example, assume
    that we have already determined the category of nouns. Then we
    might say that a syntactic criterion for an adjective in English
    is that it can occur immediately before a noun, or immediately
    following the words <literal>be</literal> or
    <literal>very</literal>. According to these tests,
    <literal>near</literal> should be categorised as an adjective:
  </para>
    <orderedlist>
      <listitem><para>the near window</para></listitem>
      <listitem><para>The end is (very) near.</para></listitem>
    </orderedlist>


  <para>
    A familiar example of a <firstterm>notional</firstterm> criterion
    is that a noun is <quote>the name of a person, place or
    thing</quote>. Within modern linguistics, notional criteria for
    word classes have be viewed with considerable suspicion, mainly
    because they are hard to formalise. Nevertheless, notional
    criteria underpin many of our intuitions about word classes, and
    enable us to make a good guess about the categorisation of words
    in languages that we are unfamiliar with; that is, if we all we
    know about the Dutch <literal>verjaardag</literal> is that it
    means the same as the English word <literal>birthday</literal>,
    then we can guess that <literal>verjaardag</literal> is a noun in
    Dutch. However, some care is needed: although we might translate
    <literal>zij is van dag jarig</literal> as <literal>it's her
    birthday today</literal>, the word <literal>jarig</literal> is in
    fact an adjective in Dutch, and has no exact equivalent in
    English.
  </para>

<!--http://www.askoxford.com/pressroom/archive/odelaunch/-->

  <para>
    All languages acquire new lexical items. A list of words recently
    added to the Oxford Dictionary of English includes
    <literal>cyberslacker, fatoush, blamestorm, SARS, cantopop, bupkis,
    noughties, muggle</literal>, and <literal>robata</literal>. Notice
    that all these new words are nouns, and this is reflected in calling
    nouns an <glossterm>open class</glossterm>. By contrast,
    prepositions are regarded as a <glossterm>closed
    class</glossterm>. That is, there is a limited set of words
    belonging to the class (e.g., <literal>above, along, at, below,
    beside, between, during, for, from, in, near, on, outside, over,
    past, through, towards, under, up, with</literal>), and membership
    of the set only changes very gradually over time.
<!--    
    Some word classes consist of a limited set of so-called
    <firstterm>function</firstterm>
    words. Prepositions are one such class, comprising items like
     etc.  These are called
    <glossterm>closed classes</glossterm>, in the sense that although
    languages acquire new lexical items.  Content words such as
    nouns are not limited in this way, and are continually being
    extended with the invention of new words.  These are called
    <glossterm>open classes</glossterm>.
-->
  </para>

</section>

<section id="pos.english">
  <title>English Word Classes</title>

  <para>
    This section presents a brief overview of English word classes.
    Readers requiring more detail are encouraged to consult a grammar
    of English.
  </para>

  <para>
    Linguists commonly recognize four major categories of open class
    words in English, namely nouns, verbs, adjectives and adverbs.
    Nouns generally refer to people, places, things, or concepts, e.g.:
    <emphasis>woman, Scotland, book, intelligence</emphasis>.  In the
    context of a sentence, nouns can appear after determiners and
    adjectives, and can be the subject or object of the verb:
  </para>

  <table id="table.nouns">
    <title>Syntactic Patterns involving some Nouns</title>
    <tgroup cols="3">
      <colspec colwidth='2cm'/>
      <thead>
        <row>
          <entry>Word</entry>
          <entry>After a determiner</entry>
          <entry>Subject of the verb</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>woman</entry>
          <entry><emphasis>the</emphasis> woman who I saw yesterday ...</entry>
          <entry>the woman <emphasis>sat</emphasis> down</entry>
        </row>
        <row>
          <entry>Scotland</entry>
          <entry><emphasis>the</emphasis> Scotland I remember as a child ...</entry>
          <entry>Scotland <emphasis>has</emphasis> five million people</entry>
        </row>
        <row>
          <entry>book</entry>
          <entry><emphasis>the</emphasis> book I bought yesterday ...</entry>
          <entry>this book <emphasis>recounts</emphasis> the colonisation of Australia</entry>
        </row>
        <row>
          <entry>intelligence</entry>
          <entry><emphasis>the</emphasis> intelligence displayed by the child ...</entry>
          <entry>Mary's intelligence <emphasis>impressed</emphasis> her teachers</entry>
        </row>
      </tbody>
    </tgroup>
  </table>

  <para>
    English nouns can be morphologically complex.  For example, words like
    <literal>books</literal> and <literal>women</literal> are plural.
    As we saw earlier, words with the <literal>-ness</literal> suffix are
    nouns that have been derived from adjectives, e.g. <literal>happiness</literal>
    and <literal>illness</literal>.  The <literal>-ment</literal> suffix
    appears on certain nouns derived from verbs, e.g. <literal>government</literal>
    and <literal>establishment</literal>.
  </para>

  <para>
    Nouns are usually further classified as <glossterm>common
    nouns</glossterm> and <glossterm>proper nouns</glossterm>.  Proper
    nouns identify particular individuals or entities,
    e.g. <literal>Moses</literal> and <literal>Scotland</literal>,
    while common nouns are all the rest.  Another important
    distinction exists between <glossterm>count nouns</glossterm> and
    <glossterm>mass nouns</glossterm>.  Count nouns are thought of as
    distinct entities which can be counted, such as
    <literal>pig</literal> (e.g. <literal>one pig, two pigs, many
    pigs</literal>).  They cannot occur with the word
    <literal>much</literal> (i.e. *<literal>much pigs</literal>).
    Mass nouns, on the other hand, are not thought of as distinct
    entities (e.g.  <literal>sand</literal>).  They cannot be
    pluralised, and do not occur with numbers (e.g. *<literal>two
    sands</literal>, *<literal>many sands</literal>).  However, they
    can occur with <literal>much</literal> (i.e. <literal>much sand</literal>).
  </para>
    
  <para>
    Verbs are words which describe events and actions,
    e.g. <literal>fall</literal>, <literal>eat</literal>.
    In the context of a sentence, verbs express a relation involving the
    referents of one or more
    noun phrases.
  </para>


  <table id="table.verbs">
    <title>Syntactic Patterns involving some Verbs</title>
    <tgroup cols="3">
      <colspec colwidth='2cm'/>
      <colspec colwidth='5cm'/>
      <thead>
        <row>
          <entry>Word</entry>
          <entry>Simple</entry>
          <entry>With modifiers and adjuncts (italicised)</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>fall</entry>
<!-- probably more plausible to think of "last week" as a sentence
modifier -->
          <entry>Rome fell</entry>
	      <entry>Last week, dot com stocks
	      <emphasis>suddenly</emphasis> fell <emphasis>like a
	      stone</emphasis></entry>
        </row>
        <row>
          <entry>eat</entry>
          <entry>Mice eat cheese</entry>
          <entry>John ate the pizza <emphasis>with gusto</emphasis></entry>
        </row>
      </tbody>
    </tgroup>
  </table>

  <para>
    Verbs can be classified according to the number of arguments
    (usually noun phrases) that
    they co-occur with.  The word <literal>fall</literal> is
    <glossterm>intransitive</glossterm>, requiring exactly one
    argument (the entity which falls).  The word
    <literal>eat</literal> is <glossterm>transitive</glossterm>,
    requiring two arguments (the eater and the eaten).  Other verbs
    are more complex; for instance <literal>put</literal> requires
    three arguments, the agent doing the putting, the entity being put
    somewhere, and a location.  The <literal>-ing</literal> suffix
    appears on nouns derived from verbs, e.g. <literal>the
    falling of the leaves</literal> (this is known as the
    <glossterm>gerund</glossterm>).
  </para>

  <para>
    English verbs can be morphologically complex.  For instance,
    the <glossterm>present participle</glossterm> of a verb ends
    in <literal>-ing</literal>, and expresses the idea of ongoing,
    incomplete action (e.g. <literal>falling, eating</literal>).
    The <glossterm>past participle</glossterm> of a verb often
    ends in <literal>-ed</literal>, and expresses the idea of a
    completed action (e.g. <literal>fell, ate</literal>).
  </para>

  <para>
    Two other important word classes are
    <glossterm>adjectives</glossterm> and
    <glossterm>adverbs</glossterm>.  Adjectives describe nouns, and
    can be used as modifiers (e.g. <literal>large</literal> in
    <literal>the large pizza</literal>), or in predicates
    (e.g. <literal>the pizza is large</literal>).  English adjectives
    can be morphologically complex (e.g.
    <literal>fall<subscript>V</subscript>+ing</literal> in
    <literal>the falling stocks</literal>).
    Adverbs modify verbs to specify the time, manner, place or
    direction of the event described by the verb
    (e.g. <literal>quickly</literal> in <literal>the stocks fell quickly</literal>).
    Adverbs may also modify adjectives (e.g. <literal>really</literal>
    in <literal>Mary's teacher was really nice</literal>).
  </para>

  <para>
    English has several categories of closed class words in addition to
    prepositions, and each dictionary and grammar classifies them
    differently.  <xref linkend="table.closed_class"/> gives a sample of
    closed class words, following the classification of the Brown
    Corpus.<footnote>
     <para>Note that part-of-speech tags may be presented as either
     upper-case or lower-case strings &mdash; there is no significance
     attached to this difference.</para>
     </footnote>
  </para>


<table id="table.closed_class">
  <title>Some English Closed Class Words, with Brown Tag</title>
  <tgroup cols="3">
    <colspec colwidth='1.5cm'/>
    <colspec colwidth='6cm'/>
    <tbody>
      <row>
        <entry> ap </entry>
        <entry> determiner/pronoun, post-determiner </entry>
        <entry> many other next more last former little several enough
                 most least only very few fewer past same </entry>
      </row>
      <row>
        <entry> at </entry>
        <entry> article </entry>
        <entry> the an no a every th' ever' ye </entry>
      </row>
      <row>
        <entry> cc </entry>
        <entry> conjunction, coordinating </entry>
        <entry> and or but plus & either neither nor yet 'n' and/or minus an' </entry>
      </row>
      <row>
        <entry> cs </entry>
        <entry> conjunction, subordinating </entry>
        <entry> that as after whether before while like because if since for
                than until so unless though providing once lest
                till whereas whereupon supposing albeit then </entry>
      </row>
      <row>
        <entry> in </entry>
        <entry> preposition </entry>
        <entry> of in for by considering to on among at through with under
                into regarding than since despite ... </entry>
      </row>
      <row>
        <entry> md </entry>
        <entry> modal auxiliary </entry>
        <entry> should may might will would must can could shall ought need 
                 wilt </entry>
      </row>
      <row>
        <entry> pn </entry>
        <entry> pronoun, nominal </entry>
        <entry> none something everything one anyone nothing nobody everybody
                everyone anybody anything someone no-one nothin' </entry>
      </row>
      <row>
        <entry> ppl </entry>
        <entry> pronoun, singular, reflexive </entry>
        <entry> itself himself myself yourself herself oneself ownself </entry>
      </row>
      <row>
        <entry> pp$ </entry>
        <entry> determiner, possessive </entry>
        <entry> our its his their my your her out thy mine thine </entry>
      </row>
      <row>
        <entry> pp$$ </entry>
        <entry> pronoun, possessive </entry>
        <entry> ours mine his hers theirs yours </entry>
      </row>
      <row>
        <entry> pps </entry>
        <entry> pronoun, personal, nominative, 3rd person singular </entry>
        <entry> it he she thee </entry>
      </row>
      <row>
        <entry> ppss </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular </entry>
        <entry> they we I you ye thou you'uns </entry>
      </row>
      <row>
        <entry> wdt </entry>
        <entry> WH-determiner </entry>
        <entry> which what whatever whichever</entry>
      </row>
      <row>
        <entry> wps </entry>
        <entry> WH-pronoun, nominative </entry>
        <entry> that who whoever whosoever what whatsoever </entry>
      </row>
    </tbody>
  </tgroup>
</table>

</section>

<section id="tagging">
  <title> Part-of-Speech Tag Sets </title>

  <para>
    Most part-of-speech tag sets make use of the same basic categories,
    such as noun, verb, adjective, and preposition. However, tag sets
    differ both in how finely they divide words into categories; and in
    how they define their categories. For example, <literal>is</literal>
    might be tagged as a verb in one tag set; but as a distinct form of
    <literal>to be</literal> in another tag set &mdash; in fact, we just
    observed the latter situation in the Brown Corpus tag set.  This
    variation in tag sets is unavoidable, since part-of-speech tags are
    used in different ways for different tasks. In other words, there is
    no one 'right way' to assign tags, only more or less useful ways,
    depending on one's goals.
<!--
  <note><para> There are several part-of-speech tag sets in widespread
  use, because there are different schemes for classifying words
  (owing to the different weight given to formal, syntactic and
  notional criteria), and because different processing tasks call for
  finer or coarser classification.</para></note>
-->
  </para>

  <para>
    Observe that the tagging process simultaneously collapses
    distinctions (i.e., lexical identity is usually lost when all
    personal pronouns are tagged <literal>prp</literal>), while
    introducing distinctions and removing ambiguities
    (e.g. <literal>deal</literal> tagged as <literal>vb</literal> or
    <literal>nn</literal>).  This move facilitates classification and
    prediction.  Observe that when we introduce finer distinctions in
    a tag set, we get better information about linguistic context, but
    we have to do more work to classify the current token (there are
    more tags to choose from).  Conversely, with fewer distinctions,
    we have less work to do for classifying the current token, but
    less information about the context to draw on.
  </para>

    <para>
      In this tutorial, we will use the following tags:
      <literal>at</literal> (article)
      <literal>nn</literal> (Noun),
      <literal>vb</literal> (Verb),
      <literal>jj</literal> (Adjective),
      <literal>in</literal> (Preposition),
      <literal>cd</literal> (Number), and
      <literal>end</literal> (Sentence-ending punctuation).
      As we mentioned, this is a
      radically simplified version of the Brown Corpus tag set, which in
      its entirety has 87 basic tags plus many
      combinations.  A fuller list is given in the Appendix of the
      tagging tutorial.
    </para>

  </section> <!-- part-of-speech tagsets -->

</section> <!-- Word Classes and Parts of Speech -->


<section id="basics.representation">
  <title> Representing Tagged Tokens and Tagged Corpora </title>

  <para>
    The preceding sections have discussed the nature and use of
    tags in language processing.  In this section the computational
    representation of tags is presented.  First we consider individual
    tagged tokens, and show how they are created and accessed.  Then
    we address tagged corpora.
  </para>

  <para>
    Recall that an NLTK token is a kind of Python dictionary,
    and we can associate arbitrary additional properties with a token.
    By convention, a tagged token is represented by adding a <literal>TAG</literal>
    property to the token.  This can be done when the token is constructed, as follows:
  </para>

<programlisting><![CDATA[
>>> tok = Token(TEXT='fly', TAG='nn')
>>> print tok
<fly/nn>
]]></programlisting>

  <para>
    We can access the properties of this token in the usual way, as
    shown below:
  </para>

<programlisting><![CDATA[
>>> print tok['TEXT']
fly
>>> print tok['TAG']
nn
]]></programlisting>

  <para>
    Sometimes we wish to add a tag to an existing token that lacks
    a tag.  This can be done as follows.  First we create a token
    consisting of text alone, and then add the tag:
  </para>

<programlisting><![CDATA[
>>> tok = Token(TEXT='fly')
<fly>
>>> tok['TAG'] = 'nn'
>>> print tok
<fly/NN>
]]></programlisting>

  <para> Several large corpora (such as the Brown Corpus and portions of the Wall
    Street Journal) have been manually tagged with part-of-speech tags.
    Before we can use these corpora, we must read them from files
    and tokenize them.
  </para>

  <para> Tagged texts are usually stored in files as sequences of
    whitespace-separated tokens, where each token is of the form
    <literal>text/tag</literal>, as illustrated below for a sample
    from the Brown Corpus.
  </para>

<para><literal>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps
said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb
accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt
interest/nn of/in both/abx governments/nns ''/'' ./.
</literal></para>

  <para> It is possible to use the <literal>nltk.corpus</literal> module to read and
    tokenize data from a tagged corpus, as shown below: </para>

<programlisting><![CDATA[
>>> from nltk.corpus import brown
>>> tok = brown.read('ca01')
>>> print tok['WORDS']
[<The/at>, <Fulton/np-tl>, <County/nn-tl>, <Grand/jj-tl>, <Jury/nn-tl>,
<said/vbd>, <Friday/nr>, <an/at>, <investigation/nn>, <of/in>, <Atlanta's/np$>,
<recent/jj>, <primary/nn>, <election/nn>, <produced/vbd>, <``/``>, <no/at>,
<evidence/nn>, <''/''>, <that/cs>, <any/dti>, <irregularities/nns>, <took/vbd>,
<place/nn>, <./.>, ...]
]]></programlisting>

  <para>
    Observe that tokenizing a tagged text produces a single token
    which contains a sequence of tagged
    tokens, stored in its <literal>WORDS</literal> property.
    Here is another example which constructs a single token for the
    text string, then calls the <literal>TaggedTokenReader()</literal>
    to add the subtokens.
  </para>

<programlisting><![CDATA[
>>> from nltk.token import *
>>> from nltk.tokenreader.tagged import TaggedTokenReader
>>> text_str = """
... John/nn saw/vb the/at book/nn on/in the/at table/nn ./end  He/nn sighed/vb ./end
... """
>>> reader = TaggedTokenReader(SUBTOKENS='WORDS')
>>> text_token = reader.read_token(text_str)
>>> print text_token['WORDS']
[<John/nn>, <saw/vb>, <the/at>, <book/nn>, <on/in>, <the/at>, 
 <table/nn>, <./end>, <He/nn>, <sighed/vb>, <./end>]
]]></programlisting>

  <important><para> If <literal>TaggedTokenReader</literal> encounters a word without a tag, it
    will assign the word the default tag <literal>None</literal>. </para></important>

  <para>The subtokens are the individual words of interest, and they are accessed
    using the <literal>WORDS</literal> property.  Each word has a
    <literal>TEXT</literal> and <literal>TAG</literal> property.
    A list of all properties defined for a token can be obtained using
    the <literal>properties()</literal> function.
  </para>

<programlisting><![CDATA[
>>> print text_token['WORDS'][1]
<saw/vb>
>>> print text_token['WORDS'][1]['TAG']
'vb'
>>> print text_token.properties()
['TEXT', 'WORDS']
>>> print text_token['WORDS'][1].properties()
['TEXT', 'TAG']
]]></programlisting>

</section> <!-- Tagged Tokens and Tagged Corpora -->

<section id="more-applications">

  <title>More Applications</title>

  <para>Now that we can access tagged text, it is possible to do a
    variety of useful processing tasks.  Here we consider just two:
    guessing the part-of-speech tag of a word, and exploring the
    frequency distribution of modal verbs according to text genre.
  </para>

<section id="more-applications.classifying">
  <title>Classifying Words Automatically</title>

  <para>A tagged corpus can be used to <emphasis>train</emphasis>
  a simple classifier, which can then be used to guess the tag for
  untagged words.  For each word, we can count the number of times
  it is tagged with each tag.  For instance, the word <literal>deal</literal>
  is tagged 89 times as <literal>nn</literal> and 41 times as <literal>vb</literal>.
  On this evidence, if we were asked to guess the tag for
  <literal>deal</literal> we would choose <literal>nn</literal>, and
  we would be right over two-thirds of the time.  The following
  program performs this tagging task, when trained on one section
  of the Brown Corpus (so-called <emphasis>belles lettres</emphasis>,
  creative writing valued for its esthetic content).
  </para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import brown
>>> cfdist = ConditionalFreqDist()

>>> for item in brown.items('belles-lettres'):
...     text_token = brown.read(item)
...     for word_token in text_token['WORDS']:
...         word = word_token['TEXT']
...         tag = word_token['TAG']
...         cfdist[word].inc(tag)

>>> for word in "John saw 3 polar bears".split():
...     print word, cfdist[word].max()
John np
saw vbd
3 cd-tl
polar jj
bears vbz
]]></programlisting>
    
    <para>Note that <literal>bears</literal> was incorrectly tagged as the
    3rd person singular form of a verb, since this word appears more
    frequently as a verb than a noun in esthetic writing.
    </para>

    <para>A problem with this approach is that it creates a
      huge model, with an entry for every possible combination of word and
      tag.  For certain tasks it is possible to construct reasonably
      good models which are tiny in comparison.  For instance, let's
      try to guess whether a verb is a noun or adjective from the last
      letter of the word alone.  We can do this as follows:
    </para>      

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import brown

>>> tokens = []
>>> for item in brown.items('belles-lettres'):
...     for tok in brown.read(item)['WORDS']:
...         if tok['TAG'] in ['nn', 'jj'] and len(tok['TEXT']) > 3:
...             tokens.append(tok)

>>> split = len(tokens)*9/10
>>> train, test = tokens[:split], tokens[split:]

>>> cfdist = ConditionalFreqDist()

>>> def condition(tok):
...     return tok['TEXT'][-1]

>>> for tok in train:
...     cond = condition(tok)
...     cls = tok['TAG']
...     cfdist[cond].inc(cls)

>>> correct = total = 0
>>> for tok in test:
...     cond = condition(tok)
...     cls = tok['TAG']
...     if cls == cfdist[cond].max():
...         correct += 1
...     total += 1

>>> print correct*100/total
71
]]></programlisting>

    <para>This result of 71% is marginally better than the result of
      65% that we get if we assign the <literal>nn</literal> tag to
      every word.  We can inspect the model to see which tag is
      assigned to a word given its final letter.  Here we learn that
      words which end in <literal>c</literal> or <literal>l</literal>
      are more likely to be adjectives than nouns.
    </para>

<programlisting><![CDATA[
>>> print [(c, cfdist[c].max()) for c in cfdist.conditions()]
[..., ('a', 'nn'), ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'),
('m', 'nn'), ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ...]
]]></programlisting>

</section> <!-- classifying words automatically -->

  <section id="more-applications.modals">
  <title>Exploring text genres</title>

  <para>
    Now that we can load a significant quantity of tagged text, we can process it
    and extract items of interest.  The following code iterates over the fifteen
    genres of the Brown Corpus (accessed using <literal>brown.groups()</literal>).
    The material for each genre lives in a set of files (accessed using
    <literal>brown.items()</literal>).  Each of these is
    tokenized in turn, and stored to <literal>text_token</literal>.
  </para>

<programlisting><![CDATA[
>>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> cfdist = ConditionalFreqDist()
>>> for genre in brown.groups():               # each genre
...     for item in brown.items(genre):        # each file
...         text_token = brown.read(item)      # tokenize
]]></programlisting>

  <para>
    The next step is to construct a list of the modals that
    were found.  To do this we extract and normalize the text
    from each token found to have the <literal>md</literal> tag.
    For each of these words we increment a count.  This uses the
    conditional frequency distribution, where the condition is the
    current genre, and the event is the modal.
  </para>

<programlisting><![CDATA[
...         found = [token['TEXT'].lower()             # normalize
...                  for token in text_token['WORDS']  # each token
...                  if token['TAG'] == 'md']          # that's a modal
...         for modal in found:
...             cfdist[genre].inc(modal)               # increment count
]]></programlisting>

  <para>
    The conditional frequency distribution is nothing more than
    a mapping from each genre to the
    distribution of modals in that genre.  The following code
    fragment identifies a small set of modals of interest, and
    processes the data structure to output the required counts.
  </para>

<programlisting><![CDATA[
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']

>>> print "%40s" % 'Genre',              # generate column headings
>>> for modal in modals:
...     print "%6s" % modal,
>>> print

>>> for genre in cfdist.conditions():    # generate rows
...     print "%40s" % genre,
...     for modal in modals:
...         print "%6d" % cfdist[genre].count(modal),
...     print

                                   Genre    can  could    may  might   must   will
                       skill and hobbies    273     59    130     22     83    259
                                   humor     17     33      8      8      9     13
                            popular lore    168    142    165     45     95    163
                          belles-lettres    249    216    213    113    169    222
                        fiction: science     16     49      4     12      8     16
                        press: reportage     94     86     66     36     50    387
miscellaneous: government & house organs    115     37    152     13     99    237
                      fiction: adventure     48    154      6     58     27     48
                        fiction: mystery     44    145     13     57     31     17
                        fiction: romance     79    195     11     51     46     43
                                religion     84     59     79     12     54     64
                                 learned    366    159    325    126    202    330
                          press: reviews     44     40     45     26     18     56
                        press: editorial    122     56     74     37     53    225
                        fiction: general     39    168      8     42     55     50
]]></programlisting>

  <para>
    There are some interesting patterns in this table.  For instance,
    compare the rows for government literature and adventure
    literature; the former is dominated by the use of <literal>can, may, must,
    will</literal> while the latter is characterised by the use
    of <literal>could</literal> and <literal>might</literal>.  With
    some further work it might be possible to guess the genre of a new
    text automatically, according to its distribution of modals.
  </para>

  <para>
    Now that we have seen how tagged tokens and tagged corpora are
    created and accessed, we are ready to take a look at the automatic
    categorization of words.
  </para>

  <important><para>
    In NLTK, tokenization and tagging are operations which
    <emphasis>annotate</emphasis> existing data.  For instance,
    when a document is stored in a single text token, then tokenized
    to create a set of subtokens, the original text is still available
    via the <literal>TEXT</literal> property.  Note that the default
    printing method for tokenized text simply does not print this material.
  </para></important>


</section> <!-- Exploring text genres -->

</section>  <!-- More applications -->

<section id="reading">
  <title>Further Reading</title>

<para>John Hopkins Center for Language and Speech Processing, 1999
Summer Workshop on Normalization of Non-Standard Words<ulink
url="http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf"><citetitle>Final
Report</citetitle></ulink>.
</para>

  <para>SIL Glossary of Linguistic Terms:
    http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/
  </para>

  <para>
    Language Files: Materials for an
    Introduction to Language and Linguistics (Eighth Edition),
    The Ohio State University Department of Linguistics,
    http://www.ling.ohio-state.edu/publications/files/
  </para>

</section>

<section id="exercises">
  <title>Exercises</title>

  <orderedlist>

    <listitem>
      <formalpara>
        <title> Accessing and tokenizing a text file </title>
        <para> Obtain some plain text data (e.g. visit a web-page and
          save it as plain text), and store it in a file 'corpus.txt'.</para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Using the <literal>open()</literal> and
          <literal>read()</literal> functions, load the text into
          a string variable and print it.
        </para></listitem>
        <listitem><para>
          Now, initialize a new token with <literal>Token()</literal>,
          using this text.  Tokenize the text with
          <literal>WhitespaceTokenizer</literal>, 
          and specify that the
          result should be stored in the <literal>WORDS</literal>
          property.  Print the result.
        </para></listitem>
        <listitem><para>
          Next, compute the number of tokens, using the
          <literal>len()</literal> function, and print the result.
        </para></listitem>
        <listitem><para>
          Finally, discuss shortcomings of this method for tokenizing
          text.  In particular, identify any material which has not
          been correctly tokenized.  (You may need to look for a more
          complex text.)
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Tokenizing text using regular expressions </title>
        <para> Obtain some plain text data (e.g. visit a web-page and
          save it as plain text), and store it in a file 'corpus.txt'.
          so that you can answer the following questions.</para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Word processors typically hyphenate words when they are
          split across a linebreak.  When word-processed documents
          are converted to plain text, the pieces are usually
          not recombined.  It is easy to discover such texts by
          searching on the web for broken words,
          e.g. <literal>depart- ment</literal>.
          Create a <literal>RegexpTokenizer</literal> which
          treats such broken words as a single token.
        </para></listitem>
        <listitem><para>
          Consider the following book title:
          <emphasis>This Is the Beat Generation: New York-San Francisco-Paris</emphasis>.
          What would it take to be able to tokenize such strings so
          that each city name was stored as a single token?
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Working with tagged text </title>
        <para> Write a program which loads the Brown corpus,
          then, given a word, lists the possible tags
          for the word each with a frequency count.  For example,
          for the word <literal>strike</literal> the program would
          generate: <literal>[('nn', 25), ('vb', 21)]</literal>.
          (Hint: this task involves sorting and reversing a list
          of tuples which has the form
          <literal>[(21, 'vb'), (25, 'nn')]</literal>.
          To convert such lists into the required form, use
          <literal>word_freq = [(y,x) for (x,y) in freq_word]</literal>.)
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Use your program to print the tags and their frequencies
          for the following words:
          <literal>can, fox, get, lift, like, but, frank, line, interest</literal>.
          Check that you know the meaning of the high-frequency tags.
        </para></listitem>
        <listitem><para>
          Write a program to find the 20 words which have the greatest
          variety of different possible tags.
        </para></listitem>
        <listitem><para>
          Pick words which can be either a noun or a verb
          (e.g. <literal>deal</literal>).  Guess which is the
          most likely tag for each word, then check whether you
          were right.
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Predicting the next word </title>
        <para> The word prediction program we saw in this chapter
          quickly gets stuck in a cycle.  Modify the program to choose
          the next word randomly, from a list of the <replaceable>n</replaceable>
          most likely words in the given context.  (Hint: store the
          <replaceable>n</replaceable> most likely words in a list
          <literal>lwords</literal> then randomly choose a word from
          the list using <literal>random.choice()</literal>.)
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Select a particular genre, such as a section of the Brown
          Corpus, or a genesis translation, or one of the newsgroups
          corpora.  Train your system on this corpus and get it to generate random
          text.  You may have to experiment with different start words. 
          How intelligible is the text?  Examine the strengths
          and weaknesses of this method of generating random text.
        </para></listitem>
        <listitem><para>
          Try the same approach with different genres, and with
          different amounts of training data.  What do you observe?
        </para></listitem>
        <listitem><para>
          Now train your system using two distinct genres and
          experiment with generating text in the hybrid genre.
          As before, discuss your observations.
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Classifying words automatically </title>
        <para> 
          The program for classifying words as nouns or adjectives
          scored 71%.  We will try to come up with better conditions,
          to get the system to score 80% or better.
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Revise the condition to use a longer suffix of the word,
          such as the last two characters, or the last three characters.
          What happens to the performance?  Which suffixes are
          diagnostic for adjectives?
        </para></listitem>
        <listitem><para>
          Explore other conditions, such as variable length prefixes
          of a word, or the length of a word, or the number of vowels
          in a word.
        </para></listitem>
        <listitem><para>
          Finally, combine multiple conditions into a tuple, and
          explore which combination of conditions gives the best
          result.
        </para></listitem>
      </orderedlist>
    </listitem>

  </orderedlist>
</section> <!-- Exercises -->

<appendix>
  <title>Frequently Used Property Names</title>

<table id="table.properties"> 
  <title>Frequently Used Property Names</title> 
  <tgroup cols="3">
    <colspec colwidth='3cm'/>
    <colspec colwidth='6cm'/>
    <thead> 
      <row> 
        <entry>Property</entry> 
        <entry>Module</entry> 
      </row> 
    </thead> 
    <tbody> 
 
      <row>
        <entry> CHUNK </entry>
        <entry><literal>parser.chunk</literal></entry>
      </row>

      <row>
        <entry> CLASS </entry>
        <entry><literal>classifier</literal></entry>
      </row>

      <row>
        <entry> CLUSTER </entry>
        <entry><literal>clusterer</literal></entry>
      </row>

      <row>
        <entry> CONTEXT </entry>
        <entry><literal>feature.word, parser.chunk</literal></entry>
      </row>

      <row>
        <entry> FEATURES </entry>
        <entry><literal>clusterer, classifier, feature</literal></entry>
      </row>

      <row>
        <entry> LEAF </entry>
        <entry><literal>parser</literal></entry>
      </row>

      <row>
        <entry> LOC </entry>
        <entry><literal>tokenizer</literal></entry>
      </row>

      <row>
        <entry> PROB </entry>
        <entry><literal>clusterer, classifier, parser</literal></entry>
      </row>

      <row>
        <entry> STEM </entry>
        <entry><literal>stemmer</literal></entry>
      </row>

      <row>
        <entry> TAG </entry>
        <entry><literal>tagger</literal></entry>
      </row>

      <row>
        <entry> TEXT </entry>
        <entry><literal>tokenizer</literal></entry>
      </row>

      <row>
        <entry> TREE </entry>
        <entry><literal>parser</literal></entry>
      </row>

      <row>
        <entry> WORDS, LINES </entry>
        <entry><literal>tokenizer</literal></entry>
      </row>

    </tbody>
  </tgroup>
</table>

</appendix>

&index;

</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
adaptive-fill-mode:nil
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:t
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/Users/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

