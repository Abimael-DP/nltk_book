<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <title>Elementary Language Processing: Tokenizing and Classifying Text</title>
    &copyright;
  </articleinfo>

<!------------------------------------------------------------------------>

<section id="intro"><title>Introduction</title>

<para>
</para>

</section> <!-- Introduction -->

<!------------------------------------------------------------------------>

<section id="tokenization"><title>Tokenization</title>

<para> This chapter addresses the following questions: how do we know
that piece of text is a <glossterm>word</glossterm>, and how do we
represent words and associated information in a machine?
</para>

<!--
<para> This chapter and the following one address the following questions: How do we know
that piece of text is a <glossterm>word</glossterm>? And once we know
that something is a word, what linguistic
<glossterm>category</glossterm> should it be assigned to?
In this chapter, we will also look at a powerful technique for
describing textual patterns called <glossterm>regular expressions</glossterm>.
</para>
-->

<para> It might seem needlessly picky to ask what a word is. Can't we
just say the following?
<orderedlist>
<listitem id="word_def">
<para>A word is a string of characters which has white space
before and after it</para>
</listitem>
</orderedlist>

However, it turns out that things are quite a bit
more complex. To get a flavour of the problems, consider the following text.
<example id="wsj_0034">
<title>Paragraph 12 from <filename>wsj_0034</filename></title>
<literallayout><literal>
It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

"I'm very alarmed to see these rich valuations," says Smith Barney's
Mr. Porter.
</literal></literallayout>
</example>
</para>

<para>
Let's start with the string <literal>aren't</literal>. According to
<xref linkend="word_def"/>, it counts as only one word. But consider a
situation where we wanted to check whether all the words in our text
occurred in a dictionary, and our dictionary had entries for <literal
lang="en">are</literal> and <literal lang="en">not</literal>, but not
for <literal lang="en">aren't</literal>.  In this case, we would
probably be happy to say that <literal>aren't</literal> is a contraction
of two distinct words. <!--We can make a similar point about
<literal>1992's</literal>. We might want to run a small program over our
text to extract all words which express dates. In this case, we would
get achieve more generality by first stripping oexcept in this case, we
would not expect to find <literal>1992</literal> in a dictionary.--> 

If we take <xref linkend="word_def"/> literally (as we should, if we are
thinking of implementing it in code), then there are some other minor
but real problems. For example, assuming our file consists of a number
of separate lines, as indicated in <xref linkend="wsj_0034"/>, then all
the words which come at the beginning of a line will fail to be preceded
by whitespace (unless we treat the newline character as a
whitespace). Second, according to our criterion, punctuation symbols
will form part of words; that is, a string like
<literal>investors,</literal> will also count as a word, since there is
no whitespace between <literal>investors</literal> and the following
comma.

<!--
Conversely, we might want to look up names of places in a gazeteer (a
list of places together with their associated geographic location), and
in this case, we would probably want to treat <literal lang="en">South
Korea</literal> as a single <quote>word</quote>.
-->
</para>

<para>
A slightly different challenge is raised by examples such as the
following (drawn from the MedLine [ref] corpus):
<orderedlist>
<listitem>
<para>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.
</para>
</listitem>
<listitem>
<para>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.
</para>
</listitem>
</orderedlist>

In these cases, we encounter terms which are unlikely to be found in any
general purpose English lexicon. Moreover, we will have no success in
trying to syntactically analyse these strings using a standard grammar
of English. Now for some applications, we would like to <quote>bundle
up</quote> expressions such as
<literal>alpha-galactosyl-1,4-beta-galactosyl-specific adhesin</literal>
and <literal>4.53 +/- 0.15%</literal> so that they are presented as
unanalysable atoms to the parser. That is, we want to treat them as
single <quote>words</quote> for the purposes of subsequent processing.
</para>

<para>The upshot is that, even if we confine our attention to English
text, the question of what we treat as word may depend a great deal on
what our purposes are. If we turn to languages other than English,
segmenting words can be even more of a challenge. For example, in
Chinese orthography, characters correspond to monosyllabic
morphemes. Many morphemes are words in their own right, but many words
contain more than one morpheme; most of them consist of two
morphemes. However, there is no visual representation of word boundaries
in Chinese text.

<note><para>give example
</para>
</note>

</para>

<para>
Let's look in more detail at the words in the text <xref linkend="wsj_0034"/>.
Suppose we use white space as the delimiter for words, and then list all
the words; we would expect to get something like the following:
<informalexample>
<programlisting><![CDATA[
120 1992 And Barney But European European Fund I It It Japanese Korea
Mr Porter Smith South Spain a a ...
]]></programlisting>
</informalexample>

<!--
We could also be slightly more clever, and produce a listing which 
<programlisting><![CDATA[
% tr -sc 'A-Za-z0-9' '\012' < wsj_0034 | sort | uniq -c
1 120
1 1992
1 And
1 Barney
1 But
2 European
1 Fund
1 I
2 It
1 Japanese
1 Korea
1 Mr
1 Porter
1 Smith
1 South
1 Spain
3 a
1 alarmed
1 are
1 aren
...
]]></programlisting>

Words according to the Unix <command>wc</command>:
<programlisting><![CDATA[
wc -w wsj_0034_pars12-13
    90 wsj_0034_pars12-13
]]></programlisting>
-->
Now, if we ask a program utility to tell us how many words there in the
text, it will probably come back with something like the following:

<itemizedlist>
<listitem>
<para>There are 90 words in <xref linkend="wsj_0034"/>.</para>
</listitem>	

<!--	  <listitem>
<para>the word `European' occurs twice in <xref
linkend="wsj_0034"/> </para></listitem>  
-->
</itemizedlist>
<indexterm><primary>word token</primary></indexterm>
<indexterm><primary>word type</primary></indexterm>
This calculation depends on treating each of the three occurrences of
<literal>a</literal> as a separate word. Yet what do we mean by saying
that is some object <literal>a</literal> which occurs three times? Are
there three words <literal>a</literal> or just one? We can in fact
answer "Both" if we draw a distinction between a word
<emphasis>token</emphasis> versus a word <emphasis>type</emphasis>.
A word type is
somewhat abstract; it's what we're talking about when we say that we
know the meaning of the word <literal>deprecate</literal>, or when we
say that the words <literal>barf</literal> and <literal>vomit</literal>
are synonyms. On the other hand, a word token is something which exists in
time and space. For example, we could talk about my uttering a token of
the word <literal>grunge</literal> in Edinburgh on July 14 2003;
equally, we can say that the second word token in <xref
linkend="wsj_0034"/> is a token of the word type
<literal>probably</literal>, or that there are two tokens of the type
<literal>European</literal> in the text.
More generally, we want to say that there are 90 word tokens in <xref
linkend="wsj_0034"/>, but only 76 word types. 
</para>

<para> The terms <glossterm>token</glossterm> and
<glossterm>type</glossterm> can also be applied to other linguistic
entities.  For example, a <glossterm>sentence token</glossterm> is an
individual occurrence of a sentence; but a <glossterm>sentence
type</glossterm> is an abstract sentence, without context.  If someone
repeats a sentence twice, they have uttered two sentence tokens, but
only one sentence type.  When the kind of token or type is obvious from
context, we will simply use the terms <glossterm>token</glossterm> and
<glossterm>type</glossterm>.
  </para>

<!-- SUBSECTION: Strings -->
<section id="tokenization.strings">
<title>Strings and Tokens</title>

<para> When written language is stored in a machine it is normally
represented as a sequence (or <glossterm>string</glossterm>) of
characters.  Individual words are strings.  A list of words is a string.
Entire texts are also strings. Strings can include special characters
which represent space, tab and newline.</para>

<para> Most computational processing is performed above the level of
characters.  In compiling a programming language, for example, the
compiler expects its input to be a sequence of
<glossterm>tokens</glossterm> that it knows how to deal with; for
example, the classes of identifiers, string constants and numerals.
Analogously, a parser will expect its input to be a sequence of word
tokens rather than a sequence of individual characters.  At its
simplest, then, <glossterm>tokenization</glossterm> of a text involves
searching for locations in the string of characters containing
whitespace (space, tab, or newline) or certain punctuation symbols, and
breaking the string into word tokens at these points.
</para>

</section><!--Strings and Tokens-->

<section id="words">
<title>Representing Words</title>

<para>
Earlier we said that written language can be stored in a machine as a character string.
We can represent words in Python using strings, such as <literal>'dog'</literal>.
Furthermore, we can represent a sentence as a list of word strings:</para>

<programlisting><![CDATA[
>>> words = ['the', 'cat', 'climbed', 'the', 'tree']
['the', 'cat', 'climbed', 'the', 'tree']
]]></programlisting>

<para> Another way to represent a word is as an integer, a pointer
into a wordlist.  Although this is less readable, it has the advantage
of permitting us to distinguish homographs.  Two words that are spelled the
same might be given different indices.</para>

<para> It turns out that both of these representations -- strings and integers -- are
too impoverished for our purposes.  They fail to distinguish between word types and
word tokens.  Additionally, they do not make it easy to associate other information
(such as a part-of-speech tag) with a word token.

In NLTK, we usually represent words using
<ulink url="&refdoc;/nltk.token.Token-class.html"><literal>Token</literal>s</ulink>.
In the next section we will learn more about <literal>Tokens</literal>, and how they are
created and manipulated.
</para>

<para> In NLTK, word <ulink url="&refdoc;/nltk.token.Token-class.html">
<literal>Token</literal>s</ulink> can be constructed from word strings
using the <ulink url="&refdoc;/nltk.token.Token-class.html#__init__">
<literal>Token</literal> constructor</ulink>, which is defined in the
<ulink url="&refdoc;/nltk.token-module.html"><literal>nltk.token</literal></ulink>
module.  Here is a simple example of the use of this constructor:
</para>

<programlisting><![CDATA[
>>> from nltk.token import * 
>>> my_word_type = 'dog' 
'dog'
>>> my_word_token = Token(TEXT=my_word_type) 
<dog>
]]></programlisting>

<para>
A token is a kind of Python dictionary, and we can associate arbitrary
additional properties with a token:
</para>

<programlisting><![CDATA[
>>> my_word_token['TAG'] = 'Noun' 
'Noun'
>>> my_word_token['REFERENT'] = 'entity123' 
'entity123'
]]></programlisting>

<para>
However, none of this permits us to distinguish two instances of the same word.
For that, we need to use <emphasis>locations</emphasis>, which we turn to next.
</para>
</section> <!-- Words -->

<section> <title> Texts </title>

<para> Many natural language processing tasks involve analyzing
    texts of varying sizes, ranging from single sentences to very
    large corpora.  There are a number of ways to represent texts
    using NLTK.  The simplest is as a single
    <literal>string</literal>.  These strings can be loaded directly
    from files: </para>

<programlisting><![CDATA[
>>> text_str = open('corpus.txt').read() 
'Hello world.  This is a test file.\n'
]]></programlisting>

<para> However, it is usually preferrable to represent a text as a
    <literal>list</literal> of <literal>Token</literal>s.  These
    <literal>list</literal>s are typically created using a
    <glossterm>tokenizer</glossterm>, such as <ulink
    url="&refdoc;/nltk.tokenizer.WhitespaceTokenizer-class.html"
    ><literal>WhitespaceTokenizer</literal></ulink> (which splits words apart
    based on whitespace): </para>

<programlisting><![CDATA[
>>> from nltk.tokenizer import *
>>> text_token = Token(TEXT='Hello world.  This is a test file.')
<Hello world.  This is a test file.>
>>> WhitespaceTokenizer().tokenize(text_token)
>>> print text_token 
<[<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]>
]]></programlisting>

<para> Observe that the <literal>text_token</literal> initially contains
a single token for the entire text.  We further tokenize this into its
component word tokens, and the result is stored as an annotation of the
original token.  The original source token and the resulting tokens are all stored
together, as the following code illustrates:
</para>

<programlisting><![CDATA[
>>> print text_token['TEXT'] 
<Hello world.  This is a test file.>
>>> print text_token['SUBTOKENS'] 
[<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]
]]></programlisting>

<note><para>The default method for printing a token containing subtokens
is just to print the subtokens.</para></note>

<para> If we want all tokens to contain location data, then we
specify the <literal>addlocs</literal> option:</para>

<programlisting><![CDATA[
>>> WhitespaceTokenizer().tokenize(text_token, addlocs=True) 
>>> print text_token 
<[<Hello>@[0:5c], <world.>@[6:12c], <This>@[14:18c], <is>@[19:21c], <a>@[22:23c],
<test>@[24:28c], <file.>@[29:34c]]>
]]></programlisting>

<para> Texts can also be represented as sets of word tokens, which is
useful in document classification applications: </para>

<programlisting><![CDATA[
>>> from nltk.set import * 
>>> print Set(*text_token.freeze()['SUBTOKENS']) 
{<a>, <This>, <is>, <file.>, <test>, <Hello>, <world.>}
]]></programlisting>

<note>
<para> Tokenization may normalize the text, mapping all words to
lowercase, expanding contractions, and possibly even stemming the
words.  An example for stemming is shown below:
<programlisting><![CDATA[
>>> text_token = Token(TEXT='stemming can be fun and exciting') 
>>> WhitespaceTokenizer().tokenize(text_token) 
>>> from nltk.stemmer.porter import * 
>>> stemmer = PorterStemmer() 
>>> for word_token in text_token['SUBTOKENS']: 
...     stemmer.stem(word_token)
>>> print text_token 
<[<TEXT='stemming', STEM='stem'>, <TEXT='can', STEM='can'>, <TEXT='be', STEM='be'>,
<TEXT='fun', STEM='fun'>, <TEXT='and', STEM='and'>, <TEXT='exciting', STEM='excit'>]>
]]></programlisting>
</para>
</note>

</section> <!-- Texts -->

<section> <title> The regular expression tokenizer </title>

<para> The <literal>RegexpTokenizer</literal> is a more powerful
      tokenizer.  It uses a regular expression to determine how text
      should be split up.  This regular expression specifies the
      format of a valid word.  For example, if we wanted to mimic the
      behavior or <literal>WhitespaceTokenizer</literal>, we could define the
      following <literal>RegexpTokenizer</literal>: </para>

<programlisting><![CDATA[
>>> text_token = Token(TEXT='Hello. Isn't this fun?') 
>>> tokenizer = RegexpTokenizer(r'[^\s]+') 
>>> tokenizer.tokenize(text_token) 
>>> print text_token 
<[<Hello.>, <Isn't>, <this>, <fun?>]>
]]></programlisting>

<para> (The regular expression <literal>\s</literal> matches any
      whitespace character.) </para>

<para> To define a tokenizer that includes punctuation as
      separate tokens, we could use: </para>

<programlisting><![CDATA[
>>> regexp = r'\w+|[^\w\s]+'
'\w+|[^\w\s]+'
>>> tokenizer = RegexpTokenizer(regexp) 
>>> tokenizer.tokenize(text_token) 
>>> print text_token 
<[<Hello>, <.>, <Isn>, <'>, <t>, <this>, <fun>, <?>]>
]]></programlisting>

<para> The regular expression in this example will match
      <emphasis>either</emphasis> a sequence of alphanumeric
      characters (letters and numbers); <emphasis>or</emphasis> a
      sequence of punctuation characters. </para>

<para> There are a number of ways we might want to improve this
      regular expression.  For example, it currently breaks the string
      "$22.50" into four tokens; but we might want it to include this
      as a single token.  One approach to making this change would be
      to add a new clause to the tokenizer's regular expression, which
      is specialized for handling strings of this form: </para>

<programlisting><![CDATA[
>>> text_token = Token(TEXT='That poster costs $22.40.') 
>>> regexp = r'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
>>> tokenizer = RegexpTokenizer(regexp) 
>>> tokenizer.tokenize(text_token) 
>>> print text_token 
<[<That>, <poster>, <costs>, &lt$22.40>, <.>]>
]]></programlisting>

<para> Of course, more general solutions to this problem are
      also possible, using different regular expressions. </para>

<note><para>It is sometimes more convenient to write a regular expression
matching the material that appears <emphasis>between</emphasis> tokens, such as whitespace
and punctuation.  The <literal>RegexpTokenizer()</literal> constructor permits
an optional parameter <literal>negative</literal> which inverts the meaning
of the regular expression.  For example, the following two tokenizers are
equivalent:
<literal>RegexpTokenizer(r'[^\s]+')</literal>,
<literal>RegexpTokenizer(r'\s+', negative=1)</literal>.
</para></note>

<note>
<para> 
The <literal>nltk.corpus</literal> module provides ready access to several
corpora included with NLTK, along with built-in tokenizers.  Here is an
example of its use:
<programlisting><![CDATA[
>>> from nltk.corpus import gutenberg 
>>> print gutenberg.tokenize('milton-paradise.txt') 
<[<**This>@[2:8c], <is>@[9:11c], <the>@[12:15c], <Project>@[16:23c], <Gutenberg>@[24:33c],
<Etext>@[34:39c], <of>@[40:42c], <Paradise>@[43:51c], <Lost(Raben)**> ... ]>
]]></programlisting>
</para></note>

</section> <!-- Regexp tokenizer -->

</section>  <!-- Tokenization in NLTK -->

<section id="frequency">
<title> Word Frequency and Text Categorization </title>

<para>INTRO</para>

    <para> A <glossterm>frequency distribution</glossterm> records the
    number of times each outcome of an experiment has occured.  For
    example, a frequency distribution could be used to record the
    frequency of each word type in a document.  Frequency
    distributions are encoded by the <ulink
    url="&refdoc;/nltk.probability.FreqDist-class.html"
    ><literal>FreqDist</literal></ulink> class, which is defined by
    the <ulink url="&refdoc;/nltk.probability-module.html"
    ><literal>nltk.probability</literal> module</ulink>. </para>

    <section id="FreqDist.constructing"> 
      <title> Constructing a Frequency Distribution </title>

      <para> Frequency distributions are generally initialized by
      repeatedly running an experiment, and incrementing the count for
      a sample every time it is an outcome of the experiment.  For
      example, the following code will produce a frequency
      distribution that records how often each word type occurs in a
      text: </para>

<programlisting><![CDATA[
>>> freq_dist = FreqDist()
>>> for token in document['SUBTOKENS']:
...     freq_dist.inc(token['TEXT'])
]]></programlisting>

    </section> <!-- Constructing a FreqDist -->

    <section id="FreqDist.using"> 
      <title> Using a Frequency Distribution </title>

      <para> Once we construct a frequency distribution that records
      the outcomes of an experiment, we can use it to examine a number
      of interesting properties of the experiment.  This section
      describes the most important accessors that are defined for
      frequency distributions. </para>

<table id="table.freqdist"> 
  <title>Frequency Distribution Module</title> 
  <tgroup cols="2">
    <colspec colwidth='2cm'/>
    <colspec colwidth='6cm'/>
    <colspec colwidth='6cm'/>
    <thead> 
      <row> 
        <entry>Name</entry> 
        <entry>Code Sample</entry> 
        <entry>Description</entry> 
      </row> 
    </thead> 

    <tbody>
      <row><entry>Count</entry><entry>
<programlisting><![CDATA[
>>> freq_dist.count('the')
6
]]></programlisting>
</entry>
<entry><para>The number of times a given sample occurred
</para></entry></row>

      <row><entry>Frequency</entry><entry>
<programlisting><![CDATA[
>>> freq_dist.freq('the')
0.012
]]></programlisting>
</entry>
<entry><para>The frequency of a given sample
</para></entry></row>

      <row><entry>N</entry><entry>
<programlisting><![CDATA[
>>> freq_dist.N()
500
]]></programlisting>
</entry>
<entry><para>The number of samples
</para></entry></row>

      <row><entry>Samples</entry><entry>
<programlisting><![CDATA[
>>> freq_dist.samples()
['happy', 'but', 'the', 'in', 'of', ...]
]]></programlisting>
</entry>
<entry><para>All the distinct samples that were recorded
</para></entry></row>

      <row><entry>Max</entry><entry>
<programlisting><![CDATA[
>>> freq_dist.max()
'the'
]]></programlisting>
</entry>
<entry><para>The sample with the greatest number of outcomes
</para></entry></row>
</tbody>
</tgroup>
</table>

    </section> <!-- Using Frequency Distributions -->

    <section id="FreqDist.example"> 
      <title> Example: Word Lengths </title>

      <para> In this section, we use a <literal>FreqDist</literal> to
      examine the distribution of word lengths in a corpus.
      We construct a frequency distribution whose
      samples are word lengths; and plot the results. </para>

      <para> To begin, we import the classes we'll be using, and
      load a corpus from a text file:</para>

      <para> We examine each token
        in the corpus, and find the length of its text.  This length
        is the "outcome" for our experiment, so we use
        <literal>inc()</literal> to increment its count in a frequency
        distribution. </para>

<programlisting><![CDATA[
>>> from nltk.probability import FreqDist 
>>> from nltk.corpus import genesis

>>> def length_dist(text):
...     freq_dist = FreqDist()
...     corpus = genesis.tokenize(text)
...     for token in corpus['SUBTOKENS']:
...         freq_dist.inc(len(token['TEXT']))
...     for i in range(15):
...         print "%2d" % int(100*freq_dist.freq(i)),
...     print

>>> length_dist('english-kjv.txt')
0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
>>> length_dist('finnish.txt')
0  0  9  6 11 16 16 12  9  6  3  2  2  1  0
]]></programlisting>

    <para> A <glossterm>condition</glossterm> specifies the context in
    which an experiment is performed.  Often, we are interested in the
    effect that conditions have on the outcome for an experiment.  For
    example, we might want to examine how the distribution of a word's
    length (the outcome) is affected by the word's initial letter (the
    condition).  Conditional frequency distributions provide a tool
    for exploring this type of question. </para>

    <para> A <glossterm>conditional frequency distribution</glossterm>
    is a collection of frequency distributions for the same
    experiment, run under different conditions.  The individual
    frequency distributions are indexed by the condition.</para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import genesis
>>> from nltk.draw.plot import Plot 

>>> cfdist = ConditionalFreqDist()

>>> for text in genesis.items():
...     corpus = genesis.tokenize(text)
...     for token in corpus['SUBTOKENS']:
...         cfdist[text].inc(len(token['TEXT']))
]]></programlisting>

        <para> To plot the results, we
        construct a list of points, where the x coordinate is the
        word length, and the y coordinate is the frequency with which
        that word length is used: </para>

<programlisting><![CDATA[
>>> for cond in cfdist.conditions():
...     wordlens = cfdist[cond].samples()
...     wordlens.sort()
...     points = [(i, cfdist[cond].freq(i)) for i in wordlens]
...     Plot(points).mainloop()
]]></programlisting>

      <para>PICTURES</para>

    </section> <!-- Example -->

    <section id="ConditionalFreqDist.predict"> 
      <title> Prediction </title>

      <para> Conditional frequency distributions are often used for
      prediction.  <glossterm>Prediction</glossterm> is the problem of
      deciding a likely outcome for a given run of an experiment.  The
      decision of which outcome to predict is usually based on the
      context in which the experiment is performed.  For example, we
      might try to predict a word's text (outcome), based on the text
      of the word that it follows (context). </para>

      <para> To predict the outcomes of an experiment, we first
      examine a representative <glossterm>training corpus</glossterm>,
      where the context and outcome for each run of the experiment are
      known.  When presented with a new run of the experiment, we
      simply choose the outcome that occured most frequently for the
      experiment's context. </para>

      <para> We can use a <literal>ConditionalFreqDist</literal> to
      find the most frequent occurance for each context.  First, we
      record each outcome in the training corpus, using the context
      that the experiment was run under as the condition.  Then, we
      can access the frequency distribution for a given context with
      the indexing operator, and use the <literal>max()</literal>
      method to find the most likely outcome. </para>

      <para> We will now use a
        <literal>ConditionalFreqDist</literal> to predict the most
        likely next word in a text.  To
        begin, we load a corpus from a text file, and create an empty
        <literal>ConditionalFreqDist</literal>:</para>

<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.corpus import genesis
>>> corpus = genesis.tokenize('english-kjv.txt')
>>> cfdist = ConditionalFreqDist()
]]></programlisting>

        <para> We then examine each token in the corpus, and increment
        the appropriate sample's count.  We use the variable
        <literal>prev</literal> to record the previous word.</para>

<programlisting><![CDATA[
>>> prev = None
>>> for token in corpus['SUBTOKENS']:
...     word = token['TEXT']
...     cfdist[prev].inc(word)
...     prev = word
]]></programlisting>

        <note> 
          <para> Sometimes the context for an experiment is
          unavailable, or does not exist.  For example, the first
          token in a text does not follow any word.  In these cases,
          we must decide what context to use.  For this example, we
          use <literal>None</literal> as the context for the first
          token.  Another option would be to simply discard the first
          token. </para>
        </note>

        <para> Once we have constructed a conditional frequency
        distribution for the training corpus, we can use it to find
        the most likely word for any given context.
        We can set up a simple loop to generate text, by using
        the most likely token for each word as the context for the
        next word: </para>

<programlisting><![CDATA[
>>> word = 'living'
>>> for i in range(20):
...     print word,
...     word = cfdist[word].max()
living creature that he said, I will not be a wife of the land of the land of the land
]]></programlisting>

        <para> This simple approach to text generation tends to
        get stuck in loops, as demonstrated by the text generated
        above.  A more advanced approach would be to randomly choose
        each word, with more frequent words chosen more often. </para>

        <note><para> Some of the contexts that we are interested in may not
        be represented in the training corpus.  For example, if the
        training corpus does not contain the word "mango," then the
        conditional frequency distribution cannot predict which words
        are likely to follow it.  If a conditional frequency
        distribution has no information about a context, then the
        <literal>max</literal> method of that context's distribution
        returns <literal>None</literal>.</para></note>

    </section> <!-- Predicting -->

  </section> <!-- ConditionalFreqDist -->





<appendix>
<title> Locations </title>

<para> <glossterm>Span locations</glossterm> specify regions of
      texts, using a <glossterm>start index</glossterm> and an
      <glossterm>end index</glossterm>.  A location in a stream of
      text starting at character
      <replaceable>s</replaceable> and ending at character
      <replaceable>e</replaceable> is written
      <literal>@[</literal><replaceable>s</replaceable><literal>:</literal><replaceable>e</replaceable><literal>c]</literal>.
      This location specifies the region of the text beginning at
      <replaceable>s</replaceable>, and including everything up to
      (but not including) <replaceable>e</replaceable>.
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html"
      ><literal>CharSpanLocations</literal></ulink> are created using the
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html#__init__"
      ><literal>CharSpanLocation</literal> constructor</ulink>, which is
      defined in the <ulink url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module:</para>

<programlisting><![CDATA[
    >>> from nltk.token import * 
    >>> my_loc = CharSpanLocation(1, 5) 
    @[1:5]c@'corpus.txt'
    >>> another_loc = CharSpanLocation(0, 2) 
    @[0:2]c@'corpus.txt'
    >>> yet_another_loc = CharSpanLocation(22, 22) 
    @[22:22]c@'corpus.txt'
]]></programlisting>

<para> Note that a text location does <emphasis>not</emphasis> include
the text at its end location.  This convention may seem unintuitive at
first, but it has a number of advantages.  First, it is consistent with
Python's slice notation (e.g., <literal>x[1:3]</literal> specifies
elements 1 and 2 of <literal>x</literal>).

<footnote><para>Unlike Python slices, text locations do
<emphasis>not</emphasis> support negative indexes.</para></footnote> 

Second, it allows text locations to specify points <emphasis>between</emphasis> tokens,
instead of just ranges.  For example, <literal>Location(3,3)</literal> specifies the
point just before the text at index 3.  Finally, it simplifies arithmetic on
indices; for example, the length of <literal>Location(5,10)</literal> is
<literal>10-5</literal>, and two locations are contiguous if the start
of one equals the end of the other. </para>

<para>A text location may be tagged with a
<glossterm>source</glossterm>, which gives an indication of where the
text was derived from.  A typical example of a source would be
the name of the file from which the element of text was read. </para>

<programlisting><![CDATA[
    >>> my_loc = CharSpanLocation(1, 5, 'foo.txt') 
    @[1:5c]@'foo.txt'
    >>> another_loc = CharSpanLocation(3, 72, 'bar.txt') 
    @[3:72c]@'bar.txt'
]]></programlisting>

<note>
<para> By default, the location's source is unspecified. </para>
</note>

<para> Sometimes, it is useful to use text locations as the
        sources for other text locations.  For example, we could
        specify the third character of the fourth word of the first
        sentence in the file <literal>foo.txt</literal> with
        <literal>char_loc</literal>: </para>

<programlisting><![CDATA[
    >>> sent_loc = CharSpanLocation(0, 25, 'foo.txt') 
    [0:25]c@'foo.txt'
    >>> word_loc = CharSpanLocation(5, 10, sent_loc) 
    [5:10c]@[0:25c]@'foo.txt'
    >>> char_loc = CharSpanLocation(6, 7, word_loc) 
    [6:7c]@[5:10c]@[0:25c]@'foo.txt'
]]></programlisting>

<note>
<para> The location indexes are zero-based, so the
        first sentence starts at an index of zero, not one.  </para>
</note>

<para> As discussed above, a text token represents a single
      occurrence of a piece of text.  In NLTK, a token is defined by a
      type, together with a location at which that type occurs.  A
      token with text <replaceable>t</replaceable> and location
      <literal>@[<replaceable>l</replaceable>]</literal> can be
      written as
      <literal>&lt;<replaceable>t</replaceable>>@[<replaceable>l</replaceable>]</literal>.
      This location information can be used to distinguish two instances of
      the same text.
      Tokens are constructed with the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal> constructor</ulink>: </para>

<programlisting><![CDATA[
    >>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
    'hello'@[0,5c]
    >>> token2 = Token(TEXT='world', LOC=CharSpanLocation(6, 11)) 
    'world'@[6,11c]
]]></programlisting>

<para> Two tokens are only equal if they are equal on all their attributes,
      in this case, for the attributes <literal>text</literal> and <literal>loc</literal>.
</para>

<programlisting><![CDATA[
    >>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
    'hello'@[0,5c]
    >>> token2 = Token(TEXT='hello', LOC=CharSpanLocation(6, 11)) 
    'world'@[6,11c]
    >>> token1 == token2 
    False
    >>> token1['TEXT'] == token2['TEXT'] 
    True
]]></programlisting>

<note>
<para> When a token's location is unknown or unimportant, it may be omitted.
However, the distinction between a word token and a word type is lost in this
case.
</para>
</note>

<para> To access a location's start index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#start"
      ><literal>start</literal></ulink> member function; and to access
      a location's end index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#end"
      ><literal>end</literal></ulink> member function: </para>

<programlisting><![CDATA[
    >>> loc1 = CharSpanLocation(0, 8) 
    @[0:8c]
    >>> loc1.start()
    0
    >>> loc1.end()
    8
]]></programlisting>

<para> To access a location's source, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#source"
      ><literal>source</literal></ulink> member function: </para>

<programlisting><![CDATA[
    >>> loc1 = CharSpanLocation(3, 6, 'corpus.txt') 
    @[3,6c]@'corpus.txt'
    >>> loc1.source()
    'corpus.txt'
]]></programlisting>

<para> For more information about tokens and locations, see the
      reference documentation for the <ulink
      url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module. </para>

</appendix>

<appendix>
  <title>Frequently Used Property Names</title>

<table id="table.properties"> 
  <title>Frequently Used Property Names</title> 
  <tgroup cols="3">
    <colspec colwidth='2cm'/>
    <colspec colwidth='4cm'/>
    <colspec colwidth='6cm'/>
    <thead> 
      <row> 
        <entry>Name</entry> 
        <entry>Module</entry> 
        <entry>Description</entry> 
      </row> 
    </thead> 
    <tbody> 
 
      <row>
        <entry> CHUNK </entry>
        <entry><literal>parser.chunk</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> CLASS </entry>
        <entry><literal>classifier</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> CLUSTER </entry>
        <entry><literal>clusterer</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> CONTEXT </entry>
        <entry><literal>feature.word, parser.chunk</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> FEATURES </entry>
        <entry><literal>clusterer, classifier, feature</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> LEAF </entry>
        <entry><literal>parser</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> LOC </entry>
        <entry><literal>tokenizer</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> PROB </entry>
        <entry><literal>clusterer, classifier, parser</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> STEM </entry>
        <entry><literal>stemmer</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> SUBTOKENS </entry>
        <entry><literal>tokenizer</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> TAG </entry>
        <entry><literal>tagger</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> TEXT </entry>
        <entry><literal>tokenizer</literal></entry>
        <entry></entry>
      </row>

      <row>
        <entry> TREE </entry>
        <entry><literal>parser</literal></entry>
        <entry></entry>
      </row>
    </tbody>
  </tgroup>
</table>

</appendix>

&index;

</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
adaptive-fill-mode:nil
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:nil
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/home/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

