<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "http://nltk.sourceforge.net/ref">
<!ENTITY tutdoc "http://nltk.sourceforge.net/tutorial">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article><title>Tokenization and Regular Expressions</title>


<section id="intro"><title>Introduction</title>


<para> This chapter and the following one address the following questions: How do we know
that piece of text is a <glossterm>word</glossterm>? And once we know
that something is a word, what linguistic
<glossterm>category</glossterm> should it be assigned to?
In this chapter, we will also look at a powerful technique for
describing textual patterns called <glossterm>regular expressions</glossterm>.
</para>

<para> It might seem needlessly picky to ask what a word is. Can't we
just say the following?
<orderedlist>
<listitem id="word_def">
<para>A word is a string of characters which has white space
before and after it</para>
</listitem>
</orderedlist>

However, it turns out that things are quite a bit
more complex. To get a flavour of the problems, consider the following text.
<example id="wsj_0034">
<title>Paragraph 12 from <filename>wsj_0034</filename></title>
<literallayout>
It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

"I'm very alarmed to see these rich valuations," says Smith Barney's
Mr. Porter.
</literallayout>
</example>
</para>

<para>
Let's start with the string <literal>aren't</literal>. According to
<xref linkend="word_def"/>, it counts as only one word. But consider a
situation where we wanted to check whether all the words in our text
occurred in a dictionary, and our dictionary had entries for <literal
lang="en">are</literal> and <literal lang="en">not</literal>, but not
for <literal lang="en">aren't</literal>.  In this case, we would
probably be happy to say that <literal>aren't</literal> is a contraction
of two distinct words. <!--We can make a similar point about
<literal>1992's</literal>. We might want to run a small program over our
text to extract all words which express dates. In this case, we would
get achieve more generality by first stripping oexcept in this case, we
would not expect to find <literal>1992</literal> in a dictionary.--> 

If we take <xref linkend="word_def"/> literally (as we should, if we are
thinking of implementing it in code), then there are some other minor
but real problems. For example, assuming our file consists of a number
of separate lines, as indicated in <xref linkend="wsj_0034"/>, then all
the words which come at the beginning of a line will fail to be preceded
by whitespace (unless we treat the newline character as a
whitespace). Second, according to our criterion, punctuation symbols
will form part of words; that is, a string like
<literal>investors,</literal> will also count as a word, since there is
no whitespace between <literal>investors</literal> and the following
comma.

<!--
Conversely, we might want to look up names of places in a gazeteer (a
list of places together with their associated geographic location), and
in this case, we would probably want to treat <literal lang="en">South
Korea</literal> as a single <quote>word</quote>.
-->
</para>

<para>
A slightly different challenge is raised by examples such as the
following (drawn from the MedLine [ref] corpus):
<orderedlist>
<listitem>
<para>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.
</para>
</listitem>
<listitem>
<para>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.
</para>
</listitem>
</orderedlist>

In these cases, we encounter terms which are unlikely to be found in any
general purpose English lexicon. Moreover, we will have no success in
trying to syntactically analyse these strings using a standard grammar
of English. Now for some applications, we would like to <quote>bundle
up</quote> expressions such as
<literal>alpha-galactosyl-1,4-beta-galactosyl-specific adhesin</literal>
and <literal>4.53 +/- 0.15%</literal> so that they are presented as
unanalysable atoms to the parser. That is, we want to treat them as
single <quote>words</quote> for the purposes of subsequent processing.
</para>

<para>The upshot is that, even if we confine our attention to English
text, the question of what we treat as word may depend a great deal on
what our purposes are. If we turn to languages other than English,
segmenting words can be even more of a challenge. For example, in
Chinese orthography, characters correspond to monosyllabic
morphemes. Many morphemes are words in their own right, but many words
contain more than one morpheme; most of them consist of two
morphemes. However, there is no visual representation of word boundaries
in Chinese text.

<note><para>give example
</para>
</note>

</para>

<!--
<section>
<title>Type and Tokens</title>
-->
<para>
Let's look in more detail at the words in the text <xref linkend="wsj_0034"/>.
Suppose we use white space as the delimiter for words, and then list all
the words; we would expect to get something like the following:
<informalexample>
<programlisting>
120
1992
And
Barney
But
European
European
Fund
I
It
It
Japanese
Korea
Mr
Porter
Smith
South
Spain
a
a
...
</programlisting>
</informalexample>

<!--
We could also be slightly more clever, and produce a listing which 
<programlisting>
% tr -sc 'A-Za-z0-9' '\012' &lt; wsj_0034 | sort | uniq -c
1 120
1 1992
1 And
1 Barney
1 But
2 European
1 Fund
1 I
2 It
1 Japanese
1 Korea
1 Mr
1 Porter
1 Smith
1 South
1 Spain
3 a
1 alarmed
1 are
1 aren
...
</programlisting>

Words according to the Unix <command>wc</command>:
<programlisting>
wc -w wsj_0034_pars12-13
    90 wsj_0034_pars12-13
</programlisting>
-->
Now, if we ask a program utility to tell us how many words there in the
text, it will probably come back with something like the following:
Now, it seems that we can make two different sorts of observations about
<itemizedlist>
<listitem>
<para>There are 90 words in <xref linkend="wsj_0034"/>.</para>
</listitem>	

<!--	  <listitem>
<para>the word `European' occurs twice in <xref
linkend="wsj_0034"/> </para></listitem>  
-->
</itemizedlist>
This calculation depends on treating each of the three occurrences of
<literal>a</literal> as a separate word. Yet what do we mean by saying
that is some object <literal>a</literal> which occurs three times? Are
there three words <literal>a</literal> or just one? We can in fact
answer "Both" if we draw a distinction between a word
<emphasis>token</emphasis> versus aword <emphasis>type</emphasis>.
<indexterm><primary>word token</primary></indexterm>
<indexterm><primary>word type</primary></indexterm> A word type is
somewhat abstract; it's what we're talking about when we say that we
know the meaning of the word <literal>deprecate</literal>, or when we
say that the words <literal>barf</literal> and <literal>vomit</literal>
are synonyms. On the other, a word token is something which exists in
time and space. For example, we could talk about my uttering a token of
the word <literal>grunge</literal> in Edinburgh on July 14 2003;
equally, we can say that the second word token in <xref
linkend="wsj_0034"/> is a token of the word type
<literal>probably</literal>, or that there are two tokens of the type
<literal>European</literal> in the text.
 
<!--
<variablelist>
<varlistentry><term>Word token</term>
  <listitem><para>an occurrence of a word at a particular
  spatio-temporal location (e.g. a sequential position in a text</para></listitem>
</varlistentry>
<varlistentry><term>Word type</term> <listitem><para>a more abstract
notion, also termed <glossterm>lexeme</glossterm>
<indexterm><primary>lexeme</primary></indexterm>&mdash;we speak of two
tokens belonging to the same type.</para></listitem>
</varlistentry>
</variablelist>-->
More generally, we want to say that there are 90 word tokens in <xref
linkend="wsj_0034"/>, but only 76 word types. 
<!--For example, there are
   five tokens of the type <literal>to</literal>. This distinction
between types and tokens is fundamental, and we will see it being used
many times again.
-->
</para>

<!-- <para> The term "word" can actually be used in two different ways: to refer
to an individual occurrence of a word; or to refer to an abstract
vocabulary item.  For example, the phrase "my dog likes his dog"
contains five occurrences of words, but only four vocabulary items
(since the vocabulary item "dog" appears twice).  In order to make this
distinction clear, we will use the term <glossterm>word
token</glossterm> to refer to occurrences of words, and the term
<glossterm>word type</glossterm> to refer to vocabulary
items. </para> -->

<indexterm><primary>token</primary></indexterm>
<indexterm><primary>type</primary></indexterm>
<indexterm><primary>sentence token</primary></indexterm>
<indexterm><primary>sentence type</primary></indexterm>

<para> The terms <glossterm>token</glossterm> and
<glossterm>type</glossterm> can also be applied to other linguistic
entities.  For example, a <glossterm>sentence token</glossterm> is an
individual occurrence of a sentence; but a <glossterm>sentence
type</glossterm> is an abstract sentence, without context.  If someone
repeats a sentence twice, they have uttered two sentence tokens, but
only one sentence type.  When the kind of token or type is obvious from
context, we will simply use the terms <glossterm>token</glossterm> and
<glossterm>type</glossterm>.
  </para>


</section>
<!-- ********************** /Introduction ************************* -->



<!-- ********************** Tokenization *************************** -->
<section id="tokenization"> 
<title>Tokenization</title>

<!-- SUBSECTION: Strings -->
<section>
<title>Strings and Tokens</title>



<para> When written language is stored in a machine it is normally
represented as a sequence (or <glossterm>string</glossterm>) of
characters.  Individual words are strings.  A list of words is a string.
Entire texts are also strings. Strings can include special characters
which represent space, tab and newline.</para>
<indexterm><primary>tokenization</primary></indexterm>

<para> Most computational processing is performed above the level of
characters.  In compiling a programming language, for example, the
compiler expects its input to be a sequence of
<glossterm>tokens</glossterm> that it knows how to deal with; for
example, the classes of identifiers, string constants and numerals.
Analogously, a parser will expect its input to be a sequence of word
tokens rather than a sequence of individual characters.  At its
simplest, then, <glossterm>tokenization</glossterm> of a text involves
searching for locations in the string of characters containing
whitespace (space, tab, or newline) or certain punctuation symbols, and
breaking the string into word tokens at these points.
</para>

</section><!--Strings and Tokens-->
</section>
<!-- ********************** /Tokenization ************************* -->


<!-- ********************** Regular Expressions ******************* -->
<section id="regex"> 
<title>Regular Expressions</title>


<section id="regex-overview">
<title>Overview</title>

<para> This section provides an introduction to regular expressions
illustrated with examples from language processing.
    
<footnote>
<para>For a more concise introduction please see the Python
documentation <ulink
		     url="http://www.python.org/doc/current/lib/module-re.html"> re -- Regular expression operations</ulink>.</para>
</footnote>
</para>

<para>
We have already noted that a text can be viewed as a string of
characters. What kinds of processing are performed at the character
level?  Perhaps word games are the most familiar example of such
processing.  In completing a crossword we may want to know which
3-letter English words end with the letter <literal>c</literal> (e.g.
<literal>arc</literal>).  We might want to know how many words can be
formed from the letters: <literal>a</literal>, <literal>c</literal>,
<literal>e</literal>, <literal>o</literal>, and <literal>n</literal>
(e.g. <literal>ocean</literal>).  We may want to find out which unique
English word contains the substring <literal>gnt</literal> (left as an
exercise for the reader).  In all these examples, we are considering
which word &mdash; drawn from a large set of candidates &mdash; matches
a given pattern. To put this in a more computational framework, we could
imagine searching through a large digital corpus in order to find all
words that match a particular pattern. There are many serious uses of
this so-called <glossterm>pattern matching</glossterm>.  
</para>

<para>
One instructive example (due to <citation>Friedl:2002:MRA</citation>) is
the task of find all doubled words in a text; an example would be the
string <literal>for for example</literal>. Notice that we would be
particularly interested in finding cases where the words were split
across a linebreak (in practise, most erroneously double words occur in
this context). Consequently, even with such a relatively banal task, we
need to be able to describe patterns which refer not just to
<quote>ordinary</quote> characters, but also to formatting information.
</para>

<para>
There are conventions for indicating structure in strings, also known as
<glossterm>formatting</glossterm>. For example, there are a number of
alternative ways of formatting a <quote>date string</quote>, such as
<literal>23/06/2002</literal> or <literal>2002-06-23</literal>.  Whole
texts may be formatted, such as an email message which contains header
fields followed by the message body. Another familiar form of formatting
involves visual structure, such as tabular format and bulleted
lists.</para>

<para>Finally, texts may contain explicit <quote>markup</quote>, such as
<sgmltag class="starttag">abbrev</sgmltag>Phil<sgmltag
class="endtag">abbrev</sgmltag>, which provides information about the
interpretation or presentation of some piece of text.  To summarize, in
language processing, strings are ubiquitous, and they often contain
important structure.
</para>

<!--
<para>
... stuff on tokenization moved earlier

Alternatively, we may want to compute the relative frequency of
particular letters in the text in order to guess the language of the
text.  For this we must total up the number of matches for each
character of interest.

</para>
-->

<para> So far we have seen elementary examples of pattern matching, the
matching of individual characters.  More often we are interested in
matching <emphasis>sequences</emphasis> of characters.  For example,
part of the operation of a naive spell-checker could be to remove a
word-final <literal>s</literal> from a suspect word token, in case the
word is a plural, and see if the putative singular form exists in the
dictionary.  For this we must locate <literal>s</literal> and remove it,
but only if it precedes a word boundary.  This requires matching a
pattern consisting of two characters.
</para>

<para> Beyond this pattern matching on the <emphasis>content</emphasis>
of a text, we often want to process the <emphasis>formatting</emphasis>
and <emphasis>markup</emphasis> of a text.  We may want to check the
formatting of a document (e.g. to ensure that every sentence begins with
a capital letter) or to reformat a document (e.g. replacing sequences of
space characters with a single space).  We may want to find all date
strings and extract the year.  We may want to extract all words
contained inside the <sgmltag class="starttag">abbrev</sgmltag> <sgmltag
class="endtag">abbrev</sgmltag> markup in order to construct a list of
abbreviations.
</para>

<para> Processing the content, format and markup of strings is a central
task in most kinds of NLP.  The most widespread method for string
processing uses <glossterm>regular expressions</glossterm>.
    </para>

</section> <!-- Overview -->

<section id="simple"> 
<title> Simple Regular Expressions </title>

<para>In this section we will see the building blocks for simple regular
expressions, along with a selection of linguistic examples. We can think
of a regular expression as <emphasis>a specialised notation for
describing patterns that we want to match</emphasis>. In order to make
explicit when we are talking about a pattern <varname>patt</varname>, we
will use the notation &ulcorn;<literal>patt</literal>&drcorn;. The first
thing to say about regular expressions is that most letters match
themselves. For example, the pattern
&ulcorn;<literal>sing</literal>&drcorn; exactly matches the string
<literal>sing</literal>. In addition, regular expressions provide us
with a set of <emphasis>special characters</emphasis>

<footnote>
<para>
These are often called <glossterm>metacharacters</glossterm>; that is,
characters which express properties of (ordinary) characters.
</para>
</footnote>

 which give us a
ways to match <emphasis>sets of strings</emphasis>, and we will now look
at these.
</para>

<section id="dot">
<title>The Wildcard</title>

<para> The <quote><literal>.</literal></quote> symbol is called a
<firstterm>wildcard</firstterm>: it matches any single character. For
example, the regular expression &ulcorn;<literal>s.ng</literal>&drcorn;
matches the following English words: <literal>sang</literal>,
<literal>sing</literal>, <literal>song</literal>, and
<literal>sung</literal>. Note that &ulcorn;<literal>.</literal>&drcorn;
will match not only alphabetic characters, but also numeric and
whitespace characters. Consequently,
&ulcorn;<literal>s.ng</literal>&drcorn; will also match non-words such as
<literal>s3ng</literal>.
</para>

<para> We can also use the wildcard symbol for counting characters. For
instance &ulcorn;<literal>....zy</literal>&drcorn; matches six-letter
strings that end in <literal>zy</literal>.  The pattern
&ulcorn;<literal>....berry</literal>&drcorn; finds words like
<literal>cranberry</literal>. In our example text <xref
linkend="wsj_0034"/>, the pattern
&ulcorn;<literal>t...</literal>&drcorn; will match the words
<literal>that</literal> and <literal>term</literal>, and will also match
the word sequence <literal>to a</literal> (since the third
"<literal>.</literal>" in the pattern can match the space character).
</para>


<note><para> Note that &ulcorn;<literal>.</literal>&drcorn; matches
<emphasis>exactly</emphasis> one character, and must be repeated for as
many characters as should be matched.  To match a variable number of
characters we must use notation for <emphasis>optionality</emphasis>.
</para></note>

</section>

<section id="qmk">
<title>Optionality</title>

<para> The <quote><literal>?</literal></quote> symbol indicates that the immediately
preceding regular expression is optional.  The regular expression
&ulcorn;<literal>colou?r</literal>&drcorn; matches both British and
American spellings, <literal>colour</literal> and
<literal>color</literal>.  The expression that precedes the
<literal>?</literal> may be punctuation, such as an optional hyphen.
For instance &ulcorn;<literal>e-?mail</literal>&drcorn; matches both
<literal>e-mail</literal> and <literal>email</literal>.
      </para>

</section>

<section id="plus">
<title>Repeatability</title>

<para> The "<literal>+</literal>" symbol indicates that the immediately
preceding expression is repeatable, up to an arbitrary number of times.
For example, the regular expression
&ulcorn;<literal>coo+l</literal>&drcorn; matches
<literal>cool</literal>, <literal>coool</literal>, and so on.  This
symbol is particularly effective when combined with the
<literal>.</literal> symbol.  For example,
&ulcorn;<literal>f.+f</literal>&drcorn; matches all strings that begin
and end with the letter <literal>f</literal> (e.g.
<literal>foolproof</literal>).  The expression
&ulcorn;<literal>.+ed</literal>&drcorn; finds strings that potentially
have the past-tense <literal>-ed</literal> suffix.
      </para>

<para> The <quote><literal>*</literal></quote> symbol indicates that the immediately
preceding expression is both optional and repeatable. For example
&ulcorn;<literal>.*gnt.*</literal>&drcorn; matches all strings that
contain <literal>gnt</literal>. 
      </para>



</section>

<section id="choices">
<title>Choices</title>

<para> Patterns using the wildcard symbol are very effective, but there
are many instances where we want to limit the set of characters that the
wildcard can match.  In such cases we can use the <literal>[]</literal>
notation, which enumerates the set of characters to be matched &mdash;
this is called a <firstterm>character class</firstterm>.  For example,
we can match any English vowel, but no consonant, using
&ulcorn;<literal>[aeiou]</literal>&drcorn;. Note that this pattern can
be interpreted as saying <quote>match <literal>a</literal> or
<literal>e</literal> or &hellip; or <literal>u</literal></quote>; that
is, the pattern resembles the wildcard pattern
&ulcorn;<literal>.</literal>&drcorn; in only matching a string of length
one; unlike the wildcard, it restricts the characters matched to a
specific class (in this case, the vowels).   Note that the order of
vowels in the regular expression is insignificant, and we would have had
the same result with the expression
&ulcorn;<literal>[uoiea]</literal>&drcorn;. As a second example, the
expression &ulcorn;<literal>p[aeiou]t</literal>&drcorn; matches the
words: <literal>pat</literal>, <literal>pet</literal>,
<literal>pit</literal>, <literal>pot</literal>, and
<literal>put</literal>.
</para>

<para> We can combine the <literal>[]</literal> notation with our
notation for repeatability.  For example, expression
&ulcorn;<literal>p[aeiou]+t</literal>&drcorn; matches the words listed
above, along with: <literal>peat</literal>, <literal>poet</literal>, and
<literal>pout</literal>.
</para>

<!--
<note><para> Note that the order of vowels in the regular expression is
insignificant, and we would have had the same result with the expression
&ulcorn;<literal>p[uoiea]+t</literal>&drcorn;.  Thus, inside these
brackets, the characters are interpreted not as a string but as a set of
choices.
</para></note>
-->

<para> Often the choices we want to describe cannot be expressed at the
level of individual characters.  As we will see in [xref to tagging
chapter], different parts of speech are often
<emphasis>tagged</emphasis> using labels from a tagset. In the Brown
tagset, for example, singular nouns have the tag <literal>NN1</literal>,
while plural nouns have the tag <literal>NN2</literal>, while nouns
which are unspecified for number (e.g., <literal>aircraft</literal>) are
tagged <literal>NN0</literal>. So we might use
&ulcorn;<literal>NN.*</literal>&drcorn; as a pattern which will match
any nominal tag. Now, suppose we were processing the output of a tagger
to extract string of tokens corresponding to noun phrases, we might want
to find all nouns (<literal>NN.*</literal>), adjectives
(<literal>JJ.*</literal>), determiners (<literal>DT</literal>) and
cardinals (<literal>CD</literal>), while excluding all other word types
(e.g. verbs <literal>VB.*</literal>).  It is possible, using a single
regular expression, to search for this set of candidates using the
<firstterm>choice operator</firstterm> <literal>|</literal> as follows:
&ulcorn;<literal>NN.*|JJ.*|DT|CD</literal>&drcorn;. This says: match
<literal>NN.*</literal> <emphasis>or</emphasis>
<literal>JJ.*</literal><emphasis>or</emphasis> &hellip; <literal>CD</literal>.
 
</para>

<para> As another example of multi-character choices, suppose that we
wanted to create a program to simplify English prose, replacing rare
words (like <literal>habitation</literal>) with a more frequent,
synonymous word (like <literal>home</literal>).  In this situation, we
need to map from a potentially large set of words to an individual word.
We can match the set of words using the choice operator.  In the case
of the word <literal>home</literal>, we would want to match the regular
expression
&ulcorn;<literal>dwelling|domicile|abode|habitation</literal>&drcorn;.
</para>

<note><para> Note that the choice operator has wide scope, so that
&ulcorn;<literal>abc|def</literal>&drcorn; is a choice between
<literal>abd</literal> and <literal>def</literal>, and not between
<literal>abced</literal> and <literal>abdef</literal>.  The latter
choice must be written using parentheses:
&ulcorn;<literal>ab(c|d)ed</literal>&drcorn;.
</para></note>

</section>

</section>

<section id="intermediate">
<title> More Complex Regular Expressions
</title>

<para>In this section we will cover operators which can be used to
construct more powerful and useful regular expressions.
</para>

<section id="ranges"> <title>Ranges</title>

<para>In section <xref linkend="choices"/> we saw how the
<literal>[]</literal> notation could be used to express a set of choices
between individual characters.  Instead of listing each character, it is
also possible to express a <emphasis>range</emphasis> of characters,
using the <literal>-</literal> operator.  For example,
&ulcorn;<literal>[a-z]</literal>&drcorn; matches any lowercase letter.
This allows us to avoid the overpermissive matching we noted above
with the pattern &ulcorn;<literal>t...</literal>&drcorn;. If we were to
use the pattern &ulcorn;<literal>t[a-z][a-z][a-z]</literal>&drcorn;,
then we would no longer match the two word sequence <literal>to a</literal>.
</para>

<para>As expected, ranges can be combined with other operators. For
example &ulcorn;<literal>[A-Z][a-z]*</literal>&drcorn; matches words
that have an initial capital letter followed by any number of lowercase
letters.  The pattern &ulcorn;<literal>20[0-4][0-9]</literal>&drcorn;
matches year expressions in the range 2000 to 2049.
  </para>

<para>Ranges can be combined, e.g.
&ulcorn;<literal>[a-zA-Z]</literal>&drcorn; which matches any lowercase
or uppercase letter.  The expression
&ulcorn;<literal>[b-df-hj-np-tv-z]+</literal>&drcorn; matches words
consisting only of consonants (e.g. <literal>pygmy</literal>).
  </para>

</section>

<section id="complementation"> <title> Complementation </title>

<para>
We just saw that the character class
&ulcorn;<literal>[b-df-hj-np-tv-z]</literal>&drcorn; allows us to match
consonants. However, this expression is quite cumbersome. A better
alternative is to say: let's match anything which isn't a vowel. To do
this, we need a way of expressing
<firstterm>complementation</firstterm>. 

<footnote>
<para>Complementation is a notion drawn from set theory.
Let's assume that our domain of all objects is the set
<varname>U</varname> = &lcub;a, b, c, d, e&rcub;, and let
<varname>V</varname> be the set &lcub;a, e&rcub;. Then the complement of
<varname>V</varname>, often written <varname>V</varname>&apos; is the
result of <quote>subtracting</quote> <varname>V</varname> from
<varname>U</varname>; i.e., the set <varname>V</varname>&apos; =
&lcub;b, c, d &rcub;.</para> </footnote> We do this using the symbol
<quote><literal>^</literal></quote> as the first character within the
class expression <literal>[]</literal>. Let's look at an example.
&ulcorn;<literal>[^aeiou]</literal>&drcorn; is just like our earlier
character class, except now the set of vowels is preceded by
<literal>^</literal>. The expression as a whole is interpreted as
matching anything which <emphasis>fails</emphasis> to match
&ulcorn;<literal>[aeiou]</literal>&drcorn;. In other words, it matches
all consonants.</para>

<para>As another example, suppose we want to match any string which is
enclosed by the HTML tags for boldface, namely <sgmltag class="starttag">B</sgmltag> and
<sgmltag class="endtag">B</sgmltag>, We might try something like this:
&ulcorn;<literal>&lt;B&gt;.*&lt;/B&gt;</literal>&drcorn;. This would
successfully match <literal>&lt;B&gt;important&lt;/B&gt;</literal>, but
would also match <literal>&lt;B&gt;important&lt;/B&gt; and
&lt;B&gt;urgent&lt;/B&gt;</literal>, since the
&ulcorn;<literal>.*</literal>&drcorn; subpattern will happily match all
the characters from the end of <literal>important</literal> to the end
of <literal>urgent</literal>. One way of ensuring that we only look at
matched pairs of tags would be to use the expression
&ulcorn;<literal>&lt;B&gt;[^<]*&lt;/B&gt;</literal>&drcorn;, where the
chararacter class matches anything other than a left angle bracket.
</para>


<para>Finally, note that character class complementation also works with
ranges. Thus &ulcorn;<literal>[^a-z]</literal>&drcorn; matches
anything other than the lower case alphabetic characters
<literal>a</literal> through <literal>z</literal>.
</para>

</section>

<section id="special"> <title> Common Special Symbols </title>

<para>So far, we have only looked at patterns which match with the
content of character strings. However, it is also useful to be able to
refer to formatting properties of texts. Two important symbols in this
regard are <quote><literal>^</literal></quote> and
<quote><literal>$</literal></quote> which are used to
<emphasis>anchor</emphasis> matches to the beginnings or ends of lines
in a file. 
</para>

<note>
<para><quote><literal>^</literal></quote> has two quite distinct
uses: it is interpreted as complementation when it occurs as the
first symbol within a character class, and as matching the beginning of
lines when it occurs elsewhere in a pattern.</para>
</note>

<para>For example, suppose we wanted to find all the words that
occur at the beginning of lines in  <xref
linkend="wsj_0034"/>. Our first attempt might look like
&ulcorn;<literal>^[A-Za-z]+ </literal>&drcorn;. This says: starting at
the beginning of a line, look for one or more alphabetic characters
(upper or lower case), followed by a space. This will match the words 
<literal>that</literal>, <literal>some</literal>,
<literal>been</literal>, and <literal>even</literal>. However, it fails
to match <literal>It's</literal>, since <literal>'</literal> isn't an
alphabetic character. A second attempt might be &ulcorn;<literal>^[^ ]+
</literal>&drcorn;, which says to match any string starting at the
beginning of a line, followed by one or more characters which are
<emphasis>not</emphasis> the space character, followed by a space. This
matches all the previous words, together with 
<literal>It's</literal>,
<literal>skyrocketed</literal>,
<literal>1992s</literal>,
<literal>I'm</literal> and
<literal>"Mr.</literal>. As a second example, &ulcorn;<literal>
[a-z]*s$</literal>&drcorn; will match words ending in
<literal>s</literal> that occur at the end of a line. Finally, consider
the pattern &ulcorn;<literal>^$</literal>&drcorn;; this matches strings where no
character occurs between the beginning and the end of a line &mdash; in
other words, empty lines!
</para>

<!--
<para>In section <xref linkend="intro"/>, we mentioned the problem of
trying to find occurrences of doubled words separated by a line
break. In order to find such cases, we need to a special character to
match linebreaks, and this is accomplished with the symbol
<quote><literal>\n</literal></quote>. Then one way of expressing our
desired pattern would be &ulcorn;<literal>[A-Za-z]+\n</literal>&drcorn;
</para>
<para>^, $, \, \n </para>
-->

<para>
As we have seen, special characters like
<quote><literal>.</literal></quote>,
<quote><literal>*</literal></quote>, <quote><literal>+</literal></quote>
and <quote><literal>$</literal></quote> give us powerful means to
generalise over character strings. But suppose we wanted to match
against a string which itself contains one or more special characters? 
An example would be the arithmetic statement <literal>$5.00 * ($3.05 +
$0.85)</literal>. In this case, we need to resort to the so-called
<glossterm>escape</glossterm> character
<quote><literal>\</literal></quote> (<quote>backslash</quote>). For
example, to match a dollar amount, we might use
&ulcorn;<literal>\$[1-9][0-9]*\.[0-9][0-9]</literal>&drcorn;.
</para>
</section>

</section>

<section id="advanced"> 
<title> Advanced Regular Expressions </title>

<para>greedy vs non-greedy matching</para>

<para>zero-width assertions</para>

<para>more special symbols: \b etc</para>

</section>

<section id="python_interface"> 
<title> Python Interface </title>

<para> How to do regexps in Python - give a couple of code samples for
the above prose illustrations.  Use /usr/dict/words - local copy.
</para>


<para>
The Python <application>re</application> module provides a convenient
interface to an underlying regular expression engine. The module allows a
regular expression pattern to be compiled into a object whose methods
can then be called.
</para>

<para>In the next example, we assume that we have  a local copy
(i.e., <literal>words</literal>) of the Unix dictionary, usually found
in <file>/usr/dict/words</file> or
<literal>/usr/share/dict/words</literal>/
</para>

<programlisting>
<emphasis># Load the regular expression module:</emphasis> 
&prompt;<command>from re import *</command> 

<emphasis># Read a big list of words:</emphasis>
&prompt; <command>words = open('/home/sb/nltk/data/words').read()</command> 

<emphasis># How many words are there?</emphasis>
&prompt; <command>len(words)</command> 409093 

<emphasis># Compile a regexp for words containing a sequence of two 'a's</emphasis>
&prompt; <command>r1 = compile('.*aa.*')</command> 

<emphasis># Find all matches</emphasis> 
&prompt; <command>print r1.findall(words)</command> 
['Afrikaans', 'bazaar', 'bazaars', 'Canaan', 'Haag', 'Haas', 'Isaac', 'Isaacs', 'Isaacson', 'Izaak', 'Salaam', 'Transvaal',
'Waals']
</programlisting>

<para>Suppose now that we want to find all three-letter words ending in
the letter <quote><literal>c</literal></quote>. Our first attempt might
be as follows:
</para>
<programlisting>
<emphasis># Try to compile a regexp for 3-letter words ending in c</emphasis>
&prompt; <command>r1 = compile('..c')</command> 

<emphasis># Find all matches</emphasis> 
&prompt; <command>print r1.findall(words)</command> 
['bac', 'duc', 'duc', 'duc', 'duc', 'duc', 'duc', 'duc', 'anc', 'jec',
'jec', 'jec', 'jec', 'jec', 'eac', 'eac', 'bsc', 'bsc', 'bsc', 'bsc',
'bsc', 'bsc', 'bsc', 'bsc', 'bsc', 'enc', 'enc', 'enc', 'enc', 'rac',
'rac', 'rac', 'rac', 'rac', 'rac', 'rac', 'rac', 'rac', 'rac', 'rac',
'rac', 'anc', 'cac', 'mic', 'mic', ...
</programlisting>
<para>The problem is that we have matched three-letter sequences which
occur <emphasis>anywhere within a word</emphasis>. For example, the
pattern will match <quote><literal>c</literal></quote> in words like
<literal>aback</literal>, <literal>Aerobacter</literal> and
<literal>albacore</literal>. Inspection of the file
<filename>words</filename> reveals that words are listed one to a
line. So we might want to revise our pattern so that the match string is
anchored to the beginning and ends of the line: 
&ulcorn;<literal>^...$</literal>&drcorn;. However, the
<filename>words</filename> is read into Python as a single string
(containing line breaks), and the Python 
<application>re</application> module interprets
&ulcorn;<literal>^</literal>&drcorn; as matching only only at the
beginning of the string as a whole; i.e., in this case, the pattern
would only try to match against the first word of the file. To deal with
this, we use the <firstterm>compilation flag</firstterm>
<literal>MULTILINE</literal> which makes
&ulcorn;<literal>^</literal>&drcorn; also match at the beginning of each
line with a string:
</para>
<programlisting>
<emphasis># Compile the regexp so that it is anchored at the beginning
of lines</emphasis> 
&prompt;<command>r2 = compile('^..c$',MULTILINE)</command>

&prompt;<command>print r2.findall(words)</command>
['arc', 'Doc', 'Lac', 'Mac', 'Vic']
</programlisting>


<para>
In Section <xref linkend="complementation"/>, we briefly looked at the
task of matching strings which were enclosed by HTML markup. Our first
attempt is illustrated in  the following code example, where we
incorrectly match the whole string, rather than just the substring
<quote><literal><B>important</B></literal></quote>.
</para>

<programlisting>
&prompt;<command>html = '<B>important</B> and <B>urgent</B>'</command>
&prompt;<command>r2 = compile('<B>.*</B>')</command>
&prompt;<command>print r3.findall(html)</command>
['<B>important</B> and <B>urgent</B>']
</programlisting>

<para>
As we pointed out, one solution is to use a character class which
matches with the complement of <quote><literal><</literal></quote>:
</para>
<programlisting>
&prompt;<command>r4 = compile('<B>[^<]*</B>')</command>
&prompt;<command>print r4.findall(html)</command>
['<B>important</B>', '<B>urgent</B>']
</programlisting>

<para>However, there is another way of approaching this problem.
&ulcorn;<literal>&lt;B&gt;.*&lt;/B&gt;</literal>&drcorn; gets the wrong
results because the &ulcorn;<literal>*</literal>&drcorn; operator tries
to consume as much input as possible. That is, the matching is said to
be <firstterm>greedy</firstterm>. In the current case,
&ulcorn;<literal>*</literal>&drcorn; matches everything after 

</para>

</section> <!--Python Interface-->

</section><!-- Regular Expressions-->

<section> <title> Tokenization in NLTK </title>

<section>
<title>Words</title>

<para> There are a number of reasonable ways to represent words in
Python.  Perhaps the simplest is as string values, such as
<literal>'dog'</literal>; this is how words (or more correctly, word
types) are typically represented when using NLTK. </para>

<programlisting>
    &prompt;<command> words = ['the', 'cat', 'climbed', 'the', 'tree']</command>
    ['the', 'cat', 'climbed', 'the', 'tree']
</programlisting>

<para> However, NLTK also allows for other representations.  For
example, you could store words as integers, with some mapping between
integers and words. </para>
</section>
<section> <title> Types and Tokens in NLTK</title>
<!--
<indexterm><primary>word token</primary></indexterm>
<indexterm><primary>word type</primary></indexterm>
<para> The term "word" can actually be used in two different
ways: to refer to an individual occurrence of a word; or to
refer to an abstract vocabulary item.  For example, the phrase
"my dog likes his dog" contains five occurrences of words, but
only four vocabulary items (since the vocabulary item "dog"
appears twice).  In order to make this distinction clear, we
will use the term <glossterm>word token</glossterm> to refer to
occurrences of words, and the term <glossterm>word
type</glossterm> to refer to vocabulary items. </para>

<indexterm><primary>token</primary></indexterm>
<indexterm><primary>type</primary></indexterm>
<indexterm><primary>sentence token</primary></indexterm>
<indexterm><primary>sentence type</primary></indexterm>
<para> The terms <glossterm>token</glossterm> and
<glossterm>type</glossterm> can also be applied in other
domains.  For example, a <glossterm>sentence token</glossterm>
is an individual occurrence of a sentence; but a
<glossterm>sentence type</glossterm> is an abstract sentence,
without context.  If someone repeats a sentence twice, they have
uttered two sentence tokens, but only one sentence type.  When
the kind of token or type is obvious from context, we will
simply use the terms <glossterm>token</glossterm> and
<glossterm>type</glossterm>. </para>
-->

<para> In NLTK, <ulink url="&refdoc;/nltk.token.Token.html">
<literal>Token</literal>s</ulink> are constructed from their
corresponding types,
using the <ulink url="&refdoc;/nltk.token.Token.html#__init__">
<literal>Token</literal> constructor</ulink>, which is defined in the
<ulink
url="&refdoc;/nltk.token.html"><literal>nltk.token</literal></ulink>
module. The key idea is that a token of, say, a word type in a given text is an
occurrence of the type at a particular location in the text. In the next
section, we will explain how text locations are represented in NLTK.
</para>
<!--
<programlisting>
    &prompt;<command> from nltk.token import * </command>
    &prompt;<command> my_word_type = 'dog' </command>
    'dog'
    &prompt;<command> my_word_token = Token(my_word_type) </command>
    'dog'@[?]
</programlisting>

<para> The reason that the token is displayed this way will be
      explained in the next two sections. </para>
-->

</section> <!-- Tokens and Types -->

<section> <title> Text Locations </title>

<indexterm><primary>Text locations</primary></indexterm>
<indexterm><primary>start index</primary></indexterm>
<indexterm><primary>end index</primary></indexterm>
<para> <glossterm>Text locations</glossterm> specify regions of
      texts, using a <glossterm>start index</glossterm> and an
      <glossterm>end index</glossterm>.  A location with start index
      <replaceable>s</replaceable> and end index
      <replaceable>e</replaceable> is written
      <literal>@[</literal><replaceable>s</replaceable><literal>:</literal><replaceable>e</replaceable><literal>]</literal>,
      and specifies the region of the text beginning at
      <replaceable>s</replaceable>, and including everything up to
      (but not including) the text at <replaceable>e</replaceable>.
      <ulink url="&refdoc;/nltk.token.Location.html"
      ><literal>Locations</literal></ulink> are created using the
      <ulink url="&refdoc;/nltk.token.Location.html#__init__"
      ><literal>Location</literal> constructor</ulink>, which is
      defined in the <ulink url="&refdoc;/nltk.token.html"
      ><literal>nltk.token</literal></ulink> module:</para>

<programlisting>
    &prompt;<command> from nltk.token import * </command>
    &prompt;<command> my_loc = Location(1, 5) </command>
    @[1:5]
    &prompt;<command> another_loc = Location(0, 2) </command>
    @[0:2]
    &prompt;<command> yet_another_loc = Location(22, 22) </command>
    @[22:22]
</programlisting>

<para> Note that a text location does <emphasis>not</emphasis> include
the text at its end location.  This convention may seem unintuitive at
first, but it has a number of advantages.  It is consistent with
Python's slice notation (e.g., <literal>x[1:3]</literal> specifies
elements 1 and 2 of <literal>x</literal>).

<footnote><para>But unlike Python slices, text locations do
<emphasis>not</emphasis> support negative indexes.</para></footnote> 

It allows text locations to specify points between tokens, instead of
just ranges; for example, <literal>Location(3,3)</literal> specifies the
point just before the text at index 3.  And it simplifies arithmetic on
indices; for example, the length of <literal>Location(5,10)</literal> is
<literal>10-5</literal>, and two locations are contiguous if the start
of one equals the end of the other. </para>

<para> To create a text location specifying the text at a single index,
you can use the <literal>Location</literal> constructor with a single
argument.  For example, the fourth word in a text could be specified
with <literal>loc1</literal>: </para>

<programlisting>
    &prompt;<command> loc1 = Location(4) </command>     <emphasis># location width = 1</emphasis>
    @[4]
</programlisting>

<para> NLTK uses the shorthand notation
      <literal>@[<replaceable>s</replaceable>]</literal> for locations
      whose width is one.  Note that
      <literal>Location(<replaceable>s</replaceable>)</literal> is
      equivalent to <literal>Location(<replaceable>s</replaceable>,
      <replaceable>s+1</replaceable>)</literal>,
      <emphasis>not</emphasis>
      <literal>Location(<replaceable>s</replaceable>,
      <replaceable>s</replaceable>)</literal>:</para>

<programlisting>
    &prompt;<command> loc2 = Location(4, 5) </command>  <emphasis># location width = 1</emphasis>
    @[4]
    &prompt;<command> loc3 = Location(4, 4) </command>  <emphasis># location width = 0</emphasis>
    @[4:4]
</programlisting>

<section> <title> Units </title>

<indexterm><primary>units</primary></indexterm>
<para> The start and end indices can be based on a variety of
        different <glossterm>units</glossterm>, such as character
        number, word number, or sentence number.  By default, the unit
        of a text location is left unspecified, but locations can be
        explicitly tagged with information about what unit their
        indices use: </para>

<programlisting>
    &prompt;<command> my_loc = Location(1, 5, unit='w') </command>
    @[1w:5w]
    &prompt;<command> another_loc = Location(3, 72, unit='c') </command>
    @[3c:72c]
    &prompt;<command> my_loc = Location(6, unit='s') </command>
    @[6s]
    &prompt;<command> my_loc = Location(10, 11, unit='s') </command>
    @[10s]
</programlisting>

<para> Unit labels take the form of case-insensitive
        <literal>string</literal>s.  Typical examples of unit labels
        are <literal>'c'</literal> (for character number),
        <literal>'w'</literal> (for word number), and
        <literal>'s'</literal> (for sentence number). </para>

</section>  <!-- Units -->
<section> <title> Sources </title>

<indexterm><primary>source</primary></indexterm>
<para> A text location may also be tagged with a
        <glossterm>source</glossterm>, which gives an indication of
        where the text was derived from.  A typical example of a
        source would be a <literal>string</literal> containing the
        name of the file from which the element of text was
        read. </para>

<programlisting>
    &prompt;<command> my_loc = Location(1, 5, source='foo.txt') </command>
    @[1:5]@'foo.txt'
    &prompt;<command> another_loc = Location(3, 72, unit='c', source='bar.txt') </command>
    @[3c:72c]@'bar.txt'
    &prompt;<command> my_loc = Location(6, unit='s', source='baz.txt') </command>
    @[6s]@'baz.txt'
</programlisting>

<para> By default, a text location's source is
        unspecified. </para>

<para> Sometimes, it is useful to use text locations as the
        sources for other text locations.  For example, we could
        specify the third character of the fourth word of the first
        sentence in the file <literal>foo.txt</literal> with
        <literal>char_loc</literal>: </para>

<programlisting>
    &prompt;<command> sentence_loc = Location(0, unit='s', source='foo.txt') </command>
    @[0s]@'foo.txt'
    &prompt;<command> word_loc = Location(3, unit='w', source=sentence_loc) </command>
    @[3w]@[0s]@'foo.txt'
    &prompt;<command> char_loc = Location(2, unit='c', source=word_loc) </command>
    @[2c]@[3w]@[0s]@'foo.txt'
</programlisting>

<para> Note that the location indexes are zero-based, so the
        first sentence starts at an index of zero, not one.  </para>

</section>  <!-- Sources -->
</section> <!-- Text Locations -->

<section> <title> Tokens and Locations </title>

<para> As discussed above, a text token represents a single
      occurrence of a text type.  In NLTK, a token is defined by a
      type, together with a location at which that type occurs.  A
      token with type <replaceable>t</replaceable> and location
      <literal>@[<replaceable>l</replaceable>]</literal> can be
      written as
      <literal><replaceable>t</replaceable>@[<replaceable>l</replaceable>]</literal>.
      Tokens are constructed with the <ulink
      url="&refdoc;/nltk.token.Token.html#__init__"
      ><literal>Token</literal> constructor</ulink>: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token2 = Token('world', Location(1, unit='w')) </command>
    'world'@[1w]
</programlisting>

<para> Two tokens are only equal if both their type and their
      location are equal:</para>

<programlisting>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token2 = Token('hello', Location(1, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token3 = Token('world', Location(0, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token4 = Token('hello', Location(0, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token1 == token2 </command>
    0
    &prompt;<command> token1 == token3 </command>
    0
    &prompt;<command> token1 == token4 </command>
    1
</programlisting>

<para> When a token's location is unknown or unimportant, the
      special location <literal>None</literal> may be used.  A token
      with type <replaceable>t</replaceable> and location
      <literal>None</literal> is written as
      <literal><replaceable>t</replaceable>@[?]</literal>.  If a
      token's location is not specified, it defaults to
      <literal>None</literal>: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello', None) </command>
    'hello'@[?]
    &prompt;<command> token2 = Token('world') </command>
    'world'@[?]
</programlisting>

<para> A Token with a location of <literal>None</literal> is not
      considered to be equal to any other token.  In particular, even
      if two tokens have the same type, and both have a location of
      <literal>None</literal>, they are not equal: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello') </command>
    'hello'@[?]
    &prompt;<command> token2 = Token('hello') </command>
    'hello'@[?]
    &prompt;<command> token1 == token2 </command>
    0
</programlisting>

<para> To access a token's type, use its <ulink
      url="&refdoc;/nltk.token.Token.html#type"
      ><literal>type</literal></ulink> member function; and to access a
      token's location, use its <ulink
      url="&refdoc;/nltk.token.Token.html#loc"
      ><literal>loc</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token1.type()</command>
    'hello'
    &prompt;<command> token1.loc()</command>
    @[0w]
</programlisting>

<para> To access a location's start index, use its <ulink
      url="&refdoc;/nltk.token.Location.html#start"
      ><literal>start</literal></ulink> member function; and to access
      a location's end index, use its <ulink
      url="&refdoc;/nltk.token.Location.html#end"
      ><literal>end</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> loc1 = Location(0, 8, unit='w') </command>
    @[0w:8w]
    &prompt;<command> loc1.start()</command>
    0
    &prompt;<command> loc1.end()</command>
    8
</programlisting>

<para> To access a location's unit index, use its <ulink
      url="&refdoc;/nltk.token.Location.html#unit"
      ><literal>unit</literal></ulink> member function; and to access
      a location's source index, use its <ulink
      url="&refdoc;/nltk.token.Location.html#source"
      ><literal>source</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> loc1 = Location(3, unit='w', source='foo.txt') </command>
    @[3w]@'foo.txt'
    &prompt;<command> loc1.unit()</command>
    'w'
    &prompt;<command> loc1.source()</command>
    'foo.txt'
</programlisting>

<para> For more information about tokens and locations, see the
      reference documentation for the <ulink
      url="&refdoc;/nltk.token.html"
      ><literal>nltk.token</literal></ulink> module. </para>

</section>  <!-- Tokens and Locations -->


<section> <title> Texts </title>

<para> Many natural language processing tasks involve analyzing
    texts of varying sizes, ranging from single sentences to very
    large corpora.  There are a number of ways to represent texts
    using NLTK.  The simplest is as a single
    <literal>string</literal>.  These strings are typically loaded
    from files: </para>

<programlisting>
    &prompt;<command> text_str = open('corpus.txt').read() </command>
    'Hello world.  This is a test file.\n'
</programlisting>

<indexterm><primary>tokenizer</primary></indexterm>
<para> It is often more convenient to represent a text as a
    <literal>list</literal> of <literal>Token</literal>s.  These
    <literal>list</literal>s are typically created using a
    <glossterm>tokenizer</glossterm>, such as <ulink
    url="&refdoc;/nltk.token.WSTokenizer.html"
    ><literal>WSTokenizer</literal></ulink> (which splits words apart
    based on whitespace): </para>

<programlisting>
    &prompt;<command> text_tok_list = WSTokenizer().tokenize(text_str) </command>
    ['Hello'@[0w], 'world.'@[1w], 'This'@[2w], 'is'@[3w], 
     'a'@[4w], 'test'@[5w], 'file.'@[6w]]
</programlisting>

<para> Texts can also be represented as sets of word tokens or
    sets of word types: </para>

<programlisting>
    &prompt;<command> text_tok_set = Set(*text_tok_list) </command>
    {'This'@[2w], 'a'@[4w], 'Hello'@[0w], 'world.'@[1w], 
     'is'@[3w], 'file.'@[6w], 'test'@[5w]}
</programlisting>

<note> <para> For example, this representation might be convenient for a
search engine which is trying to find all documents containing some
keyword. (For more information on why a "<literal>*</literal>" is used
in the call to the Set constructor, see the Set tutorial.)  <!-- WRITE
THE SET TUTORIAL!! -->
</para> </note>

</section> <!-- Texts -->

<section> <title>NLTK Tokenizers </title>

<!-- As mentioned in the previous section, it is often useful to
represent a text as a list of tokens.  The process of breaking a text up
into its constituent tokens is known as
<glossterm>tokenization</glossterm> -->

<para> As we saw in Section <xref
	linkend="tokenization"/>, tokenization involves breaking a text
up into its constituent tokens prior to further processing. Tokenization
in NLTK can occur at a number of different levels: a text could be
broken up into paragraphs, sentences, words, syllables, or phonemes. And
for any given level of tokenization, there are many different algorithms
for breaking up the text.  For example, at the word level, it is not
immediately clear how to treat such strings as <literal>can't</literal>
<literal>$22.50</literal> <literal>New York</literal> and
<literal>so-called</literal>.
</para>

<para> NLTK defines a general interface for tokenizing texts, the
    <ulink url="&refdoc;/nltk.token.TokenizerI.html"
    ><literal>TokenizerI</literal></ulink> class.  This interface is
    used by all tokenizers, regardless of what level they tokenize at
    or what algorithm they use.  It defines a single method, <ulink
    url="&refdoc;/nltk.token.TokenizerI.html#tokenize"
    ><literal>tokenize</literal></ulink>, which takes a
    <literal>string</literal>, and returns a list of
    <literal>Token</literal>s.  </para>

<section> <title>NLTK Interfaces</title>

<para> <literal>TokenizerI</literal> is the first "interface"
      class we've encountered; at this point, we'll take a short
      digression to explain how interfaces are implemented in
      NLTK. </para>

<indexterm><primary>interface</primary></indexterm>
<para> An <glossterm>interface</glossterm> gives a partial
      specification of the behavior of a class, including
      specifications for methods that the class should implement.  For
      example, a "comparable" interface might specify that a class
      must implement a comparison method.  Interfaces do not give a
      complete specification of a class; they only specify a minimum
      set of methods and behaviors which should be implemented by the
      class.  For example, the <literal>TokenizerI</literal> interface
      specifies that a tokenizer class must implement a
      <literal>tokenize</literal> method, which takes a
      <literal>string</literal>, and returns a list of
      <literal>Token</literal>s; but it does not specify what other
      methods the class should implement (if any).  </para>

<para> The notion of "interfaces" can be very useful in ensuring
      that different classes work together correctly.  Although the
      concept of "interfaces" is supported in many languages, such as
      Java, there is no native support for interfaces in
      Python. </para>

<para> NLTK therefore implements interfaces using classes, all
      of whose methods raise the
      <literal>NotImplementedError</literal> exception.  To
      distinguish interfaces from other classes, they are always named
      with a trailing "<literal>I</literal>".  If a class implements
      an interface, then it should be a subclass of the interface.
      For example, the <literal>WSTokenizer</literal> class implements
      the <literal>TokenizerI</literal> interface, and so it is a
      subclass of <literal>TokenizerI</literal>.  </para>

</section> <!-- NLTK Interfaces -->

<section> <title> The whitespace tokenizer </title> 

<para> A simple example of a tokenizer is the
      <ulink url="&refdoc;/nltk.token.WSTokenizer.html"
      ><literal>WSTokenizer</literal></ulink>, which breaks a text into
      words, assuming that words are separated by whitespace (space,
      enter, and tab characters).  We can use the
      <ulink url="&refdoc;/nltk.token.WSTokenizer.html#__init__"
      ><literal>WSTokenizer</literal> constructor</ulink> to build a
      new whitespace tokenizer: </para>

<programlisting>
    &prompt;<command> tokenizer = WSTokenizer() </command>
</programlisting>

<para> Once we have built the tokenizer, we can use it to
      process texts: </para>

<programlisting>
    &prompt;<command> tokenizer.tokenize(text_str) </command>
    ['Hello'@[0w], 'world.'@[1w], 'This'@[2w], 'is'@[3w], 
     'a'@[4w], 'test'@[5w], 'file.'@[6w]]
</programlisting>

<para> However, this tokenizer is not ideal for many tasks.  For
      example, we might want punctuation to be included as separate
      tokens; or we might want names like "New York" to be included as
      single tokens. </para>

</section> <!-- Whitespace tokenizer -->
<section> <title> The regular expression tokenizer </title>

<para> The <literal>RETokenizer</literal> is a more powerful
      tokenizer, which uses a regular expression to determine how text
      should be split up.  This regular expression specifies the
      format of a valid word.  For example, if we wanted to mimic the
      behavior or <literal>WSTokenizer</literal>, we could define the
      following <literal>RETokenizer</literal>: </para>

<programlisting>
    &prompt;<command> tokenizer = RETokenizer(r'[^\s]+') </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['Hello.'@[0w], "Isn't"@[1w], 'this'@[2w], 'fun?'@[3w]]
</programlisting>

<para> (The regular expression <literal>\s</literal> matches any
      whitespace character.) </para>

<para> To define a tokenizer that includes punctuation as
      separate tokens, we could use: </para>

<programlisting>
    &prompt;<command> regexp = r'\w+|[^\w\s]+'</command>
    '\w+|[^\w\s]+'
    &prompt;<command> tokenizer = RETokenizer(regexp) </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['Hello'@[0w], '.'@[1w], 'Isn'@[2w], "'"@[3w], 't'@[4w], 
     'this'@[5w], 'fun'@[6w], '?'@[7w]]
</programlisting>

<para> The regular expression in this example will match
      <emphasis>either</emphasis> a sequence of alphanumeric
      characters (letters and numbers); <emphasis>or</emphasis> a
      sequence of punctuation characters. </para>

<para> There are a number of ways we might want to improve this
      regular expression.  For example, it currently breaks the string
      "$22.50" into four tokens; but we might want it to include this
      as a single token.  One approach to making this change would be
      to add a new clause to the tokenizer's regular expression, which
      is specialized for handling strings of this form: </para>

<programlisting>
    &prompt;<command> example_text = 'That poster costs $22.40.'</command>
    &prompt;<command> regexp = r'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'</command>
    '(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
    &prompt;<command> tokenizer = RETokenizer(regexp) </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['That'@[0w], 'poster'@[1w], 'costs'@[2w], '$22.40'@[3w], '.'@[4w]]
</programlisting>

<para> Of course, more general solutions to this problem are
      also possible, using different regular expressions. </para>
</section> <!-- Regexp tokenizer -->

</section> <!-- Tokenization -->

<section> <title> Example: Processing Tokenized Text </title>

<para> In this section, we show how you can use NLTK to
    examine the distribution of word lengths in a document.  This is
    meant to be a simple example of how the tools we have introduced
    can be used to solve a simple NLP problem.  The distribution of
    word lengths in a document can give clues to other properties,
    such as the document's style, or the document's language. </para>

<para> We present three different approaches to solving this
    problem; each one illustrates different techniques, which might be
    useful for other problems. </para>

<section> <title> Word Length Distributions 1: Using a List </title>

<para> To begin with, we'll need to extract the words from a
      corpus that we wish to test.  We'll use the
      <literal>WSTokenizer</literal> to tokenize the corpus: </para>

<programlisting>
    &prompt;<command> corpus = open('corpus.txt').read() </command>
    &prompt;<command> tokens = WSTokenizer().tokenize(corpus) </command>
</programlisting>

<para> Now, we will construct a list
      <literal>wordlen_count_list</literal>, which gives the number of
      words that have a given length.  In particular,
      <literal>wordlen_count_list[<replaceable>i</replaceable>]</literal>
      is the number of words whose length is
      <replaceable>i</replaceable>. </para>

<para> When constructing this list, we must be careful not to
      try to add a value past the end of the list.  Therefore,
      whenever we encounter a word that is longer than any previous
      words, we will add enough zeros to
      <literal>wordlen_count_list</literal> that we can store the
      occurrence of the new word:

<programlisting>
    &prompt;<command> wordlen_count_list = []</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     wordlen = len(token.type())</command>
    &prompt2;     <emphasis># Add zeros until wordlen_count_list is long enough</emphasis>
    &prompt2;<command>     while wordlen >= len(wordlen_count_list):</command>
    &prompt2;<command>         wordlen_count_list.append(0)</command>
    &prompt2;     <emphasis># Increment the count for this word length</emphasis>
    &prompt2;<command>     wordlen_count_list[wordlen] += 1</command>
</programlisting>
</para>

<para> We can plot the results, using the <ulink
      url="&refdoc;/nltk.draw.plot.Plot.html"
      ><literal>Plot</literal></ulink> class, defined in the
      <ulink url="&refdoc;/nltk.draw.plot.html"
      ><literal>nltk.draw.plot</literal></ulink> module: </para>

<programlisting>
    &prompt;<command> Plot(wordlen_count_list)</command>
</programlisting>

<note> <para> We are currently using a fairly simple class to
      plot functions.  We will likely replace it with a more advanced
      plotting system in the future. </para> </note>

<para> The complete code for this example, including necessary
      <literal>import</literal> statements, is: </para>

<programlisting>
from nltk.token import WSTokenizer
from nltk.draw.plot import Plot

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Count up how many times each word length occurs</emphasis>
wordlen_count_list = []
for token in tokens:
     wordlen = len(token.type())
     <emphasis># Add zeros until wordlen_count_list is long enough</emphasis>
     while wordlen >= len(wordlen_count_list):
         wordlen_count_list.append(0)
     <emphasis># Increment the count for this word length</emphasis>
     wordlen_count_list[wordlen] += 1

Plot(wordlen_count_list)
</programlisting>

</section> <!-- Word length distributions: list -->

<section> <title> Word Length Distributions 2: Using a Dictionary </title>

<para> We have been examining the function from word lengths to
      token counts.  In this example, the range of the function (i.e.,
      the set of word lengths) is ordered and relatively small.
      However, we often wish to examine functions whose ranges are not
      so well behaved.  In such cases, dictionaries can be a powerful
      tool.  The following code uses a dictionary to count up the
      number of times each word length occurs: </para>

<programlisting>
    &prompt;<command> wordlen_count_dict = {}</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     word_length = len(token.type())</command>
    &prompt2;<command>     if wordlen_count_dict.has_key(word_length):</command>
    &prompt2;<command>         wordlen_count_dict[word_length] += 1</command>
    &prompt2;<command>     else:</command>
    &prompt2;<command>         wordlen_count_dict[word_length] = 1</command>
</programlisting>

<para> To plot the results, we can use a list of (wordlen,
      count) pairs.  This is simply the <literal>items</literal> of
      the dictionary: </para>

<programlisting>
    &prompt;<command> points = wordlen_count_dict.items() </command>
    &prompt;<command> Plot(points)</command>
</programlisting>

<para> The complete code for this example, including necessary
      <literal>import</literal> statements, is: </para>

<programlisting>
from nltk.token import WSTokenizer
from nltk.draw.plot import Plot

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Construct a dictionary mapping word lengths to token counts</emphasis>
wordlen_count_dict = {}
for token in tokens:
    word_length = len(token.type())
    if wordlen_count_dict.has_key(word_length):
        wordlen_count_dict[word_length] += 1
    else:
        wordlen_count_dict[word_length] = 1

<emphasis># Construct a list of (wordlen, count) and plot the results.</emphasis>
points = wordlen_count_dict.items() 
Plot(points)
</programlisting>

</section> <!-- Word length distributions: dictionary -->

<section> <title> Word Length Distributions 3: Using a Frequency Distribution </title>

<para> The <ulink url="&refdoc;/nltk.probability.html"
      ><literal>nltk.probability</literal></ulink> module defines two
      interfaces, <ulink
      url="&refdoc;/nltk.probability.FreqDistI.html"
      ><literal>FreqDistI</literal></ulink> and <ulink
      url="&refdoc;/nltk.probability.ProbDistI.html"
      ><literal>ProbDistI</literal></ulink>, for modeling frequency
      distributions and probability distributions, respectively.  In
      this example, we use a frequency distribution to find the
      relationship between word lengths and token counts. </para>

<para> We will use a <ulink
      url="&refdoc;/nltk.probability.SimpleFreqDist.html"
      ><literal>SimpleFreqDist</literal></ulink>, which is a simple
      (but sometimes inefficient) implementation of the
      <literal>FreqDistI</literal> interface.  For this example, three
      methods of <literal>SimpleFreqDist</literal> are
      relevant:</para>

<itemizedlist>
<listitem><para><ulink url="&refdoc;/nltk.probability.SimpleFreqDist.html#inc"><literal>inc(<replaceable>sample</replaceable>)</literal></ulink>
          increments the frequency of a given sample.</para>
</listitem>
<listitem><para><ulink url="&refdoc;/nltk.probability.SimpleFreqDist.html#samples"><literal>samples()</literal></ulink> returns a list of
          the samples covered by a frequency distribution.</para>
</listitem>
<listitem><para><ulink url="&refdoc;/nltk.probability.SimpleFreqDist.html#count"><literal>count(<replaceable>sample</replaceable>)</literal></ulink>
          returns the number of times a given sample occurred.</para>
</listitem>
</itemizedlist>

<para> First, we construct the frequency distribution for the
      word lengths: </para>

<programlisting>
    &prompt;<command> wordlen_freqs = SimpleFreqDist()</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     wordlen_freqs.inc(len(token.type()))</command>
</programlisting>

<para> Next, we extract the set of word lengths that were found
      in the corpus: </para>

<programlisting>
    &prompt;<command> wordlens = wordlen_freqs.samples()</command>
</programlisting>

<para> Finally, we construct a list of (wordlen, count) pairs,
      and plot it: </para>

<programlisting>
    &prompt;<command> points = [(wordlen, wordlen_freqs.count(wordlen))</command>
    &prompt2;<command>           for wordlen in wordlens]</command>
    &prompt;<command> Plot(points)</command>
</programlisting>

<note> <para> The expression
      <literal>[...&nbsp;for&nbsp;...&nbsp;in&nbsp;...]</literal> is
      called a "list comprehension."  For more information on list
      comprehensions, see the <ulink
      url="&tutdoc;/advpython/t1.html">"New Python Features"
      tutorial</ulink>. </para>
</note>

<para> The complete code for this example, including necessary
      <literal>import</literal> statements, is: </para>

<programlisting>
from nltk.token import WSTokenizer
from nltk.draw.plot import Plot
from nltk.probability import SimpleFreqDist

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Construct a frequency distribution of word lengths</emphasis>
wordlen_freqs = SimpleFreqDist()
for token in tokens:
    wordlen_freqs.inc(len(token.type()))

<emphasis># Exctract the set of word lengths found in the corpus</emphasis>
wordlens = wordlen_freqs.samples()

<emphasis># Construct a list of (wordlen, count) and plot the results.</emphasis>
points = [(wordlen, wordlen_freqs.count(wordlen))
          for wordlen in wordlens]
Plot(points)
</programlisting>

<para> For more information about frequency distributions, see
      the Probability Tutorial.</para>

</section> <!-- Word length distributions: freqdist -->
</section> <!-- Word length distributions example -->

</section>  <!-- Tokenization in NLTK -->

&index;
</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
adaptive-fill-mode:nil
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:nil
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/home/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

