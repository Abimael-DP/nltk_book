<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.1//EN" [
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<!-- Memo to self: add xrefs.  
Targets like: <section id="foo">
Refs like: <xref linkend="foo"></>
-->

<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Tagging</title>
  </articleinfo>

  <section> <title> Introduction </title>

    <para> When processing a text, it is often useful to associate
    auxiliary information with each token.  For example, we might
    want to label each token with its part of speech; or we might want
    to disambiguate homonyms by associating them with "word sense"
    labels.  This kind of auxiliary information is typically used in
    later stages of text processing.  For example, part of speech
    labels could be used to derive the internal structure of a
    sentence; and "word sense" labels could be used to allow a
    question-answering system to distinguish homonyms. </para>

    <para> The process of associating labels with each token in a text
    is called <glossterm>tagging</glossterm>, and the labels are
    called <glossterm>tags</glossterm>.  The collection of tags used
    for a particular task is known as a <glossterm>tag
    set</glossterm>. </para>
   
    <para> <glossterm>Part-of-speech tagging</glossterm> is the most
    common example of tagging, and it is the example we will examine
    in this tutorial.  But you should keep in mind that most of the
    techniques we discuss here can also be applied to many other
    tagging problems. </para>

  </section> <!-- Intro -->

  <section> <title> Part-of-Speech Tagging </title>

    <para> Part-of-speech tags divide words into categories, based on
    how they can be combined to form sentences.  For example,
    articles can combine with nouns, but not verbs.  Part-of-speech
    tags also give information about the semantic content of a word.
    For example, nouns typically express "things," and prepositions
    express relationships between "things." </para>

    <para> Most part-of-speech tag sets make use of the same basic
    categories, such as "noun," "verb," "adjective," and
    "preposition."  However, tag sets differ both in how finely they
    divide words into categories; and in how define their categories.
    For example, "is" might be tagged as a verb in one tag set; but as
    a form of "to be" in another tag set.  This variation in tag sets
    is reasonable, since part-of-speech tags are used in different
    ways for different tasks. </para>

    <!-- How do I make "Table1" not use a hard-coded number?? -->
    <para> In this tutorial, we will use the tag set listed in Table
    1.  This tag set is a simplification of the commonly used
    <glossterm>Brown Corpus tag set</glossterm>.  The complete Brown
    Corpus tag set has 87 basic tags.  For more information on tag
    sets, see <citetitle>Foundations of Statistical Natural Language
    Processing</citetitle>
    (<author><surname>Manning</surname></author> &amp;
    <author><surname>Schutze</surname></author>), pp. 139-145. </para>

    <table id="table.tagset"> <title>Tag Set</title> 
      <tgroup cols="2"><tbody>
          <row>
            <entry>AT</entry>
            <entry>Article</entry>
          </row>
          <row>
            <entry>NN</entry>
            <entry>Noun</entry>
          </row>
          <row>
            <entry>VB</entry>
            <entry>Verb</entry>
          </row>
          <row>
            <entry>JJ</entry>
            <entry>Adjective</entry>
          </row>
          <row>
            <entry>IN</entry>
            <entry>Preposition</entry>
          </row>
          <row>
            <entry>CD</entry>
            <entry>Number</entry>
          </row>
          <row>
            <entry>END</entry>
            <entry>Sentence-ending punctuation</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

  </section> <!-- Part-of-Speech Tagging -->

  <section> <title> The nltk.tagger Module </title>

    <para> The <ulink
    url="http://nltk.sourceforge.net/ref/nltk.tagger.html">
    <literal>nltk.tagger</literal></ulink> module defines the classes
    and interfaces used by NLTK to perform tagging.  </para>

    <section> <title> TaggedType </title>

      <para> NLTK defines a simple class, <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.TaggedType.html">
      <literal>TaggedType</literal></ulink>, for representing the
      types of tagged tokens.  A <literal>TaggedType</literal>
      consists of a <glossterm>base type</glossterm> and a
      <glossterm>tag</glossterm>.  Typically, the base type and the
      tag will both be strings.  For example, the tagged type for the
      noun "dog" would have the base type <literal>'dog'</literal> and
      the tag <literal>'NN'</literal>.  A tagged type with base type
      <replaceable>b</replaceable> and tag
      <replaceable>t</replaceable> is written
      <replaceable>b</replaceable><literal>/</literal><replaceable>t</replaceable>.
      Tagged types are created with the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.TaggedType.html#__init__">
      <literal>TaggedType</literal></ulink> constructor: </para>

<programlisting>
    &prompt;<command> ttype1 = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttype2 = TaggedType('runs', 'VB') </command>
    'runs'/'VB'
</programlisting>

      <para> A <literal>TaggedType</literal>'s base type is accessed
      via the
      <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.TaggedType.html#base">
      <literal>base</literal></ulink> member function; and its tag is
      accessed via the
      <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.TaggedType.html#tag">
      <literal>tag</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> ttype1.base() </command>
    'dog'
    &prompt;<command> ttype2.tag() </command>
    'VB'
</programlisting>

      <para> To construct a tagged token, simply use the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.token.Token.html#__init__">
      <literal>Token</literal></ulink> constructor with a
      <literal>TaggedType</literal>: </para>

<programlisting>
    &prompt;<command> ttype = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttoken = Token(ttype, Location(5)) </command>
    'dog'/'NN'@[5]
</programlisting>

    </section> <!-- TaggedType -->

    <section> <title> Reading Tagged Corpora </title>

      <para> Several large corpora (such as the Brown Corpus and
      portions of the Wall Street Journal) have been manually tagged
      with part-of-speech tags.  These corpora are primarily useful
      for testing taggers and for training statistical taggers.
      However, before we can use these corpora, we must read them from
      files and tokenize them. </para>

      <para> Tagged texts are usually stored in files as a sequences
      of whitespace-separated tokens, where each token is of the form
      <replaceable>base</replaceable><literal>/</literal><replaceable>tag</replaceable>.
      Figure 1 shows an example of some tagged text, taken from the
      Brown corpus.  To tokenize tagged texts of this form, the
      <literal>nltk.tagger</literal> module defines the
      <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.TaggedTokenizer.html">
      <literal>TaggedTokenizer</literal></ulink> class: </para>

<programlisting>
    &prompt;<command> text_str = open('corpus.txt').read() </command>
    'John/NN saw/VB the/AT book/NN on/IN the/AT 
     table/NN ./END  He/NN sighed/VB ./END'
    &prompt;<command> tokens = TaggedTokenizer().tokenize(text_str) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

      <para> If <literal>TaggedTokenizer</literal> encounters a word without
      a tag, it will assign it the default tag
      <literal>'UNK'</literal> (for "unknown"). </para>

      <figure> <title> An Example of Tagged Text (excerpted from the Brown Corpus) </title>
        <titleabbrev>Example of Tagged Text</titleabbrev>
<screen>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt
it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb
generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in
the/at best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.
</screen>
      </figure>

    </section> <!-- TaggedTokenizer -->

    <section> <title> The TaggerI Interface </title>

      <para> The <literal>nltk.tagger</literal> module defines <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.TaggerI.html">
      <literal>TaggerI</literal></ulink>, a general interface for
      tagging texts.  This interface is used by all taggers.  It
      defines a single method, <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.TaggerI.html#tag">
      <literal>tag</literal></ulink>, which assigns a tag to each
      token in a list, and returns the resulting list of tagged
      tokens.</para>

      <!-- I'm using my_tagger without defining it.  Is that ok?  It
      means that if someone's following along, and typing everything
      in, this listing won't work.  But there's no reasonable way to
      define it at this point..  So I won't worry about it for now -->
<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(text_str) </command>
    ['John'@[0w], 'saw'@[1w], 'the'@[2w], 'book'@[3w], 
     'on'@[4w], 'the'@[5w], 'table'@[6w], '.'@[7w], 
     'He'@[8w], 'sighed'@[9w], '.'@[10w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

    </section> <!-- TaggerI -->

  </section> <!-- The nltk.tagger module -->

  <section> <title> Taggers </title>

    <para> The <literal>nltk.tagger</literal> module currently defines
    four taggers; this list will likely grow in the future.  This
    section describes the taggers currently implemented by
    <literal>nltk.tagger</literal>, and how they are used. </para>
    
    <section> <title> A Default Tagger </title>

      <para> The simplest tagger defined by
      <literal>nltk.tagger</literal> is <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.NN_CD_Tagger.html">
      <literal>NN_CD_Tagger</literal></ulink>.  This tagger assigns a
      tag to each token on the basis of its type.  If its type appears
      to be a number, it assigns the type "CD."  Otherwise, it assigns
      the type "NN." </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(text_str) </command>
    ['John'@[0w], 'saw'@[1w], '3'@[2w], 
     'polar'@[3w], 'bears'@[4w], '.'@[5w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'NN'@[1w], '3'/'CD'@[2w], 
     'polar'/'NN'@[3w], 'bears'/'NN'@[4w], '.'/'NN'@[5w]]
</programlisting>

      <para> This is a simple algorithm, but it yields quite poor
      performance when used by itself.  On a typical corpus, it will
      tag only 20%-30% of the tokens correctly.  However, it is a very
      reasonable tagger to use as a default, if a more advanced tagger
      fails to determine a token's tag.  When used in conjunction with
      other taggers, <literal>NN_CD_Tagger</literal> can significantly
      improve performance. </para>
      
    </section> <!-- NN_CD_Tagger -->

    <section> <title> Unigram Tagging </title>

      <para> The <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.UnigramTagger.html">
      <literal>UnigramTagger</literal></ulink> class implements a
      simple statistical tagging algorithm: for each token, it assigns
      the tag that is most likely for that token's type.  For example,
      it will assign the tag "JJ" to any occurrence of the word
      "frequent," since "frequent" is used as an adjective (e.g. "a
      frequent word") more often than it is used as a verb (e.g. "I
      frequent this cafe"). </para>

      <para> Before a <literal>UnigramTagger</literal> can be used to
      tag data, it must be trained on a <glossterm>training
      corpus</glossterm>.  It uses this corpus to determine which tags
      are most common for each word.
      <literal>UnigramTaggers</literal> are trained using the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.UnigramTagger.html#train">
      <literal>train</literal></ulink> method: </para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize('train.txt')</command>
    &prompt;<command> tagger = UnigramTagger()</command>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once a <literal>UnigramTagger</literal> has been trained,
      it can be used to tag other corpora: </para>

<programlisting>
    &prompt;<command> tokens = TaggedTokenizer().tokenize('corpus.txt')</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>UnigramTagger</literal> will assign the tag
      <literal>'UNK'</literal> (unknown) to any token whose type was
      not encountered in the training data. </para>

      <para> Note that, like almost all statistical taggers, the
      performance of <literal>UnigramTagger</literal> is highly
      dependent on the quality of its training set.  In particular, if
      the training set is too small, it will not be able to reliably
      estimate the most likely tag for each word.  Performance will
      also suffer if the training set is significantly different than
      the texts we wish to tag. </para>
      
    </section> <!-- UnigramTagger -->

    <section> <title> Nth Order Tagging </title>

      <para> The <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.NthOrderTagger.html">
      <literal>NthOrderTagger</literal></ulink> class implements a
      more advanced statistical tagging algorithm.  In addition to
      considering the token's type, it also considers the
      part-of-speech tags of the <replaceable>n</replaceable> preceding
      tokens. </para>

      <para> To decide which tag to assign to a token,
      <literal>NthOrderTagger</literal> first constructs a
      <glossterm>context</glossterm> for the token.  This context
      consists of the token's type, along with the part-of-speech tags
      of the <replaceable>n</replaceable> preceding tags.  It then
      picks the tag which is most likely for that context.  Note that
      a 0th order tagger is equivalent to a unigram tagger, since a
      0th context is just a token's type.  1st order taggers are
      sometimes called <glossterm>bigram taggers</glossterm>, and 2nd
      order taggers are called <glossterm>trigram taggers</glossterm>.
      </para>

      <para> <literal>NthOrderTagger</literal> uses a training corpus
      to determine which part-of-speech tag is most likely for each
      context:</para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize('train.txt')</command>
    &prompt;<command> tagger = NthOrderTagger(3)</command>         <emphasis># 3rd order tagger</emphasis>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once an <literal>NthOrderTagger</literal> has been trained,
      it can be used to tag other corpora: </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize('corpus.txt')</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>NthOrderTagger</literal> will assign the tag
      <literal>'UNK'</literal> (unknown) to any token whose context was
      not encountered in the training data. </para>

      <para> Note that as <replaceable>n</replaceable> gets larger,
      the specificity of the contexts increases; and with it, the
      chance that the data we wish to tag will contain contexts that
      were not present in the training data.  Thus, there is a
      trade-off between the accuracy and the coverage of our results.
      This is a common type of trade-off in natural language
      processing.  It is closely related to the
      <glossterm>precision/recall trade-off</glossterm> that we'll
      encounter later when we discuss information retrieval. </para>

    </section> <!-- NthOrderTagger -->

    <section> <title> Combining Taggers </title>

      <para> One way to address the trade-off between accuracy and
      coverage is to use the more accurate algorithms when we can, but
      to fall back on algorithms with wider coverage when necessary.
      For example, we could combine the results of a 1st order tagger,
      a 0th order tagger, and an <literal>NN_CD_Tagger</literal>, as
      follows:</para>

      <orderedlist>
        <listitem> 
          <para> Try tagging the token with the 1st order
          tagger. </para>
        </listitem>
        <listitem> 
          <para> If the 1st order tagger is unable to find a tag for
          the token, try finding a tag with the 0th order
          tagger. </para>
        </listitem> 
        <listitem> 
          <para> If the 0th order tagger is also unable to find a tag,
          use the <literal>NN_CD_Tagger</literal> to find a tag. </para>
        </listitem>
      </orderedlist>

      <para> NLTK defines the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tagger.BackoffTagger.html">
      <literal>BackoffTagger</literal></ulink> class for combining
      taggers in this way.  A <literal>BackoffTagger</literal> is
      constructed from an ordered list of one or more
      <glossterm>subtaggers</glossterm>.  For each token in the input,
      the <literal>BackoffTagger</literal> uses the result of the
      first tagger in the list that successfully found a tag.  Taggers
      indicate that they are unable to tag a token by assigning it the
      special token <literal>'UNK'</literal> (for "unknown").  We can
      use a <literal>BackoffTagger</literal> to implement the strategy
      proposed above: </para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize('train.txt')</command>

    <emphasis># Construct the taggers</emphasis>
    &prompt;<command> tagger1 = NthOrderTagger(1)</command>         <emphasis># 1st order tagger</emphasis>
    &prompt;<command> tagger2 = UnigramTagger()</command>           <emphasis># 0th order tagger</emphasis>
    &prompt;<command> tagger3 = NN_CD_Tagger()</command>

    <emphasis># Train the taggers</emphasis>
    &prompt;<command> tagger1.train(train_toks)</command>
    &prompt;<command> tagger2.train(train_toks)</command>

    <emphasis># Combine the taggers</emphasis>
    &prompt;<command> tagger = BackoffTagger([tagger1, tagger2, tagger3])</command>
</programlisting>

      <para> We can then use the combined tagger to tag new corpora:
      </para>

<programlisting>
    &prompt;<command> tokens = TaggedTokenizer().tokenize('corpus.txt')</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>
      
    </section> <!-- Combining Tagger -->

  </section> <!-- Taggers -->

  <section> <title> Tagging: A Closer Look </title>

    <para> In the next four sections, we will discuss how each of the
    taggers introduced in the previous section are implemented.  This
    discussion serves several purposes: </para>

    <itemizedlist>
      <listitem>
        <para> It demonstrates how to write classes implementing the
        interfaces defined by NLTK. </para>
      </listitem>
      <listitem>
        <para> It provides you with a better understanding of the
        algorithms and data structures underlying each approach to
        tagging. </para>
      </listitem>
      <listitem>
        <para> It gives you a chance to see some of the code used to
        implement NLTK.  We have tried very hard to ensure that the
        implementation of every class in NLTK is easy to understand.
        </para>
      </listitem>
    </itemizedlist>

    <para> Before you read this section, you may wish to read the
    tutorial "<ulink
    url="http://nltk.sourceforge.net/tutorial/writing_classes/t1.html">
    Writing Classes For NLTK"</ulink>, which describes how to create
    classes that interface with the toolkit. </para>

  </section> <!-- Tagging: A Closer Look -->

  <section> <title> NN_CD_Tagger </title>

    <para> <literal>NN_CD_Tagger</literal> assigns the tag "CD" to any
    token whose type appears to be a number; and "NN" to any other
    token.  It uses a simple regular expression to test whether a
    token's type is a number:</para>

<programlisting>
    r'^[0-9]+(.[0-9]+)?$'
</programlisting>

    <para> This regular expression matches one or more digits, followed
    by an optional period and one or more digits (e.g.,
    "<literal>12</literal>" or "<literal>732.42</literal>").  Note the
    use of "<literal>^</literal>" (which matches the beginning of a
    string) and "<literal>$</literal>" (which matches the end of a
    string) to ensure that the regular expression will only match
    complete token types. </para>

    <para> The <ulink
    url="http://nltk.sourceforge.net/ref/nltk.tagger.NN_CD_Tagger.html#tag">
    <literal>NN_CD_Tagger.tag</literal></ulink> method simply loops
    through each token in the list of untagged tokens.  For each
    untagged token, it uses the regular expression described above to
    determine which tag to use, constructs a new tagged token, and
    appends it to a list of tagged tokens.  After looping through all
    of the untagged tokens, it returns the newly constructed list of
    tagged tokens. </para>

<programlisting>
class NN_CD_Tagger:
    <emphasis># ...</emphasis>
    def tag(self, tokens):
        tagged_tokens = []

        for token in tokens:
            base_type = token.type()
            if re.match(r'^[0-9]+(.[0-9]+)?$', base_type):
                tag = TaggedType(base_type, 'CD')
            else:
                tag = TaggedType(base_type, 'NN')
            tagged_tokens.append(Token(tag, token.loc()))

        return tagged_tokens
</programlisting>

    <para> Since <literal>NN_CD_Tagger</literal>s are stateless, and
    have no customization parameters, the <ulink
    url="http://nltk.sourceforge.net/ref/nltk.tagger.NN_CD_Tagger.html#__init__">
    <literal>NN_CD_Tagger constructor</literal></ulink> is empty:
    </para>

<programlisting>
class NN_CD_Tagger:
    <emphasis># ...</emphasis>
    def __init__(self): pass
</programlisting>

    <para> The complete listing for the
    <literal>NN_CD_Tagger</literal> class is:</para>

    <figure><title>The NN_CD_Tagger Implementation</title>
<programlisting> 
<emphasis><command>class NN_CD_Tagger(TaggerI):</command></emphasis>
    """
    A \"default\" tagger, which will assign the tag C{CD} to numbers,
    and C{NN} to anything else.  This tagger expects a list of
    C{strings}s as its inputs.
    """
<emphasis><command>    def __init__(self): </command></emphasis>
        <emphasis># Inherit documentation string from the TaggerI interface.</emphasis>
<command>        pass</command>
<command>    </command>
<emphasis><command>    def tag(self, tokens):</command></emphasis>
        <emphasis># Inherit documentation string from the TaggerI interface.</emphasis>
<command>        tagged_tokens = []</command>
<command></command>
<command>        for token in tokens:</command>
<command>            base_type = token.type()</command>
<command>            if re.match(r'^[0-9]+(.[0-9]+)?$', base_type):</command>
<command>                tag = TaggedType(base_type, 'CD')</command>
<command>            else:</command>
<command>                tag = TaggedType(base_type, 'NN')</command>
<command>            tagged_tokens.append(Token(tag, token.loc()))</command>
<command></command>
<command>        return tagged_tokens</command>
</programlisting>
    </figure>

  </section> <!-- NN_CD_Tagger -->

  <section> <title> UnigramTagger </title>

    <para> <literal>UnigramTagger</literal> tags tokens by assigning
    the tag that is most likely to go with each token's type.  It uses
    a frequency distribution to model the likelihood of tags, given
    tokens.  This frequency distribution is constructed from a
    training corpus, using the <ulink
    url="http://nltk.sourceforge.net/ref/nltk.tagger.UnigramTagger.html#train">
    <literal>train</literal></ulink> method. </para>

    <section><title> Features and Contexts </title>

      <para> Unigram tagging is a "prediction problem."  A
      <glossterm>prediction problem</glossterm> is one in which
      we try to make predictions about the outcome of a process.  It
      is instructive to think about prediction problems in terms of
      "features" and "contexts."  A <glossterm>feature</glossterm> is
      an aspect of the outcome that we wish to predict; and a
      <glossterm>context</glossterm> is an aspect of the outcome that
      we can base our prediction on.  In the case of the unigram
      tagger, the feature we wish to predict is the token's tag, and
      the context that we will base our prediction on is the token's
      base type. </para>

    </section> <!-- Features and Contexts -->

    <section> <title> Context/Feature Samples </title>

      <para> NLTK defines the class <ulink
      url="http://nltk.sourceforge.net/ref/nltk.probability.CFSample.html">
      <literal>CFSample</literal></ulink> for representing samples in
      terms of their features and contexts.  Context/feature samples
      are constructed with the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.probability.CFSample.html#__init__">
      <literal>CFSample constructor</literal></ulink>:</para>

<programlisting>
    &prompt;<command> context = 'bank'</command>  <emphasis># the base type</emphasis>
    &prompt;<command> feature = 'NN'</command>    <emphasis># the tag</emphasis>
    &prompt;<command> sample = CFSample(context, feature)</command>
    &lt;CFSample 'bank', 'NN'&gt;
</programlisting>

      <para> A <literal>CFSample</literal>'s context is accessed via
      the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.probability.CFSample.html#context">
      <literal>context</literal></ulink> member function; and its
      feature is accessed via the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.probability.CFSample.html#feature">
      <literal>feature</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> sample.context()</command>
    'bank'
    &prompt;<command> sample.feature()</command>
    'NN'
</programlisting>

      <important>
        <para> <literal>CFSample</literal>s should always have
        <emphasis>immutable</emphasis> contexts and features.  If you
        wish to use a sequence as a <literal>CFSample</literal>'s
        context or feature, you must use a tuple (and not a list).  If
        necessary, you can convert lists to tuples:</para>

<programlisting>
    &prompt;<command> my_list = [1, 2, 3]</command>
    [1, 2, 3]
    &prompt;<command> tuple(my_list)</command>
    (1, 2, 3)
</programlisting>

      </important>

    </section> <!-- CFSample -->

    <section> <title> Training the Unigram Tagger </title>

      <para> We can use <literal>CFSample</literal>s to populate a
      frequency distribution that models the distribution of tags and
      tokens in the training text: </para>

<programlisting>
class UnigramTagger:
    <emphasis># ...</emphasis>
    def train(self, tagged_tokens):
        for token in tagged_tokens:
            context = token.type().base()
            feature = token.type().tag()
            self._freqdist.inc(CFSample(context, feature))
</programlisting>

    </section> <!-- Training -->

    <section> <title> Tagging with the Unigram Tagger </title>
      
      <para> To find the most likely tag for a given token, we can
      just use the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.probability.FreqDistI.html#cond_max">
      <literal>cond_max</literal></ulink> member of our frequency
      distribution.  This method returns the most frequent sample that
      is contained in a conditioning event.  For example, using a
      predicated event for our condition, we could find the most
      likely tag for the base type "bank" as follows:</para>

<programlisting>
    &prompt;<command> def context_is_bank(w): </command>
    &prompt2;<command>     return w.context() == bank</command>
    &prompt2;
    &prompt;<command> condition = PredEvent(is_bank)</command>
    {Event x: context_is_bank(x)}
    &prompt;<command> freqdist.cond_max(condition)</command>
    'NN'
</programlisting>

      <para> However, using predicated events for this purpose is
      cumbersome.  Instead, we can use the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.probability.ContextEvent.html">
      <literal>ContextEvent</literal></ulink> class, which represents
      the event that the context has a given value.
      <literal>ContextEvent</literal>s are created with the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.probability.ContextEvent.html#__init__">
      <literal>ContextEvent constructor</literal></ulink>: </para>

<programlisting>
    &prompt;<command> condition = ContextEvent('bank')</command>
    {Event x: &lt;CFSample 'bank', x&gt;}
    &prompt;<command> freqdist.cond_max(condition)</command>
    'NN'
</programlisting>

      <para> To tag a text, we can simply loop through each token,
      using <literal>cond_max</literal> with
      <literal>ContextEvent</literal>s to find the most likely tag for
      each token's base type:</para>

<programlisting>
class UnigramTagger:
    <emphasis># ...</emphasis>
    def tag(self, tokens):
        tagged_tokens = []
      
        for token in tokens:
            <emphasis># Predict the next tag</emphasis>
            context_event = probability.ContextEvent(token.type())
            sample = self._freqdist.cond_max(context_event)
            if sample: tag = sample.feature()
            else: tag = 'UNK'

            <emphasis># Add the newly tagged token to tagged_tokens</emphasis>
            token_type = TaggedType(token.type(), tag)
            tagged_tokens.append(Token(token_type, token.loc()))

        <emphasis># Return the list of tagged tokens</emphasis>
        return tagged_tokens
</programlisting>
      
      <para> Note that we use the default tag <literal>'UNK'</literal>
      whenever we encounter a token whose base type was not present in
      the training corpus. </para>

    </section> <!-- Tagging Words -->

    <section> <title> Initializing the Unigram Tagger </title>

      <para> The constructor for <literal>UnigramTagger</literal>
      simply initializes <literal>self._freqdist</literal> with a new
      frequency distribution.  However, there are several different
      implementations of the frequency distribution interface, and the
      choice of which implementation to use can have an enormous
      impact on the efficiency of the tagger. </para>

      <section> <title> Efficiency Analysis </title>

        <itemizedlist>
          <listitem>
            <para> Let
            <replaceable>n<subscript>tag</subscript></replaceable> be
            the number of tags in our tag set</para>
          </listitem>
          <listitem>
            <para> Let
            <replaceable>n<subscript>train</subscript></replaceable>
            be the number of training tokens.  </para>
          </listitem>
        </itemizedlist>

        <para> The easiest solution is to use a
        <literal>SimpleFreqDist</literal>.
        <literal>SimpleFreqDist</literal> is a good general-purpose
        frequency distribution, and it will work with any kinds of
        samples and events. Unfortunately, this generality comes at a
        price.  For each token, we need to find the tag that is most
        frequently paired with a base type in the training data.  But
        <literal>SimpleFreqDist</literal> has no efficient way to
        extract <replaceable>n<subscript>tag</subscript></replaceable>
        training samples that match the base type we are looking for.
        Instead, it must search through all
        <replaceable>n<subscript>train</subscript></replaceable>
        samples.  Thus, if we use a <literal>SimpleFreqDist</literal>,
        then the unigram tagger will take time proportional to
        <replaceable>n<subscript>train</subscript></replaceable>
        (which is typically at least 10,000) to tag each word. </para>

        <para> A better option is to use a <ulink
        url="http://nltk.sourceforge.net/ref/nltk.probability.CFFreqDist.html">
        <literal>CFFreqDist</literal></ulink>.  This implementation of
        the frequency distribution interface requires that all samples
        be <literal>CFSample</literal>s; furthermore, the only event
        type supported by <literal>CFFreqDist</literal> is
        <literal>ContextEvent</literal>.  However, these restrictions
        allow <literal>CFFreqDist</literal> to use a more structured
        internal representation, resulting in much more efficient
        implementations of the conditional methods
        (<literal>cond_max</literal>, <literal>cond_freq</literal>,
        and <literal>cond_samples</literal>).  For the unigram tagger,
        this allows us to tag each word in time proportional to
        <replaceable>n<subscript>tag</subscript></replaceable> (which
        is typically less than 100). </para>

      </section> <!-- Efficiency Analysis -->

      <section> <title> The UnigramTagger Constructor </title>

        <para> We therefore use a <literal>CFFreqDist</literal> to
        implement the UnigramTagger's frequency distribution: </para>

<programlisting>
class UnigramTagger:
    <emphasis># ...</emphasis>
    def __init__(self):
        self._freqdist = probability.CFFreqDist()
</programlisting>

      </section> <!-- The UnigramTagger Constructor -->

    </section> <!-- Initializing the UnigramTagger -->

    <section><title>The UnigramTagger Implementation</title>

      <para> The complete listing for the
      <literal>UnigramTagger</literal> class is:</para>
      
      <figure><title>The UnigramTagger Implementation</title>
<programlisting> 
<command>class UnigramTagger(TaggerI):</command>
    """
    A unigram stochastic tagger.  Before a C{UnigramTagger} can be
    used, it should be trained on a list of C{TaggedToken}s.  Using
    this training data, it will find the most likely tag for each word
    type.  It will then use this information to assign the most
    frequent tag to each word.  If the C{NthOrderTagger} encounters a
    word in a context for which it has no data, it will assign it the
    tag \"UNK\".
    
    This tagger expects a list of C{Token}s as its
    input, and generates a list of C{TaggedToken}s as its
    output.
    """
<emphasis><command>    def __init__(self):</command></emphasis>
        <emphasis># Inherit documentation string from the TaggerI interface.</emphasis>
<command>        self._freqdist = probability.CFFreqDist()</command>
<command>    </command>
<emphasis><command>    def train(self, tagged_tokens):</command></emphasis>
        """
        Train this C{UnigramTagger} using the given
        training data.  If this method is called multiple times, then
        the training data from every call will be used.
        
        @param tagged_tokens: The training data.
        @type tagged_tokens: list of TaggedToken
        @returntype: None
        """
<command>        for token in tagged_tokens:</command>
<command>            context = token.type().base()</command>
<command>            feature = token.type().tag()</command>
<command>            self._freqdist.inc( probability.CFSample(context, feature) )</command>
<command></command>
<emphasis><command>    def tag(self, tokens):</command></emphasis>
        <emphasis># Inherit documentation string from the TaggerI interface.</emphasis>
<command>        tagged_tokens = []</command>
<command>      </command>
<command>        for token in tokens:</command>
            <emphasis># Predict the next tag</emphasis>
<command>            context = token.type()</command>
<command>            context_event = probability.ContextEvent(context)</command>
<command>            sample = self._freqdist.cond_max(context_event)</command>
<command>            if sample: tag = sample.feature()</command>
<command>            else: tag = 'UNK'</command>
<command></command>
            <emphasis># Add the newly tagged token to tagged_tokens</emphasis>
<command>            token_type = TaggedType(token.type(), tag)</command>
<command>            tagged_tokens.append(Token(token_type, token.loc()))</command>
<command></command>
<command>        return tagged_tokens</command>
</programlisting>
      </figure>
    </section> <!-- UnigramTagger Implementation -->
  </section> <!-- UnigramTagger -->

  <section> <title> NthOrderTagger </title>

    <para> The <literal>NthOrderTagger</literal> is a generalization
    of the <literal>UnigramTagger</literal>.  Instead of using the
    token's base type as a context, it uses a tuple consisting of the
    token's base type and the tags of the <replaceable>n</replaceable>
    preceding tokens.  This generalization creates two new
    issues. </para>

    <para> First, we must decide how to handle the first
    <replaceable>n</replaceable> tokens, since they do not have
    <replaceable>n</replaceable> preceding tokens.
    <literal>NthOrderTagger</literal> uses special tag
    <literal>'UNK'</literal> for these unavailable tokens.  Another
    option would be to simply ignore the first
    <replaceable>n</replaceable> tokens.  As it turns out, which
    approach we take will not have much of an impact, since
    <replaceable>n</replaceable> (the order of the tagger) is
    generally much less than
    <replaceable>n<subscript>train</subscript></replaceable> (the
    number of training samples). </para>

    <para> The second issue is that, when tagging a text, we do not
    have access the the actual tags of the
    <replaceable>n</replaceable> preceding tokens.  However, we do
    have access to our predicted values for these tags.
    <literal>NthOrderTagger</literal> uses these predicted tags, since
    they are likely to be correct.  Assuming that our predictions are
    good, the use of predicted tags instead of actual tags will have a
    relatively minor impact on performance. </para>

    <section> <title> Initializing the Nth Order Tagger</title>

      <para> Having addressed these two issues, we can examine the
      implementation of the <literal>NthOrderTagger</literal>.  The
      constructor simply records <replaceable>n</replaceable>, and
      constructs a new frequency distribution: </para>

<programlisting>
class NthOrderTagger:
    <emphasis># ...</emphasis>
    def __init__(self, n):
        self._n = n
        self._freqdist = probability.CFFreqDist()
</programlisting>

    </section> <!-- NthOrderTagger Constructor -->

    <section> <title> Training the Nth Order Tagger </title>

      <para> To train the <literal>NthOrderTagger</literal>, we
      construct a <literal>CFSample</literal> for each token.  For
      contexts, we use a tuple consisting of the
      <replaceable>n</replaceable> previous tags and the current
      token's base type. </para>

<programlisting>
class NthOrderTagger:
    <emphasis># ...</emphasis>
    def train(self, tagged_tokens):
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = ['UNK'] * self._n
      
        for token in tagged_tokens:
            context = tuple(prev_tags+[token.type().base()])
            feature = token.type().tag()
            self._freqdist.inc( probability.CFSample(context, feature) )

            <emphasis># Update prev_tags</emphasis>
            if len(prev_tags) > 0:
                del prev_tags[0]
                prev_tags.append(token.type().tag())
</programlisting>

    </section> <!-- NthOrderTagger.train -->

    <section> <title> Tagging with the Nth Order Tagger </title>

      <para> As with the <literal>UnigramTagger</literal>, we can find
      the most likely tag for each token using the
      <literal>cond_max</literal> member of our frequency
      distribution.  But instead of using each token's base type as a
      context, we use a tuple consisting of the
      <replaceable>n</replaceable> previous predicted tags and the
      token's base type.</para>

<programlisting>
class NthOrderTagger:
    <emphasis># ...</emphasis>
    def tag(self, tokens):
        tagged_tokens = []
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = ['UNK'] * self._n

        for token in tokens:
            <emphasis># Predict the next tag</emphasis>
            context = tuple(prev_tags+[token.type()])
            context_event = probability.ContextEvent(context)
            sample = self._freqdist.cond_max(context_event)
            if sample: tag = sample.feature()
            else: tag = 'UNK'

            <emphasis># Add the newly tagged token to tagged_tokens</emphasis>
            token_type = TaggedType(token.type(), tag)
            tagged_tokens.append(Token(token_type, token.loc()))

            <emphasis># Update prev_tags</emphasis>
            if len(prev_tags) > 0:
                del prev_tags[0]
                prev_tags.append(tag)

        <emphasis># Return the list of tagged tokens</emphasis>
        return tagged_tokens
</programlisting>

    </section> <!-- NthOrderTagger.tag -->

    <section><title>The NthOrderTagger Implementation</title>

      <para> The complete listing for the
      <literal>NthOrderTagger</literal> class is:</para>
      
      <figure><title>The NthOrderTagger Implementation</title>
<programlisting> 
<emphasis><command>class NthOrderTagger(TaggerI):</command></emphasis>
    """
    An I{n}-th order stochastic tagger.  Before an
    C{NthOrderTagger} can be used, it should be trained on a 
    list of C{TaggedToken}s.  Using this list, it will
    construct a frequency distribution describing the frequencies with 
    each word is tagged in different contexts.  The context considered 
    consists of the word to be tagged and the I{n} previous words' 
    tags.  Once it has constructed this frequency distribution, it
    uses it to tag words by assigning each word the tag with the
    maximum frequency given its context.  If the
    C{NthOrderTagger} encounters a word in a context for
    which it has no data, it will assign it the tag \"UNK\".

    This tagger expects a list of C{Token}s as its
    input, and generates a list of C{TaggedToken}s as its
    output.
    """
<emphasis><command>    def __init__(self, n):</command></emphasis>
        """
        Construct a new I{n}-th order stochastic tagger.  The
        new tagger should be trained, using the train() method, before
        it is used to tag data.
        
        @param n: The order of the new C{NthOrderTagger}.
        @type n: int
        """
<command>        self._n = n</command>
<command>        self._freqdist = probability.CFFreqDist()</command>
<command></command>
<emphasis><command>    def train(self, tagged_tokens):</command></emphasis>
        """
        Train this C{NthOrderTagger} using the given
        training data.  If this method is called multiple times, then
        the training data from every call will be used.
        
        @param tagged_tokens: The training data.
        @type tagged_tokens: list of TaggedToken
        @returntype: None
        """
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
<command>        prev_tags = ['UNK'] * self._n</command>
<command>      </command>
<command>        for token in tagged_tokens:</command>
<command>            context = tuple(prev_tags+[token.type().base()])</command>
<command>            feature = token.type().tag()</command>
<command>            self._freqdist.inc( probability.CFSample(context, feature) )</command>
<command></command>
            <emphasis># Update prev_tags</emphasis>
<command>            if len(prev_tags) > 0:</command>
<command>                del prev_tags[0]</command>
<command>                prev_tags.append(token.type().tag())</command>
<command></command>
<emphasis><command>    def tag(self, tokens):</command></emphasis>
        <emphasis># Inherit documentation string from the TaggerI interface.</emphasis>
<command>        tagged_tokens = []</command>
<command>      </command>
<command>        prev_tags = ['UNK'] * self._n</command>
<command>        for token in tokens:</command>
            <emphasis># Predict the next tag</emphasis>
<command>            context = tuple(prev_tags+[token.type()])</command>
<command>            context_event = probability.ContextEvent(context)</command>
<command>            sample = self._freqdist.cond_max(context_event)</command>
<command>            if sample: tag = sample.feature()</command>
<command>            else: tag = 'UNK'</command>
<command></command>
            <emphasis># Add the newly tagged token to tagged_tokens</emphasis>
<command>            token_type = TaggedType(token.type(), tag)</command>
<command>            tagged_tokens.append(Token(token_type, token.loc()))</command>
<command></command>
            <emphasis># Update prev_tags</emphasis>
<command>            if len(prev_tags) > 0:</command>
<command>                del prev_tags[0]</command>
<command>                prev_tags.append(tag)</command>
<command></command>
<command>        return tagged_tokens</command>
</programlisting>
      </figure>

    </section> <!-- NthOrderTagger Implementation -->

  </section> <!-- NthOrderTagger -->

  <section> <title> BackoffTagger </title>

    <para> The <literal>BackoffTagger</literal> is used to combine the
    results of a list of <glossterm>subtaggers</glossterm>.  For each
    token in the input, the <literal>BackoffTagger</literal> uses the
    result of the most first tagger in the list that successfully
    found a tag.  The implementation of
    <literal>BackoffTagger</literal> is relatively straight-forward.
    When tagging a text, it first runs all of its subtaggers on the
    text.  It then constructs a list of tagged tokens by combining the
    results of these taggers. </para>

    <para> There are two disadvantages to this approach: </para>

    <itemizedlist>
      <listitem>
        <para> It can be inefficient to run all of the taggers on every
        token.  For example, if we are combing a first order tagger
        and a unigram tagger, and the first order tagger successfully
        tags 98% of the tokens, then 98% of the unigram tagger's work
        is unnecessary. </para>
      </listitem>
      <listitem>
        <para> It does not allow for any interaction between the
        taggers.  To see why we might want interaction between
        taggers, consider a <literal>BackoffTagger</literal> that
        combines a first order tagger with a unigram tagger.  As we
        saw in the last section, the first order tagger must use the
        <emphasis>predicted</emphasis> tag of the preceding token to
        predict each new tag.  If the first order tagger was unable to
        tag the prior token, but the unigram tagger did successfully
        find a tag, then we might want to use the unigram tagger's
        prediction to help the first order tagger predict the next
        tag.</para>
      </listitem>
    </itemizedlist>

    <para> However, this approach has the major advantage of
    generality: it can be used to combine <emphasis>any</emphasis>
    taggers.  Any combining tagger that allowed for interaction
    between subtaggers, or only used subtaggers for some tokens, could
    not be this general. </para>

    <section> <title> Initializing a Backoff Tagger </title>

    <para> The <literal>BackoffTagger</literal> constructor simply
    records the list of subtaggers and the unknown tag.  The
    <glossterm>unknown tag</glossterm> is the tag which subtaggers use
    to indicate that they are unable to tag a word.  The unknown tag
    defaults to <literal>'UNK'</literal>.</para>

<programlisting>
class BackoffTagger:
    <emphasis># ...</emphasis>
    def __init__(self, subtaggers, unknown_tag='UNK'):
        self._unk = unknown_tag
        self._taggers = subtaggers
</programlisting>

    </section> <!-- Initializing a BackoffTagger -->

    <section> <title> Tagging with the Backoff Tagger </title>

      <para> <literal>BackoffTagger</literal> uses a fairly simple
      algorithm to tag texts: </para>

      <itemizedlist>
        <listitem>
          <para> Run each subtagger on the untagged tokens. </para>
        </listitem>
        <listitem>
          <para> Check to make sure that all subtaggers returned the
          correct number of tokens. </para>
        </listitem>
        <listitem>
          <para> For each token <replaceable>tok</replaceable>:</para>
          <itemizedlist>
            <listitem>
              <para> Find the first tagger in
              <literal>self._subtaggers</literal> that successfully
              tokenized <replaceable>tok</replaceable>.</para>
            </listitem>
            <listitem>
              <para> Add that tagger's tagged version of
              <replaceable>tok</replaceable> to our return-value
              list.</para>
            </listitem>
            <listitem>
              <para> If no taggers in
              <literal>self._subtaggers</literal> successfully
              tokenized <replaceable>tok</replaceable>, add the last
              tagger's tagged version of <replaceable>tok</replaceable>
              to our return-value list.</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>

<programlisting>
class BackoffTagger:
    <emphasis># ...</emphasis>
    def tag(self, tokens):
        <emphasis># Find the output of all the taggers.</emphasis>
        tagger_outputs = []
        for tagger in self._subtaggers:
            out = tagger.tag(tokens)
            tagger_outputs.append(out)

        <emphasis># Check for consistency</emphasis>
        length = len(tokens)
        for tagger_output in tagger_outputs:
            if len(tagger_output) != length:
                raise ValueError('Broken tagger!')
          
        <emphasis># For each word, find the first tagged output value whose</emphasis>
        <emphasis># tag is not "unknown."</emphasis>
        num_tokenizers = len(self._subtaggers)
        tagged_tokens = []
        for i_token in range(len(tokens)):
            for i_tagger in range(num_tokenizers):

                <emphasis># Did this tagger successfully tag this token?</emphasis>
                token = tagger_outputs[i_tagger][i_token]
                if token.type().tag() != self._unk:
                    tagged_tokens.append(token)
                    break <emphasis># out of "for i_tagger in ..."</emphasis>

                <emphasis># If this is the last tokenizer, use its result, even</emphasis>
                <emphasis># if its tag is "unknown."</emphasis>
                if i_tagger == num_tokenizers-1:
                    tagged_tokens.append(token)
                    
        return tagged_tokens
</programlisting>    
    
    </section> <!-- BackoffTagger tagging -->

    <section><title>The BackoffTagger Implementation</title>

      <para> The complete listing for the
      <literal>BackoffTagger</literal> class is:</para>
      
      <figure><title>The BackoffTagger Implementation</title>
<programlisting> 
<emphasis><command>class BackoffTagger(TaggerI):</command></emphasis>
    """
    A C{Tagger} that tags tokens using a basic backoff
    model.  Each C{BackoffTagger} is parameterized by an
    ordered list sub-taggers.  In order to assign a tag to a token,
    each of these sub-taggers is consulted in order.  If a sub-tagger
    is unable to determine a tag for the given token, it should use a
    special \"unknown tag.\"  The first tag returned by a sub-tagger,
    other than the unknown tag, is used for each Token.

    This tagger expects a list of C{Token}s as its
    input, and generates a list of C{TaggedToken}s as its
    output.  Each sub-tagger should accept a list a list of
    C{Token}s as its input, and should generate a list
    of C{TaggedToken}s as its output.
    """
<emphasis><command>    def __init__(self, subtaggers, unknown_tag='UNK'):</command></emphasis>
        """
        Construct a new C{BackoffTagger}, from the given
        list of sub-taggers.  The unknown tag specifies which tag
        should be treated as an indication that a sub-tagger cannot
        successfully tag a C{Token}.
        
        @param subtaggers: The list of sub-taggers used by this
               C{BackoffTagger}.  These sub-taggers will be
               consulted in the order in which they appear in the
               list.
        @type subtaggers: list of TaggerI
        @param unknown_tag: The tag which indicates that a sub-tagger
               is unable to tag a C{Token}.
        @type unknown_tag: sting.
        """
<command>        self._unk = unknown_tag</command>
<command>        self._subtaggers = subtaggers</command>
<command></command>
<emphasis><command>    def tag(self, tokens):</command></emphasis>
        <emphasis># Inherit documentation string from the TaggerI interface.</emphasis>
<command></command>
        <emphasis># Find the output of all the taggers.</emphasis>
<command>        tagger_outputs = []</command>
<command>        for tagger in self._subtaggers:</command>
<command>            out = tagger.tag(tokens)</command>
<command>            tagger_outputs.append(out)</command>
<command></command>
        <emphasis># Check for consistency</emphasis>
<command>        length = len(tokens)</command>
<command>        for tagger_output in tagger_outputs:</command>
<command>            if len(tagger_output) != length:</command>
<command>                raise ValueError('Broken tagger!')</command>
<command>          </command>
        <emphasis># For each word, find the first tagged output value whose</emphasis>
        <emphasis># tag is not "unknown."</emphasis>
<command>        num_tokenizers = len(self._subtaggers)</command>
<command>        tagged_tokens = []</command>
<command>        for i_token in range(len(tokens)):</command>
<command>            for i_tagger in range(num_tokenizers):</command>
<command></command>
                <emphasis># Did this tagger successfully tag this token?</emphasis>
<command>                token = tagger_outputs[i_tagger][i_token]</command>
<command>                if token.type().tag() != self._unk:</command>
<command>                    tagged_tokens.append(token)</command>
<command>                    break</command> <emphasis># out of "for i_tagger in ..."</emphasis>
<command></command>
                <emphasis># If this is the last tokenizer, use its result, even</emphasis>
                <emphasis># if its tag is "unknown."</emphasis>
<command>                if i_tagger == num_tokenizers-1:</command>
<command>                    tagged_tokens.append(token)</command>
<command>                    </command>
<command>        return tagged_tokens</command>
</programlisting>
      </figure>
    </section> <!-- BackoffTagger impl -->


  </section> <!-- BackoffTagger -->

</article>
