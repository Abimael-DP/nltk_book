<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!ENTITY ling "<prompt>...</prompt>">
]>
<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Tagging</title> &copyright;
  </articleinfo>

  <section id="intro">
    <title>Introduction</title>

    <para> When processing a text, it is often useful to associate auxiliary information with
      each token. For example, we might want to label each token with its part of speech; or
      we might want to disambiguate homonyms by associating them with "word sense" labels.
      This kind of auxiliary information is typically used in later stages of text processing.
      For example, part of speech labels could be used to derive the internal structure of a
      sentence; and "word sense" labels could be used to allow a question-answering system to
      distinguish homonyms. </para>

    <para> The process of associating labels with each token in a text is called
      <glossterm>tagging</glossterm>, and the labels are called <glossterm>tags</glossterm>.
      The collection of tags used for a particular task is known as a <glossterm>tag
      set</glossterm>.
    </para>

    <para>Text can be
      decorated with <glossterm>markup</glossterm> to represent the category of each word,
      as in the following sentence from the Brown Corpus:
      <literal>
        The/at Pantheon's/np$ interior/nn ,/, still/rb in/in its/pp$
        original/jj form/nn ,/, is/bez truly/ql majestic/jj and/cc an/at
        architectural/jj triumph/nn ./.
      </literal>
      Here, the word <literal>The</literal> is tagged <literal>at</literal>, which
      is the Brown Corpus tag for "article".
      The natural language tool that assigns categories to words is
      known as a <glossterm>part-of-speech tagger</glossterm>, or a <glossterm>POS-tagger</glossterm>.
    </para>

    <para>Tagging need not assign a part of speech.  Here is a sample of sense-tagged text:
      <literal>The Pantheon's interior/a , still in its original/a form/a</literal>.
      Here, the tags refer to the particular sense of the word that is appropriate for
      the context in which it appears, as defined in a dictionary:
    </para>

    <table id="table.dictionary">
      <title>Dictionary Fragment for Sense Tagging</title>
        <tgroup cols="2">
          <colspec colwidth='2cm'/>
          <tbody>
            <row>
              <entry>interior:</entry>
              <entry>(a) inside a space; (b) inside a country and at a distance
                from the coast or border; (c) domestic; (d) private.</entry>
            </row>
            <row>
              <entry>original:</entry>
              <entry>(a) relating to the beginning of something; (b) novel;
                (c) that from which a copy is made; (d) mentally ill or eccentric.</entry>
            </row>
            <row>
              <entry>form:</entry>
              <entry>(a) definite shape or appearance; (b) body; (c) mould;
                (d) particular structural character exhibited by something;
                (e) a style as in music, art or literature; (f) homogeneous
                polynomial in two or more variables; ...</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

    <para>Words can be tagged with other information as well, such as
      pronunciation or intonation (for use by a text-to-speech system),
      or the stem of the word, or a reference to a lexical entry, or
      a reference to an antecedent.  In this tutorial our primary focus
      is on part-of-speech tagging, as an early step in language processing
      which does not depend on the deeper linguistic analysis required by
      these other kinds of tagging.
    </para>

    <note><para>
      Part-of-speech tagging is the most common example of tagging, and
      it is the example we will examine in this tutorial.  However, readers
      should keep in mind that many of the techniques we discuss here
      can also be applied to many other tagging problems.
      Following widespread practice, we will normally use the
      term <glossterm>tagging</glossterm> to refer to part-of-speech tagging.
    </para></note>

  </section>

  <section id="pos">
    <title>Word Classes and Parts of Speech</title>

<!-- 
X.2 linguistic overview (for non-linguist readers)
- how have linguists addressed the problem?
- what are the shortcomings of the non-computational approach?

% tr -sc 'A-Za-z0-9' '\012' &lt; wsj_0034 | sort 
-->

    <para>There is a long tradition within linguistics of classifying words into different
      categories. These categories are also called <firstterm>parts of speech</firstterm>.
      Familiar examples are <type>noun</type>, <type>verb</type>, <type>preposition</type>,
      <type>adjective</type> and <type>adverb</type>.  In this section we present the
      standard criteria for categorizing words in this way, then discuss the main classes
      of words in English.
    </para>

    <section id="pos.categorize">
      <title>Categorizing Words</title>

      <para>How do we know what category a word
        should belong to? In general, linguists invoke three kinds of criteria for making the
        decision:
        <itemizedlist>
          <listitem><para>formal;</para></listitem>
          <listitem><para>syntactic (or distributional);</para></listitem>
          <listitem><para>notional (or semantic).</para></listitem>
        </itemizedlist>
        A <firstterm>formal</firstterm> criterion is one which looks at the
        internal structure of a word. For example, <literal>-ness</literal> is a suffix which
        combines with an adjective to produce a noun. Examples are <literal>happy</literal>
        &rarr; <literal>happiness</literal>, <literal>ill</literal> &rarr;
        <literal>illness</literal>. So if we encounter a word which ends in
        <literal>-ness</literal>, this is very likely to be a noun.
      </para>

      <para>
        A <firstterm>syntactic</firstterm> criterion refers to the syntactic contexts in
        which a word can occur. For example, assume that we have already determined the category
        of nouns. Then we might say that a syntactic criterion for an adjective in English is
        that it can occur immediately before a noun, or immediately following the
        words <literal>be</literal> or <literal>very</literal>. According to these tests,
        <literal>near</literal> should be categorized as an adjective:
        <orderedlist>
          <listitem><para>the near window</para></listitem>
          <listitem><para>The end is (very) near.</para></listitem>
        </orderedlist>
      </para>

      <para>A familier example of a <firstterm>notional</firstterm> criterion is that a noun is
        <quote>the name of a person, place or thing</quote>. Within modern linguistics,
        notional criteria for word classes have be viewed with considerable suspicion, mainly
        because they are hard to formalize. Nevertheless, notional criteria underpin many of our
        intuitions about word classes, and enable us to make a good guess about the
        categorization of words in languages that we are unfamiliar with; that is, if we all we
        know about the Dutch <literal>verjaardag</literal> is that it means the same as the
        English word <literal>birthday</literal>, then we can guess
        that <literal>verjaardag</literal> is a noun in Dutch. However, some care is needed:
        although we might translate <literal>zij is van dag jarig</literal> as <literal>it's her
        birthday today</literal>, the word <literal>jarig</literal> is in fact an adjective
        in Dutch, and has no exact equivalent in English.
      </para>

      <para>Some word classes consist of a limited set of function words, e.g. prepositions:
        <literal>above, along, at, below, beside, between, during, for, from, in, near,
          on, outside, over, past, through, towards, under, up, with</literal> etc.
          These are called <glossterm>closed classes</glossterm>.  Other word classes such as
          nouns are not limited in this way, and are continually being extended with the
          invention of new words.  These are called <glossterm>open classes</glossterm>.
      </para>

    </section>

    <section id="pos.english">
      <title>English Word Classes</title>

      <para>
        English has four open word classes: nouns, verbs, adjectives and adverbs.
        Nouns generally refer to people, places, things, or concepts, e.g.:
        <emphasis>woman, Scotland, book, intelligence</emphasis>.
        In the context of a sentence, nouns can appear after determiners and adjectives,
        and can be the subject or object of the verb:</para>

    <table id="table.nouns">
      <title>Syntactic Distribution of Some Nouns</title>
        <tgroup cols="3">
          <colspec colwidth='2cm'/>
          <thead>
            <row>
              <entry>Word</entry>
              <entry>After a determiner</entry>
              <entry>Subject of the verb</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>woman</entry>
              <entry><emphasis>the</emphasis> woman who I saw yesterday ...</entry>
              <entry>the woman <emphasis>sat</emphasis> down</entry>
            </row>
            <row>
              <entry>Scotland</entry>
              <entry><emphasis>the</emphasis> Scotland I remember as a child ...</entry>
              <entry>Scotland <emphasis>has</emphasis> five million people</entry>
            </row>
            <row>
              <entry>book</entry>
              <entry><emphasis>the</emphasis> book I bought yesterday ...</entry>
              <entry>this book <emphasis>recounts</emphasis> the white invasion of Australia</entry>
            </row>
            <row>
              <entry>intelligence</entry>
              <entry><emphasis>the</emphasis> intelligence displayed by the child ...</entry>
              <entry>Mary's intelligence <emphasis>impressed</emphasis> her teachers</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>
        English nouns can be morphologically complex.  For example, words like
        <literal>books</literal> and <literal>women</literal> are plural.
        As we saw earlier, words with the <literal>-ness</literal> suffix are
        nouns that have been derived from adjectives, e.g. <literal>happiness</literal>
        and <literal>illness</literal>.  The <literal>-ment</literal> suffix
        appears on certain nouns derived from verbs, e.g. <literal>government</literal>
        and <literal>establishment</literal>.
      </para>

      <para>
        Nouns are usually further classified as <glossterm>common nouns</glossterm> and
        <glossterm>proper nouns</glossterm>.  Proper nouns identify particular individuals
        or entities, e.g. <literal>Moses</literal> and <literal>Scotland</literal>, while
        common nouns are all the rest.  Another important distinction exists between
        <glossterm>count nouns</glossterm> and <glossterm>mass nouns</glossterm>.
        Count nouns are thought of as distinct entities which can be counted, such as
        <literal>pig</literal> (e.g. <literal>one pig, two pigs, many pigs</literal>).
        They cannot occur with the word <literal>much</literal> (i.e. *<literal>much pigs</literal>).
        Mass nouns, on the other hand, are not thought of as distinct entities (e.g.
        <literal>sand</literal>).  They cannot be pluralized, and do not occur with
        numbers (e.g. *<literal>two sands</literal>, *<literal>many sands</literal>).
        However, the can occur with the word <literal>much</literal> (e.g. <literal>much sand</literal>).
      </para>
        
      <para>VERBS</para>

      <para>Adjectives and adverbs</para>

      <para>closed class words</para>

    </section>

  </section>

    <section id="tagging">
        <title> Computational Approaches to Tagging </title>
        <!--
X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)
-->
        <para> Part-of-speech tags divide words into categories, based on how they can be combined
            to form sentences. For example, articles can combine with nouns, but not verbs.
            Part-of-speech tags also give information about the semantic content of a word. For
            example, nouns typically express "things," and prepositions express relationships
            between "things." </para>
        <para> Most part-of-speech tag sets make use of the same basic categories, such as "noun,"
            "verb," "adjective," and "preposition." However, tag sets differ both in how finely they
            divide words into categories; and in how define their categories. For example, "is"
            might be tagged as a verb in one tag set; but as a form of "to be" in another tag set.
            This variation in tag sets is reasonable, since part-of-speech tags are used in
            different ways for different tasks. </para>
        <!-- How do I make "Table 1" not use a hard-coded number?? -->
        <para> In this tutorial, we will use the tag set listed in Table 1. This tag set is a
            simplification of the commonly used <glossterm>Brown Corpus tag set</glossterm>. The
            complete Brown Corpus tag set has 87 basic tags. For more information on tag sets, see
                <citetitle>Foundations of Statistical Natural Language Processing</citetitle> (<author>
                <surname>Manning</surname>
            </author> &amp; <author>
                <surname>Schutze</surname>
            </author>), pp. 139-145, or <citetitle>Speech and Language Processing</citetitle> (<author>
                <surname>Jurafsky</surname>
            </author> &amp; <author>
                <surname>Martin</surname>
            </author>) p. 297.
         </para>

        <table id="table.tagset">
            <title>Tag Set</title>
            <tgroup cols="2">
                <tbody>
                    <row>
                        <entry>AT</entry>
                        <entry>Article</entry>
                    </row>
                    <row>
                        <entry>NN</entry>
                        <entry>Noun</entry>
                    </row>
                    <row>
                        <entry>VB</entry>
                        <entry>Verb</entry>
                    </row>
                    <row>
                        <entry>JJ</entry>
                        <entry>Adjective</entry>
                    </row>
                    <row>
                        <entry>IN</entry>
                        <entry>Preposition</entry>
                    </row>
                    <row>
                        <entry>CD</entry>
                        <entry>Number</entry>
                    </row>
                    <row>
                        <entry>END</entry>
                        <entry>Sentence-ending punctuation</entry>
                    </row>
                </tbody>
            </tgroup>
        </table>
    </section>
    <!--
    <section> <title> Advanced Topics in Tagging (optional) </title>

    <para></para>

X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions


    </section>
-->


    <section id="basics.representation">
      <title> Representing Tagged Tokens </title>

      <para>
        A tagged token is represented by adding a <literal>TAG</literal> property
        to the token:
      </para>

<programlisting><![CDATA[
>>> tok = Token(TEXT='fly', TAG='N')
>>> print tok['TEXT']
fly
>>> print tok['TAG']
N
>>> print tok
<fly/N>
]]></programlisting>




      <para>Tagged tokens are represented in the <literal>SUBTOKENS</literal> property,
        and that the tags themselves are stored
        in the <literal>TAG</literal> property. The <literal>properties</literal> member
        function of a token lists the properties defined for that token, in case there
        is ever any doubt about which property names have been used: </para>

<programlisting>
    &prompt;<command> print text_token['SUBTOKENS'][1] </command>
    &lt;saw/VB>
    &prompt;<command> print text_token['SUBTOKENS'][1]['TAG'] </command>
    'VB'
    &prompt;<command> print text_token.properties() </command>
    ['TEXT', 'SUBTOKENS']
    &prompt;<command> print text_token['SUBTOKENS'][1].properties() </command>
    ['TEXT', 'TAG']
</programlisting>



    </section>



    <section id="basics.corpora">
      <title> Tagged Corpora </title>
      <para> Several large corpora (such as the Brown Corpus and portions of the Wall
        Street Journal) have been manually tagged with part-of-speech tags. These
        corpora are primarily useful for testing taggers and for training statistical
        taggers. However, before we can use these corpora, we must read them from files
        and tokenize them. </para>
      <para> Tagged texts are usually stored in files as a sequences of
        whitespace-separated tokens, where each token is of the form <replaceable>base</replaceable>
        <literal>/</literal>
        <replaceable>tag</replaceable>, as illustrated below for a sample from the Brown Corpus.</para>

<para><literal>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps
said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb
accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt
interest/nn of/in both/abx governments/nns ''/'' ./.
</literal></para>

      <para> To tokenize tagged texts of this form, the <literal>nltk.tagger</literal>
        module defines the <ulink url="&refdoc;/nltk.tagger.TaggedTokenizer-class.html">
        <literal>TaggedTokenizer</literal> </ulink> class: </para>

<programlisting>
    &prompt;<command> tagged_text_str = open('corpus.txt').read() </command>
    'John/NN saw/VB the/AT book/NN on/IN the/AT 
     table/NN ./END  He/NN sighed/VB ./END'
    &prompt;<command> tagged_text_token = Token(TEXT=tagged_text_str) </command>
    &prompt;<command> TaggedTokenizer().tokenize(tagged_text_token) </command>
    &prompt;<command> print tagged_text_token </command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, 
     &lt;table/NN>, &lt;./END>, &lt;He/NN>, &lt;sighed/VB>, &lt;./END>]>
</programlisting>

      <para> If <literal>TaggedTokenizer</literal> encounters a word without a tag, it
        will assign it the default tag <literal>None</literal>. </para>
      <para> It is possible to use the <literal>nltk.corpus</literal> module to read and
        tokenize data from a tagged corpus, as shown below: </para>

<programlisting>
    &prompt;<command> from nltk.corpus import brown</command>
    &prompt;<command> brown.tokenize('ca01')</command>
    &lt;[&lt;The/at>, &lt;Fulton/np-tl>, &lt;County/nn-tl>, &lt;Grand/jj-tl>, &lt;Jury/nn-tl>,
    &lt;said/vbd>, &lt;Friday/nr>, &lt;an/at>, &lt;investigation/nn>, &lt;of/in>, &lt;Atlanta's/np$>,
    &lt;recent/jj>, &lt;primary/nn>, &lt;election/nn>, &lt;produced/vbd>, &lt;``/``>, &lt;no/at>,
    &lt;evidence/nn>, &lt;''/''>, &lt;that/cs>, &lt;any/dti>, &lt;irregularities/nns>, &lt;took/vbd>,
    &lt;place/nn>, &lt;./.>, ...]>
</programlisting>

    </section>

    <section>
        <title> Tagging in NLTK </title>
        <!--
X.5 implementation
    - how does NLTK do it?
    - simple problems and worked solutions
    - suggested projects (e.g. for your MSc students)
-->
        <section id="basics">
            <title> The nltk.tagger Module </title>
            <para> The <ulink url="&refdoc;/nltk.tagger-module.html">
                    <literal>nltk.tagger</literal>
                </ulink> module defines the classes and interfaces used by NLTK to perform tagging. </para>
            <!--
    <section id="basics.TaggedType"> <title> TaggedType </title>

      <para> NLTK defines a simple class, <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html"
      ><literal>TaggedType</literal></ulink>, for representing the text
      type of a tagged token.  A <literal>TaggedType</literal>
      consists of a <glossterm>base type</glossterm> and a
      <glossterm>tag</glossterm>.  Typically, the base type and the
      tag will both be strings.  For example, the tagged type for the
      noun "dog" would have the base type <literal>'dog'</literal> and
      the tag <literal>'NN'</literal>.  A tagged type with base type
      <replaceable>b</replaceable> and tag
      <replaceable>t</replaceable> is written
      <replaceable>b</replaceable><literal>/</literal><replaceable>t</replaceable>.
      Tagged types are created with the <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#__init__"
      ><literal>TaggedType</literal></ulink> constructor: </para>

<programlisting>
    &prompt;<command> ttype1 = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttype2 = TaggedType('runs', 'VB') </command>
    'runs'/'VB'
</programlisting>

      <para> A <literal>TaggedType</literal>'s base type is accessed
      via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#base"
      ><literal>base</literal></ulink> member function; and its tag is
      accessed via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#tag"
      ><literal>tag</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> ttype1.base() </command>
    'dog'
    &prompt;<command> ttype2.tag() </command>
    'VB'
</programlisting>

      <para> To construct a tagged token, simply use the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal></ulink> constructor with a
      <literal>TaggedType</literal>: </para>

<programlisting>
    &prompt;<command> ttype = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttoken = Token(ttype, Location(5)) </command>
    'dog'/'NN'@[5]
</programlisting>

    </section>
-->
            <!-- TaggedTokenizer -->
            <section id="basics.TaggerI">
                <title> The TaggerI Interface </title>
                <para> The <literal>nltk.tagger</literal> module defines <ulink url="&refdoc;/nltk.tagger.TaggerI-class.html">
                        <literal>TaggerI</literal>
                    </ulink>, a general interface for tagging texts. This interface is used by all
                    taggers. It defines a single method, <ulink url="&refdoc;/nltk.tagger.TaggerI-class.html#tag">
                        <literal>tag</literal>
                    </ulink>, which assigns a tag to each token in a list, and returns the resulting
                    list of tagged tokens.</para>
                <programlisting>
    &prompt;<command> text = open('untagged.txt').read() </command>
    &prompt;<command> print text </command>
    &lt;John saw the book on the table . He sighed .>
    &prompt;<command> text_token = Token(TEXT=text) </command>
    &prompt;<command> WSTokenizer().tokenize(text_token) </command>
    &prompt;<command> print text_token </command>
    &lt;[&lt;John>, &lt;saw>, &lt;the>, &lt;book>, &lt;on>, &lt;the>, &lt;table>, &lt;.>, &lt;He>, &lt;sighed>, &lt;.>]>
    &prompt;<command> my_tagger.tag(text_token) </command>
    &prompt;<command> print text_token </command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>,
    &lt;table/NN>, &lt;./END>, &lt;He/NN>, &lt;sighed/VB>, &lt;./END>]>
</programlisting>

            </section>
            <!-- TaggerI -->
        </section>
        <!-- The nltk.tagger module -->
        <section id="taggers">
            <title> Taggers </title>
            <para> The <literal>nltk.tagger</literal> module currently defines four taggers; this
                list will likely grow in the future. This section describes the taggers currently
                implemented by <literal>nltk.tagger</literal>, and how they are used. </para>
            <section id="taggers.default">
                <title> The Default Tagger </title>
                <para> The simplest tagger defined by <literal>nltk.tagger</literal> is <ulink url="&refdoc;/nltk.tagger.DefaultTagger-class.html">
                        <literal>DefaultTagger</literal>
                    </ulink>. This tagger assigns the same tag to each token regardless of its text.</para>
                <programlisting>
    &prompt;<command> WSTokenizer().tokenize(text_token) </command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John>, &lt;saw>, &lt;3>, &lt;polar>, &lt;bears>, &lt;.>]>
    &prompt;<command> my_tagger = DefaultTagger('NN') </command>
    &prompt;<command> my_tagger.tag(text_token) </command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/NN>, &lt;3/NN>, &lt;polar/NN>, &lt;bears/NN>, &lt;./NN>]>
</programlisting>
                <para> This is a simple algorithm, and it performs poorly when used by itself. On a
                    typical corpus, it will tag only 20%-30% of the tokens correctly. However, it is
                    a very reasonable tagger to use as a default, if a more advanced tagger fails to
                    determine a token's tag. When used in conjunction with other taggers, a
                    <literal>DefaultTagger</literal> can significantly improve performance. </para>
            </section>
            <!-- DefaultTagger -->
            <section id="taggers.regexp">
                <title> The Regular Expression Tagger </title>
                <para> The regular expression tagger assigns tags to tokens on the basis of regular
                    expression matching.</para>
                <programlisting>

    &prompt;<command> NN_CD_tagger = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')]) </command>
    &prompt;<command> NN_CD_tagger.tag(text_token) </command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/NN>, &lt;3/CD>, &lt;polar/NN>, &lt;bears/NN>, &lt;./NN>]>
</programlisting>
                <para> This is a simple algorithm, and it performs poorly when used by itself. On a
                    typical corpus, it will tag only 20%-30% of the tokens correctly. However, it is
                    a very reasonable tagger to use as a default, if a more advanced tagger fails to
                    determine a token's tag. When used in conjunction with other taggers, a
                    <literal>DefaultTagger</literal> can significantly improve performance. </para>
            </section>
            <!-- DefaultTagger -->
            <section id="taggers.unigram">
                <title> The Unigram Tagger </title>
                <para> The <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html">
                        <literal>UnigramTagger</literal>
                    </ulink> class implements a simple statistical tagging algorithm: for each
                    token, it assigns the tag that is most likely for that token's text. For
                    example, it will assign the tag "JJ" to any occurrence of the word "frequent,"
                    since "frequent" is used as an adjective (e.g. "a frequent word") more often
                    than it is used as a verb (e.g. "I frequent this cafe"). </para>
                <para> Before a <literal>UnigramTagger</literal> can be used to tag data, it must be
                    trained on a <glossterm>training corpus</glossterm>. It uses this corpus to
                    determine which tags are most common for each word.
                    <literal>UnigramTaggers</literal> are trained using the <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html#train">
                        <literal>train</literal>
                    </ulink> method, which takes a tagged corpus: </para>
                <programlisting>
    <emphasis># 'train.txt' is a tagged training corpus</emphasis>
    &prompt;<command> tagged_text_str = open('train.txt').read()</command>
    &prompt;<command> tagged_text_token = Token(TEXT=tagged_text_str)</command>
    &prompt;<command> TaggedTokenizer().tokenize(tagged_text_token)</command>
    &prompt;<command> tagger = UnigramTagger()</command>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>
                <para> Once a <literal>UnigramTagger</literal> has been trained, the <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html#tag">
                        <literal>tag</literal>
                    </ulink> can be used to tag untagged corpora: </para>
                <programlisting>
    &prompt;<command> WSTokenizer().tokenize(text_token)</command>
    &prompt;<command> tagger.tag(text_token)</command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, ...]>
</programlisting>
                <para>
                    <literal>UnigramTagger</literal> will assign the default tag
                    <literal>None</literal> to any token that was not encountered in the training
                    data. </para>
                <para> Note that, like almost all statistical taggers, the performance of
                    <literal>UnigramTagger</literal> is highly dependent on the quality of its
                    training set. In particular, if the training set is too small, it will not be
                    able to reliably estimate the most likely tag for each word. Performance will
                    also suffer if the training set is significantly different than the texts we
                    wish to tag. </para>
            </section>
            <!-- UnigramTagger -->
            <section id="taggers.nthorder">
                <title> Nth-Order Tagging </title>
                <para> The <ulink url="&refdoc;/nltk.tagger.NthOrderTagger-class.html">
                        <literal>NthOrderTagger</literal>
                    </ulink> class implements a more advanced statistical tagging algorithm. In
                    addition to considering the token's text, it also considers the part-of-speech
                    tags of the <replaceable>n</replaceable> preceding tokens. </para>
                <para> To decide which tag to assign to a token, <literal>NthOrderTagger</literal>
                    first constructs a <glossterm>context</glossterm> for the token. This context
                    consists of the token's text, along with the part-of-speech tags of the
                    <replaceable>n</replaceable> preceding tags. It then picks the tag which is most
                    likely for that context. Note that a 0th order tagger is equivalent to a unigram
                    tagger, since the context used to tag a token is just its text. First order
                    taggers are sometimes called <glossterm>bigram taggers</glossterm>, and second
                    order taggers are called <glossterm>trigram taggers</glossterm>. </para>
                <para>
                    <literal>NthOrderTagger</literal> uses a tagged training corpus to determine
                    which part-of-speech tag is most likely for each context:</para>
                <programlisting>
    &prompt;<command> TaggedTokenizer().tokenize(tagged_text_token)</command>
    &prompt;<command> tagger = NthOrderTagger(3)</command>         <emphasis># 3rd order tagger</emphasis>
    &prompt;<command> tagger.train(tagged_text_token)</command>
</programlisting>
                <para> Once an <literal>NthOrderTagger</literal> has been trained, it can be used to
                    tag untagged corpora: </para>
                <programlisting>
    &prompt;<command> WSTokenizer().tokenize(text_token)</command>
    &prompt;<command> tagger.tag(text_token)</command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, ...]>
</programlisting>
                <para>
                    <literal>NthOrderTagger</literal> will assign the default tag
                    <literal>None</literal> to any token whose context was not encountered in the
                    training data. </para>
                <para> Note that as <replaceable>n</replaceable> gets larger, the specificity of the
                    contexts increases; and with it, the chance that the data we wish to tag will
                    contain contexts that were not present in the training data. Thus, there is a
                    trade-off between the accuracy and the coverage of our results. This is a common
                    type of trade-off in natural language processing. It is closely related to the
                        <glossterm>precision/recall trade-off</glossterm> that we'll encounter later
                    when we discuss information retrieval. </para>
            </section>
            <!-- NthOrderTagger -->
            <section id="tagger.backoff">
                <title> Combining Taggers </title>
                <para> One way to address the trade-off between accuracy and coverage is to use the
                    more accurate algorithms when we can, but to fall back on algorithms with wider
                    coverage when necessary. For example, we could combine the results of a first
                    order tagger, a 0th order tagger, and an <literal>NN_CD_Tagger</literal>, as follows:</para>
                <orderedlist>
                    <listitem>
                        <para> Try tagging the token with the first order tagger. </para>
                    </listitem>
                    <listitem>
                        <para> If the first order tagger is unable to find a tag for the token, try
                            finding a tag with the 0th order tagger. </para>
                    </listitem>
                    <listitem>
                        <para> If the 0th order tagger is also unable to find a tag, use the
                            <literal>NN_CD_Tagger</literal> to find a tag. </para>
                    </listitem>
                </orderedlist>
                <para> NLTK defines the <ulink url="&refdoc;/nltk.tagger.BackoffTagger-class.html">
                        <literal>BackoffTagger</literal>
                    </ulink> class for combining taggers in this way. A
                    <literal>BackoffTagger</literal> is constructed from an ordered list of one or
                    more <glossterm>subtaggers</glossterm>. For each token in the input, the
                    <literal>BackoffTagger</literal> uses the result of the first tagger in the list
                    that successfully found a tag. Taggers indicate that they are unable to tag a
                    token by assigning it the special tag <literal>None</literal>. We can use a
                    <literal>BackoffTagger</literal> to implement the strategy proposed above: </para>
                <programlisting>
    &prompt;<command> TaggedTokenizer().tokenize(text_token)</command>

    <emphasis># Construct the taggers</emphasis>
    &prompt;<command> tagger1 = NthOrderTagger(1)</command>         <emphasis># first order tagger</emphasis>
    &prompt;<command> tagger2 = UnigramTagger()</command>           <emphasis># zeroth order tagger</emphasis>
    &prompt;<command> tagger3 = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')]) </command>

    <emphasis># Train the taggers</emphasis>
    &prompt;<command> tagger1.train(train_toks)</command>
    &prompt;<command> tagger2.train(train_toks)</command>

    <emphasis># Combine the taggers</emphasis>
    &prompt;<command> tagger = BackoffTagger([tagger1, tagger2, tagger3])</command>
</programlisting>
                <para> Note that the order in which the taggers are given to
                    <literal>BackoffTagger</literal> is important: the taggers should be listed in
                    the order that they should be tried. This typically means that more specific
                    taggers should be listed before less specific taggers. </para>
                <para> Having defined a combined tagger, we can use it to tag new corpora: </para>
                <programlisting>
    &prompt;<command> TaggedTokenizer().tokenize(text_token)</command>
    &prompt;<command> tagger.tag(text_token)</command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, ...]>
</programlisting>
            </section>
            <!-- Combining Tagger -->
        </section>
        <!-- Taggers -->
        <!-- old material on implementation that appeared here to be moved to
an implementation tutorial - SB -->

    <section id="exercises">
      <title>Exercises</title>

      <orderedlist>

        <listitem>
          <formalpara>
            <title> Reading tagged corpora </title>
            <para> Tokenize the Brown Corpus and build one or more suitable data structures
              so that you can answer the following questions.</para>
          </formalpara>
          <orderedlist>
            <listitem><para>
              Which word has the most number of tags?</para>
            </listitem>
            <listitem><para>
              How many words are ambiguous, in the sense that they appear with at least two tags?</para>
            </listitem>
            <listitem><para>
              What percentage of word <emphasis>occurrences</emphasis> in the Brown Corpus involve
              these ambiguous words?</para>
            </listitem>
            <listitem><para>
              Which nouns are more common in their plural form than their singular form?
              (Only consider regular plurals, formed with the <literal>-s</literal> suffix.
            </para></listitem>
          </orderedlist>
        </listitem>

        <listitem>
          <formalpara>
            <title> Combining taggers with BackoffTagger</title>
            <para> There is typically a trade-off between the accuracy and coverage for taggers:
              taggers that use more specific contexts usually produce more accurate results,
              when they have seen those contexts in the training data; but because the
              training data is limited, they are less likely to encounter each context. The
              <literal>BackoffTagger</literal> addresses this problem by trying taggers with
              more specific contexts first; and falling back to the more general taggers when
              necessary. In this exercise, we examine the effects of using
              <literal>BackoffTagger</literal>.
            </para>
          </formalpara>

          <orderedlist>
            <listitem>
              <para> Create a <literal>DefaultTagger</literal> or a
                <literal>RegexpTagger</literal>, and a <literal>UnigramTagger</literal>,
                and a <literal>NthOrderTagger</literal>. Train the
                <literal>UnigramTagger</literal>, and the
                <literal>NthOrderTagger</literal> using a tagged section of the Brown corpus.</para>
            </listitem>
            <listitem>
              <para> Test the performance of each tagger, using a tagged section of the
                Brown corpus. Record the <glossterm>accuracy</glossterm> of the tagger
                (the percentage of tokens that are correctly tagged). Be sure to use a
                different section of the corpus for testing than you used for training. </para>
            </listitem>
            <listitem>
              <para> Use <literal>BackoffTagger</literal> to create three different
                combinations of the basic taggers. Test the accuracy of each combined
                tagger. Which combinations give the most improvement? </para>
            </listitem>
            <listitem>
              <para> Try repeating steps 1-3 with a different sized training corpus. How
                does it affect your results? </para>
            </listitem>
          </orderedlist>
        </listitem>

        <listitem>
          <formalpara>
            <title> Tagger context </title>
            <para><literal>NthOrderParser</literal> chooses a tag for a token based on its type
              and the tags of the <replaceable>n</replaceable> preceeding tokens. This is a
              common context to use for parsing, but certainly not the only possible context. </para>
          </formalpara>
          <para> Construct a new tagger, subclassed from <literal>SequentialTagger</literal>,
            that uses a different context. If your tagger's context contains multiple
            elements, then you should combine them in a <literal>tuple</literal>. Some
            possibilities for elements to include are:
          </para>

          <itemizedlist>
            <listitem>
              <para> The base type of the current token, or of a previous token. </para>
            </listitem>
            <listitem>
              <para> The length of the current token's type, or of a previous token's
                type. </para>
            </listitem>
            <listitem>
              <para> The first letter of the current token's type, or of a previous
                token's type. </para>
            </listitem>
            <listitem>
              <para> The tag a previous token. </para>
            </listitem>
          </itemizedlist>

          <para> Try to choose context elements that you believe will help the tagger decide
            which tag is appropriate. Keep in mind the trade-off between more specific
            taggers with accurate results; and more general taggers with broader coverage. </para>

          <para> Use <literal>BackoffTagger</literal> to combine your tagger with other
            taggers. How does the combined tagger's accuracy compare to the basic tagger?
            How does the combined tagger's accuracy compare to the combined taggers you
            created in the previous exercise?
          </para>
 
        </listitem>

        <listitem>
          <formalpara>
            <title> Reverse sequential taggers </title>
            <para> Since sequential taggers tag tokens in order, one at a time, they can only
              use the predicted tags to the <emphasis>left</emphasis> of the current token to
              decide what tag to assign to a token. But in some cases, the
              <emphasis>right</emphasis> context can provide more information about what tag
              should be used. A <glossterm>reverse sequential tagger</glossterm> is a tagger
              that: </para>
          </formalpara>

          <orderedlist>
            <listitem>
              <para> Assigns tags to one token at a time, starting with the last token of
                the text, and proceeding in right-to-left order. </para>
            </listitem>
            <listitem>
              <para> Decides which tag to assign a token on the basis of that token, the
                tokens that follow it, and the predicted tags for the tokens that follow
                it. </para>
            </listitem>
          </orderedlist>

          <para> There is no need to create new classes to perform reverse sequential tagging.
            By reversing texts at appropriate times, we can use sequential tagging classes
            to perform reverse sequential tagging. In particular, we should reverse the
            training text before we train the tagger; and reverse the text that we wish to
            tag both before and after we use the sequential tagger. </para>
          <para> Use this technique to create a first order reverse sequential tagger. Measure
            its accuracy on a tagged section of the Brown corpus. Be sure to use a different
            section of the corpus for testing than you used for training. How does its
            accuracy compare to a first order sequential tagger, using the same training
            data and test data? </para>
        </listitem>

        <listitem>
          <formalpara>
            <title> Processing individual sentences </title>
            <para> [to be written] Write a modified nth order tagger, that ignores tags that are
              in a previous sentence. E.g., for a 3nd order tagger, if the previous 3 words
              were "dog/NN ./. A/DT", then just use "DT" and the current token as context. </para>
          </formalpara>
        </listitem>

        <listitem>
          <formalpara>
            <title> Alternatives to backoff </title>
            <para> [to be written] Create a new kind of tagger that combines 2 or more
              subtaggers. </para>
          </formalpara>
        </listitem>
      </orderedlist>

    </section>
        <!-- Exercises -->
  </section>
    <!-- Tagging in NLTK --> &index;</article>
<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:nil
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/home/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->
