<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!ENTITY ling "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <title>NLTK Tutorial: Tagging</title>
    &copyright;
  </articleinfo>

<section id="intro">
    <title>Introduction</title>

<para> This chapter addresses the following question: Once we know
that something is a word, what linguistic
<glossterm>category</glossterm> should it be assigned to?
</para>

</section>

<section id="pos">
    <title>Word Classes and Parts of Speech</title>
<!-- 
X.2 linguistic overview (for non-linguist readers)
- how have linguists addressed the problem?
- what are the shortcomings of the non-computational approach?

% tr -sc 'A-Za-z0-9' '\012' &lt; wsj_0034 | sort 
-->
<para>There is a long tradition within linguistics of classifying
    words into different categories. These categories are also called
    <firstterm>parts of speech</firstterm>. Familiar examples are
    <type>noun</type>, <type>verb</type>, <type>preposition</type>,
    <type>adjective</type> and <type>adverb</type>. How do we know what
    category a word should belong to? In general, linguists invoke three
    kinds of criteria for making the decision:
    <itemizedlist>
     <listitem>
      <para>formal;</para>
     </listitem>
     <listitem>
      <para>syntactic (or distributional);</para>
     </listitem>
     <listitem>
      <para>notional (or semantic).</para>
     </listitem>
    </itemizedlist>

A <firstterm>formal</firstterm> criterion is one which looks at the
internal structure of a word. For example, <literal>-ness</literal> is a
suffix which combines with an adjective to produce a noun. Examples are
<literal>happy</literal> &gt; <literal>happiness</literal>,
<literal>ill</literal> &gt; <literal>illness</literal>. So if we
encounter a word which ends in <literal>-ness</literal>, this is very
likely to be a noun.
</para>

<para>
A <firstterm>syntactic</firstterm> criterion refers to the syntactic
contexts in which a word can occur. For example, assume that we have
already determined the category of nouns. Then  we might say that a
syntactic criterion for an adjective in English is that it can occur
immediately before a noun, or immediately following the words
<literal>be</literal> or <literal>very</literal>. According to these
tests, <literal>near</literal> should be categorized as an adjective:
<orderedlist>
<listitem>
<para>the near window</para>
</listitem>
<listitem>
<para>The end is (very) near.</para>
</listitem>
</orderedlist>
</para>

<para>A familier example of a <firstterm>notional</firstterm> criterion is
that a noun is <quote>the name of a person, place or thing</quote>.
Within modern linguistics, notional criteria for
word classes have be viewed with considerable suspicion, mainly because
they are hard to formalize. Nevertheless, notional criteria underpin
many of our intuitions about word classes, and enable us to make a good
guess about the categorization of words in languages that we are
unfamiliar with; that is, if we all we know about the Dutch
<literal>verjaardag</literal> is that it means the same as the English
word <literal>birthday</literal>, then we can guess that
<literal>verjaardag</literal> is a noun in Dutch. However, some care is
needed: although we might translate <literal>zij is van dag
jarig</literal> as <literal>it's her birthday today</literal>, the word
<literal>jarig</literal> is in fact an adjective in Dutch, and has no
exact equivalent in English.</para>

</section>

    <section id="tagging">
    <title> Computational Approaches to Tagging </title>
<!--
X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)
-->
   <para> When processing a text, it is often useful to associate
    auxiliary information with each token.  For example, we might
    want to label each token with its part of speech; or we might want
    to disambiguate homonyms by associating them with "word sense"
    labels.  This kind of auxiliary information is typically used in
    later stages of text processing.  For example, part of speech
    labels could be used to derive the internal structure of a
    sentence; and "word sense" labels could be used to allow a
    question-answering system to distinguish homonyms. </para>

    <para> The process of associating labels with each token in a text
    is called <glossterm>tagging</glossterm>, and the labels are
    called <glossterm>tags</glossterm>.  The collection of tags used
    for a particular task is known as a <glossterm>tag
    set</glossterm>. </para>
   
    <para> <glossterm>Part-of-speech tagging</glossterm> is the most
    common example of tagging, and it is the example we will examine
    in this tutorial.  But you should keep in mind that most of the
    techniques we discuss here can also be applied to many other
    tagging problems. </para>

    <para> Part-of-speech tags divide words into categories, based on
    how they can be combined to form sentences.  For example,
    articles can combine with nouns, but not verbs.  Part-of-speech
    tags also give information about the semantic content of a word.
    For example, nouns typically express "things," and prepositions
    express relationships between "things." </para>

    <para> Most part-of-speech tag sets make use of the same basic
    categories, such as "noun," "verb," "adjective," and
    "preposition."  However, tag sets differ both in how finely they
    divide words into categories; and in how define their categories.
    For example, "is" might be tagged as a verb in one tag set; but as
    a form of "to be" in another tag set.  This variation in tag sets
    is reasonable, since part-of-speech tags are used in different
    ways for different tasks. </para>

    <!-- How do I make "Table 1" not use a hard-coded number?? -->
    <para> In this tutorial, we will use the tag set listed in Table
    1.  This tag set is a simplification of the commonly used
    <glossterm>Brown Corpus tag set</glossterm>.  The complete Brown
    Corpus tag set has 87 basic tags.  For more information on tag
    sets, see <citetitle>Foundations of Statistical Natural Language
    Processing</citetitle>
    (<author><surname>Manning</surname></author> &amp;
    <author><surname>Schutze</surname></author>), pp. 139-145,
    or <citetitle>Speech and Language Processing</citetitle>
    (<author><surname>Jurafsky</surname></author> &amp;
    <author><surname>Martin</surname></author>) p. 297.
</para>

    <table id="table.tagset"> <title>Tag Set</title> 
      <tgroup cols="2"><tbody>
          <row>
            <entry>AT</entry>
            <entry>Article</entry>
          </row>
          <row>
            <entry>NN</entry>
            <entry>Noun</entry>
          </row>
          <row>
            <entry>VB</entry>
            <entry>Verb</entry>
          </row>
          <row>
            <entry>JJ</entry>
            <entry>Adjective</entry>
          </row>
          <row>
            <entry>IN</entry>
            <entry>Preposition</entry>
          </row>
          <row>
            <entry>CD</entry>
            <entry>Number</entry>
          </row>
          <row>
            <entry>END</entry>
            <entry>Sentence-ending punctuation</entry>
          </row>
        </tbody>
      </tgroup>
    </table>



    </section>

<!--
    <section> <title> Advanced Topics in Tagging (optional) </title>

    <para></para>

X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions


    </section>
-->

    <section> <title> Tagging in NLTK </title>

<!--
X.5 implementation
    - how does NLTK do it?
    - simple problems and worked solutions
    - suggested projects (e.g. for your MSc students)
-->



  <section id="basics">
      <title> The nltk.tagger Module </title>

    <para> The <ulink
    url="&refdoc;/nltk.tagger-module.html"
    ><literal>nltk.tagger</literal></ulink> module defines the classes
    and interfaces used by NLTK to perform tagging.  </para>

<!--
    <section id="basics.TaggedType"> <title> TaggedType </title>

      <para> NLTK defines a simple class, <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html"
      ><literal>TaggedType</literal></ulink>, for representing the text
      type of a tagged token.  A <literal>TaggedType</literal>
      consists of a <glossterm>base type</glossterm> and a
      <glossterm>tag</glossterm>.  Typically, the base type and the
      tag will both be strings.  For example, the tagged type for the
      noun "dog" would have the base type <literal>'dog'</literal> and
      the tag <literal>'NN'</literal>.  A tagged type with base type
      <replaceable>b</replaceable> and tag
      <replaceable>t</replaceable> is written
      <replaceable>b</replaceable><literal>/</literal><replaceable>t</replaceable>.
      Tagged types are created with the <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#__init__"
      ><literal>TaggedType</literal></ulink> constructor: </para>

<programlisting>
    &prompt;<command> ttype1 = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttype2 = TaggedType('runs', 'VB') </command>
    'runs'/'VB'
</programlisting>

      <para> A <literal>TaggedType</literal>'s base type is accessed
      via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#base"
      ><literal>base</literal></ulink> member function; and its tag is
      accessed via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#tag"
      ><literal>tag</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> ttype1.base() </command>
    'dog'
    &prompt;<command> ttype2.tag() </command>
    'VB'
</programlisting>

      <para> To construct a tagged token, simply use the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal></ulink> constructor with a
      <literal>TaggedType</literal>: </para>

<programlisting>
    &prompt;<command> ttype = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttoken = Token(ttype, Location(5)) </command>
    'dog'/'NN'@[5]
</programlisting>

    </section>
-->

    <section id="basics.corpora"> <title> Reading Tagged Corpora </title>

      <para> Several large corpora (such as the Brown Corpus and
      portions of the Wall Street Journal) have been manually tagged
      with part-of-speech tags.  These corpora are primarily useful
      for testing taggers and for training statistical taggers.
      However, before we can use these corpora, we must read them from
      files and tokenize them. </para>

      <para> Tagged texts are usually stored in files as a sequences
      of whitespace-separated tokens, where each token is of the form
      <replaceable>base</replaceable><literal>/</literal><replaceable>tag</replaceable>.
      Figure 1 shows an example of some tagged text, taken from the
      Brown corpus. </para>

      <figure> <title> An Example of Tagged Text (excerpted from the Brown Corpus) </title>
        <titleabbrev>Example of Tagged Text</titleabbrev>
<screen>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt
it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb
generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in
the/at best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.
</screen>
      </figure>

      <para> To tokenize tagged texts of this form, the
      <literal>nltk.tagger</literal> module defines the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedTokenizer-class.html"
      ><literal>TaggedTokenizer</literal></ulink> class: </para>

<programlisting>
    &prompt;<command> tagged_text_str = open('corpus.txt').read() </command>
    'John/NN saw/VB the/AT book/NN on/IN the/AT 
     table/NN ./END  He/NN sighed/VB ./END'
    &prompt;<command> tagged_text_token = Token(TEXT=tagged_text_str) </command>
    &prompt;<command> TaggedTokenizer().tokenize(tagged_text_token) </command>
    &prompt;<command> print tagged_text_token </command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, 
     &lt;table/NN>, &lt;./END>, &lt;He/NN>, &lt;sighed/VB>, &lt;./END>]&gt;
</programlisting>

      <para> If <literal>TaggedTokenizer</literal> encounters a word
      without a tag, it will assign it the default tag
      <literal>None</literal>. </para>

      <para> It is possible to use the <literal>nltk.corpus</literal>
      module to read and tokenize data from a tagged corpus, as shown below:
      </para>

<programlisting>
    &prompt;<command> from nltk.corpus import brown</command>
    &prompt;<command> brown.tokenize('ca01')</command>
    &lt;[&lt;The/at>, &lt;Fulton/np-tl>, &lt;County/nn-tl>, &lt;Grand/jj-tl>, &lt;Jury/nn-tl>, &lt;said/vbd>, &lt;Friday/nr>, &lt;an/at>, &lt;investigation/nn>, &lt;of/in>, &lt;Atlanta's/np$>, &lt;recent/jj>, &lt;primary/nn>, &lt;election/nn>, &lt;produced/vbd>, &lt;``/``>, &lt;no/at>, &lt;evidence/nn>, &lt;''/''>, &lt;that/cs>, &lt;any/dti>, &lt;irregularities/nns>, &lt;took/vbd>, &lt;place/nn>, &lt;./.>, ...]&gt;
</programlisting>

    </section> <!-- TaggedTokenizer -->

    <section id="basics.TaggerI"> <title> The TaggerI Interface </title>

      <para> The <literal>nltk.tagger</literal> module defines <ulink
      url="&refdoc;/nltk.tagger.TaggerI-class.html"
      ><literal>TaggerI</literal></ulink>, a general interface for
      tagging texts.  This interface is used by all taggers.  It
      defines a single method, <ulink
      url="&refdoc;/nltk.tagger.TaggerI-class.html#tag"
      ><literal>tag</literal></ulink>, which assigns a tag to each
      token in a list, and returns the resulting list of tagged
      tokens.</para>

<programlisting>
    &prompt;<command> text = open('untagged.txt').read() </command>
    &prompt;<command> print text </command>
    &lt;John saw the book on the table . He sighed .>
    &prompt;<command> text_token = Token(TEXT=text) </command>
    &prompt;<command> WSTokenizer().tokenize(text_token) </command>
    &prompt;<command> print text_token </command>
    &lt;[&lt;John>, &lt;saw>, &lt;the>, &lt;book>, &lt;on>, &lt;the>, &lt;table>, &lt;.>, &lt;He>, &lt;sighed>, &lt;.>]&gt;
    &prompt;<command> my_tagger.tag(text_token) </command>
    &prompt;<command> print text_token </command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>,
    &lt;table/NN>, &lt;./END>, &lt;He/NN>, &lt;sighed/VB>, &lt;./END>]&gt;
</programlisting>

    <para>Note that the tagged tokens are represented in the
    <literal>SUBTOKENS</literal> property, and that the tags themselves
    are stored in the <literal>POS</literal> property.  The
    <literal>properties</literal> member function of a token lists
    the properties defined for that token, in case there is ever any
    doubt about which property names have been used:
    </para>

<programlisting>
    &prompt;<command> print text_token['SUBTOKENS'][1] </command>
    &lt;saw/VB>
    &prompt;<command> print text_token['SUBTOKENS'][1]['POS'] </command>
    'VB'
    &prompt;<command> print text_token.properties() </command>
    ['TEXT', 'SUBTOKENS']
    &prompt;<command> print text_token['SUBTOKENS'][1].properties() </command>
    ['TEXT', 'POS']
</programlisting>

    </section> <!-- TaggerI -->

  </section> <!-- The nltk.tagger module -->

  <section id="taggers"> <title> Taggers </title>

    <para> The <literal>nltk.tagger</literal> module currently defines
    four taggers; this list will likely grow in the future.  This
    section describes the taggers currently implemented by
    <literal>nltk.tagger</literal>, and how they are used. </para>
    
    <section id="taggers.default"> <title> The Default Tagger </title>

      <para> The simplest tagger defined by
      <literal>nltk.tagger</literal> is <ulink
      url="&refdoc;/nltk.tagger.DefaultTagger-class.html"
      ><literal>DefaultTagger</literal></ulink>.  This tagger assigns the
      same tag to each token regardless of its text.</para>

<programlisting>
    &prompt;<command> WSTokenizer().tokenize(text_token) </command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John>, &lt;saw>, &lt;3>, &lt;polar>, &lt;bears>, &lt;.>]>
    &prompt;<command> my_tagger = DefaultTagger('NN') </command>
    &prompt;<command> my_tagger.tag(text_token) </command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/NN>, &lt;3/NN>, &lt;polar/NN>, &lt;bears/NN>, &lt;./NN>]&gt;
</programlisting>

      <para> This is a simple algorithm, and it performs poorly when
      used by itself.  On a typical corpus, it will
      tag only 20%-30% of the tokens correctly.  However, it is a very
      reasonable tagger to use as a default, if a more advanced tagger
      fails to determine a token's tag.  When used in conjunction with
      other taggers, a <literal>DefaultTagger</literal> can significantly
      improve performance. </para>
      
    </section> <!-- DefaultTagger -->

    <section id="taggers.regexp"> <title> The Regular Expression Tagger </title>

      <para> The regular expression tagger assigns tags to tokens on the
      basis of regular expression matching.</para>

<programlisting>

    &prompt;<command> NN_CD_tagger = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')]) </command>
    &prompt;<command> NN_CD_tagger.tag(text_token) </command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/NN>, &lt;3/CD>, &lt;polar/NN>, &lt;bears/NN>, &lt;./NN>]&gt;
</programlisting>

      <para> This is a simple algorithm, and it performs poorly when
      used by itself.  On a typical corpus, it will
      tag only 20%-30% of the tokens correctly.  However, it is a very
      reasonable tagger to use as a default, if a more advanced tagger
      fails to determine a token's tag.  When used in conjunction with
      other taggers, a <literal>DefaultTagger</literal> can significantly
      improve performance. </para>
      
    </section> <!-- DefaultTagger -->

    <section id="taggers.unigram"> <title> The Unigram Tagger </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger-class.html"
      ><literal>UnigramTagger</literal></ulink> class implements a
      simple statistical tagging algorithm: for each token, it assigns
      the tag that is most likely for that token's text.  For example,
      it will assign the tag "JJ" to any occurrence of the word
      "frequent," since "frequent" is used as an adjective (e.g. "a
      frequent word") more often than it is used as a verb (e.g. "I
      frequent this cafe"). </para>

      <para> Before a <literal>UnigramTagger</literal> can be used to
      tag data, it must be trained on a <glossterm>training
      corpus</glossterm>.  It uses this corpus to determine which tags
      are most common for each word.
      <literal>UnigramTaggers</literal> are trained using the <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger-class.html#train"
      ><literal>train</literal></ulink> method, which takes a
      tagged corpus: </para>

<programlisting>
    <emphasis># 'train.txt' is a tagged training corpus</emphasis>
    &prompt;<command> tagged_text_str = open('train.txt').read()</command>
    &prompt;<command> tagged_text_token = Token(TEXT=tagged_text_str)</command>
    &prompt;<command> TaggedTokenizer().tokenize(tagged_text_token)</command>
    &prompt;<command> tagger = UnigramTagger()</command>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once a <literal>UnigramTagger</literal> has been trained,
      the <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html#tag"
      ><literal>tag</literal></ulink> can be used to tag
      untagged corpora: </para>

<programlisting>
    &prompt;<command> WSTokenizer().tokenize(text_token)</command>
    &prompt;<command> tagger.tag(text_token)</command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, ...]&gt;
</programlisting>

      <para> <literal>UnigramTagger</literal> will assign the default
      tag <literal>None</literal> to any token that was not
      encountered in the training data. </para>

      <para> Note that, like almost all statistical taggers, the
      performance of <literal>UnigramTagger</literal> is highly
      dependent on the quality of its training set.  In particular, if
      the training set is too small, it will not be able to reliably
      estimate the most likely tag for each word.  Performance will
      also suffer if the training set is significantly different than
      the texts we wish to tag. </para>
      
    </section> <!-- UnigramTagger -->

    <section id="taggers.nthorder"> <title> Nth-Order Tagging </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.NthOrderTagger-class.html"
      ><literal>NthOrderTagger</literal></ulink> class implements a
      more advanced statistical tagging algorithm.  In addition to
      considering the token's text, it also considers the
      part-of-speech tags of the <replaceable>n</replaceable> preceding
      tokens. </para>

      <para> To decide which tag to assign to a token,
      <literal>NthOrderTagger</literal> first constructs a
      <glossterm>context</glossterm> for the token.  This context
      consists of the token's text, along with the part-of-speech tags
      of the <replaceable>n</replaceable> preceding tags.  It then
      picks the tag which is most likely for that context.  Note that
      a 0th order tagger is equivalent to a unigram tagger, since the
      context used to tag a token is just its text.  First order taggers
      are sometimes called <glossterm>bigram taggers</glossterm>, and
      second order taggers are called <glossterm>trigram
      taggers</glossterm>. </para>

      <para> <literal>NthOrderTagger</literal> uses a tagged training
      corpus to determine which part-of-speech tag is most likely for
      each context:</para>

<programlisting>
    &prompt;<command> TaggedTokenizer().tokenize(tagged_text_token)</command>
    &prompt;<command> tagger = NthOrderTagger(3)</command>         <emphasis># 3rd order tagger</emphasis>
    &prompt;<command> tagger.train(tagged_text_token)</command>
</programlisting>

      <para> Once an <literal>NthOrderTagger</literal> has been trained,
      it can be used to tag untagged corpora: </para>

<programlisting>
    &prompt;<command> WSTokenizer().tokenize(text_token)</command>
    &prompt;<command> tagger.tag(text_token)</command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, ...]&gt;
</programlisting>

      <para> <literal>NthOrderTagger</literal> will assign the default
      tag <literal>None</literal> to any token whose context was not
      encountered in the training data. </para>

      <para> Note that as <replaceable>n</replaceable> gets larger,
      the specificity of the contexts increases; and with it, the
      chance that the data we wish to tag will contain contexts that
      were not present in the training data.  Thus, there is a
      trade-off between the accuracy and the coverage of our results.
      This is a common type of trade-off in natural language
      processing.  It is closely related to the
      <glossterm>precision/recall trade-off</glossterm> that we'll
      encounter later when we discuss information retrieval. </para>

    </section> <!-- NthOrderTagger -->

    <section id="tagger.backoff"> <title> Combining Taggers </title>

      <para> One way to address the trade-off between accuracy and
      coverage is to use the more accurate algorithms when we can, but
      to fall back on algorithms with wider coverage when necessary.
      For example, we could combine the results of a first order tagger,
      a 0th order tagger, and an <literal>NN_CD_Tagger</literal>, as
      follows:</para>

      <orderedlist>
        <listitem> 
          <para> Try tagging the token with the first order tagger. </para>
        </listitem>
        <listitem> 
          <para> If the first order tagger is unable to find a tag for
          the token, try finding a tag with the 0th order
          tagger. </para>
        </listitem> 
        <listitem> 
          <para> If the 0th order tagger is also unable to find a tag,
          use the <literal>NN_CD_Tagger</literal> to find a tag. </para>
        </listitem>
      </orderedlist>

      <para> NLTK defines the <ulink
      url="&refdoc;/nltk.tagger.BackoffTagger-class.html"
      ><literal>BackoffTagger</literal></ulink> class for combining
      taggers in this way.  A <literal>BackoffTagger</literal> is
      constructed from an ordered list of one or more
      <glossterm>subtaggers</glossterm>.  For each token in the input,
      the <literal>BackoffTagger</literal> uses the result of the
      first tagger in the list that successfully found a tag.  Taggers
      indicate that they are unable to tag a token by assigning it the
      special tag <literal>None</literal>.  We can use a
      <literal>BackoffTagger</literal> to implement the strategy
      proposed above: </para>

<programlisting>
    &prompt;<command> TaggedTokenizer().tokenize(text_token)</command>

    <emphasis># Construct the taggers</emphasis>
    &prompt;<command> tagger1 = NthOrderTagger(1)</command>         <emphasis># first order tagger</emphasis>
    &prompt;<command> tagger2 = UnigramTagger()</command>           <emphasis># zeroth order tagger</emphasis>
    &prompt;<command> tagger3 = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')]) </command>

    <emphasis># Train the taggers</emphasis>
    &prompt;<command> tagger1.train(train_toks)</command>
    &prompt;<command> tagger2.train(train_toks)</command>

    <emphasis># Combine the taggers</emphasis>
    &prompt;<command> tagger = BackoffTagger([tagger1, tagger2, tagger3])</command>
</programlisting>

      <para> Note that the order in which the taggers are given to
      <literal>BackoffTagger</literal> is important: the taggers
      should be listed in the order that they should be tried.  This
      typically means that more specific taggers should be listed
      before less specific taggers. </para>

      <para> Having defined a combined tagger, we can use it to tag
      new corpora: </para>

<programlisting>
    &prompt;<command> TaggedTokenizer().tokenize(text_token)</command>
    &prompt;<command> tagger.tag(text_token)</command>
    &prompt;<command> print text_token</command>
    &lt;[&lt;John/NN>, &lt;saw/VB>, &lt;the/AT>, &lt;book/NN>, &lt;on/IN>, &lt;the/AT>, ...]&gt;
</programlisting>
      
    </section> <!-- Combining Tagger -->

  </section> <!-- Taggers -->

<!-- old material on implementation that appeared here to be moved to
an implementation tutorial - SB -->

  <section id="exercises">
    <title>Exercises</title>

    <section id="exercises.combine">
      <title> Combining Taggers with BackoffTagger</title>

      <para> There is typically a trade-off between the accuracy and
      coverage for taggers: taggers that use more specific contexts
      usually produce more accurate results, when they have seen those
      contexts in the training data; but because the training data is
      limited, they are less likely to encounter each context.  The
      <literal>BackoffTagger</literal> addresses this problem by
      trying taggers with more specific contexts first; and falling
      back to the more general taggers when necessary.  In this
      exercise, we examine the effects of using
      <literal>BackoffTagger</literal>. </para>
      
      <orderedlist>
        
        <listitem> <para> Create a <literal>DefaultTagger</literal> or
        a <literal>RegexpTagger</literal>, and a
        <literal>UnigramTagger</literal>, and a
        <literal>NthOrderTagger</literal>.  Train the
        <literal>UnigramTagger</literal>, and the
        <literal>NthOrderTagger</literal> using a tagged section of
        the Brown corpus.</para> </listitem>

        <listitem>
        <para> Test the performance of each tagger, using a tagged
        section of the Brown corpus.  Record the
        <glossterm>accuracy</glossterm> of the tagger (the
        percentage of tokens that are correctly tagged).  Be sure to
        use a different section of the corpus for testing than you
        used for training. </para> </listitem>
        
        <listitem> <para> Use <literal>BackoffTagger</literal> to
        create three different combinations of the basic taggers.
        Test the accuracy of each combined tagger.  Which
        combinations give the most improvement?  </para>
        </listitem>

        <listitem> <para> Try repeating steps 1-3 with a different
        sized training corpus.  How does it affect your results? </para>
        </listitem>
      </orderedlist>
    </section> <!-- Combine -->
        
    <section id="exercises.context">
      <title> Tagger Context </title>
      
      <para> <literal>NthOrderParser</literal> chooses a tag for a
      token based on its type and the tags of the
      <replaceable>n</replaceable> preceeding tokens.  This is a
      common context to use for parsing, but ceratinly not the only
      possible context. </para>

      <para> Construct a new tagger, subclassed from
      <literal>SequentialTagger</literal>, that uses a different
      context.  If your tagger's context contains multiple elements,
      then you should combine them in a <literal>tuple</literal>.
      Some possibilities for elements to include are: </para>

      <itemizedlist>
        <listitem> <para> The base type of the current token, or of
        a previous token. </para> </listitem>
        <listitem> <para> The length of the current token's type, or
        of a previous token's type. </para> </listitem>
        <listitem> <para> The first letter of the current token's
        type, or of a previous token's type. </para> </listitem>
        <listitem> <para> The tag a previous token. </para>
        </listitem>
      </itemizedlist>

      <para> Try to choose context elements that you believe will help
      the tagger decide which tag is appropriate.  Keep in mind the
      trade-off between more specific taggers with accurate results;
      and more general taggers with broader coverage. </para>

      <para> Use <literal>BackoffTagger</literal> to combine your
      tagger with other taggers.  How does the combined tagger's
      accuracy compare to the basic tagger?  How does the combined
      tagger's accuracy compare to the combined taggers you created
      in the previous exercise? </para>
    </section> <!-- Tagger Context -->

    <section id="exercises.reverse">
      <title> Reverse Sequential Taggers </title>
      
      <para> Since sequential taggers tag tokens in order, one at a
      time, they can only use the predicted tags to the
      <emphasis>left</emphasis> of the current token to decide what
      tag to assign to a token.  But in some cases, the
      <emphasis>right</emphasis> context can provide more information
      about what tag should be used.  A <glossterm>reverse sequential
      tagger</glossterm> is a tagger that: </para>

      <orderedlist>
        <listitem><para> Assigns tags to one token at a time, starting
        with the last token of the text, and proceeding in
        right-to-left order. </para> </listitem>
        <listitem><para> Decides which tag to assign a token on the
        basis of that token, the tokens that follow it, and the
        predicted tags for the tokens that follow it. </para>
        </listitem>
      </orderedlist>

      <para> There is no need to create new classes to perform reverse
      sequential tagging.  By reversing texts at appropriate times, we
      can use sequential tagging classes to perform reverse sequential
      tagging.  In particular, we should reverse the training text
      before we train the tagger; and reverse the text that we wish to
      tag both before and after we use the sequential tagger. </para>

      <para> Use this technique to create a first order reverse
      sequential tagger.  Measure its accuracy on a tagged section of
      the Brown corpus.  Be sure to use a different section of the
      corpus for testing than you used for training.  How does its
      accuracy compare to a first order sequential tagger, using the
      same training data and test data? </para>
    </section> <!-- Reverse -->

    <section id="example.restart">
      <title> Processing Individual Sentences </title>

      <para> [to be written] Write a modified nth order tagger, that
      ignores tags that are in a previous sentence.  E.g., for a 3nd
      order tagger, if the previous 3 words were "dog/NN ./.  A/DT",
      then just use "DT" and the current token as context. </para>
    </section> <!-- restart at sentences -->

    <section id="example.backoff">
      <title> Alternatives to Backoff </title>

      <para> [to be written] Create a new kind of tagger that combines
      2 or more subtaggers. </para>
    </section> <!-- Alternatives to backoff -->

  </section> <!-- Exercises -->
  </section> <!-- Tagging in NLTK -->

  &index;
</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:nil
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/home/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

