<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.1//EN" [

<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "http://nltk.sourceforge.net/ref">
<!ENTITY tutdoc "http://nltk.sourceforge.net/tutorial">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<!-- Memo to self: add xrefs.  
Targets like: <section id="foo">
Refs like: <xref linkend="foo"></>
-->

<article>

<!--
ideas for student projects:
  - combine taggers & figure out what works best
  - make new tagger:
    - ReverseTagger - like sequential, but backwards
    - "restart" at each new sentence? (punctuation)
    - other types of nth order - different contexts?
    - other types of backoff
-->

  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Tagging</title>
  </articleinfo>

  <section> <title> Introduction </title>

    <para> When processing a text, it is often useful to associate
    auxiliary information with each token.  For example, we might
    want to label each token with its part of speech; or we might want
    to disambiguate homonyms by associating them with "word sense"
    labels.  This kind of auxiliary information is typically used in
    later stages of text processing.  For example, part of speech
    labels could be used to derive the internal structure of a
    sentence; and "word sense" labels could be used to allow a
    question-answering system to distinguish homonyms. </para>

    <indexterm><primary>tagging</primary></indexterm>
    <indexterm><primary>tags</primary></indexterm>
    <indexterm><primary>tag set</primary></indexterm>
    <para> The process of associating labels with each token in a text
    is called <glossterm>tagging</glossterm>, and the labels are
    called <glossterm>tags</glossterm>.  The collection of tags used
    for a particular task is known as a <glossterm>tag
    set</glossterm>. </para>
   
    <indexterm><primary>Part-of-speech tagging</primary></indexterm>
    <para> <glossterm>Part-of-speech tagging</glossterm> is the most
    common example of tagging, and it is the example we will examine
    in this tutorial.  But you should keep in mind that most of the
    techniques we discuss here can also be applied to many other
    tagging problems. </para>

  </section> <!-- Intro -->

  <section> <title> Part-of-Speech Tagging </title>

    <para> Part-of-speech tags divide words into categories, based on
    how they can be combined to form sentences.  For example,
    articles can combine with nouns, but not verbs.  Part-of-speech
    tags also give information about the semantic content of a word.
    For example, nouns typically express "things," and prepositions
    express relationships between "things." </para>

    <para> Most part-of-speech tag sets make use of the same basic
    categories, such as "noun," "verb," "adjective," and
    "preposition."  However, tag sets differ both in how finely they
    divide words into categories; and in how define their categories.
    For example, "is" might be tagged as a verb in one tag set; but as
    a form of "to be" in another tag set.  This variation in tag sets
    is reasonable, since part-of-speech tags are used in different
    ways for different tasks. </para>

    <!-- How do I make "Table 1" not use a hard-coded number?? -->
    <indexterm><primary>Brown Corpus tag set</primary></indexterm>
    <para> In this tutorial, we will use the tag set listed in Table
    1.  This tag set is a simplification of the commonly used
    <glossterm>Brown Corpus tag set</glossterm>.  The complete Brown
    Corpus tag set has 87 basic tags.  For more information on tag
    sets, see <citetitle>Foundations of Statistical Natural Language
    Processing</citetitle>
    (<author><surname>Manning</surname></author> &amp;
    <author><surname>Schutze</surname></author>), pp. 139-145. </para>

    <table id="table.tagset"> <title>Tag Set</title> 
      <tgroup cols="2"><tbody>
          <row>
            <entry>AT</entry>
            <entry>Article</entry>
          </row>
          <row>
            <entry>NN</entry>
            <entry>Noun</entry>
          </row>
          <row>
            <entry>VB</entry>
            <entry>Verb</entry>
          </row>
          <row>
            <entry>JJ</entry>
            <entry>Adjective</entry>
          </row>
          <row>
            <entry>IN</entry>
            <entry>Preposition</entry>
          </row>
          <row>
            <entry>CD</entry>
            <entry>Number</entry>
          </row>
          <row>
            <entry>END</entry>
            <entry>Sentence-ending punctuation</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

  </section> <!-- Part-of-Speech Tagging -->

  <section> <title> The nltk.tagger Module </title>

    <para> The <ulink
    url="&refdoc;/nltk.tagger.html">
    <literal>nltk.tagger</literal></ulink> module defines the classes
    and interfaces used by NLTK to perform tagging.  </para>

    <section> <title> TaggedType </title>

      <indexterm><primary>base type</primary></indexterm>
      <indexterm><primary>tag</primary></indexterm>
      <para> NLTK defines a simple class, <ulink
      url="&refdoc;/nltk.tagger.TaggedType.html">
      <literal>TaggedType</literal></ulink>, for representing the text
      type of a tagged token.  A <literal>TaggedType</literal>
      consists of a <glossterm>base type</glossterm> and a
      <glossterm>tag</glossterm>.  Typically, the base type and the
      tag will both be strings.  For example, the tagged type for the
      noun "dog" would have the base type <literal>'dog'</literal> and
      the tag <literal>'NN'</literal>.  A tagged type with base type
      <replaceable>b</replaceable> and tag
      <replaceable>t</replaceable> is written
      <replaceable>b</replaceable><literal>/</literal><replaceable>t</replaceable>.
      Tagged types are created with the <ulink
      url="&refdoc;/nltk.tagger.TaggedType.html#__init__">
      <literal>TaggedType</literal></ulink> constructor: </para>

<programlisting>
    &prompt;<command> ttype1 = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttype2 = TaggedType('runs', 'VB') </command>
    'runs'/'VB'
</programlisting>

      <para> A <literal>TaggedType</literal>'s base type is accessed
      via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType.html#base">
      <literal>base</literal></ulink> member function; and its tag is
      accessed via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType.html#tag">
      <literal>tag</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> ttype1.base() </command>
    'dog'
    &prompt;<command> ttype2.tag() </command>
    'VB'
</programlisting>

      <para> To construct a tagged token, simply use the <ulink
      url="&refdoc;/nltk.token.Token.html#__init__">
      <literal>Token</literal></ulink> constructor with a
      <literal>TaggedType</literal>: </para>

<programlisting>
    &prompt;<command> ttype = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttoken = Token(ttype, Location(5)) </command>
    'dog'/'NN'@[5]
</programlisting>

    </section> <!-- TaggedType -->

    <section> <title> Reading Tagged Corpora </title>

      <para> Several large corpora (such as the Brown Corpus and
      portions of the Wall Street Journal) have been manually tagged
      with part-of-speech tags.  These corpora are primarily useful
      for testing taggers and for training statistical taggers.
      However, before we can use these corpora, we must read them from
      files and tokenize them. </para>

      <para> Tagged texts are usually stored in files as a sequences
      of whitespace-separated tokens, where each token is of the form
      <replaceable>base</replaceable><literal>/</literal><replaceable>tag</replaceable>.
      Figure 1 shows an example of some tagged text, taken from the
      Brown corpus. </para>

      <figure> <title> An Example of Tagged Text (excerpted from the Brown Corpus) </title>
        <titleabbrev>Example of Tagged Text</titleabbrev>
<screen>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt
it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb
generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in
the/at best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.
</screen>
      </figure>

      <para> To tokenize tagged texts of this form, the
      <literal>nltk.tagger</literal> module defines the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedTokenizer.html">
      <literal>TaggedTokenizer</literal></ulink> class: </para>

<programlisting>
    &prompt;<command> text_str = open('corpus.txt').read() </command>
    'John/NN saw/VB the/AT book/NN on/IN the/AT 
     table/NN ./END  He/NN sighed/VB ./END'
    &prompt;<command> tokens = TaggedTokenizer().tokenize(text_str) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

      <para> If <literal>TaggedTokenizer</literal> encounters a word
      without a tag, it will assign it the default tag
      <literal>None</literal>. </para>

    </section> <!-- TaggedTokenizer -->

    <section> <title> The TaggerI Interface </title>

      <para> The <literal>nltk.tagger</literal> module defines <ulink
      url="&refdoc;/nltk.tagger.TaggerI.html">
      <literal>TaggerI</literal></ulink>, a general interface for
      tagging texts.  This interface is used by all taggers.  It
      defines a single method, <ulink
      url="&refdoc;/nltk.tagger.TaggerI.html#tag">
      <literal>tag</literal></ulink>, which assigns a tag to each
      token in a list, and returns the resulting list of tagged
      tokens.</para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(text_str) </command>
    ['John'@[0w], 'saw'@[1w], 'the'@[2w], 'book'@[3w], 
     'on'@[4w], 'the'@[5w], 'table'@[6w], '.'@[7w], 
     'He'@[8w], 'sighed'@[9w], '.'@[10w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

    </section> <!-- TaggerI -->

  </section> <!-- The nltk.tagger module -->

  <section> <title> Taggers </title>

    <para> The <literal>nltk.tagger</literal> module currently defines
    four taggers; this list will likely grow in the future.  This
    section describes the taggers currently implemented by
    <literal>nltk.tagger</literal>, and how they are used. </para>
    
    <section> <title> A Default Tagger </title>

      <para> The simplest tagger defined by
      <literal>nltk.tagger</literal> is <ulink
      url="&refdoc;/nltk.tagger.NN_CD_Tagger.html">
      <literal>NN_CD_Tagger</literal></ulink>.  This tagger assigns a
      tag to each token on the basis of its type.  If its type appears
      to be a number, it assigns the type "CD."  Otherwise, it assigns
      the type "NN." </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(text_str) </command>
    ['John'@[0w], 'saw'@[1w], '3'@[2w], 
     'polar'@[3w], 'bears'@[4w], '.'@[5w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'NN'@[1w], '3'/'CD'@[2w], 
     'polar'/'NN'@[3w], 'bears'/'NN'@[4w], '.'/'NN'@[5w]]
</programlisting>

      <para> This is a simple algorithm, but it yields quite poor
      performance when used by itself.  On a typical corpus, it will
      tag only 20%-30% of the tokens correctly.  However, it is a very
      reasonable tagger to use as a default, if a more advanced tagger
      fails to determine a token's tag.  When used in conjunction with
      other taggers, <literal>NN_CD_Tagger</literal> can significantly
      improve performance. </para>
      
    </section> <!-- NN_CD_Tagger -->

    <section> <title> Unigram Tagging </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger.html">
      <literal>UnigramTagger</literal></ulink> class implements a
      simple statistical tagging algorithm: for each token, it assigns
      the tag that is most likely for that token's type.  For example,
      it will assign the tag "JJ" to any occurrence of the word
      "frequent," since "frequent" is used as an adjective (e.g. "a
      frequent word") more often than it is used as a verb (e.g. "I
      frequent this cafe"). </para>

      <indexterm><primary>training
      corpus</primary></indexterm>
      <para> Before a <literal>UnigramTagger</literal> can be used to
      tag data, it must be trained on a <glossterm>training
      corpus</glossterm>.  It uses this corpus to determine which tags
      are most common for each word.
      <literal>UnigramTaggers</literal> are trained using the <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger.html#train">
      <literal>train</literal></ulink> method, which takes a
      tagged corpus: </para>

<programlisting>
    <emphasis># 'train.txt' is a tagged training corpus</emphasis>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize('train.txt')</command>
    &prompt;<command> tagger = UnigramTagger()</command>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once a <literal>UnigramTagger</literal> has been trained,
      the <ulink url="&refdoc;/nltk.tagger.UnigramTagger.html#tag">
      <literal>tag</literal></ulink> can be used to tag
      untagged corpera: </para>

<programlisting>
    <emphasis># 'corpus.txt' is an untagged test corpus</emphasis>
    &prompt;<command> tokens = WSTokenizer().tokenize('corpus.txt')</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>UnigramTagger</literal> will assign the default
      tag <literal>None</literal> to any token whose type was not
      encountered in the training data. </para>

      <para> Note that, like almost all statistical taggers, the
      performance of <literal>UnigramTagger</literal> is highly
      dependent on the quality of its training set.  In particular, if
      the training set is too small, it will not be able to reliably
      estimate the most likely tag for each word.  Performance will
      also suffer if the training set is significantly different than
      the texts we wish to tag. </para>
      
    </section> <!-- UnigramTagger -->

    <section> <title> Nth Order Tagging </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.NthOrderTagger.html">
      <literal>NthOrderTagger</literal></ulink> class implements a
      more advanced statistical tagging algorithm.  In addition to
      considering the token's type, it also considers the
      part-of-speech tags of the <replaceable>n</replaceable> preceding
      tokens. </para>

      <indexterm><primary>context</primary></indexterm>
      <indexterm><primary>bigram taggers</primary></indexterm>
      <indexterm><primary>trigram taggers</primary></indexterm>
      <para> To decide which tag to assign to a token,
      <literal>NthOrderTagger</literal> first constructs a
      <glossterm>context</glossterm> for the token.  This context
      consists of the token's type, along with the part-of-speech tags
      of the <replaceable>n</replaceable> preceding tags.  It then
      picks the tag which is most likely for that context.  Note that
      a 0th order tagger is equivalent to a unigram tagger, since the
      context used to tag a token is just its type.  1st order taggers
      are sometimes called <glossterm>bigram taggers</glossterm>, and
      2nd order taggers are called <glossterm>trigram
      taggers</glossterm>. </para>

      <para> <literal>NthOrderTagger</literal> uses a tagged training
      corpus to determine which part-of-speech tag is most likely for
      each context:</para>

<programlisting>
    <emphasis># 'train.txt' is a tagged training corpus</emphasis>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize('train.txt')</command>
    &prompt;<command> tagger = NthOrderTagger(3)</command>         <emphasis># 3rd order tagger</emphasis>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once an <literal>NthOrderTagger</literal> has been trained,
      it can be used to tag untagged corpora: </para>

<programlisting>
    <emphasis># 'corpus.txt' is an untagged test corpus</emphasis>
    &prompt;<command> tokens = WSTokenizer().tokenize('corpus.txt')</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>NthOrderTagger</literal> will assign the default
      tag <literal>None</literal> to any token whose context was not
      encountered in the training data. </para>

      <indexterm><primary>precision/recall trade-off</primary></indexterm>
      <para> Note that as <replaceable>n</replaceable> gets larger,
      the specificity of the contexts increases; and with it, the
      chance that the data we wish to tag will contain contexts that
      were not present in the training data.  Thus, there is a
      trade-off between the accuracy and the coverage of our results.
      This is a common type of trade-off in natural language
      processing.  It is closely related to the
      <glossterm>precision/recall trade-off</glossterm> that we'll
      encounter later when we discuss information retrieval. </para>

    </section> <!-- NthOrderTagger -->

    <section> <title> Combining Taggers </title>

      <para> One way to address the trade-off between accuracy and
      coverage is to use the more accurate algorithms when we can, but
      to fall back on algorithms with wider coverage when necessary.
      For example, we could combine the results of a 1st order tagger,
      a 0th order tagger, and an <literal>NN_CD_Tagger</literal>, as
      follows:</para>

      <orderedlist>
        <listitem> 
          <para> Try tagging the token with the 1st order
          tagger. </para>
        </listitem>
        <listitem> 
          <para> If the 1st order tagger is unable to find a tag for
          the token, try finding a tag with the 0th order
          tagger. </para>
        </listitem> 
        <listitem> 
          <para> If the 0th order tagger is also unable to find a tag,
          use the <literal>NN_CD_Tagger</literal> to find a tag. </para>
        </listitem>
      </orderedlist>

      <indexterm><primary>subtaggers</primary></indexterm>
      <para> NLTK defines the <ulink
      url="&refdoc;/nltk.tagger.BackoffTagger.html">
      <literal>BackoffTagger</literal></ulink> class for combining
      taggers in this way.  A <literal>BackoffTagger</literal> is
      constructed from an ordered list of one or more
      <glossterm>subtaggers</glossterm>.  For each token in the input,
      the <literal>BackoffTagger</literal> uses the result of the
      first tagger in the list that successfully found a tag.  Taggers
      indicate that they are unable to tag a token by assigning it the
      special tag <literal>None</literal>.  We can use a
      <literal>BackoffTagger</literal> to implement the strategy
      proposed above: </para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize('train.txt')</command>

    <emphasis># Construct the taggers</emphasis>
    &prompt;<command> tagger1 = NthOrderTagger(1)</command>         <emphasis># 1st order tagger</emphasis>
    &prompt;<command> tagger2 = UnigramTagger()</command>           <emphasis># 0th order tagger</emphasis>
    &prompt;<command> tagger3 = NN_CD_Tagger()</command>

    <emphasis># Train the taggers</emphasis>
    &prompt;<command> tagger1.train(train_toks)</command>
    &prompt;<command> tagger2.train(train_toks)</command>

    <emphasis># Combine the taggers</emphasis>
    &prompt;<command> tagger = BackoffTagger([tagger1, tagger2, tagger3])</command>
</programlisting>

      <para> Note that the order in which the taggers are given to
      <literal>BackoffTagger</literal> is important: the taggers
      should be listed in the order that they should be tried.  This
      typically means that more specific taggers should be listed
      before less specific taggers. </para>

      <para> Having defined a combined tagger, we can use it to tag
      new corpora: </para>

<programlisting>
    &prompt;<command> tokens = TaggedTokenizer().tokenize('corpus.txt')</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>
      
    </section> <!-- Combining Tagger -->

  </section> <!-- Taggers -->

  <section> <title> Tagging: A Closer Look </title>

    <para> In the next five sections, we will discuss how each of the
    taggers introduced in the previous section are implemented.  This
    discussion serves several purposes: </para>

    <itemizedlist>
      <listitem>
        <para> It demonstrates how to write classes implementing the
        interfaces defined by NLTK. </para>
      </listitem>
      <listitem>
        <para> It provides you with a better understanding of the
        algorithms and data structures underlying each approach to
        tagging. </para>
      </listitem>
      <listitem>
        <para> It gives you a chance to see some of the code used to
        implement NLTK.  We have tried very hard to ensure that the
        implementation of every class in NLTK is easy to understand.
        </para>
      </listitem>
    </itemizedlist>

    <para> Before you read this section, you may wish to read the
    tutorial "<ulink url="&tutdoc;/writing_classes/t1.html">
    Writing Classes For NLTK"</ulink>, which describes how to create
    classes that interface with the toolkit. </para>

  </section> <!-- Tagging: A Closer Look -->

  <section> <title> Sequential Taggers </title>

    <indexterm><primary>sequential tagger</primary></indexterm>
    <para> The four taggers discussed in this tutorial are implemented
    as sequential taggers.  A <glossterm>sequential tagger</glossterm>
    is a tagger that: </para>

    <orderedlist>
      <listitem><para> Assigns tags to one token at a time, starting
      with the first token of the text, and proceeding in sequential
      order. </para> </listitem>
      <listitem><para> Decides which tag to assign a token on the
      basis of that token, the tokens that preceed it, and the
      predicted tags for the tokens that preceed it. </para>
      </listitem>
    </orderedlist>

    <para> To capture this commonality, we define a common base class,
    <ulink url="&refdoc;/nltk.tagger.SequentialTagger.html">
    <literal>SequentialTagger</literal></ulink>.  This base class
    defines <literal>tag</literal> using a new method,
    <ulink url="&refdoc;/nltk.tagger.SequentialTagger.html#tag_next">
    <literal>tag_next</literal></ulink>, which returns the appropriate
    tag for the next token.  However,
    <literal>SequentialTagger</literal> does not implement this new
    method itself.  Instead, each tagger subclass provides its own
    implementation. </para>

    <para> In addition to capturing the commonality between the four
    taggers, the <literal>SequentialTagger</literal> class has another
    advantage: it will allow us to define
    <literal>BackoffTagger</literal> in such a way that each subtagger
    can use the predictions made by the other taggers as context for
    deciding which tags to assign.  See the discussion of the
    <literal>BackoffTagger</literal> implementation for more
    details.</para>

    <section> <title> SequentialTagger.tag_next </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.SequentialTagger.html#tag_next">
      <literal>tag_next</literal></ulink> method decides which tag to
      assign a token, given the list of tagged tokens that preceeds
      it.  It takes two arguments: a list of tagged tokens preceeding
      the token to be tagged, and the token to be tagged; and it
      returns the appropriate tag for that token. </para>

    </section> <!-- tag_next -->

    <section> <title> SequentialTagger.tag </title>

      <para> The implementation of the <literal>tag</literal> method
      is relatively streight forward.  It simply loops through the
      untagged text, calling <literal>tag_next</literal> for each
      token.  It uses the result of each call to
      <literal>tag_next</literal> to create a tagged version of that
      token, and collects these together to form the tagged
      text. </para>

<programlisting>
    <command>def tag(self, text):</command>
        tagged_text = []

        for token in text:
            tag = self.next_tag(tagged_text, token)
            tagged_token = Token(TaggedType(token.type(), tag), token.loc())
            tagged_text.append(tagged_token)

        return tagged_text
</programlisting>

    </section> <!-- tag -->

    <section> <title> The SequentialTagger Implementation </title>

      <para> The complete listing for
      <literal>SequentialTagger</literal> is: </para>

      <figure><title>The SequentialTagger Implementation</title>
<programlisting>
<command>class SequentialTagger(TaggerI):</command>

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        assert 0, "next_tag not defined by SequentialTagger subclass"

    <command>def tag(self, text):</command>
        tagged_text = []

        <emphasis># Tag each token, in sequential order.</emphasis>
        for token in text:
            <emphasis># Get the tag for the next token.</emphasis>
            tag = self.next_tag(tagged_text, token)

            <emphasis># Use tag to build a tagged token, and add it to tagged_text.</emphasis>
            tagged_token = Token(TaggedType(token.type(), tag), token.loc())
            tagged_text.append(tagged_token)

        return tagged_text
</programlisting>
      </figure>
      
      <para> Note that SequentialTagger requires that subclasses
      define the <literal>tag_next</literal> method; otherwise, the
      <literal>assert</literal> statement will raise an
      exception when the user tries to tag a text. </para>

    </section>

    <section> <title> Subclasses </title>

      <para> The next four sections show how the
      <literal>SequentialTagger</literal> base class can be used to
      define <literal>NN_CD_Tagger</literal>,
      <literal>UnigramTagger</literal>,
      <literal>NthOrderTagger</literal>, and
      <literal>BackoffTagger</literal>.</para>

    </section> <!-- Subclasses -->

  </section> <!-- SequentialTagger -->

  <section> <title> NN_CD_Tagger </title>

    <para> <literal>NN_CD_Tagger</literal> assigns the tag
    <literal>"CD"</literal> to any token whose type appears to be a
    number; and <literal>"NN"</literal> to any other token.  It uses a
    simple regular expression to test whether a token's type is a
    number:</para>

<programlisting>
    r'^[0-9]+(.[0-9]+)?$'
</programlisting>

    <para> This regular expression matches one or more digits, followed
    by an optional period and one or more digits (e.g.,
    "<literal>12</literal>" or "<literal>732.42</literal>").  Note the
    use of "<literal>^</literal>" (which matches the beginning of a
    string) and "<literal>$</literal>" (which matches the end of a
    string) to ensure that the regular expression will only match
    complete token types. </para>

    <para> Since <literal>NN_CD_Tagger</literal> is a subclass of
    <literal>SequentialTagger</literal>, it just needs to define the
    <literal>next_tag</literal> method.  In the case of
    <literal>NN_CD_Tagger</literal>, the <literal>next_tag</literal>
    method is quite simple: </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        if re.match(r'^[0-9]+(.[0-9]+)?$', next_token.type()):
            return 'CD'
        else:
            return 'NN'
</programlisting>

    <para> Since <literal>NN_CD_Tagger</literal>s are stateless, and
    have no customization parameters, the <ulink
    url="&refdoc;/nltk.tagger.NN_CD_Tagger.html#__init__">
    <literal>NN_CD_Tagger constructor</literal></ulink> is empty:
    </para>

<programlisting>
    <command>def __init__(self):</command> pass
</programlisting>

    <para> The complete listing for the
    <literal>NN_CD_Tagger</literal> class is:</para>

    <figure><title>The NN_CD_Tagger Implementation</title>
<programlisting> 
<command>class NN_CD_Tagger(SequentialTagger):</command>

    <command>def __init__(self):</command> pass

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Assign the 'CD' tag for numbers; and 'NN' for anything else.</emphasis>
        if re.match(r'^[0-9]+(.[0-9]+)?$', next_token.type()):
            return 'CD'
        else:
            return 'NN'
</programlisting>
    </figure>

    <para> Note that <literal>NN_CD_Tagger</literal> does
    <emphasis>not</emphasis> define <literal>tag</literal>.  When the
    <literal>tag</literal> method is called, the definition given by
    <literal>SequentialTagger</literal> will be used. </para>

  </section> <!-- NN_CD_Tagger -->

  <section> <title> UnigramTagger </title>

    <para> <literal>UnigramTagger</literal> tags tokens by assigning
    the tag that is most likely to go with each token's type.  It uses
    a frequency distribution to model the likelihood of tags, given
    tokens.  This frequency distribution is constructed from a
    training corpus, using the <ulink
    url="&refdoc;/nltk.tagger.UnigramTagger.html#train">
    <literal>train</literal></ulink> method. </para>

    <section><title> Features and Contexts </title>

      <indexterm><primary>prediction problem</primary></indexterm>
      <indexterm><primary>feature</primary></indexterm>
      <indexterm><primary>context</primary></indexterm>
      <para> Unigram tagging is a "prediction problem."  A
      <glossterm>prediction problem</glossterm> is one in which
      we try to make predictions about the outcome of a process.  It
      is instructive to think about prediction problems in terms of
      "features" and "contexts."  A <glossterm>feature</glossterm> is
      an aspect of the outcome that we wish to predict; and a
      <glossterm>context</glossterm> is an aspect of the outcome that
      we can base our prediction on.  In the case of the unigram
      tagger, the feature we wish to predict is the token's tag, and
      the context that we will base our prediction on is the token's
      base type. </para>

    </section> <!-- Features and Contexts -->

    <section> <title> Context/Feature Samples </title>

      <para> NLTK defines the class <ulink
      url="&refdoc;/nltk.probability.CFSample.html">
      <literal>CFSample</literal></ulink> for representing samples in
      terms of their features and contexts.  Context/feature samples
      are constructed with the <ulink
      url="&refdoc;/nltk.probability.CFSample.html#__init__">
      <literal>CFSample constructor</literal></ulink>:</para>

<programlisting>
    &prompt;<command> context = 'bank'</command>  <emphasis># the base type</emphasis>
    &prompt;<command> feature = 'NN'</command>    <emphasis># the tag</emphasis>
    &prompt;<command> sample = CFSample(context, feature)</command>
    &lt;CFSample 'bank', 'NN'&gt;
</programlisting>

      <para> A <literal>CFSample</literal>'s context is accessed via
      the <ulink
      url="&refdoc;/nltk.probability.CFSample.html#context">
      <literal>context</literal></ulink> member function; and its
      feature is accessed via the <ulink
      url="&refdoc;/nltk.probability.CFSample.html#feature">
      <literal>feature</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> sample.context()</command>
    'bank'
    &prompt;<command> sample.feature()</command>
    'NN'
</programlisting>

      <important>
        <para> <literal>CFSample</literal>s should always have
        <emphasis>immutable</emphasis> contexts and features.  If you
        wish to use a sequence as a <literal>CFSample</literal>'s
        context or feature, you must use a tuple (and not a list).
        When necessary, you can convert lists to tuples:</para>

<programlisting>
    &prompt;<command> my_list = [1, 2, 3]</command>
    [1, 2, 3]
    &prompt;<command> tuple(my_list)</command>
    (1, 2, 3)
</programlisting>

      </important>

    </section> <!-- CFSample -->

    <section> <title> Training the Unigram Tagger </title>

      <para> We can use <literal>CFSample</literal>s to populate a
      frequency distribution that models the distribution of tags and
      tokens in the training text: </para>

<programlisting>
    <command>def train(self, tagged_tokens):</command>
        for token in tagged_tokens:
            context = token.type().base()
            feature = token.type().tag()
            self._freqdist.inc(CFSample(context, feature))
</programlisting>

    </section> <!-- Training -->

    <section> <title> Tagging with the Unigram Tagger </title>
      
      <para> To find the most likely tag for a given token, we can
      just use the <ulink
      url="&refdoc;/nltk.probability.FreqDistI.html#cond_max">
      <literal>cond_max</literal></ulink> member of our frequency
      distribution.  This method returns the most frequent sample that
      is contained in a conditioning event.  For example, using a
      predicated event for our condition, we could find the most
      likely tag for the base type "bank" as follows:</para>

<programlisting>
    &prompt;<command> def context_is_bank(w): </command>
    &prompt2;<command>     return w.context() == bank</command>
    &prompt2;
    &prompt;<command> condition = PredEvent(is_bank)</command>
    {Event x: context_is_bank(x)}
    &prompt;<command> freqdist.cond_max(condition)</command>
    'NN'
</programlisting>

      <para> However, using predicated events for this purpose is
      cumbersome.  Instead, we can use the <ulink
      url="&refdoc;/nltk.probability.ContextEvent.html">
      <literal>ContextEvent</literal></ulink> class, which represents
      the event that the context has a given value.
      <literal>ContextEvent</literal>s are created with the <ulink
      url="&refdoc;/nltk.probability.ContextEvent.html#__init__">
      <literal>ContextEvent constructor</literal></ulink>: </para>

<programlisting>
    &prompt;<command> condition = ContextEvent('bank')</command>
    {Event x: &lt;CFSample 'bank', x&gt;}
    &prompt;<command> freqdist.cond_max(condition)</command>
    'NN'
</programlisting>

      <para> The <literal>next_tag</literal> method must decide which
      tag is most likely for a given token.  It simply consults the
      tagger's frequency distribution to find the sample that is most
      likely for the context consisting of the tokens's type; and
      returns the corresponding tag.  If the token type has not been
      seen before, it returns the special tag
      <literal>None</literal>. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        context_event = ContextEvent(next_token.type())
        sample = self._freqdist.cond_max(context_event)
        if sample: 
            return sample.feature()
        else: 
            return None
</programlisting>
      
    </section> <!-- Tagging Words -->

    <section> <title> Initializing the Unigram Tagger </title>

      <para> The constructor for <literal>UnigramTagger</literal>
      simply initializes <literal>self._freqdist</literal> with a new
      frequency distribution.  However, there are several different
      implementations of the frequency distribution interface, and the
      choice of which implementation to use can have an enormous
      impact on the efficiency of the tagger. </para>

      <section> <title> Efficiency Analysis </title>

        <itemizedlist>
          <listitem>
            <para> Let
            <replaceable>n<subscript>tag</subscript></replaceable> be
            the number of tags in our tag set</para>
          </listitem>
          <listitem>
            <para> Let
            <replaceable>n<subscript>train</subscript></replaceable>
            be the number of training tokens.  </para>
          </listitem>
        </itemizedlist>

        <para> The easiest solution is to use a
        <literal>SimpleFreqDist</literal>.
        <literal>SimpleFreqDist</literal> is a good general-purpose
        frequency distribution, and it will work with any kinds of
        samples and events. Unfortunately, this generality comes at a
        price.  For each token, we need to find the tag that is most
        frequently paired with a base type in the training data.  But
        <literal>SimpleFreqDist</literal> has no efficient way to
        extract <replaceable>n<subscript>tag</subscript></replaceable>
        training samples that match the base type we are looking for.
        Instead, it must search through all
        <replaceable>n<subscript>train</subscript></replaceable>
        samples.  Thus, if we use a <literal>SimpleFreqDist</literal>,
        then the unigram tagger will take time proportional to
        <replaceable>n<subscript>train</subscript></replaceable>
        (which is typically at least 10,000) to tag each word. </para>

        <para> A better option is to use a <ulink
        url="&refdoc;/nltk.probability.CFFreqDist.html">
        <literal>CFFreqDist</literal></ulink>.  This implementation of
        the frequency distribution interface requires that all samples
        be <literal>CFSample</literal>s; furthermore, the only event
        type supported by <literal>CFFreqDist</literal> is
        <literal>ContextEvent</literal>.  However, these restrictions
        allow <literal>CFFreqDist</literal> to use a more structured
        internal representation, resulting in much more efficient
        implementations of the conditional methods
        (<literal>cond_max</literal>, <literal>cond_freq</literal>,
        and <literal>cond_samples</literal>).  For the unigram tagger,
        this allows us to tag each word in time proportional to
        <replaceable>n<subscript>tag</subscript></replaceable> (which
        is typically less than 100). </para>

      </section> <!-- Efficiency Analysis -->

      <section> <title> The UnigramTagger Constructor </title>

        <para> We therefore use a <literal>CFFreqDist</literal> to
        implement the UnigramTagger's frequency distribution: </para>

<programlisting>
    <command>def __init__(self):</command>
        self._freqdist = probability.CFFreqDist()
</programlisting>

      </section> <!-- The UnigramTagger Constructor -->

    </section> <!-- Initializing the UnigramTagger -->

    <section><title>The UnigramTagger Implementation</title>

      <para> The complete listing for the
      <literal>UnigramTagger</literal> class is:</para>
      
      <figure><title>The UnigramTagger Implementation</title>
<programlisting> 
<command>class UnigramTagger(TaggerI):</command>
class UnigramTagger(SequentialTagger):
    <command>def __init__(self):</command>
        self._freqdist = CFFreqDist()
    
    <command>def train(self, tagged_tokens):</command>
        for token in tagged_tokens:
            context = token.type().base()
            feature = token.type().tag()
            self._freqdist.inc( CFSample(context, feature) )

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Find the most likely tag for the token's type.</emphasis>
        context_event = ContextEvent(next_token.type())
        sample = self._freqdist.cond_max(context_event)

        <emphasis># If we found a tag, return it; otherwise, return None.</emphasis>
        if sample:
            return sample.feature()
        else:
            return None
</programlisting>
      </figure>
    </section> <!-- UnigramTagger Implementation -->
  </section> <!-- UnigramTagger -->

  <section> <title> NthOrderTagger </title>

    <para> The <literal>NthOrderTagger</literal> is a generalization
    of the <literal>UnigramTagger</literal>.  Instead of using the
    token's base type as a context, it uses a tuple consisting of the
    token's base type and the tags of the <replaceable>n</replaceable>
    preceding tokens.  This generalization creates two new
    issues. </para>

    <para> First, we must decide how to handle the first
    <replaceable>n</replaceable> tokens, since they do not have
    <replaceable>n</replaceable> preceding tokens.  

    <literal>NthOrderTagger</literal> simply uses the tags that are
    available.  For example, in a 3rd order tagger, the context of the
    second token will contain only the token's type and the first
    token's tag.  Another option would be to simply ignore the first
    <replaceable>n</replaceable> tokens.  As it turns out, which
    approach we take will not have much of an impact, since
    <replaceable>n</replaceable> (the order of the tagger) is
    generally much less than
    <replaceable>n<subscript>train</subscript></replaceable> (the
    number of training samples). </para>

    <para> The second issue is that, when tagging a text, we do not
    have access the the actual tags of the
    <replaceable>n</replaceable> preceding tokens.  However, we do
    have access to our predicted values for these tags.
    <literal>NthOrderTagger</literal> uses these predicted tags, since
    they are likely to be correct.  Assuming that our predictions are
    good, the use of predicted tags instead of actual tags will have a
    relatively minor impact on performance. </para>

    <section> <title> Initializing the Nth Order Tagger</title>

      <para> Having addressed these two issues, we can examine the
      implementation of the <literal>NthOrderTagger</literal>.  The
      constructor simply records <replaceable>n</replaceable>, and
      constructs a new frequency distribution: </para>

<programlisting>
    <command>def __init__(self, n):</command>
        self._n = n
        self._freqdist = probability.CFFreqDist()
</programlisting>

    </section> <!-- NthOrderTagger Constructor -->

    <section> <title> Training the Nth Order Tagger </title>

      <para> To train the <literal>NthOrderTagger</literal>, we
      construct a <literal>CFSample</literal> for each token.  For
      contexts, we use a tuple consisting of the
      <replaceable>n</replaceable> previous tags and the current
      token's base type. </para>

<programlisting>
    <command>def train(self, tagged_tokens):</command>
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = []
        
        for token in tagged_tokens:
            context = tuple(prev_tags + [token.type().base()])
            feature = token.type().tag()
            self._freqdist.inc( CFSample(context, feature) )

            <emphasis># Update prev_tags</emphasis>
            prev_tags.append(token.type().tag())
            if len(prev_tags) == (self._n+1):
                del prev_tags[0]
</programlisting>

    </section> <!-- NthOrderTagger.train -->

    <section> <title> Tagging with the Nth Order Tagger </title>

      <para> As with the <literal>UnigramTagger</literal>, we can find
      the most likely tag for each token using the
      <literal>cond_max</literal> member of our frequency
      distribution.  But instead of using each token's base type as a
      context, we use a tuple consisting of the
      <replaceable>n</replaceable> previous predicted tags and the
      token's base type.</para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Find the tags of the n previous tokens.</emphasis>
        prev_tags = []
        start = max(len(tagged_tokens) - self._n, 0)
        for token in tagged_tokens[start:]:
            prev_tags.append(token.type().tag())

        <emphasis># Construct the relevant context event.</emphasis>
        context = tuple(prev_tags + [next_token.type()])
        context_event = ContextEvent(context)

        <emphasis># Find the most likely tag for this context, and return it.</emphasis>
        sample = self._freqdist.cond_max(context_event)
        if sample: return sample.feature()
        else: return None
</programlisting>

    </section> <!-- NthOrderTagger.tag -->

    <section><title>The NthOrderTagger Implementation</title>

      <para> The complete listing for the
      <literal>NthOrderTagger</literal> class is:</para>
      
      <figure><title>The NthOrderTagger Implementation</title>
<programlisting> 
<command>class NthOrderTagger(SequentialTagger):</command>
    <command>def __init__(self, n):</command>
        if n < 0: raise ValueError('n must be non-negative')
        self._n = n
        self._freqdist = CFFreqDist()

    <command>def train(self, tagged_tokens):</command>
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = []
        
        for token in tagged_tokens:
            context = tuple(prev_tags + [token.type().base()])
            feature = token.type().tag()
            self._freqdist.inc( CFSample(context, feature) )

            <emphasis># Update prev_tags</emphasis>
            prev_tags.append(token.type().tag())
            if len(prev_tags) == (self._n+1):
                del prev_tags[0]

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Find the tags of the n previous tokens.</emphasis>
        prev_tags = []
        start = max(len(tagged_tokens) - self._n, 0)
        for token in tagged_tokens[start:]:
            prev_tags.append(token.type().tag())

        <emphasis># Construct the relevant context event.</emphasis>
        context = tuple(prev_tags + [next_token.type()])
        context_event = ContextEvent(context)

        <emphasis># Find the most likely tag for this context, and return it.</emphasis>
        sample = self._freqdist.cond_max(context_event)
        if sample: return sample.feature()
        else: return None
</programlisting>
      </figure>

    </section> <!-- NthOrderTagger Implementation -->

  </section> <!-- NthOrderTagger -->

  <section> <title> BackoffTagger </title>

    <indexterm><primary>subtaggers</primary></indexterm>
    <para> The <literal>BackoffTagger</literal> is used to combine the
    results of a list of <glossterm>subtaggers</glossterm>.  For each
    token to be tagged, the <literal>BackoffTagger</literal> consults
    each subtagger, in order.  Each token is assigned the first
    non-<literal>None</literal> tag returned by a subtagger for that
    token.  If all of the subtaggers return the tag
    <literal>None</literal> for a token, then
    <literal>BackoffTagger</literal> will assign it the tag
    <literal>None</literal>. </para>

    <section> <title> Initializing a Backoff Tagger </title>

    <para> The <literal>BackoffTagger</literal> constructor simply
    records the list of subtaggers. </para>

<programlisting>
    <command>def __init__(self, subtaggers):</command>
        self._taggers = subtaggers
</programlisting>

    </section> <!-- Initializing a BackoffTagger -->

    <section> <title> Tagging with the Backoff Tagger </title>

      <para> The implementation of <literal>BackoffTagger</literal> is
      relatively straight-forward.  Its <literal>next_tag</literal>
      method simply calls each subtagger's <literal>next_tag</literal>
      method, in order; and returns the first
      non-<literal>None</literal> tag produced by a subtagger. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        for subtagger in self._subtaggers:
            tag = subtagger.next_tag(tagged_tokens, next_token)
            if tag is not None:
                return tag

        <emphasis># Default to None if all subtaggers return None. </emphasis>
        return None
</programlisting>    
    
    </section> <!-- BackoffTagger tagging -->

    <section><title>The BackoffTagger Implementation</title>

      <para> The complete listing for the
      <literal>BackoffTagger</literal> class is:</para>
      
      <figure><title>The BackoffTagger Implementation</title>
<programlisting> 
<command>class BackoffTagger(SequentialTagger):</command>
    <command>def __init__(self, subtaggers):</command>
        self._subtaggers = subtaggers

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        for subtagger in self._subtaggers:
            tag = subtagger.next_tag(tagged_tokens, next_token)
            if tag is not None:
                return tag

        <emphasis># Default to None if all subtaggers return None. </emphasis>
        return None
</programlisting>
      </figure>
    </section> <!-- BackoffTagger impl -->


  </section> <!-- BackoffTagger -->

  &index;
</article>

