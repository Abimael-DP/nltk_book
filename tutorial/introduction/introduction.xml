<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Introduction to Natural Language Processing</title>
    &versiondate; &copyright;
  </articleinfo>

<blockquote><attribution>John Ralston Saul</attribution>
<para>
The single and shortest definition of civilization may be the word
<emphasis>language</emphasis>...
Civilization, if it means something concrete, is the conscious but
unprogrammed mechanism by which humans communicate.  And through
communication they live with each other, think, create, and act.
</para>
</blockquote>

  <section id="goals"><title>Goals</title>

    <para>
      Language is the chief manifestation of human intelligence.
      Through language we express basic needs and lofty aspirations,
      technical know-how and flights of fantasy.  Ideas are
      shared over great separations of distance and time.
      The following samples from English illustrate the richness
      of language:
    </para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para>
      Overhead the day drives level and grey, hiding the sun by a
      flight of grey spears.
      (William Faulkner, <emphasis>As I Lay Dying</emphasis>, 1935)
    </para></listitem>
    <listitem><para>
      When using the toaster please ensure that the exhaust fan is
      turned on. (sign in dormitory kitchen)
    </para></listitem>
    <listitem><para>
      Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated
      activities with Ki values of 45.1-271.6 &mu;M (Medline)
    </para></listitem>
    <listitem><para>
      Iraqi Head Seeks Arms (spoof headline,
        <literal>http://www.snopes.com/humor/nonsense/head97.htm</literal>)
    </para></listitem>
    <listitem><para>
      The earnest prayer of a righteous man has great power and wonderful
      results. (James 5:16b)
    </para></listitem>
    <listitem><para>
      Twas brillig, and the slithy toves did gyre and gimble in the wabe
      (Lewis Carroll, <emphasis>Jabberwocky</emphasis>, 1872)
    </para></listitem>
    <listitem><para>
       There are two ways to do this, AFAIK :smile:  (internet
         discussion archive)
    </para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

    <para>
      Thanks to this richness, the study of language is part of many
      disciplines outside of linguistics, including translation,
      literary criticism, philosophy, anthropology and psychology.
      Many less obvious disciplines investigate language use, such as
      law, hermeneutics, forensics, telephony, pedagogy, archaeology,
      cryptanalysis and speech pathology.  Each applies distinct
      methodologies to gather observations, develop theories and test
      hypotheses.  Yet all serve to deepen our understanding of
      language and of the intellect which is manifested in language.
    </para>

    <para>
      The importance of language to science and the arts is matched in
      significance by the cultural treasure that is inherent in
      language.  Each of the worlds ~7,000 human languages
      is rich in unique respects, in its oral
      histories and creation legends, down to its grammatical
      constructions and its very words and their nuances of meaning.
      Threatened remnant
      cultures have words to distinguish plant subspecies according to
      therapeutic uses which are unknown to science.  Languages evolve
      over time as they come into contact with each other and they
      provide a unique window onto human pre-history.  
      Technological change gives rise to new words like
      <emphasis>weblog</emphasis>
      and new morphemes like <emphasis>e-</emphasis> and <emphasis>cyber-</emphasis>.
      In many parts of the world, small linguistic variations from one
      town to the next add up to a completely different language in
      the space of a half-hour drive.  For its breathtaking complexity
      and diversity, human language is as a colourful tapestry
      stretching through time and space.
    </para>

    <para>
      Each new wave of computing
      technology has confronted new challenges for language analysis.
      Early machine languages gave way to high-level programming
      languages which are automatically parsed and interpreted.
      Databases are interrogated using linguistic expressions
      like <literal>SELECT age FROM employee</literal>.
      Recently, computing devices have become ubiquitous
      and are often equipped with multimodal interfaces
      supporting text, speech, dialogue and pen gestures.
      Building new systems for natural linguistic interaction requires
      sophisticated language analysis.
    </para>

    <para>
      Today the greatest challenge for language analysis is presented by the
      explosion of text and multimedia content on the world-wide web.
      For many people, a large and growing fraction of work and
      leisure time is spent navigating and accessing this universe of
      information.  What tourist sites can I visit between
      Philadelphia and Pittsburgh on a limited budget?  What do expert
      critics say about Canon digital cameras?  What predictions about
      the steel market were made by credible commentators in the past
      week?  Answering such questions requires a combination of
      language processing tasks including information extraction,
      inference, and summarisation.  This scale of language
      analysis increasingly depends on sophisticated automatic
      language processing.
    </para>

    <para>
      As we have seen, <glossterm>natural language
      processing</glossterm>, or NLP, is important for scientific,
      economic, social, and cultural reasons.  NLP is experiencing
      rapid growth as its theories and methods are deployed in a
      variety of new language technologies.  Thus it is important for
      a wide range of people to have a working knowledge of NLP.
      Within academia, this includes people working in areas from
      humanities computing and corpus linguistics to
      artificial intelligence and information extraction.  Within
      industry, this includes people working in human-computer
      interaction, business information analysis, and web software
      development.  We hope this book will permit our diverse audience
      to appreciate the workings of this rapidly growing field and to
      apply its techniques to real-world problems in language
      analysis.  The book presents a carefully balanced selection of
      theoretical foundations and practical application, and equips
      readers to work with large datasets, to create robust models of
      linguistic phenomena, and to deploy them in working language
      technologies.  By integrating all of this with the Natural
      Language Toolkit, we hope this book opens up the exciting
      endeavour of practical natural language processing to a broader
      audience than ever before.
    </para>

  </section> <!-- Goals -->

  <section><title>A Brief History of Natural Language Processing</title>

    <para>A long-standing challenge within computer science has been
    to build intelligent machines.  The chief measure of machine
    intelligence has been a linguistic one, namely the
    <glossterm>Turing Test</glossterm>.  Can a dialogue system,
    responding to a user's typed input with its own textual output,
    perform so naturally that users cannot distinguish it from
    a human interlocutor using the same interface?
    Today, there is substantial
    ongoing research and development in such areas as machine
    translation and spoken dialogue, and significant commercial
    systems are in widespread use [XREF].  The following dialogue
    illustrates a typical application:
    </para>

<programlisting>
S: How may I help you?

U: When is Saving Private Ryan playing?

S: For what theater?

U: The Paramount theater.

S: Saving Private Ryan is not playing at the Paramount theater, but
   it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30. 
</programlisting>

    <para>
   Today's commercial dialogue systems are strictly limited to
   narrowly-defined domains. We could not ask the above system to
   provide driving instructions or details of nearby restaurants unless
   the requisite information had already been stored and suitable
   question and answer sentences had been incorporated into the language
   processing system.  Observe that the dialogue system appears
   understand the user's goals: the user asks when a movie is showing
   and the system correctly determines from this that the user wants to
   see the movie. This step seems so obvious to humans that we do not
   notice it has been made, yet a natural language system needs to be
   endowed with this capability in order to interact naturally. Without
   it, when asked <literal>Do you know when Saving Private Ryan is
    playing</literal>, a system might simply &mdash; and unhelpfully
   &mdash; respond with a cold <literal>Yes</literal>. While it appears
   that this dialogue system can perform simple inferences, such
   sophistication is only found in cutting edge research prototypes.
   Instead, the developers of commercial systems use contextual
   assumptions and simple business logic to ensure that the different
   ways in which a user might express requests or provide information
   are handled in a way that makes sense for the particular application.
   Thus, whether the user says <literal>When is ... </literal>, or
   <literal>I want to know when ... </literal>, or <literal>Can you tell
    me when ... </literal>, simple rules will always result in users
   being presented with movie showing times, and this is sufficient for
   the system to provide a useful service.
    </para>

    <para>
   Although recent advances have been made, it is still largely true
   that deployed natural language systems cannot perform common-sense
   reasoning or draw on world knowledge.  We can wait for these general
   problems of artificial intelligence to be solved, but in the meantime
   it is necessary to live with some severe limitations on the reasoning
   and knowledge capabilities of natural language systems. Since the
   very beginning, an important goal of NLP research has been to
   make progress on the holy grail of natural linguistic interaction
   <emphasis>without</emphasis> recourse to this unrestricted knowledge
   and reasoning capability.  This is an old challenge, and so it is
   instructive to review the history of the field.
    </para>

    <para>
   The very notion that natural language could be treated in a
   computational manner grew out of a research programme, dating back to
   the early 1900s, to reconstruct mathematical reasoning using logic
   (Frege, Russell, Wittgenstein, Tarski, Lambek, Carnap).  This led to
   the notion of language as a formal system amenable to automatic
   processing.  Three further developments laid the foundation for
   natural language processing.  The first was <emphasis>formal language
    theory</emphasis>.  This defined a language as a set of sentences
   (or strings) accepted by a class of automata (e.g. regular languages
   and finite state machines, context-free languages and pushdown
   automata, Chomsky hierarchy, etc). This provided the underpinnings
   for computational syntax.  [MORE].  The second was <emphasis>symbolic
    logic</emphasis>. This provided a formal method of capturing
   selected aspects of natural language that were held to be relevant
   for modeling the notion of proof. A formal calculus in symbolic logic
   provides the syntax of a language, together with rules of inference
   and, possibly, rules of interpretation in a set-theoretic model;
   examples are propositional logic and first-order logic. Given such a
   calculus, with a well-defined syntax and semantics, it becomes
   possible to associate meanings with expressions of natural language
   by translating them into expressions of the formal calculus. For
   example, if we translate <emphasis>John saw Mary</emphasis> into a
   formula <literal>saw(j,m)</literal>, we (implicitly or explicitly)
   intepret the English verb <emphasis>saw</emphasis> as a binary
   relation, and <emphasis>John</emphasis> and <emphasis>Mary</emphasis>
   as denoting individuals.  More general statements like <emphasis>All birds
    fly</emphasis> require quantifiers, in this case &forall; meaning
   <emphasis>for all</emphasis>: <literal>&forall;x: bird(x) &rarr;
    fly(x)</literal>.  This use of logic provided the technical
   machinery to perform inferences that are an important part of
   language understanding. The third was the <emphasis>principle of
    compositionality</emphasis>. This was the notion that the meaning of
   a complex expression is comprised of the meaning of its parts and
   their mode of combination. This provided a useful correspondence
   between syntax and semantics, that the meaning of a complex
   expression could be computed recursively.  Given the representation
   of <emphasis>It is not true that &blank;<subscript>p</subscript></emphasis>
   as <literal>not(p)</literal> and <emphasis>John saw
    Mary</emphasis> as <literal>saw(j,m)</literal>, we can compute the
   interpretation of <emphasis>It is not true that John saw Mary</emphasis>
   recursively using the above information to get
   <literal>not(saw(j,m))</literal>. Today, this approach is most
   clearly manifested in a family of grammar formalisms known as
   unification-based grammar, and NLP applications implemented in the
   Prolog programming language.
    </para>

    <para>
    A separate strand of development in the 1960s and 1970s eschewed
    the declarative/procedural distinction and the principle of
    compositionality.  They seemed to get in the way of building
    practical systems.  For instance, early question answering systems
    employed <emphasis>semantic grammars</emphasis>, consisting of
    fixed templates such as:
    <literal>How many &blank;<subscript>i</subscript>
    does &blank;<subscript>j</subscript> have?</literal>
    Each template comes with a predefined semantic functions,
    such as <literal>count(i,j)</literal>.  A user's question which
    matched the template would be mapped to a semantic function,
    then ``executed'' to obtain an answer <literal>k = count(i,j)</literal>.
    This can then be substituted into a new template:
    <literal>&blank;<subscript>j</subscript> has
    &blank;<subscript>k</subscript> &blank;<subscript>i</subscript></literal>.
    Thus, the question
    <emphasis>How many airports<subscript>i</subscript>
    does London<subscript>j</subscript> have?</emphasis>
    can be trivially mapped onto a template (as shown by the
    subscripts) and translated to an executable program.  The result can be
    returned in English as:
    <emphasis>London<subscript>j</subscript> has five <subscript>k</subscript>
    airports<subscript>i</subscript></emphasis>.
    </para>
    
<!--  
    <para>
    (call this the low-church AI approach, cf the high-church AI
    approach of formal language theory and symbolic logic;
    neither approach worked - both left out the same thing)
    </para>
-->

  </section> <!-- A Brief History of Natural Language Processing -->

<!--
  <section><title>Linguistics and Natural Language Processing (draft)</title>

  <para>
    [What is the relationship between linguistics and NLP?
    Goal of (generative) linguistics to account for the grammaticality
    judgements of the ideal monolingual speaker/hearer, vs goal of
    NLP to build systems to map between the (linguistic) systems of
    humans and machines.  Challenge of linguistics is to balance
    descriptive and explanatory adequacy; challenge of NLP to balance
    expressiveness and tractability.]
  </para>

    <para>
    [Grammar as a definition of well-formed
    sentences along with a semantic translation, versus
    an implementation which (say) maps from sentences to
    meanings (parser) or vice versa (generator).
    declarative vs procedural;
    system of rewriting rules vs automaton;
    perspective on NLP: relating the declarative to the procedural;
    distinguish this constrast from competence vs performance.]
    </para>

    <para>
    In the late 1980s and early 1990s there was a promising
    convergence between the fields of linguistics and NLP.  (This had
    been a feature of the 1960s, e.g. with the application of the SPE
    model in speech synthesis systems.)  Computational linguists often
    looked to linguistics as a source of knowledge about language.
    Over the last decade we have seen a new divergence, as
    computational linguists have discovered that linguistic analyses
    often failed to account for the linguistic patterns attested in
    the large corpora used to develop their systems.  However, once
    linguists learn to work with these large datasets, their own
    analytical work will benefit, leading to broader coverage of their
    theories, and earlier refutation of false hypotheses.  The result,
    we expect, will be new opportunities for cross-fertilization
    between linguistics and NLP.
    </para>

    <para>
    [Opportunities for linguists to contribute their insights to the
    future development of NLP and, in the reverse direction, to apply
    the results of NLP research back in linguistics.]
    </para>

    <para>
    [rationalism vs empiricism; realism vs idealism; other balancing acts]
    </para>

<para>
rationalism vs empiricism:
significant distinction coming out of the history of the natural
sciences:
to what extent does our experience of the world provide the basis for
our knowledge?

rationalism - all truth has its origins in human thought
  (doesn't require input from supernatural beings or the
  experience of our senses)
Descartes, Leibniz
``innate ideas'' implanted in our minds from birth.
E.g. principles of Euclidean geometry arose through the
process of reason, and not divine revelation or sensory experience.
Enlightenment - priority of human reason over revelation/experience.

empiricism
John Locke - primary source of knowledge is the experience of our faculties
- reason is secondary - reflecting on that experience

linguistic implications:
Chomsky, poverty of the stimulus, innate language faculty
to account for universal grammar

vs humans have general learning methods (analogical, memory based),
and use these to identify meaningful patterns in language
and ground them in sensory experience.

</para>

<para>
realism vs idealism;
Kant - phenomena (appearances and representations that we can
experience)
vs the ``things in themselves'' which can never been known directly

what is the metaphysical status of the constructs of our NLP models?
Are things like ``noun phrase'' real world entities, or just
theory-internal constructs, products of thought?
useful fictions
unobservables (e.g. null elements, underlying forms)

realism - theories can establish constructs and these are
existent entities, which actually cause the observed phenomena.

- entities exist in the real world independently of human perception
and human reason
</para>

<para>
Balancing acts:
statistical vs symbolic
(artificial: should be statistical vs ? (gradient vs binary)
symbolic vs non-symbolic
deep vs shallow processing;
science vs engineering
</para>


  </section>
-->


  <section><title>The Architecture of linguistic and NLP systems</title>

    <para>
      Within the approach to linguistic theory known as <glossterm>
	generative grammar</glossterm>, it is claimed that humans have
      distinct kinds of linguistic knowledge, organised into different
      modules: for example, knowledge of phonology and morphology,
      knowledge of syntax, knowledge of semantics. In a formal theory of
      linguistic competence, each of these different kinds of implicit
      knowledge is made explicit as different module of the theory.
      By `module' here we mean at least a set of primitive elements,
      together with a way of combining the elements into derived
      elements. For example, a phonological module might provide a
      set of phonemes, together with an operation for concatenating
      phonemes into phonological strings. Similarly, a syntactic
      module might provide labeled nodes as primitives, together wih
      a mechanism for assembling nodes into arbitrarily complex trees. A
      set of linguistic primitives, together with some operators for
      defining complex elements, is often called <glossterm>a level of
      representation</glossterm>. 
    </para>

    <para>
      As well as defining modules, a generative grammar will prescribe
      how the modules interact. For example, well-formed phonological
      strings will provide the phonological content of words, and words
      will provide the terminal elements of syntax trees. Similarly, a
      linguistic theory needs to specify some manner of mapping
      well-formed syntactic trees into semantic representations, and
      equally an account of how contextual or pragmatic information can
      resolve indeterminacies in these semantic representations.
      </para>

    <para>As we indicated above, an important aspect of theories of
   generative grammar is that they are intended to model the linguistic
   knowledge of speakers and hearers; they are not intended to explain
   how humans actually process linguistic information. This is, in part,
   reflected in the claim that a generative grammer encodes the
   <emphasis>competence</emphasis> of an idealized native speaker,
   rather than the speaker's <emphasis>competence</emphasis>. A closely
   related distinction is to say that a generative grammar encodes
   <glossterm>declarative</glossterm> rather than
   <glossterm>procedural</glossterm> knowledge.  As you might expect,
   computational linguistics has the crucial role of proposing
   procedural models of language. A central example is
   <glossterm>parsing</glossterm>, where we have to develop
   computational mechanisms which convert, say, strings of words into
   structural representations such as syntax trees. Nevertheless, it is
   widely accepted that well-engineered computational models of language
   contain both declarative and procedural aspects. Thus, a full account
   of parsing will say how declarative knowledge in the form of a
   grammar and lexicon combines with procedural knowledge which
   determines how a syntactic analysis should be assigned to a given
   string of words. This procedural knowledge will be expressed as an
   <glossterm>algorithm</glossterm>: that is, an explicit recipe for
   mapping some input into an appropriate output in a finite number of
   steps. A simple parsing algorithm for context free gramars, for
   instance, looks first for a rule of the form <literal>S &rarr;
    X<subscript>1</subscript> &mldr;
    X<subscript>n</subscript></literal>, and builds a partial tree
   structure. It then steps through the grammar rules one-by-one,
   looking for a rule of the form <literal>X<subscript>1</subscript>
    &rarr; Y<subscript>1</subscript> ...
    Y<subscript>j</subscript></literal> which will expand the leftmost
   daughter introduced by the <literal>S</literal> rule, and further
   extends the partial tree. This process continues, for example by
   looking  for a rule of the form <literal>Y<subscript>1</subscript>
    &rarr; Z<subscript>1</subscript> ...
    Z<subscript>k</subscript></literal> and expanding the partial tree
   appropriately,  until the leftmost node label in the partial tree is
   a lexical category; the parser then checks to see if the first word
   of the input can belong to the category. To illustrate, let's suppose
   that the first grammer rule chosen by the parser is <literal>S &rarr;
    NP VP</literal> and the second rule chosen is <literal>NP &rarr; Det
    N</literal>; then the partial tree will the one in
   <xref linkend="partialtree"/>.
 
    </para>

    <figure id="partialtree">
      <title>Partial Parse Tree</title>
      <graphic fileref="images/partialtree" scale="20"/>
    </figure>

    <para>
      If we assume that the input string we are trying to parse is
      <emphasis>the cat slept</emphasis>,  we will succeed in
      identifying <emphasis>the</emphasis> as a word which can belong to
      the category <literal>Det</literal>. In this case, the parser goes
      on to the next node of the tree (e.g., <literal>N</literal>) and
      next input word (e.g., <emphasis>cat</emphasis>). However, if we
      had built the same partial tree with an input string <emphasis>did
	the cat sleep</emphasis>, the parse would fail at this point
      (since <emphasis>did</emphasis> is not of category
      <literal>Det</literal>); so it would throw away the structure
      built so far, and look for an alternative way of going from the
      <literal>S</literal> node down to a leftmost lexical category
      (e.g., using a rule <literal>S &rarr; V NP VP</literal>). The
      important point for now is not the details of this or other
      parsing algorithms; we discuss this topic much more fully in
      (parsing-chap xref). Rather, we just want to illustrate the idea
      that an algorithm can be broken down into a fixed number of steps
      which produce a definite result at the end.
    </para>

    <para>In <xref linkend="dialogue"/> we further illustrate some of
    these points in the context of a spoken dialogue system, such as our
    earlier example of an application that offers the user information
    about movies currently on show.  
    </para>
    <figure id="dialogue">
      <title>Architecture of Spoken Dialogue System</title>

		<graphic fileref="images/dialogue" scale="20"/>

    </figure>

    <para> Down the lefthand side of the diagram, we have shown a
      pipeline of some representative speech understanding <emphasis>component</emphasis>s.
      These map from speech input via syntactic parsing to some kind of
      meaning representation. Up the righthand side is an inverse
      pipeline of components for concept-to-speech generation. These
      components constitute the procedural aspect of the system's
      natural language processing. In the central column of the diagram
      are some representative declaratives aspects: the repositories of
      language-related information which are called upon by the
      processing components. 
    </para>
    <para>In addition to embodying the declarative/procedural
      distinction, the diagram also illustrates that linguistically
      motivated ways of modularizing linguistic knowledge are often
      reflected in computational systems. That is, the various
      components are organized so that the data which they  exchang
      corresponds roughly to different levels of representation. For
      example, the output of the speech analysis component will contain
      sequences of phonological representations of words, and the output
      of the parser will be a logical form that represents the
      compositional semantics of the output. Of course the parallel is
      not precise, in part because it is often a matter of practical
      expedience where to place the boundaries between different
      processing components. For example, we can assume that within the
      parsing component of <xref
      linkend="dialogue"/>, there is a level of syntactic
      representation; however, we have chosen not to expose this at the
      level of the system diagram. Perhaps the most important points to
      note are that first, computational systems typically assume that
      the processing of natural language inputs can be usefully broken
      down into a series of discrete steps; and second, that in the
      process of natural language understanding, these steps go from
      more concrete levels to more abstract ones, while in natural
      language production, the direction is reversed.</para>

  </section> <!-- The Architecture of linguistic and NLP systems -->

  <section><title>Language Technologies</title>

    <para>
    a selection of: machine translation, dialog systems, document summarization,
    information extraction, text retrieval, question answering
    </para>

  </section> <!-- Language Technologies -->

  <section><title>NLTK: The Natural Language Toolkit</title>

    <para>
    The Natural Language Toolkit (NLTK) was originally developed as
    part of a computational linguistics course at the University of
    Pennsylvania in 2001.  Over the years it has been developed and
    expanded with the help of dozens of contributors.  It is now used
    as the basis of research projects and has been adopted in courses
    in dozens of Universities.
    </para>

    <para>
    NLTK is available for free, and runs on most operating systems,
    including Linux/Unix, Mac OSX, and Microsoft Windows.  NTLK is
    available online at <literal>nltk.sourceforge.net</literal>.
    </para>

    <para>
    NLP is often taught within the confines of a single-semester
    course, either at advanced undergraduate level, or at postgraduate
    level.  Unfortunately, it turns out to be rather difficult to
    cover both the theoretical and practical sides of the subject in
    such a short span of time.  Some courses focus on theory to the
    exclusion of practical exercises, and deprive students of the
    challenge and excitement of writing programs to automatically
    process natural language.  Other courses are simply designed to
    teach programming for linguists, and get too caught up in the
    mechanics of programming to get very far with NLP.  NLTK was
    developed, in part, to address this problem, making it feasible
    to cover a substantial amount of theory and practice within a
    single-semester course.
    </para>

    <para>
    A significant fraction of any NLP course is made up of fundamental
    data structures and algorithms.  These are usually taught with the
    help of formal notations and complex diagrams.  Large trees and
    charts are copied onto the board and edited in tedious slow
    motion, or laboriously prepared for presentation slides.  A more
    effective method is to use live demonstrations.  NLTK provides
    interactive graphical user interfaces, making it possible to view
    program state, and to study program execution step-by-step.  Most
    NLTK modules have a demonstration mode, where they will perform an
    interesting task without requiring any special input from the
    user.  It is even possible to make minor modifications to programs
    in response to ``what if'' questions from the audience.  Not only
    do students learn the mechanics of NLP more quickly, they gain
    deeper insights into the data structures and algorithms, and they
    acquire new problem-solving skills.
    </para>

    <para>
    Thus NLTK offers a fresh approach to NLP pedagogy, in which
    theoretical content is tightly integrated with application.
    Practical work is supported by significant corpora distributed
    with NLTK, including samples from the Linguistic Data Consortium,
    the leading publisher of linguistic data.
    </para>
 
    <para>
    NLTK supports assignments of varying difficulty and scope.  In the
    simplest assignments, students experiment with existing components
    to perform a wide variety of NLP tasks.  This may involve no
    programming at all, in the case of the existing demonstrations, or
    simply changing a line or two of program code.  As students become
    more familiar with the toolkit they can be asked to modify
    existing components or to create complete systems out of existing
    components.  NLTK also provides students with a flexible framework
    for advanced projects, such as developing a multi-component
    system, by integrating and extending NLTK components, and adding
    on entirely new components.  Here NLTK helps by providing standard
    implementations of all the basic data structures and algorithms,
    interfaces to standard corpora, and a flexible and extensible
    architecture.
    </para>

   <para>In summary NLTK defines a basic infrastructure that can be
   used to build NLP programs in Python.  It provides: </para>

    <itemizedlist>
      <listitem><para>Basic classes for representing data relevant to
      natural language processing.</para>
      </listitem>
      <listitem><para>Standard interfaces for performing tasks, such
      as tokenization, tagging, and parsing.</para>
      </listitem>
      <listitem><para>Standard implementations for each task, which
      can be combined to solve complex problems.</para>
      </listitem>
      <listitem><para>Extensive documentation, including tutorials
      and reference documentation.</para>
      </listitem>
    </itemizedlist>

  </section> <!-- NLTK: The Natural Language Toolkit -->

  <section><title>The Python Programming Language</title>

    <para>
    NLTK is written in the Python language, a simple yet powerful
    scripting language with excellent functionality for processing
    linguistic data.  Python can be downloaded for free from
    <literal>www.python.org</literal>.
    Here is a five-line Python program which
    takes text input and prints all the words ending in <literal>ing</literal>:
    </para>

<programlisting><![CDATA[
import sys                            # load the system library
for line in sys.stdin.readlines():    # for each line of input
    for word in line.split():         # for each word in the line
        if word.endswith('ing'):      # does the word end in 'ing'?
            print word                # if so, print the word
]]></programlisting>

    <para>
    This program illustrates some of the main features of Python.
    First, whitespace is used to <emphasis>nest</emphasis> lines of
    code, thus the line starting with <literal>if</literal> falls
    inside the scope of the previous line starting with
    <literal>for</literal>, so the <literal>ing</literal> test is
    performed for each word.  Second, Python is
    <emphasis>object-oriented</emphasis>; each variable is an entity
    which has certain defined attributes and methods.  For example,
    <literal>line</literal> is more than a sequence of characters.  It
    has a method (or operation) called <literal>split</literal> which
    can break a line into its words.  To apply a method to an object,
    we give the object name, followed by a period, followed by the
    method name.  Third, methods have <emphasis>arguments</emphasis>
    expressed inside parentheses.  For instance,
    <literal>split</literal> had no argument because we were splitting
    the string wherever there was white space.  To split a string into
    sentences delimited by a period, we could write
    <literal>split('.')</literal>.  Finally, and most importantly,
    Python is highly readable, so much so that it is fairly easy to guess
    what the above program does without knowing any Python.
    </para>

    <para>
    The readability of Python is particularly striking in comparison
    to other languages which have been used for NLP, such as Perl.
    Here is a Perl program which prints words ending in
    <literal>ing</literal>:
    </para>

<programlisting><![CDATA[
while (<>) {                          # for each line of input
    foreach my $word (split) {        # for each word in a line
        if ($word =~ /ing$/) {        # does the word end in 'ing'?
            print "$word\n";          # if so, print the word
        }
    }
}
]]></programlisting>

    <para>
    Like Python, Perl is a scripting language.  However, it is not an
    object-oriented language, and its syntax is obscure.  For
    instance, it is difficult to guess what kind of entities are
    represented by: <literal>&lt;&gt;</literal>,
    <literal>my</literal>, and <literal>split</literal>.  We agree
    with Hammond, the author of a book on Perl programming for
    linguists, who writes that <emphasis>it is quite easy in Perl to
    write programs that simply look like raving gibberish, even to
    experienced Perl programmers</emphasis> (Hammond 2003:47).  Our
    own experience, as heavy users of Perl since the 1980s, is that it
    is inordinately difficult to maintain and re-use Perl programs of
    any size.  We believe it is not an optimal choice of programming
    language for linguists or for language processing.
    </para>

    <para>
    Several other languages are used for NLP, including Prolog,
    Java, LISP, C and Haskell.  In the appendix we have provided translations of our five-line
    Python program into these languages.
    We chose Python as the implementation language for NLTK for a number
    of reasons: in particular, it has a shallow learning curve, its
    syntax and semantics are transparent, and it has good
    string-handling functionality.  As a scripting language, Python
    facilitates interactive exploration.  As an object-oriented
    language, Python permits data and methods to be encapsulated and
    re-used easily.  Python comes with an extensive standard library,
    including tools for graphical programming and numerical
    processing.
    </para>

    <para>
    NLTK was designed with six requirements in mind.  First, NLTK is
    <emphasis>easy to use</emphasis>.  The primary purpose of the
    toolkit is to allow students to concentrate on building natural
    language processing systems.  The more time students must spend
    learning to use the toolkit, the less useful it is.  Second, we
    have made a significant effort to ensure that all the data
    structures and interfaces are <emphasis>consistent</emphasis>,
    making it easy to carry out a variety of tasks using a uniform
    framework.  Third, the toolkit is <emphasis>extensible</emphasis>,
    easily accommodating new components, whether those components
    replicate or extend the toolkit's existing functionality.
    Moreover, the toolkit is organized so that it is usually obvious
    where extensions would fit into the toolkit's infrastructure.
    Fourth, the toolkit is designed to be <emphasis>simple</emphasis>,
    providing an intuitive and appealing framework along with
    substantial building blocks, for students to gain a practical
    knowledge of NLP without having to write mountains of code.  Fifth, the toolkit
    is <emphasis>modular</emphasis>, so that the interaction between
    different components of the toolkit is minimized, and uses simple,
    well-defined interfaces.  In particular, it should be possible to
    complete individual projects using small parts of the toolkit,
    without needing to understand how they interact with the rest of
    the toolkit.  This allows students to learn how to use the toolkit
    incrementally throughout a course.  Modularity also makes it
    easier to change and extend the toolkit.  Finally, the toolkit is
    <emphasis>well documented</emphasis>, including nomenclature, data
    structures, and implementations.
    </para>

    <para>
    Contrasting with these requirements are three non-requirements.
    First, while the toolkit provides a wide range of functions, it is
    not intended to be encyclopedic.  There should be a wide variety
    of ways in which students can extend the toolkit.  Second, while
    the toolkit should be efficient enough that students can use their
    NLP systems to perform meaningful tasks, it does not need to be
    highly optimized for runtime performance.  Such optimizations
    often involve more complex algorithms, and sometimes require the
    use of C or C++, making the toolkit harder to install.  Third, we have
    avoided the the use of clever programming tricks, since clear
    implementations are far preferable to ingenious yet indecipherable
    ones.
    </para>


 

  <!--  <para> This tutorial introduces natural language processing in Python,
      NLTK, and the <literal>nltk.token</literal> module. </para>-->

  </section> <!-- The Choice of Python -->

  <section id="overview"><title>Overview of NLTK</title>

    <para>NLTK provides basic classes for representing data relevant to NLP.
    NLTK also provides standard interfaces for performing NLP tasks, along
    with standard implementations of each task.</para>

    <para>NLTK is organized into a collection of task-specific modules
    (Python packages).  Each contains data-oriented classes to
    represent NLP information, and task-oriented classes to
    encapsulate the resources and methods needed to perform a
    particular task.</para>

    <para>NLTK has the following modules:

<itemizedlist>
<listitem><para>token: classes for representing and processing
  individual elements of text, such as words and
  sentences
</para></listitem>

<listitem><para>probability: classess for representing and processing
  probabilistic information.
</para></listitem>

<listitem><para>tree: classes for representing and processing hierarchical
information over text.
</para></listitem>

<listitem><para>cfg: classes for representing and processing context
free grammars.</para></listitem>

<listitem><para>fsa: finite state automata</para></listitem>

<listitem><para>tagger: tagging each word with a part-of-speech, a sense, etc
</para></listitem>

<listitem><para>parser: building trees over text (includes chart, chunk and
  probabilistic parsers)
</para></listitem>

<listitem><para>classifier: classify text into categories
  (includes feature, featureSelection, maxent, naivebayes</para></listitem>

<listitem><para>draw: visualize NLP structures and processes</para></listitem>

<listitem><para>corpus: access (tagged) corpus data</para></listitem>
</itemizedlist>
    </para>

  </section> <!-- NLTK -->

  <section id="corpora"><title>Corpora</title>

<figure id="nltk-corpora">
  <title>Corpora and Corpus Samples Distributed with NLTK</title>
  <informaltable frame="all">
    <tgroup cols="4">
      <tbody>
        <row>
          <entry>Corpus</entry>
          <entry>Compiler</entry>
          <entry>Contents</entry>
          <entry>Example Application</entry>
        </row>
        <row>
          <entry>20 Newsgroups (selection)</entry>
          <entry>Ken Lang and Jason Rennie</entry>
          <entry>3 newsgroups, 4000 posts, 780k words</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Brown Corpus</entry>
          <entry>Nelson Francis and Henry Kucera</entry>
          <entry>15 genres, 1.15M words, tagged</entry>
          <entry>training and testing taggers, text classification, language modelling</entry>
        </row>
        <row>
          <entry>CoNLL 2000 Chunking Data</entry>
          <entry>Erik Tjong Kim Sang</entry>
          <entry>270k words, tagged and chunked</entry>
          <entry>training and testing chunk parsers</entry>
        </row>
        <row>
          <entry>Genesis Corpus</entry>
          <entry>Misc web sources</entry>
          <entry>6 texts, 200k words, 6 languages</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Project Gutenberg (selection)</entry>
          <entry>Michael Hart, Gregory Newby, et al</entry>
          <entry>14 texts, 1.7M words</entry>
          <entry>text classification, language modelling</entry>
        </row>
        <row>
          <entry>NIST 1999 Information Extraction (selection)</entry>
          <entry>John Garofolo</entry>
          <entry>63k words, newswire and named-entity SGML markup</entry>
          <entry>training and testing named-entity recognizers</entry>
        </row>
        <row>
          <entry>Levin Verb Index</entry>
          <entry>Beth Levin</entry>
          <entry>3k verbs with Levin classes</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Lexicon Corpus</entry>
          <entry></entry>
          <entry>Words, tags and frequencies from Brown Corpus and WSJ</entry>
          <entry>general purpose</entry>
        </row>
        <row>
          <entry>Names Corpus</entry>
          <entry>Mark Kantrowitz and Bill Ross</entry>
          <entry>8k male and female names</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>PP Attachment Corpus</entry>
          <entry>Adwait Ratnaparkhi</entry>
          <entry>28k prepositional phrases, tagged as noun or verb modifiers</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Roget's Thesaurus</entry>
          <entry>Project Gutenberg</entry>
          <entry>200k words, formatted text</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SEMCOR</entry>
          <entry>Vasile Rus and Rada Mihalcea</entry>
          <entry>880k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SENSEVAL 2 Corpus</entry>
          <entry>Ted Pedersen</entry>
          <entry>600k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>Stopwords Corpus</entry>
          <entry>Martin Porter et al</entry>
          <entry>2,400 stopwords for 11 languages</entry>
          <entry>text retrieval</entry>
        </row>
        <row>
          <entry>Penn Treebank (selection)</entry>
          <entry>LDC</entry>
          <entry>40k words, tagged and parsed</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Wordnet 1.7</entry>
          <entry>George Miller and Christiane Fellbaum</entry>
          <entry>180k words in a semantic network</entry>
          <entry>word-sense disambiguation, natural language understanding</entry>
        </row>
        <row>
          <entry>Wordlist Corpus</entry>
          <entry>OpenOffice.org et al</entry>
          <entry>960k words and 20k affixes for 8 languages</entry>
          <entry></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>
 
</section> <!-- Corpora -->

<section><title> Further Reading </title>

  <para>
  Dialogue example is from
  Bob Carpenter and Jennifer Chu-Carroll's ACL-99 Tutorial on Spoken Dialogue Systems
  </para>

</section> <!-- Further Reading -->

<section><title>Appendix: NLP in other Programming Languages</title>

<para>
Earlier we explained the thinking that lay behind our choice of the
Python programming language.  We showed a simple Python program that
reads in text and prints the words that end with <literal>ing</literal>.
In this appendix we provide equivalent programs in other languages, so
that readers can gain a sense of the appeal of Python.
</para>

<para>
Prolog is a logic programming language which has been popular for
developing natural language parsers and feature-based grammars, given
the inbuilt support for search and the
<emphasis>unification</emphasis> operation which combines two feature
structures into one.  Unfortunately Prolog is not easy to use for
string processing or input/output, as the following program code
demonstrates:
</para>

<programlisting><![CDATA[
main :-
    current_input(InputStream),
    read_stream_to_codes(InputStream, Codes),
    codesToWords(Codes, Words),
    maplist(string_to_list, Words, Strings),
    filter(endsWithIng, Strings, MatchingStrings),
    writeMany(MatchingStrings),
    halt.

codesToWords([], []).
codesToWords([Head | Tail], Words) :-
    ( char_type(Head, space) ->
        codesToWords(Tail, Words)
    ;
        getWord([Head | Tail], Word, Rest),
        codesToWords(Rest, Words0),
        Words = [Word | Words0]
    ).

getWord([], [], []).
getWord([Head | Tail], Word, Rest) :-
    (
        ( char_type(Head, space) ; char_type(Head, punct) )
    ->
        Word = [],
        Tail = Rest
    ;
        getWord(Tail, Word0, Rest),
        Word = [Head | Word0]
    ).

filter(Predicate, List0, List) :-
    ( List0 = [] ->
        List = []
    ;
        List0 = [Head | Tail],
        ( apply(Predicate, [Head]) ->
            filter(Predicate, Tail, List1),
            List = [Head | List1]
        ;
            filter(Predicate, Tail, List)
        )
    ).

endsWithIng(String) :- sub_string(String, _Start, _Len, 0, 'ing').

writeMany([]).
writeMany([Head | Tail]) :- write(Head), nl, writeMany(Tail).

]]></programlisting>

<para>
LISP is a so-called functional language, in which all objects
are lists, and all operations are performed by (nested) functions
of the form <literal>(function arg1 arg2 ...)</literal>:
</para>

<programlisting><![CDATA[
(defpackage "REGEXP-TEST" (:use "LISP" "REGEXP"))
(in-package "REGEXP-TEST")

(defun has-suffix (string suffix)
  "Open a file and look for words ending in _ing."
  (with-open-file (f string)
     (with-loop-split (s f " ")
        (mapcar #'(lambda (x) (has_suffix suffix x)) s))))

(defun has_suffix (suffix string)
  (let* ((suffix_len (length suffix))
   (string_len (length string))
    (base_len (- string_len suffix_len)))
    (if (string-equal suffix string :start1 0 :end1 NIL :start2 base_len :end2 NIL)
        (print string))))

(has-suffix "test.txt" "ing")

]]></programlisting>

<para>
Java is an object-oriented language that is designed for portability ...
</para>

<programlisting><![CDATA[
import java.io.*;

public class IngWords {
    public static void main(String[] args) {
        BufferedReader in = new BufferedReader(new
	    InputStreamReader(
                 System.in));
        String line = in.readLine();
        while (line != null) {
            for (String word : line.split(" ")) {
                if (word.endsWith("ing"))
                    System.out.println(word);
            }
            line = in.readLine();
        }
    }
}
]]></programlisting>


<para>
C:
</para>


<programlisting><![CDATA[
#include <sys/types.h>
#include <regex.h>
#include <stdio.h>

#define BUFFER_SIZE 1024

int main(int argc, char **argv)
{
     regex_t space_pat, ing_pat;
     char buffer[BUFFER_SIZE];

     regcomp(&space_pat, "[, \t\n]+", REG_EXTENDED);
     regcomp(&ing_pat, "ing$", REG_EXTENDED | REG_ICASE);

     while (fgets(buffer, BUFFER_SIZE, stdin) != NULL)
     {
         char *start = buffer;
         regmatch_t space_match;
         while (regexec(&space_pat, start, 1, &space_match, 0) == 0)
         {
             if (space_match.rm_so > 0)
             {
                 regmatch_t ing_match;
                 start[space_match.rm_so] = '\0';
                 if (regexec(&ing_pat, start, 1, &ing_match, 0) == 0)
                     printf("%s\n", start);
             }
             start += space_match.rm_eo;
         }
     }

     regfree(&space_pat);
     regfree(&ing_pat);

     return 0;
}
]]></programlisting>

<para>
Haskell:
</para>


<programlisting><![CDATA[
module Main where
main = interact (unlines.(filter ing).(map (filter isAlpha)).words)
 where
    ing = (=="gni").(take 3).reverse
]]></programlisting>



<para>
We are grateful to the following people for providing program samples:
Trevor Cohn, Edward Ivanovic, Olivia March, Tim Baldwin, Rod Farmer.
</para>



</section> <!-- Appendix -->


&index;
</article>

<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:2
sgml-indent-data:t
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
End:
-->
