<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Introduction to Natural Language Processing</title>
    &versiondate; &copyright;
    <title>NLTK Tutorial: Introduction to Natural Language Processing</title>
<!--
    <title>Introduction to Natural Language Processing</title>
-->
  </articleinfo>
 
<!--
  <para><emphasis>Sample chapter for Natural Language Processing,
    by Steven Bird, Ewan Klein and Edward Loper, April 2005</emphasis></para>
-->

<blockquote><attribution>John Ralston Saul</attribution>
<para>
The single and shortest definition of civilization may be the word
<emphasis>language</emphasis>...
Civilization, if it means something concrete, is the conscious but
unprogrammed mechanism by which humans communicate.  And through
communication they live with each other, think, create, and act.
</para>
</blockquote>

  <section id="goals"><title>The Language Challenge</title>

    <para>
      Language is the chief manifestation of human intelligence.
      Through language we express basic needs and lofty aspirations,
      technical know-how and flights of fantasy.  Ideas are
      shared over great separations of distance and time.
      The following samples from English illustrate the richness
      of language:
    </para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para>
      Overhead the day drives level and grey, hiding the sun by a
      flight of grey spears.
      (William Faulkner, <emphasis>As I Lay Dying</emphasis>, 1935)
    </para></listitem>
    <listitem><para>
      When using the toaster please ensure that the exhaust fan is
      turned on. (sign in dormitory kitchen)
    </para></listitem>
    <listitem><para>
      Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated
      activities with Ki values of 45.1-271.6 &mu;M (Medline)
    </para></listitem>
    <listitem><para>
      Iraqi Head Seeks Arms (spoof headline,
        <literal>http://www.snopes.com/humor/nonsense/head97.htm</literal>)
    </para></listitem>
    <listitem><para>
      The earnest prayer of a righteous man has great power and wonderful
      results. (James 5:16b)
    </para></listitem>
    <listitem><para>
      Twas brillig, and the slithy toves did gyre and gimble in the wabe
      (Lewis Carroll, <emphasis>Jabberwocky</emphasis>, 1872)
    </para></listitem>
    <listitem><para>
       There are two ways to do this, AFAIK :smile:  (internet
         discussion archive)
    </para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

    <para>
      Thanks to this richness, the study of language is part of many
      disciplines outside of linguistics, including translation,
      literary criticism, philosophy, anthropology and psychology.
      Many less obvious disciplines investigate language use, such as
      law, hermeneutics, forensics, telephony, pedagogy, archaeology,
      cryptanalysis and speech pathology.  Each applies distinct
      methodologies to gather observations, develop theories and test
      hypotheses.  Yet all serve to deepen our understanding of
      language and of the intellect which is manifested in language.
    </para>

    <para>
      The importance of language to science and the arts is matched in
      significance by the cultural treasure that is inherent in
      language.  Each of the world's ~7,000 human languages
      is rich in unique respects, in its oral
      histories and creation legends, down to its grammatical
      constructions and its very words and their nuances of meaning.
      Threatened remnant
      cultures have words to distinguish plant subspecies according to
      therapeutic uses which are unknown to science.  Languages evolve
      over time as they come into contact with each other and they
      provide a unique window onto human pre-history.  
      Technological change gives rise to new words like
      <emphasis>weblog</emphasis>
      and new morphemes like <emphasis>e-</emphasis> and <emphasis>cyber-</emphasis>.
      In many parts of the world, small linguistic variations from one
      town to the next add up to a completely different language in
      the space of a half-hour drive.  For its breathtaking complexity
      and diversity, human language is as a colourful tapestry
      stretching through time and space.
    </para>

    <para>
      Each new wave of computing technology has faced new challenges
      for language analysis.  Early machine languages gave way to
      high-level programming languages which are automatically parsed
      and interpreted.  Databases are interrogated using linguistic
      expressions like <literal>SELECT age FROM employee</literal>.
      Recently, computing devices have become ubiquitous and are often
      equipped with multimodal interfaces supporting text, speech,
      dialogue and pen gestures.  One way or another, building new
      systems for natural linguistic interaction will require
      sophisticated language analysis.
    </para>

    <para>
      Today, the greatest challenge for language analysis is presented by the
      explosion of text and multimedia content on the world-wide web.
      For many people, a large and growing fraction of work and
      leisure time is spent navigating and accessing this universe of
      information.  <emphasis>What tourist sites can I visit between
      Philadelphia and Pittsburgh on a limited budget?  What do expert
      critics say about Canon digital cameras?  What predictions about
      the steel market were made by credible commentators in the past
      week?</emphasis>  Answering such questions requires a combination of
      language processing tasks including information extraction,
      inference, and summarisation.  The scale of such tasks often
      calls for high-performance computing.
    </para>

    <para>
      As we have seen, <glossterm>natural language
      processing</glossterm>, or NLP, is important for scientific,
      economic, social, and cultural reasons.  NLP is experiencing
      rapid growth as its theories and methods are deployed in a
      variety of new language technologies.  For this reason it is
      important for a wide range of people to have a working knowledge
      of NLP.  Within academia, this includes people in areas from
      humanities computing and corpus linguistics through to computer
      science and artificial intelligence.  Within industry, this
      includes people in human-computer interaction, business
      information analysis, and web software development.  We hope
      that you, a member of this diverse audience reading these
      materials, will come to appreciate the workings of this rapidly
      growing field of NLP and will apply its techniques in the solution of
      real-world problems.  The following chapters present a
      carefully-balanced selection of theoretical foundations and
      practical application, and equips readers to work with large
      datasets, to create robust models of linguistic phenomena, and
      to deploy them in working language technologies.  By integrating
      all of this with the Natural Language Toolkit (NLTK), we hope
      this book opens up the exciting endeavour of practical natural
      language processing to a broader audience than ever before.
    </para>

    <note><para>
      An important aspect of learning NLP using these materials is
      to experience both the challenge and &mdash; we hope &mdash; the satisfaction
      of creating software to process natural language.  The accompanying
      software, NLTK, is available for free and runs on most operating systems
      including Linux/Unix, Mac OSX and Microsoft Windows.  You can
      download NTLK from <literal>nltk.sourceforge.net</literal>, along
      with extensive documentation.  We encourage you to install NLTK
      on your machine before reading beyond the end of this chapter.
    </para></note>

  </section> <!-- The Language Challenge -->

  <section><title>A Brief History of Natural Language Processing</title>

    <para>A long-standing challenge within computer science has been
    to build intelligent machines.  The chief measure of machine
    intelligence has been a linguistic one, namely the
    <glossterm>Turing Test</glossterm>.  Can a <glossterm>dialogue system</glossterm>,
    responding to a user's typed input with its own textual output,
    perform so naturally that users cannot distinguish it from
    a human interlocutor using the same interface?
    Today, there is substantial
    ongoing research and development in such areas as machine
    translation and spoken dialogue, and significant commercial
    systems are in widespread use.  The following dialogue
    illustrates a typical application:
    </para>

<programlisting>
S: How may I help you?

U: When is Saving Private Ryan playing?

S: For what theater?

U: The Paramount theater.

S: Saving Private Ryan is not playing at the Paramount theater, but
   it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30. 
</programlisting>

    <para>
      Today's commercial dialogue systems are strictly limited to
      narrowly-defined domains. We could not ask the above system to
      provide driving instructions or details of nearby restaurants
      unless the requisite information had already been stored and
      suitable question and answer sentences had been incorporated
      into the language processing system.  Observe that the above
      system appears to understand the user's goals: the user asks
      when a movie is showing and the system correctly determines from
      this that the user wants to see the movie. This inference seems
      so obvious to humans that we usually do not even notice it has
      been made, yet a natural language system needs to be endowed
      with this capability in order to interact naturally. Without it,
      when asked <literal>Do you know when Saving Private Ryan is
      playing</literal>, a system might simply &mdash; and unhelpfully
      &mdash; respond with a cold <literal>Yes</literal>. While it
      appears that this dialogue system can perform simple inferences,
      such sophistication is only found in cutting edge research
      prototypes.  Instead, the developers of commercial dialogue
      systems use contextual assumptions and simple business logic to
      ensure that the different ways in which a user might express
      requests or provide information are handled in a way that makes
      sense for the particular application.  Thus, whether the user
      says <literal>When is ... </literal>, or <literal>I want to know
      when ... </literal>, or <literal>Can you tell me when
      ... </literal>, simple rules will always result in users being
      presented with screening times.  This is sufficient for the
      system to provide a useful service.
    </para>

    <para>
      Despite some recent advances, it is generally true that those
      natural language systems which have been fully deployed still
      cannot perform common-sense reasoning or draw on world
      knowledge.  We can wait for these difficult artificial
      intelligence problems to be solved, but in the meantime it is
      necessary to live with some severe limitations on the reasoning
      and knowledge capabilities of natural language
      systems. Accordingly, right from the beginning, an important
      goal of NLP research has been to make progress on the holy grail
      of natural linguistic interaction <emphasis>without</emphasis>
      recourse to this unrestricted knowledge and reasoning
      capability.  This is an old challenge, and so it is instructive
      to review the history of the field.
    </para>

    <para>
      The very notion that natural language could be treated in a
      computational manner grew out of a research program, dating back
      to the early 1900s, to reconstruct mathematical reasoning using
      logic, most clearly manifested in the work by Frege, Russell,
      Wittgenstein, Tarski, Lambek and Carnap.  This work led to the
      notion of language as a formal system amenable to automatic
      processing.  Three later developments laid the foundation for
      natural language processing.  The first was
      <emphasis><glossterm>formal language
      theory</glossterm></emphasis>.  This defined a language as a set
      of strings accepted by a class of automata, such as context-free
      languages and pushdown automata, and provided the underpinnings
      for computational syntax.
    </para>

    <para>
      The second development was <emphasis><glossterm>symbolic
      logic</glossterm></emphasis>. This provided a formal method for
      capturing selected aspects of natural language that are relevant
      for expressing logical proofs. A formal calculus in symbolic
      logic provides the syntax of a language, together with rules of
      inference and, possibly, rules of interpretation in a
      set-theoretic model; examples are propositional logic and
      first-order logic.  Given such a calculus, with a well-defined
      syntax and semantics, it becomes possible to associate meanings
      with expressions of natural language by translating them into
      expressions of the formal calculus. For example, if we translate
      <emphasis>John saw Mary</emphasis> into a formula
      <literal>saw(j,m)</literal>, we (implicitly or explicitly)
      intepret the English verb <emphasis>saw</emphasis> as a binary
      relation, and <emphasis>John</emphasis> and
      <emphasis>Mary</emphasis> as denoting individuals.  More general
      statements like <emphasis>All birds fly</emphasis> require
      quantifiers, in this case &forall; meaning <emphasis>for
      all</emphasis>: <literal>&forall;x: bird(x) &rarr;
      fly(x)</literal>.  This use of logic provided the technical
      machinery to perform inferences that are an important part of
      language understanding. The third development was the
      <emphasis><glossterm>principle of
      compositionality</glossterm></emphasis>. This was the notion
      that the meaning of a complex expression is comprised of the
      meaning of its parts and their mode of combination. This
      principle provided a useful correspondence between syntax and
      semantics, namely that the meaning of a complex expression could
      be computed recursively.  Given the representation of
      <emphasis>It is not true that
      &blank;<subscript>p</subscript></emphasis> as
      <literal>not(p)</literal> and <emphasis>John saw Mary</emphasis>
      as <literal>saw(j,m)</literal>, we can compute the
      interpretation of <emphasis>It is not true that John saw
      Mary</emphasis> recursively using the above information to get
      <literal>not(saw(j,m))</literal>. Today, this approach is most
      clearly manifested in a family of grammar formalisms known as
      unification-based grammar, and NLP applications implemented in
      the Prolog programming language.  This approach has been called
      <emphasis>high-church</emphasis> NLP, to highlight its attention
      to order and its ritualized methods for ensuring correctness.
    </para>

    <para>
      A separate strand of development in the 1960s and 1970s eschewed
      the declarative/procedural distinction and the principle of
      compositionality.  They only seemed to get in the way of
      building practical systems.  For example, early question
      answering systems employed fixed pattern-matching templates such
      as: <literal>How many &blank;<subscript>i</subscript> does
      &blank;<subscript>j</subscript> have?</literal>, where slot
      <literal>i</literal> is a feature or service, and slot
      <literal>j</literal> is a person or place.  Each template came
      with a predefined semantic function, such as
      <literal>count(i,j)</literal>.  A user's question which matched
      the template would be mapped to the corresponding semantic
      function and then ``executed'' to obtain an answer, <literal>k =
      count(i,j)</literal>.  This answer would be substituted into a
      new template: <literal>&blank;<subscript>j</subscript> has
      &blank;<subscript>k</subscript>
      &blank;<subscript>i</subscript></literal>.  For example, the
      question <emphasis>How many airports<subscript>i</subscript>
      does London<subscript>j</subscript> have?</emphasis> can be
      mapped onto a template (as shown by the subscripts) and
      translated to an executable program.  The result can be
      substituted into a new template and returned to the user:
      <emphasis>London has five airports</emphasis>.  Finally, the
      subscripts are removed and the natural language answer is
      returned to the user.
    </para>

    <para>
      This approach to NLP is known as <emphasis><glossterm>semantic
      grammar</glossterm></emphasis>.  Such grammars are formalized
      like phrase-structure grammars, but their constituents are no
      longer grammatical categories like noun phrase, but semantic
      categories like <emphasis>Airport</emphasis> and
      <emphasis>City</emphasis>.  These grammars work very well in
      limited domains, and are still widely used in spoken language
      systems.  However, they suffer from brittleness, duplication of
      grammatical structure in different semantic categories, and lack
      of portability.  This approach has been called
      <emphasis>low-church</emphasis> NLP, to highlight its readiness
      to adopt new methods regardless of their prestige, and a
      concomitant disregard for tradition.
    </para>

    <para>
      In the preceding paragraphs we mentioned a distinction between
      high-church and low-church approaches to NLP.  This distinction
      relates back to early metaphysical debates about
      <emphasis><glossterm>rationalism</glossterm></emphasis> versus
      <emphasis><glossterm>empiricism</glossterm></emphasis> and
      <emphasis><glossterm>realism</glossterm></emphasis> versus
      <emphasis><glossterm>idealism</glossterm></emphasis> that
      occurred in the Enlightenment period of Western philosophy.
      These debates took place against a backdrop of orthodox thinking
      in which the source of all knowledge was believed to be divine
      revelation.  During this period of the seventeenth and
      eighteenth centuries, philosophers argued that human reason or
      sensory experience has priority over revelation.  Descartes and
      Leibniz, amongst others, took the rationalist position,
      asserting that all truth has its origins in human thought, and
      in the existence of ``innate ideas'' implanted in our minds from
      birth.  For example, they saw that the principles of Euclidean
      geometry were developed using human reason, and were not the
      result of supernatural revelation or sensory experience.  In
      contrast, Locke and others took the empiricist view, that our
      primary source of knowledge is the experience of our faculties,
      and that human reason plays a secondary role in reflecting on
      that experience.  Prototypical evidence for this position was
      Galileo's discovery &mdash; based on careful observation of the
      motion of the planets &mdash; that the solar system is
      heliocentric and not geocentric.  In the context of linguistics,
      this debate leads to the following question: to what extent does
      human linguistic experience, versus our innate ``language
      faculty'', provide the basis for our knowledge of language?
      In NLP this matter surfaces as differences in the priority of
      corpus data versus linguistic introspection in the construction
      of computational models.
    </para>

    <para>
      A further concern, enshrined in the debate between
      <emphasis><glossterm>realism</glossterm></emphasis> and
      <emphasis><glossterm>idealism</glossterm></emphasis>, was the
      metaphysical status of the constructs of a theory.  Kant argued
      for a distinction between phenomena, the manifestations we can
      experience, and ``things in themselves'' which can never been
      known directly.  A linguistic realist would take a theoretical
      construct like ``noun phrase'' to be real world entity that
      exists independently of human perception and reason, and which
      actually <emphasis>causes</emphasis> the observed linguistic
      phenomena.  A linguistic idealist, on the other hand, would
      argue that noun phrases, along with more abstract constructs
      like semantic representations, are intrinsically unobservable,
      and simply play the role of useful fictions.  The way linguists
      write about theories often betrays a realist position, while
      NLP practitioners occupy neutral territory or else lean towards
      the idealist position.
    </para>

    <para>
      These issues are still alive today, and show up in the
      distinctions between symbolic vs statistical methods, deep vs
      shallow processing, binary vs gradient classifications, and
      scientific vs engineering goals.  However, these contrasts are
      highly nuanced, and the debate is no longer as polarised as it
      once was.  In fact, most of the discussions &mdash; and most of
      the advances even &mdash; involve a <emphasis>balancing
      act</emphasis> of the two extremes.  For example, one
      intermediate position is to assume that humans are innately
      endowed with analogical and memory-based learning methods (weak
      rationalism), and use these methods to identify meaningful
      patterns in their sensory language experience (empiricism).  For
      a more concrete illustration, consider the way in which
      statistics from large corpora may serve as evidence for binary
      choices in a symbolic grammar.  For instance, dictionaries
      describe the words <emphasis>absolutely</emphasis> and
      <emphasis>definitely</emphasis> as nearly synonymous, yet their
      patterns of usage are quite distinct when combined with a
      following verb, as shown in <xref linkend="absolutely"/>.
    </para>

<figure id="absolutely">
  <title>Absolutely vs Definitely (Liberman 2005, LanguageLog.org)</title>
  <informaltable frame="all">
    <tgroup cols="5">
      <tbody>
        <row>
          <entry>Google hits</entry>
          <entry>adore</entry>
          <entry>love</entry>
          <entry>like</entry>
          <entry>prefer</entry>
        </row>
        <row>
          <entry>absolutely</entry>
          <entry>289,000</entry>
          <entry>905,000</entry>
          <entry>16,200</entry>
          <entry>644</entry>
        </row>
        <row>
          <entry>definitely</entry>
          <entry>1,460</entry>
          <entry>51,000</entry>
          <entry>158,000</entry>
          <entry>62,600</entry>
        </row>
        <row>
          <entry>ratio</entry>
          <entry>198/1</entry>
          <entry>18/1</entry>
          <entry>1/10</entry>
          <entry>1/97</entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>

<!--
      http://itre.cis.upenn.edu/~myl/languagelog/archives/002022.html
-->

    <para>
      Observe that <emphasis>absolutely adore</emphasis> is about 200
      times as popular as <emphasis>definitely adore</emphasis>, while
      <emphasis>absolutely prefer</emphasis> is about 100 times rarer
      then <emphasis>definitely prefer</emphasis>.  This information
      is used by statistical language models, but it also counts as
      evidence for a symbolic account of word combination in which
      <emphasis>absolutely</emphasis> can only modify extreme actions
      or attributes.  This information could be represented as a
      binary-valued feature of certain lexical items.  Thus, we see
      statistical data informing symbolic models.  Now that
      this information is codified, it is available to be exploited as
      a contextual feature for a statistical language modelling,
      alongside many other rich sources of symbolic information, like
      hand-constructed parse trees and semantic representations.  Now
      the circle is closed, and we see symbolic information informing
      statistical models.
    </para>

    <para>
      This new rapprochement between high-church and low-church NLP
      is giving rise to many exciting new developments.  We will touch
      on some of these in the ensuing pages.  We too will perform this balancing
      act, employing approaches to NLP that integrate these
      historically-opposed philosophies and methodologies.
    </para>

  </section> <!-- A Brief History of Natural Language Processing -->

  <section><title>An NLP Application: Information Extraction</title>

    <para>
      For many NLP applications, the priority is to extract meaning
      from written text. This must be done
      <emphasis>robustly</emphasis>; processing should not fail when
      unexpected or ill-formed input is received.  Today, robust
      semantic interpretation is easiest when shallow processing
      methods are used.  As we saw in the previous section, these
      methods severely limit (or even omit) syntactic analysis and
      restrict the complexity of the target semantic representations.
    </para>

    <para>
      One well-known instance of shallow semantics is
      <glossterm>Information Extraction</glossterm> (IE). In contrast
      to full text understanding, IE systems only try to recognise a
      limited number of pre-specified semantic topics and use a
      constrained semantic representation.  The initial phase of
      information extraction involves the detection of
      <glossterm>named entities</glossterm>, expressions that denote
      locations, people, companies, times, and monetary amounts.  (In
      fact, these named entities are nothing other than the low-level
      categories of the semantic grammars we saw in the previous
      section, but under a new guise.)  To illustrate, the following
      text contains several expressions which have been tagged as
      entities of types <literal>time</literal>,
      <literal>company</literal> and <literal>location</literal>:
    </para>

<programlisting><![CDATA[
The incident occurred <time>around 5.30pm</time> when a man walked into
<company>William Hill bookkeepers</company> on <location>Ware Road</location>,
and threatened a member of staff with a small black handgun. 
]]></programlisting>


    <para>
      The final output of an information extraction system, usually
      referred to as a <glossterm>template</glossterm>, consists of
      fixed number of slots that must be filled. More semantically, we
      can think of the template as denoting an event in which the
      participating entities play specific roles.  Here is an example
      corresponding to the above text:
    </para>

<programlisting><![CDATA[
Crime event
     Time: around 5.30pm
     Location: William Hill bookkeepers on Ware Road
     Suspect: man
     Weapon: small black handgun
]]></programlisting>

    <para>
      Early work in IE demonstrated that hand-built rules could
      achieve good accuracy in the tasks defined by the DARPA Message
      Understanding Conferences (MUC).  In general, highest accuracy
      has been achieved in identifying named entities, such as
      <literal>William Hill bookkeepers</literal>.  However, it is harder to
      identify which slots the entities should fill; and harder still
      to accurately identify which event the entities are
      participating in.  In recent years attention has shifted to
      making IE systems more portable to new domains by using some
      form of machine learning.  These systems work moderately well on
      marked-up text (e.g. HTML-formatted text), but their performance
      on free text still needs improvement before they can be adopted
      more widely.
    </para>

    <para>
      IE systems commonly use external knowledge sources in addition
      to the input text.  For example, named entity recognizers often
      use name or location gazetteers, extensive catalogues of people
      and places such as the Getty Thesaurus of Geographic Names.  In
      the bioinformatics domain it is common to draw on external
      knowledge bases for standard names of genes and proteins.  A
      general purpose lexical ontology called WordNet is often used in
      IE systems.  Sometimes formal domain ontologies are used as a
      knowledge source for IE, permitting systems to perform simple
      taxonomic inferences and resolve ambiguities in the
      classification of slot-fillers.
    </para>

    <para>
      For example, an IE system for extracting basketball statistics
      from news reports on the web might have to interpret the
      sentence <emphasis>Quincy played three games last
	weekend</emphasis>, where the ambiguous subject
      <emphasis>Quincy</emphasis> might be either a player or a team.
      An ontology in which <emphasis>Quincy</emphasis>
      is identified as the name of a college basketball team would
      permit the ambiguity to be resolved.  In other cases the
      template disambiguates the slot filler.  Thus, in the basketball domain,
      <emphasis>Washington</emphasis> might refer to one of many
      possible players, teams or locations.  If we know that the
      verb <emphasis>won</emphasis> requires an entity whose role in
      the taxonomy is a team, then we can resolve the ambiguity in
      the sentence <emphasis>Washington won</emphasis>.
    </para>

  </section> <!-- Language Technologies -->

<!--

  <section><title>A Sample of NLP Tasks</title>

    <para>
      
    </para>

    <para>
    a selection of: machine translation, dialog systems, document summarization,
    information extraction, text retrieval, question answering
    </para>

  </section>
-->

  <section><title>The Architecture of linguistic and NLP systems</title>

    <para>
      Within the approach to linguistic theory known as <glossterm>
      generative grammar</glossterm>, it is claimed that humans have
      distinct kinds of linguistic knowledge, organised into different
      modules: for example, knowledge of a language's sound structure
      (phonology), knowledge of word structure (morphology), knowledge
      of phrase structure (syntax), and knowledge of meaning
      (semantics). In a formal linguistic theory, each
      kind of linguistic knowledge is made explicit
      as different <emphasis>module</emphasis> of the theory, consisting
      of a collection of basic elements together with a way of
      combining them into complex structures. For example, a
      phonological module might provide a set of phonemes together
      with an operation for concatenating phonemes into phonological
      strings. Similarly, a syntactic module might provide labelled
      nodes as primitives together wih a mechanism for assembling
      them into trees. A set of linguistic
      primitives, together with some operators for defining complex
      elements, is often called <glossterm>a level of
      representation</glossterm>.
    </para>

    <para>
      As well as defining modules, a generative grammar will prescribe
      how the modules interact. For example, well-formed phonological
      strings will provide the phonological content of words, and
      words will provide the terminal elements of syntax
      trees. Well-formed syntactic trees will be mapped to semantic
      representations, and contextual or pragmatic information will
      ground these semantic representations in some real-world
      situation.
      </para>

    <para>
      As we indicated above, an important aspect of theories of
      generative grammar is that they are intended to model the
      linguistic knowledge of speakers and hearers; they are not
      intended to explain how humans actually process linguistic
      information. This is, in part, reflected in the claim that a
      generative grammer encodes the
      <emphasis><glossterm>competence</glossterm></emphasis> of an
      idealized native speaker, rather than the speaker's
      <emphasis><glossterm>performance</glossterm></emphasis>. A closely related distinction
      is to say that a generative grammar encodes
      <emphasis><glossterm>declarative</glossterm></emphasis> rather than
      <emphasis><glossterm>procedural</glossterm></emphasis> knowledge.  As you might
      expect, computational linguistics has the crucial role of
      proposing procedural models of language. A central example is
      <glossterm>parsing</glossterm>, where we have to develop
      computational mechanisms which convert strings of words
      into structural representations such as syntax
      trees. Nevertheless, it is widely accepted that well-engineered
      computational models of language contain both declarative and
      procedural aspects. Thus, a full account of parsing will say how
      declarative knowledge in the form of a grammar and lexicon
      combines with procedural knowledge which determines how a
      syntactic analysis should be assigned to a given string of
      words. This procedural knowledge will be expressed as an
      <glossterm>algorithm</glossterm>: that is, an explicit recipe
      for mapping some input into an appropriate output in a finite
      number of steps.
    </para>

    <para>
      A simple parsing algorithm for context-free gramars, for
      instance, looks first for a rule of the form <literal>S &rarr;
      X<subscript>1</subscript> &mldr;
      X<subscript>n</subscript></literal>, and builds a partial tree
      structure. It then steps through the grammar rules one-by-one,
      looking for a rule of the form
      <literal>X<subscript>1</subscript> &rarr;
      Y<subscript>1</subscript> ...
      Y<subscript>j</subscript></literal> which will expand the
      leftmost daughter introduced by the <literal>S</literal> rule,
      and further extends the partial tree. This process continues,
      for example by looking for a rule of the form
      <literal>Y<subscript>1</subscript> &rarr;
      Z<subscript>1</subscript> ...
      Z<subscript>k</subscript></literal> and expanding the partial
      tree appropriately, until the leftmost node label in the partial
      tree is a lexical category; the parser then checks to see if the
      first word of the input can belong to the category. To
      illustrate, let's suppose that the first grammer rule chosen by
      the parser is <literal>S &rarr; NP VP</literal> and the second
      rule chosen is <literal>NP &rarr; Det N</literal>; then the
      partial tree will the one in <xref linkend="partialtree"/>.
 
    </para>

    <figure id="partialtree">
      <title>Partial Parse Tree</title>
      <graphic fileref="images/partialtree" scale="15"/>
    </figure>

    <para>
      If we assume that the input string we are trying to parse is
      <emphasis>the cat slept</emphasis>,  we will succeed in
      identifying <emphasis>the</emphasis> as a word which can belong to
      the category <literal>Det</literal>. In this case, the parser goes
      on to the next node of the tree, <literal>N</literal>, and
      next input word, <emphasis>cat</emphasis>. However, if we
      had built the same partial tree with an input string <emphasis>did
	the cat sleep</emphasis>, the parse would fail at this point,
      since <emphasis>did</emphasis> is not of category
      <literal>Det</literal>.  The parser would throw away the structure
      built so far and look for an alternative way of going from the
      <literal>S</literal> node down to a leftmost lexical category
      (e.g., using a rule <literal>S &rarr; V NP VP</literal>). The
      important point for now is not the details of this or other
      parsing algorithms; we discuss this topic much more fully in
      the chapter on parsing. Rather, we just want to illustrate the idea
      that an algorithm can be broken down into a fixed number of steps
      which produce a definite result at the end.
    </para>

    <para>
      In <xref linkend="dialogue"/> we further illustrate some of
      these points in the context of a <glossterm>spoken dialogue
      system</glossterm>, such as our earlier example of an
      application that offers the user information about movies
      currently on show.
    </para>
    <figure id="dialogue">
      <title>Architecture of Spoken Dialogue System</title>

		<graphic fileref="images/dialogue" scale="20"/>

    </figure>

    <para>
      Down the lefthand side of the diagram we have shown a pipeline
      of some representative speech understanding
      <emphasis>component</emphasis>s.  These map from speech input
      via syntactic parsing to some kind of meaning representation. Up
      the righthand side is an inverse pipeline of components for
      concept-to-speech generation. These components constitute the
      procedural aspect of the system's natural language
      processing. In the central column of the diagram are some
      representative declaratives aspects: the repositories of
      language-related information which are called upon by the
      processing components.
    </para>

    <para>
      In addition to embodying the declarative/procedural distinction,
      the diagram also illustrates that linguistically motivated ways
      of modularizing linguistic knowledge are often reflected in
      computational systems. That is, the various components are
      organized so that the data which they exchange corresponds
      roughly to different levels of representation. For example, the
      output of the speech analysis component will contain sequences
      of phonological representations of words, and the output of the
      parser will be a semantic representation.  Of course the
      parallel is not precise, in part because it is often a matter of
      practical expedience where to place the boundaries between
      different processing components. For example, we can assume that
      within the parsing component of <xref linkend="dialogue"/> there
      is a level of syntactic representation, although we have chosen
      not to expose this at the level of the system diagram.  Despite
      such idiosyncracies, most NLP systems break down their work into
      a series of discrete steps.  In the process of natural language
      understanding, these steps go from more concrete levels to more
      abstract ones, while in natural language production, the
      direction is reversed.
    </para>

  </section> <!-- The Architecture of linguistic and NLP systems -->

<!--
  <section><title>Linguistics and Natural Language Processing (draft)</title>

  <para>
    [What is the relationship between linguistics and NLP?
    Goal of (generative) linguistics to account for the grammaticality
    judgements of the ideal monolingual speaker/hearer, vs goal of
    NLP to build systems to map between the (linguistic) systems of
    humans and machines.  Challenge of linguistics is to balance
    descriptive and explanatory adequacy; challenge of NLP to balance
    expressiveness and tractability.]
  </para>

    <para>
    [Grammar as a definition of well-formed
    sentences along with a semantic translation, versus
    an implementation which (say) maps from sentences to
    meanings (parser) or vice versa (generator).
    declarative vs procedural;
    system of rewriting rules vs automaton;
    perspective on NLP: relating the declarative to the procedural;
    distinguish this constrast from competence vs performance.]
    </para>

    <para>
    In the late 1980s and early 1990s there was a promising
    convergence between the fields of linguistics and NLP.  (This had
    been a feature of the 1960s, e.g. with the application of the SPE
    model in speech synthesis systems.)  Computational linguists often
    looked to linguistics as a source of knowledge about language.
    Over the last decade we have seen a new divergence, as
    computational linguists have discovered that linguistic analyses
    often failed to account for the linguistic patterns attested in
    the large corpora used to develop their systems.  However, once
    linguists learn to work with these large datasets, their own
    analytical work will benefit, leading to broader coverage of their
    theories, and earlier refutation of false hypotheses.  The result,
    we expect, will be new opportunities for cross-fertilization
    between linguistics and NLP.
    </para>

    <para>
    [Opportunities for linguists to contribute their insights to the
    future development of NLP and, in the reverse direction, to apply
    the results of NLP research back in linguistics.]
    </para>

  </section>
-->

  <section><title>Learning NLP with the Natural Language Toolkit</title>

    <para>
      The <glossterm>Natural Language Toolkit (NLTK)</glossterm> was
      originally created as part of a computational linguistics course
      in the Department of Computer and Information Science at the
      University of Pennsylvania in 2001.  Since then it has been
      developed and expanded with the help of dozens of contributors.
      It has now been adopted in courses in dozens of universities,
      and serves as the basis of many research projects.  In this
      section we will discuss some of the benefits of learning (and
      teaching) NLP using NLTK.
    </para>

    <para>
      NLP is often taught within the confines of a single-semester
      course, either at advanced undergraduate level, or at
      postgraduate level.  Unfortunately, it turns out to be rather
      difficult to cover both the theoretical and practical sides of
      the subject in such a short span of time.  Some courses focus on
      theory to the exclusion of practical exercises, and deprive
      students of the challenge and excitement of writing programs to
      automatically process natural language.  Other courses are
      simply designed to teach programming for linguists, and do not
      get past the mechanics of programming to cover significant NLP.
      NLTK was developed to address this very problem, making it
      feasible to cover a substantial amount of theory and practice
      within a single-semester course.
    </para>

    <para>
      A significant fraction of any NLP course is made up of
      fundamental data structures and algorithms.  These are usually
      taught with the help of formal notations and complex diagrams.
      Large trees and charts are copied onto the board and edited in
      tedious slow motion, or laboriously prepared for presentation
      slides.  A more effective method is to use live demonstrations
      in which those diagrams are generated and updated automatically.
      NLTK provides interactive graphical user interfaces, making it
      possible to view program state and to study program execution
      step-by-step.  Most NLTK components have a demonstration mode,
      and will perform an interesting task without requiring any
      special input from the user.  It is even possible to make minor
      modifications to programs in response to ``what if'' questions.
      In this way, students learn the mechanics of NLP quickly, gain
      deeper insights into the data structures and algorithms, and
      acquire new problem-solving skills.
    </para>

    <para>
      NLTK supports assignments of varying difficulty and scope.  In
      the simplest assignments, students experiment with existing
      components to perform a wide variety of NLP tasks.  This may
      involve no programming at all, in the case of the existing
      demonstrations, or simply changing a line or two of program
      code.  As students become more familiar with the toolkit they
      can be asked to modify existing components or to create complete
      systems out of existing components.  NLTK also provides students
      with a flexible framework for advanced projects, such as
      developing a multi-component system, by integrating and
      extending NLTK components, and adding on entirely new
      components.  Here NLTK helps by providing standard
      implementations of all the basic data structures and algorithms,
      interfaces to standard corpora, substantial corpus samples, and
      a flexible and extensible architecture.  Thus, as we have seen,
      NLTK offers a fresh approach to NLP pedagogy, in which
      theoretical content is tightly integrated with application.
    </para>

  </section> <!-- Learning NLP with the Natural Language Toolkit -->

  <section><title>The Python Programming Language</title>

    <para>
      NLTK is written in the <glossterm>Python</glossterm> language, a
      simple yet powerful scripting language with excellent
      functionality for processing linguistic data.  Python can be
      downloaded for free from <literal>www.python.org</literal>.
      Here is a five-line Python program which takes text input and
      prints all the words ending in <literal>ing</literal>:
    </para>

<programlisting><![CDATA[
import sys                            # load the system library
for line in sys.stdin.readlines():    # for each line of input
    for word in line.split():         # for each word in the line
        if word.endswith('ing'):      # does the word end in 'ing'?
            print word                # if so, print the word
]]></programlisting>

    <para>
      This program illustrates some of the main features of Python.
      First, whitespace is used to <emphasis>nest</emphasis> lines of
      code, thus the line starting with <literal>if</literal> falls
      inside the scope of the previous line starting with
      <literal>for</literal>, so the <literal>ing</literal> test is
      performed for each word.  Second, Python is
      <emphasis>object-oriented</emphasis>; each variable is an entity
      which has certain defined attributes and methods.  For example,
      <literal>line</literal> is more than a sequence of characters.
      It is a string object that has a method (or operation) called
      <literal>split</literal> that we can use to break a line into
      its words.  To apply a method to an object, we give the object
      name, followed by a period, followed by the method name.  Third,
      methods have <emphasis>arguments</emphasis> expressed inside
      parentheses.  For instance, <literal>split</literal> had no
      argument because we were splitting the string wherever there was
      white space.  To split a string into sentences delimited by a
      period, we could write <literal>split('.')</literal>.  Finally,
      and most importantly, Python is highly readable, so much so that
      it is fairly easy to guess what the above program does even if
      you have never written a program before.
    </para>

    <para>
      This readability of Python is striking in comparison to other
      languages which have been used for NLP, such as
      <glossterm>Perl</glossterm>.  Here is a Perl program which
      prints words ending in <literal>ing</literal>:
    </para>

<programlisting><![CDATA[
while (<>) {                          # for each line of input
    foreach my $word (split) {        # for each word in a line
        if ($word =~ /ing$/) {        # does the word end in 'ing'?
            print "$word\n";          # if so, print the word
        }
    }
}
]]></programlisting>

    <para>
      Like Python, Perl is a scripting language.  However, it is not a
      real object-oriented language, and its syntax is obscure.  For
      instance, it is difficult to guess what kind of entities are
      represented by: <literal>&lt;&gt;</literal>,
      <literal>$</literal>, <literal>my</literal>, and
      <literal>split</literal>.  We agree that ``it is quite easy in
      Perl to write programs that simply look like raving gibberish,
      even to experienced Perl programmers'' (Hammond 2003:47).
      Having used Perl ourselves in research and teaching since the
      1980s, we have found that Perl programs of any size are
      inordinately difficult to maintain and re-use.  Therefore we
      believe Perl is not an optimal choice of programming language
      for linguists or for language processing.  Several other
      languages are used for NLP, including Prolog, Java, LISP and C.
      In the appendix we have provided translations of our five-line
      Python program into these and other languages, and invite you to
      compare them for readability.
    </para>

    <para>
      We chose Python as the implementation language for NLTK because
      it has a shallow learning curve, its syntax and semantics are
      transparent, and it has good string-handling functionality.  As
      a scripting language, Python facilitates interactive
      exploration.  As an object-oriented language, Python permits
      data and methods to be encapsulated and re-used easily.  Python
      comes with an extensive standard library, including components
      for graphical programming, numerical processing, and web data
      processing.
    </para>

   <para>NLTK defines a basic infrastructure that can be
   used to build NLP programs in Python.  It provides: </para>

    <itemizedlist>
      <listitem><para>Basic classes for representing data relevant to
      natural language processing.</para>
      </listitem>
      <listitem><para>Standard interfaces for performing tasks, such
      as tokenization, tagging, and parsing.</para>
      </listitem>
      <listitem><para>Standard implementations for each task, which
      can be combined to solve complex problems.</para>
      </listitem>
      <listitem><para>Extensive documentation, including tutorials
      and reference documentation.</para>
      </listitem>
    </itemizedlist>

  </section> <!-- The Python Programming Language -->

  <section><title>The Design of NLTK</title>

    <para>
      NLTK was designed with six requirements in mind.  First, NLTK is
      <emphasis>easy to use</emphasis>.  The primary purpose of the
      toolkit is to allow students to concentrate on building natural
      language processing systems.  The more time students must spend
      learning to use the toolkit, the less useful it is.  Second, we
      have made a significant effort to ensure that all the data
      structures and interfaces are <emphasis>consistent</emphasis>,
      making it easy to carry out a variety of tasks using a uniform
      framework.  Third, the toolkit is
      <emphasis>extensible</emphasis>, easily accommodating new
      components, whether those components replicate or extend the
      toolkit's existing functionality.  Moreover, the toolkit is
      organized so that it is usually obvious where extensions would
      fit into the toolkit's infrastructure.  Fourth, the toolkit is
      designed to be <emphasis>simple</emphasis>, providing an
      intuitive and appealing framework along with substantial
      building blocks, for students to gain a practical knowledge of
      NLP without having to write mountains of code.  Fifth, the
      toolkit is <emphasis>modular</emphasis>, so that the interaction
      between different components of the toolkit is minimized, and
      uses simple, well-defined interfaces.  In particular, it should
      be possible to complete individual projects using small parts of
      the toolkit, without needing to understand how they interact
      with the rest of the toolkit.  This allows students to learn how
      to use the toolkit incrementally throughout a course.
      Modularity also makes it easier to change and extend the
      toolkit.  Finally, the toolkit is <emphasis>well
      documented</emphasis>, including nomenclature, data structures,
      and implementations.
    </para>

    <para>
      Contrasting with these requirements are three non-requirements.
      First, while the toolkit provides a wide range of functions, it
      is not intended to be encyclopedic.  There should be a wide
      variety of ways in which students can extend the toolkit.
      Second, while the toolkit should be efficient enough that
      students can use their NLP systems to perform meaningful tasks,
      it does not need to be highly optimized for runtime performance.
      Such optimizations often involve more complex algorithms, and
      sometimes require the use of C or C++, making the toolkit harder
      to install.  Third, we have avoided clever programming tricks,
      since clear implementations are far preferable to ingenious yet
      indecipherable ones.
    </para>

    <para>
      NLTK is organized into a collection of task-specific components.
      Each module is a combination of data structures for representing
      a particular kind of information such as trees, and
      implementations of standard algorithms involving those
      structures such as parsers.  This approach is a standard feature
      of <glossterm>object-oriented design</glossterm>, in which
      components encapsulate both the resources and methods needed to
      accomplish a particular task.
    </para>

    <para>
    The most fundamental NLTK components are for identifying and
    manipulating individual words of text.  These include:
    <literal>tokenizer</literal>, for breaking up strings of
      characters into word tokens;
    <literal>tokenreader</literal>, for reading tokens from
      different kinds of corpora;
    <literal>stemmer</literal>, for stripping affixes from tokens,
      useful in some text retrieval applications;
    and
    <literal>tagger</literal>, for adding part-of-speech tags,
      including regular-expression taggers, n-gram taggers and
      Brill taggers.
    </para>

    <para>
    The second kind of module is for creating and manipulating
    structured linguistic information.  These components include:
    <literal>tree</literal>, for representing and processing
      parse trees;
    <literal>featurestructure</literal>, for building and
      unifying nested feature structures (or
      attribute-value matrices);
    <literal>cfg</literal>, for specifying free grammars;
    and
    <literal>parser</literal>, for creating parse trees over
      input text, including chart parsers, chunk parsers and
      probabilistic parsers.
    </para>

    <para>
    Several utility components are provided to facilitate processing
    and visualization.  These include:
    <literal>draw</literal>, to visualize NLP structures and
      processes;
    <literal>probability</literal>, to count and collate events,
      and perform statistical estimation;
    and
    <literal>corpus</literal>, to access tagged linguistic corpora.
    </para>

    <para>
      Finally, several advanced components are provided, mostly
      demonstrating NLP applications of machine learning techniques.
      These include: <literal>clusterer</literal>, for discovering
      groups of similar items within a large collection, including
      k-means and expectation maximisation;
      <literal>classifier</literal>, for categorising text into
      different types, including naive Bayes and maximum entropy; and
      <literal>hmm</literal>, for Hidden Markov Models, useful for a
      variety of sequence classification tasks.
    </para>

    <para>
      A further group of components is not part of NLTK proper.  These
      are a wide selection of third-party contributions, often
      developed as student projects at various institutions where NLTK
      is used, and distributed in a separate package called
      <emphasis>NLTK Contrib</emphasis>.  Several of these student
      contributions, such as the Brill tagger and the HMM module, have
      now been incorporated into NLTK.  Although these components are
      not maintained, they may serve as a useful starting point for
      future student projects.
    </para>

    <para>
      In addition to software and documentation, NLTK provides
      substantial <glossterm>corpus</glossterm> samples, listed in
      <xref linkend="nltk-corpora"/>.  Many of these can be accessed
      using the <literal>corpus</literal> module, avoiding the need to
      write specialized file parsing code before you can do NLP tasks.
    </para>


<figure id="nltk-corpora">
  <title>Corpora and Corpus Samples Distributed with NLTK</title>
  <informaltable frame="all">
    <tgroup cols="4">
      <tbody>
        <row>
          <entry>Corpus</entry>
          <entry>Compiler</entry>
          <entry>Contents</entry>
          <entry>Example Application</entry>
        </row>
        <row>
          <entry>20 Newsgroups (selection)</entry>
          <entry>Ken Lang and Jason Rennie</entry>
          <entry>3 newsgroups, 4000 posts, 780k words</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Brown Corpus</entry>
          <entry>Nelson Francis and Henry Kucera</entry>
          <entry>15 genres, 1.15M words, tagged</entry>
          <entry>training and testing taggers, text classification, language modelling</entry>
        </row>
        <row>
          <entry>CoNLL 2000 Chunking Data</entry>
          <entry>Erik Tjong Kim Sang</entry>
          <entry>270k words, tagged and chunked</entry>
          <entry>training and testing chunk parsers</entry>
        </row>
        <row>
          <entry>Genesis Corpus</entry>
          <entry>Misc web sources</entry>
          <entry>6 texts, 200k words, 6 languages</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Project Gutenberg (selection)</entry>
          <entry>Michael Hart, Gregory Newby, et al</entry>
          <entry>14 texts, 1.7M words</entry>
          <entry>text classification, language modelling</entry>
        </row>
        <row>
          <entry>NIST 1999 Information Extraction (selection)</entry>
          <entry>John Garofolo</entry>
          <entry>63k words, newswire and named-entity SGML markup</entry>
          <entry>training and testing named-entity recognizers</entry>
        </row>
        <row>
          <entry>Levin Verb Index</entry>
          <entry>Beth Levin</entry>
          <entry>3k verbs with Levin classes</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Lexicon Corpus</entry>
          <entry></entry>
          <entry>Words, tags and frequencies from Brown Corpus and WSJ</entry>
          <entry>general purpose</entry>
        </row>
        <row>
          <entry>Names Corpus</entry>
          <entry>Mark Kantrowitz and Bill Ross</entry>
          <entry>8k male and female names</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>PP Attachment Corpus</entry>
          <entry>Adwait Ratnaparkhi</entry>
          <entry>28k prepositional phrases, tagged as noun or verb modifiers</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Roget's Thesaurus</entry>
          <entry>Project Gutenberg</entry>
          <entry>200k words, formatted text</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SEMCOR</entry>
          <entry>Vasile Rus and Rada Mihalcea</entry>
          <entry>880k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SENSEVAL 2 Corpus</entry>
          <entry>Ted Pedersen</entry>
          <entry>600k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>Stopwords Corpus</entry>
          <entry>Martin Porter et al</entry>
          <entry>2,400 stopwords for 11 languages</entry>
          <entry>text retrieval</entry>
        </row>
        <row>
          <entry>Penn Treebank (selection)</entry>
          <entry>LDC</entry>
          <entry>40k words, tagged and parsed</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Wordnet 1.7</entry>
          <entry>George Miller and Christiane Fellbaum</entry>
          <entry>180k words in a semantic network</entry>
          <entry>word-sense disambiguation, natural language understanding</entry>
        </row>
        <row>
          <entry>Wordlist Corpus</entry>
          <entry>OpenOffice.org et al</entry>
          <entry>960k words and 20k affixes for 8 languages</entry>
          <entry></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>
 
</section> <!-- The Design of NLTK -->

<section><title> Further Reading </title>

  <para>
    The Association for Computational Linguistics (ACL) is the peak
    professional body in NLP.  Its journal and conference proceedings,
    approximately 10,000 articles, are available online with a
    full-text search interface, via
    <literal>http://www.aclweb.org/anthology/</literal>.
  </para>

  <para>
    Several NLP systems have online interfaces that you might
    like to experiment with, e.g.:
  </para>

    <itemizedlist>
      <listitem><para>WordNet: <literal>http://wordnet.princeton.edu/</literal>
      </para></listitem>
      <listitem><para>Translation: <literal>http://world.altavista.com/</literal>
      </para></listitem>
      <listitem><para>ChatterBots: <literal>http://www.loebner.net/Prizef/loebner-prize.html</literal>
      </para></listitem>
      <listitem><para>Question Answering: <literal>http://www.answerbus.com/</literal>
      </para></listitem>
      <listitem><para>Summarisation: <literal>http://tangra.si.umich.edu/clair/md/demo.cgi</literal>
      </para></listitem>
    </itemizedlist>

  <para>
    Useful websites with substantial information about NLP:
    <literal>http://www.hltcentral.org/</literal>,
    <literal>http://www.lt-world.org/</literal>,
    <literal>http://www.aclweb.org/</literal>,
    <literal>http://www.elsnet.org/</literal>.
    The ACL website contains an overview of computational linguistics,
    including copies of introductory chapters from recent textbooks, at
    <literal>http://www.aclweb.org/archive/what.html</literal>.
  </para>

<!--
  <para>
    Key papers that cover historical developments, and
  </para>
-->

  <para>
    Acknowledgements: The dialogue example is taken from Bob Carpenter
    and Jennifer Chu-Carroll's ACL-99 Tutorial on Spoken Dialogue
    Systems; The terms high-church and low-church were used by Mark
    Liberman in a seminar at the University of Pennsylvania; the
    following people kindly provided program samples: Tim Baldwin,
    Trevor Cohn, Rod Farmer, Edward Ivanovic, Olivia March, and Lars
    Yencken.
  </para>

  <section><title> NLP Textbooks and Surveys </title>

  <para>
    This section will give a brief overview of other NLP textbooks,
    and field-wide surveys <emphasis>(to be written).</emphasis>
  </para>

  <para>
    Recent textbooks:
      Manning &amp; Schutze,
      Jurafsky &amp; Martin.
    Older textbooks:
      Allen (1995), Charniak (1993), Grishman.
    Prolog-based:
      <citation>Covington94</citation>,
      <citation>GazdarMellish89</citation>,
      Pereira &amp; Shieber.
    Mathematical foundations:
      Partee et al.
  </para>

  <para>
    Recent field-wide surveys: Mitkov, Dale et al
  </para>

<bibliography>
<biblioentry>
  <abbrev>Covington94</abbrev>
  <author><surname>Covington</surname><firstname>Michael A.</firstname></author>
  <pubdate>1994</pubdate>
  <title>Natural Language Processing for Prolog Programmers</title>
  <publishername>Prentice-Hall</publishername>
</biblioentry>

<biblioentry>
  <abbrev>GazdarMellish89</abbrev>
  <author><surname>Gazdar</surname><firstname>Gerald</firstname></author>
  <author><surname>Mellish</surname><firstname>Chris</firstname></author>
  <pubdate>1989</pubdate>
  <title>Natural Language Processing in Prolog: An Introduction to Computational Linguistics</title>
  <publishername>Addison-Wesley</publishername>
</biblioentry>

<biblioentry>
  <abbrev>Dale00</abbrev>
  <editor><surname>Dale</surname><firstname>Robert</firstname></editor>
  <editor><surname>Moisl</surname><firstname>Hermann</firstname></editor>
  <editor><surname>Somers</surname><firstname>Harold</firstname></editor>
  <pubdate>2000</pubdate>
  <title>Handbook of Natural Language Processing</title>
  <publishername>Marcel Dekker</publishername>
</biblioentry>

<biblioentry>
  <abbrev>Mitkov03</abbrev>
  <editor><surname>Mitkov</surname><firstname>Ruslan</firstname></editor>
  <pubdate>2003</pubdate>
  <title>The Oxford Handbook of Computational Linguistics</title>
  <publishername>Oxford University Press</publishername>
</biblioentry>

<!-- TEMPLATE
<biblioentry>
  <abbrev></abbrev>
  <author><surname></surname><firstname></firstname></author>
  <author><surname></surname><firstname></firstname></author>
  <pubdate></pubdate>
  <title></title>
  <publishername></publishername>
</biblioentry>
-->

</bibliography>


  </section>

</section>

<section><title>Appendix: NLP in other Programming Languages</title>

<para>
Earlier we explained the thinking that lay behind our choice of the
Python programming language.  We showed a simple Python program that
reads in text and prints the words that end with <literal>ing</literal>.
In this appendix we provide equivalent programs in other languages, so
that readers can gain a sense of the appeal of Python.
</para>

<para>
Prolog is a logic programming language which has been popular for
developing natural language parsers and feature-based grammars, given
the inbuilt support for search and the
<emphasis>unification</emphasis> operation which combines two feature
structures into one.  Unfortunately Prolog is not easy to use for
string processing or input/output, as the following program code
demonstrates:
</para>

<programlisting><![CDATA[
main :-
    current_input(InputStream),
    read_stream_to_codes(InputStream, Codes),
    codesToWords(Codes, Words),
    maplist(string_to_list, Words, Strings),
    filter(endsWithIng, Strings, MatchingStrings),
    writeMany(MatchingStrings),
    halt.

codesToWords([], []).
codesToWords([Head | Tail], Words) :-
    ( char_type(Head, space) ->
        codesToWords(Tail, Words)
    ;
        getWord([Head | Tail], Word, Rest),
        codesToWords(Rest, Words0),
        Words = [Word | Words0]
    ).

getWord([], [], []).
getWord([Head | Tail], Word, Rest) :-
    (
        ( char_type(Head, space) ; char_type(Head, punct) )
    ->  Word = [], Tail = Rest
    ;   getWord(Tail, Word0, Rest), Word = [Head | Word0]
    ).

filter(Predicate, List0, List) :-
    ( List0 = [] -> List = []
    ;   List0 = [Head | Tail],
        ( apply(Predicate, [Head]) ->
            filter(Predicate, Tail, List1),
            List = [Head | List1]
        ;   filter(Predicate, Tail, List)
        )
    ).

endsWithIng(String) :- sub_string(String, _Start, _Len, 0, 'ing').

writeMany([]).
writeMany([Head | Tail]) :- write(Head), nl, writeMany(Tail).

]]></programlisting>

<para>
Java is an object-oriented language incorporating native support for
the internet, that was originally designed to permit the same
executable program to be run on most computer platforms.  Java is
quickly replacing COBOL as the standard language for business
enterprise software.
</para>

<programlisting><![CDATA[
import java.io.*;
public class IngWords {
    public static void main(String[] args) {
        BufferedReader in = new BufferedReader(new
	    InputStreamReader(
                 System.in));
        String line = in.readLine();
        while (line != null) {
            for (String word : line.split(" ")) {
                if (word.endsWith("ing"))
                    System.out.println(word);
            }
            line = in.readLine();
        }
    }
}
]]></programlisting>


<para>
The C programming language is a highly-efficient low-level
language that is popular for operating system software and for
teaching the fundamentals of computer science.
</para>


<programlisting><![CDATA[
#include <sys/types.h>
#include <regex.h>
#include <stdio.h>
#define BUFFER_SIZE 1024

int main(int argc, char **argv) {
     regex_t space_pat, ing_pat;
     char buffer[BUFFER_SIZE];
     regcomp(&space_pat, "[, \t\n]+", REG_EXTENDED);
     regcomp(&ing_pat, "ing$", REG_EXTENDED | REG_ICASE);

     while (fgets(buffer, BUFFER_SIZE, stdin) != NULL) {
         char *start = buffer;
         regmatch_t space_match;
         while (regexec(&space_pat, start, 1, &space_match, 0) == 0) {
             if (space_match.rm_so > 0) {
                 regmatch_t ing_match;
                 start[space_match.rm_so] = '\0';
                 if (regexec(&ing_pat, start, 1, &ing_match, 0) == 0)
                     printf("%s\n", start);
             }
             start += space_match.rm_eo;
         }
     }
     regfree(&space_pat);
     regfree(&ing_pat);

     return 0;
}
]]></programlisting>

<para>
LISP is a so-called functional programming language, in which all objects
are lists, and all operations are performed by (nested) functions
of the form <literal>(function arg1 arg2 ...)</literal>.
Many of the earliest NLP systems were implemented in LISP.
</para>

<programlisting><![CDATA[
(defpackage "REGEXP-TEST" (:use "LISP" "REGEXP"))
(in-package "REGEXP-TEST")

(defun has-suffix (string suffix)
  "Open a file and look for words ending in _ing."
  (with-open-file (f string)
     (with-loop-split (s f " ")
        (mapcar #'(lambda (x) (has_suffix suffix x)) s))))

(defun has_suffix (suffix string)
  (let* ((suffix_len (length suffix))
   (string_len (length string))
    (base_len (- string_len suffix_len)))
    (if (string-equal suffix string :start1 0 :end1 NIL :start2 base_len :end2 NIL)
        (print string))))

(has-suffix "test.txt" "ing")

]]></programlisting>

<para>
Haskell is another functional programming language which permits
a much more compact solution of our simple task.
</para>


<programlisting><![CDATA[
module Main
  where main = interact (unlines.(filter ing).(map (filter isAlpha)).words)
    where ing = (=="gni").(take 3).reverse
]]></programlisting>



</section> <!-- Appendix -->


&index;
</article>

<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:2
sgml-indent-data:t
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
End:
-->
