<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Introduction to Natural Language Processing</title>
    &versiondate; &copyright;
  </articleinfo>

  <section id="goals">
    <title> Goals </title>

    <para>
      Language is the chief manifestation of human intelligence.
      Through language we express basic needs and lofty aspirations,
      technical know-how and flights of fantasy.  Ideas are
      shared over great separations of distance and time.
      The following samples from English illustrate the richness
      of language:
    </para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para>
      Overhead the day drives level and grey, hiding the sun by a
      flight of grey spears.
      (William Faulkner, <emphasis>As I Lay Dying</emphasis>, 1935)
    </para></listitem>
    <listitem><para>
      When using the toaster please ensure that the exhaust fan is
      turned on. (sign in dormitory kitchen)
    </para></listitem>
    <listitem><para>
      Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated
      activities with Ki values of 45.1-271.6 &mu;M (Medline)
    </para></listitem>
    <listitem><para>
      Iraqi Head Seeks Arms (spoof headline,
        <literal>http://www.snopes.com/humor/nonsense/head97.htm</literal>)
    </para></listitem>
    <listitem><para>
      The earnest prayer of a righteous man has great power and wonderful
      results. (James 5:16b)
    </para></listitem>
    <listitem><para>
      Twas brillig, and the slithy toves did gyre and gimble in the wabe
      (Lewis Carroll, <emphasis>Jabberwocky</emphasis>, 1872)
    </para></listitem>
    <listitem><para>
       There are two ways to do this, AFAIK :smile:  (internet
         discussion archive)
    </para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

    <para>
      Thanks to this richness, the study of language is part of many
      disciplines outside of linguistics, including translation,
      literary criticism, philosophy, anthropology and psychology.
      Many less obvious disciplines investigate language use, such as
      law, hermeneutics, forensics, telephony, pedagogy, archaeology,
      cryptanalysis and speech pathology.  Each applies distinct
      methodologies to gather observations, develop theories and test
      hypotheses.  Yet all serve to deepen our understanding of
      language and of the intellect which is manifested in language.
    </para>

    <para>
      The importance of language to science and the arts is matched in
      significance by the cultural treasure that is inherent in
      language.  Each of the worlds ~7,000 human languages
      is rich in unique respects, in its oral
      histories and creation legends, down to its grammatical
      constructions and its very words and their nuances of meaning.
      Threatened remnant
      cultures have words to distinguish plant subspecies according to
      therapeutic uses which are unknown to science.  Languages evolve
      over time as they come into contact with each other and they
      provide a unique window onto human pre-history.  
      Technological change gives rise to new words like
      <emphasis>weblog</emphasis>
      and new morphemes like <emphasis>e-</emphasis> and <emphasis>cyber-</emphasis>.
      In many parts of the world, small linguistic variations from one
      town to the next add up to a completely different language in
      the space of a half-hour drive.  For its breathtaking complexity
      and diversity, human language is as a colourful tapestry
      stretching through time and space.
    </para>

    <para>
      Each new wave of computing
      technology has confronted new challenges for language analysis.
      Early machine languages gave way to high-level programming
      languages which are automatically parsed and interpreted.
      Databases are interrogated using linguistic expressions
      like <literal>SELECT age FROM employee</literal>.
      Recently, computing devices have become ubiquitous
      and are often equipped with multimodal interfaces
      supporting text, speech, dialogue and pen gestures.
      Building new systems for natural linguistic interaction requires
      sophisticated language analysis.
    </para>

    <para>
      Today the greatest challenge for language analysis is presented by the
      explosion of text and multimedia content on the world-wide web.
      For many people, a large and growing fraction of work and
      leisure time is spent navigating and accessing this universe of
      information.  What tourist sites can I visit between
      Philadelphia and Pittsburgh on a limited budget?  What do expert
      critics say about Canon digital cameras?  What predictions about
      the steel market were made by credible commentators in the past
      week?  Answering such questions requires a combination of
      language processing tasks including information extraction,
      inference, and summarisation.  This scale of language
      analysis increasingly depends on sophisticated automatic
      language processing.
    </para>

    <para>
      As we have seen, <glossterm>natural language
      processing</glossterm>, or NLP, is important for scientific,
      economic, social, and cultural reasons.  NLP is experiencing
      rapid growth as its theories and methods are deployed in a
      variety of new language technologies.  Thus it is important for
      a wide range of people to have a working knowledge of NLP.
      Within academia, this includes people working in areas from
      humanities computing and corpus linguistics to
      artificial intelligence and information extraction.  Within
      industry, this includes people working in human-computer
      interaction, business information analysis, and web software
      development.  We hope this book will permit our diverse audience
      to appreciate the workings of this rapidly growing field and to
      apply its techniques to real-world problems in language
      analysis.  The book presents a carefully balanced selection of
      theoretical foundations and practical application, and equips
      readers to work with large datasets, to create robust models of
      linguistic phenomena, and to deploy them in working language
      technologies.  By integrating all of this with the Natural
      Language Toolkit, we hope this book opens up the exciting
      endeavour of practical natural language processing to a broader
      audience than ever before.
    </para>

  </section>

  <section>
    <title>A Brief History of Natural Language Processing</title>

    <para>A long-standing challenge within computer science has been
    to build intelligent machines.  The chief measure of machine
    intelligence has been a linguistic one, namely the
    <glossterm>Turing Test</glossterm>.  Can a dialogue system,
    responding to a user's typed input with its own textual output,
    perform so naturally that users cannot distinguish it from
    a human interlocutor using the same interface?
    Today, there is substantial
    ongoing research and development in such areas as machine
    translation and spoken dialogue, and significant commercial
    systems are in widespread use [XREF].  The following dialogue
    illustrates a typical application:
    </para>

<programlisting>
S: How may I help you?

U: When is Saving Private Ryan playing?

S: For what theater?

U: The Paramount theater.

S: Saving Private Ryan is not playing at the Paramount theater, but
   it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30. 
</programlisting>

    <para>
   Today's commercial dialogue systems are strictly limited to
   narrowly-defined domains. We could not ask the above system to
   provide driving instructions or details of nearby restaurants unless
   the requisite information had already been stored and suitable
   question and answer sentences had been incorporated into the language
   processing system.  Observe that the dialogue system appears
   understand the user's goals: the user asks when a movie is showing
   and the system correctly determines from this that the user wants to
   see the movie. This step seems so obvious to humans that we do not
   notice it has been made, yet a natural language system needs to be
   endowed with this capability in order to interact naturally. Without
   it, when asked <literal>Do you know when Saving Private Ryan is
    playing</literal>, a system might simply &mdash; and unhelpfully
   &mdash; respond with a cold <literal>Yes</literal>. While it appears
   that this dialogue system can perform simple inferences, such
   sophistication is only found in cutting edge research prototypes.
   Instead, the developers of commercial systems use contextual
   assumptions and simple business logic to ensure that the different
   ways in which a user might express requests or provide information
   are handled in a way that makes sense for the particular application.
   Thus, whether the user says <literal>When is ... </literal>, or
   <literal>I want to know when ... </literal>, or <literal>Can you tell
    me when ... </literal>, simple rules will always result in users
   being presented with movie showing times, and this is sufficient for
   the system to provide a useful service.
    </para>

    <para>
   Although recent advances have been made, it is still largely true
   that deployed natural language systems cannot perform common-sense
   reasoning or draw on world knowledge.  We can wait for these general
   problems of artificial intelligence to be solved, but in the meantime
   it is necessary to live with some severe limitations on the reasoning
   and knowledge capabilities of natural language systems. Since the
   very beginning, an important goal of NLP research has been to
   make progress on the holy grail of natural linguistic interaction
   <emphasis>without</emphasis> recourse to this unrestricted knowledge
   and reasoning capability.  This is an old challenge, and so it is
   instructive to review the history of the field.
    </para>

    <para>
   The very notion that natural language could be treated in a
   computational manner grew out of a research programme, dating back to
   the early 1900s, to reconstruct mathematical reasoning using logic
   (Frege, Russell, Wittgenstein, Tarski, Lambek, Carnap).  This led to
   the notion of language as a formal system amenable to automatic
   processing.  Three further developments laid the foundation for
   natural language processing.  The first was <emphasis>formal language
    theory</emphasis>.  This defined a language as a set of sentences
   (or strings) accepted by a class of automata (e.g. regular languages
   and finite state machines, context-free languages and pushdown
   automata, Chomsky hierarchy, etc). This provided the underpinnings
   for computational syntax.  [MORE].  The second was <emphasis>symbolic
    logic</emphasis>. This provided a formal method of capturing
   selected aspects of natural language that were held to be relevant
   for modeling the notion of proof. A formal calculus in symbolic logic
   provides the syntax of a language, together with rules of inference
   and, possibly, rules of interpretation in a set-theoretic model;
   examples are propositional logic and first-order logic. Given such a
   calculus, with a well-defined syntax and semantics, it becomes
   possible to associate meanings with expressions of natural language
   by translating them into expressions of the formal calculus. For
   example, if we translate <emphasis>John saw Mary</emphasis> into a
   formula <literal>saw(j,m)</literal>, we (implicitly or explicitly)
   intepret the English verb <emphasis>saw</emphasis> as a binary
   relation, and <emphasis>John</emphasis> and <emphasis>Mary</emphasis>
   as denoting individuals.  More general statements like <emphasis>All birds
    fly</emphasis> require quantifiers, in this case &forall; meaning
   <emphasis>for all</emphasis>: <literal>&forall;x: bird(x) &rarr;
    fly(x)</literal>.  This use of logic provided the technical
   machinery to perform inferences that are an important part of
   language understanding. The third was the <emphasis>principle of
    compositionality</emphasis>. This was the notion that the meaning of
   a complex expression is comprised of the meaning of its parts and
   their mode of combination. This provided a useful correspondence
   between syntax and semantics, that the meaning of a complex
   expression could be computed recursively.  Given the representation
   of <emphasis>It is not true that &blank;<subscript>p</subscript></emphasis>
   as <literal>not(p)</literal> and <emphasis>John saw
    Mary</emphasis> as <literal>saw(j,m)</literal>, we can compute the
   interpretation of <emphasis>It is not true that John saw Mary</emphasis>
   recursively using the above information to get
   <literal>not(saw(j,m))</literal>. Today, this approach is most
   clearly manifested in a family of grammar formalisms known as
   unification-based grammar, and NLP applications implemented in the
   Prolog programming language.
    </para>

    <para>
    A separate strand of development in the 1960s and 1970s eschewed
    the declarative/procedural distinction and the principle of
    compositionality.  They seemed to get in the way of building
    practical systems.  For instance, early question answering systems
    employed <emphasis>semantic grammars</emphasis>, consisting of
    fixed templates such as:
    <literal>How many &blank;<subscript>i</subscript>
    does &blank;<subscript>j</subscript> have?</literal>
    Each template comes with a predefined semantic functions,
    such as <literal>count(i,j)</literal>.  A user's question which
    matched the template would be mapped to a semantic function,
    then ``executed'' to obtain an answer <literal>k = count(i,j)</literal>.
    This can then be substituted into a new template:
    <literal>&blank;<subscript>j</subscript> has
    &blank;<subscript>k</subscript> &blank;<subscript>i</subscript></literal>.
    Thus, the question
    <emphasis>How many airports<subscript>i</subscript>
    does London<subscript>j</subscript> have?</emphasis>
    can be trivially mapped onto a template (as shown by the
    subscripts) and translated to an executable program.  The result can be
    returned in English as:
    <emphasis>London<subscript>j</subscript> has five <subscript>k</subscript>
    airports<subscript>i</subscript></emphasis>.
    </para>
    
<!--  
    <para>
    (call this the low-church AI approach, cf the high-church AI
    approach of formal language theory and symbolic logic;
    neither approach worked - both left out the same thing)
    </para>
-->

</section>

<!--
<section>
  <title>Linguistics and Natural Language Processing (draft)</title>

  <para>
    [What is the relationship between linguistics and NLP?
    Goal of (generative) linguistics to account for the grammaticality
    judgements of the ideal monolingual speaker/hearer, vs goal of
    NLP to build systems to map between the (linguistic) systems of
    humans and machines.  Challenge of linguistics is to balance
    descriptive and explanatory adequacy; challenge of NLP to balance
    expressiveness and tractability.]
  </para>

    <para>
    [Grammar as a definition of well-formed
    sentences along with a semantic translation, versus
    an implementation which (say) maps from sentences to
    meanings (parser) or vice versa (generator).
    declarative vs procedural;
    system of rewriting rules vs automaton;
    perspective on NLP: relating the declarative to the procedural;
    distinguish this constrast from competence vs performance.]
    </para>

    <para>
    In the late 1980s and early 1990s there was a promising
    convergence between the fields of linguistics and NLP.  (This had
    been a feature of the 1960s, e.g. with the application of the SPE
    model in speech synthesis systems.)  Computational linguists often
    looked to linguistics as a source of knowledge about language.
    Over the last decade we have seen a new divergence, as
    computational linguists have discovered that linguistic analyses
    often failed to account for the linguistic patterns attested in
    the large corpora used to develop their systems.  However, once
    linguists learn to work with these large datasets, their own
    analytical work will benefit, leading to broader coverage of their
    theories, and earlier refutation of false hypotheses.  The result,
    we expect, will be new opportunities for cross-fertilization
    between linguistics and NLP.
    </para>

    <para>
    [Opportunities for linguists to contribute their insights to the
    future development of NLP and, in the reverse direction, to apply
    the results of NLP research back in linguistics.]
    </para>

    <para>
    [rationalism vs empiricism; realism vs idealism; other balancing acts]
    </para>

<para>
rationalism vs empiricism:
significant distinction coming out of the history of the natural
sciences:
to what extent does our experience of the world provide the basis for
our knowledge?

rationalism - all truth has its origins in human thought
  (doesn't require input from supernatural beings or the
  experience of our senses)
Descartes, Leibniz
``innate ideas'' implanted in our minds from birth.
E.g. principles of Euclidean geometry arose through the
process of reason, and not divine revelation or sensory experience.
Enlightenment - priority of human reason over revelation/experience.

empiricism
John Locke - primary source of knowledge is the experience of our faculties
- reason is secondary - reflecting on that experience

linguistic implications:
Chomsky, poverty of the stimulus, innate language faculty
to account for universal grammar

vs humans have general learning methods (analogical, memory based),
and use these to identify meaningful patterns in language
and ground them in sensory experience.

</para>

<para>
realism vs idealism;
Kant - phenomena (appearances and representations that we can
experience)
vs the ``things in themselves'' which can never been known directly

what is the metaphysical status of the constructs of our NLP models?
Are things like ``noun phrase'' real world entities, or just
theory-internal constructs, products of thought?
useful fictions
unobservables (e.g. null elements, underlying forms)

realism - theories can establish constructs and these are
existent entities, which actually cause the observed phenomena.

- entities exist in the real world independently of human perception
and human reason
</para>

<para>
Balancing acts:
statistical vs symbolic
(artificial: should be statistical vs ? (gradient vs binary)
symbolic vs non-symbolic
deep vs shallow processing;
science vs engineering
</para>


  </section>
-->


  <section>
    <title>The Architecture of linguistic and NLP systems</title>
    <para>
      Within the approach to linguistic theory known as <glossterm>
	generative grammar</glossterm>, it is claimed that humans have
      distinct kinds of linguistic knowledge, organised into different
      modules: for example, knowledge of phonology and morphology,
      knowledge of syntax, knowledge of semantics. In a formal theory of
      linguistic competence, each of these different kinds of implicit
      knowledge is made explicit as different module of the theory.
      By `module' here we mean at least a set of primitive elements,
      together with a way of combining the elements into derived
      elements. For example, a phonological module might provide a
      set of phonemes, together with an operation for concatenating
      phonemes into phonological strings. Similarly, a syntactic
      module might provide labeled nodes as primitives, together wih
      a mechanism for assembling nodes into arbitrarily complex trees. A
      set of linguistic primitives, together with some operators for
      defining complex elements, is often called <glossterm>a level of
      representation</glossterm>. 
    </para>

    <para>
      As well as defining modules, a generative grammar will prescribe
      how the modules interact. For example, well-formed phonological
      strings will provide the phonological content of words, and words
      will provide the terminal elements of syntax trees. Similarly, a
      linguistic theory needs to specify some manner of mapping
      well-formed syntactic trees into semantic representations, and
      equally an account of how contextual or pragmatic information can
      resolve indeterminacies in these semantic representations.
      </para>

    <para>As we indicated above, an important aspect of theories of
   generative grammar is that they are intended to model the linguistic
   knowledge of speakers and hearers; they are not intended to explain
   how humans actually process linguistic information. This is, in part,
   reflected in the claim that a generative grammer encodes the
   <emphasis>competence</emphasis> of an idealized native speaker,
   rather than the speaker's <emphasis>competence</emphasis>. A closely
   related distinction is to say that a generative grammar encodes
   <glossterm>declarative</glossterm> rather than
   <glossterm>procedural</glossterm> knowledge.  As you might expect,
   computational linguistics has the crucial role of proposing
   procedural models of language. A central example is
   <glossterm>parsing</glossterm>, where we have to develop
   computational mechanisms which convert, say, strings of words into
   structural representations such as syntax trees. Nevertheless, it is
   widely accepted that well-engineered computational models of language
   contain both declarative and procedural aspects. Thus, a full account
   of parsing will say how declarative knowledge in the form of a
   grammar and lexicon combines with procedural knowledge which
   determines how a syntactic analysis should be assigned to a given
   string of words. This procedural knowledge will be expressed as an
   <glossterm>algorithm</glossterm>: that is, an explicit recipe for
   mapping some input into an appropriate output in a finite number of
   steps. A simple parsing algorithm for context free gramars, for
   instance, looks first for a rule of the form <literal>S &rarr;
    X<subscript>1</subscript> &mldr;
    X<subscript>n</subscript></literal>, and builds a partial tree
   structure. It then steps through the grammar rules one-by-one,
   looking for a rule of the form <literal>X<subscript>1</subscript>
    &rarr; Y<subscript>1</subscript> ...
    Y<subscript>j</subscript></literal> which will expand the leftmost
   daughter introduced by the <literal>S</literal> rule, and further
   extends the partial tree. This process continues, for example by
   looking  for a rule of the form <literal>Y<subscript>1</subscript>
    &rarr; Z<subscript>1</subscript> ...
    Z<subscript>k</subscript></literal> and expanding the partial tree
   appropriately,  until the leftmost node label in the partial tree is
   a lexical category; the parser then checks to see if the first word
   of the input can belong to the category. If the first rule is, say,
   <literal>S &rarr; NP VP</literal> and the second rule is <literal>NP
    &rarr; Det N</literal>, then the partial tree will be as shown in
   <xref
    linkend="partialtree"/>. </para>
  <figure>
   <title>Partial Parse Tree</title>
   <graphic/>
  </figure>

  <para>
Assuming an input string <emphasis>the cat
    slept</emphasis>, then we will succeed in identifying
   <emphasis>the</emphasis> as a word which can belong to the category
   <literal>Det</literal>. In this case, the parser goes on to the next
   node of the tree (e.g., <literal>N</literal>) and input word (e.g.,
   <emphasis>cat</emphasis>, otherwise it throws away the structure
   built so far, and looks for an alternative way of going from the
   <literal>S</literal> node down to a leftmost lexical category. The
   important point for now is not the details of this algorithm; we
   discuss this much more fully in (parsing-chap xref). Rather, we just
   want to illustrate the idea that an algorithm can be broken down into
   a fixed number of steps which produce a definite result at the end. 
    </para>

    <para>In <xref linkend="dialogue"/> we further illustrate some of
    these points in the context of a spoken dialogue system, such as our
    earlier example of an application that offers the user information
    about movies currently on show.  
    </para>
    <figure id="dialogue">
      <title>Architecture of Spoken Dialogue System</title>
      <informaltable frame="topbot">
	<tgroup cols="1">
	  <tbody>
	    <row>
	      <entry>
		<graphic fileref="images/dialogue" scale="20"/>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>
    </figure>

    <para> Down the lefthand side of the diagram, we have shown a
      pipeline of some representative speech understanding component</emphasis>s.
      These map from speech input via syntactic parsing to some kind of
      meaning representation. Up the righthand side is an inverse
      pipeline of components for concept-to-speech generation. These
      components constitute the procedural aspect of the system's
      natural language processing. In the central column of the diagram
      are some representative declaratives aspects: the repositories of
      language-related information which are called upon by the
      processing components. 
    </para>
    <para>In addition to embodying the declarative/procedural
      distinction, the diagram also illustrates that linguistically
      motivated ways of modularizing linguistic knowledge are often
      reflected in computational systems. That is, the various
      components are organized so that the data which they  exchang
      corresponds roughly to different levels of representation. For
      example, the output of the speech analysis component will contain
      sequences of phonological representations of words, and the output
      of the parser will be a logical form that represents the
      compositional semantics of the output. Of course the parallel is
      not precise, in part because it is often a matter of practical
      expedience where to place the boundaries between different
      processing components. For example, we can assume that within the
      parsing component of <xref
      linkend="dialogue"/>, there is a level of syntactic
      representation; however, we have chosen not to expose this at the
      level of the system diagram. Perhaps the most important points to
      note are that first, computational systems typically assume that
      the processing of natural language inputs can be usefully broken
      down into a series of discrete steps; and second, that in the
      process of natural language understanding, these steps go from
      more concrete levels to more abstract ones, while in natural
      language production, the direction is reversed.</para>

  </section>
    

  <section>
    <title>Language Technologies</title>

    <para>
    a selection of: machine translation, dialog systems, document summarization,
    information extraction, text retrieval, question answering
    </para>

  </section>

  <section>
    <title>NLTK: The Natural Language Toolkit</title>

    <para>
    The Natural Language Toolkit (NLTK) was developed in conjunction
    with a computational linguistics course at the University of
    Pennsylvania in 2001.  Since then it has been used as the basis
    of research projects and been adopted in courses in dozens of
    Universities.  NLTK can be downloaded for free from
    <literal>nltk.sourceforge.net</literal>.
    </para>

    <section><title>Teaching with NLTK</title>

    <para>
    Integrating theory and practice is a difficult in the framework of
    a single semester, introductory course on NLP.  Common approaches
    are to focus on theory to the exclusion of practical exercises, or
    to focus exclusively on teaching linguists how to write programs.
    However, we believe that neither approach is suitable for a
    one-semester course on NLP.  The former provides the students with
    no practical experience, while the latter usually gets bogged down
    in the mechanics of programming and doesn't succeed in teaching
    any significant NLP.  The teacher of a one-semester course faces
    an acute dilemma: forget the practical component and just teach
    the theory or teach programming and try to fit in some NLP at the
    end once they can manage all the "housekeeping" functions (file
    I/O, representing and displaying grammars and trees, etc).  It is
    clear that these considerable overheads and shortcomings warrant
    the fresh approach to NLP pedagogy offered here.
    </para>

    <para>
    Apart from the practical component, NLP courses may also depend on
    software for in-class demonstrations.  This context calls for
    interactive graphical user interfaces, making it possible to view
    program state (e.g. the chart of a chart parser), observe program
    execution step-by-step (e.g. execution of a finite-state machine),
    and even make minor modifications to programs in response to
    ``what if'' questions from the class.  Because of these
    difficulties it is common to avoid live demonstrations, and keep
    classes for theoretical presentations only.  Apart from being
    dull, this approach leaves students to solve important practical
    problems on their own, or to deal with them less efficiently in
    office hours.
    </para>
 
    <para>
    NLTK supports assignments of varying difficulty and scope.  In the
    simplest assignments, students experiment with existing components
    to perform a wide variety of NLP tasks.  As students become more
    familiar with the toolkit, they can be asked to modify existing
    components, or to create complete systems out of existing
    components.
    NLTK also provides students with a flexible framework for advanced
    projects.  Typical projects might involve implementing a new
    algorithm, developing a new component, or implementing a new task.
    </para>

    <para>
    This book confronts these problems by tightly integrating the
    theoretical content with a practical component based on the
    Natural Language Toolkit.  The toolkit was developed in
    conjunction with a NLP course at the University of Pennsylvania, a
    leading centre for NLP teaching and research.  The practical
    component is supported by significant corpora distributed with
    NLTK, including samples from the Linguistic Data Consortium, the
    leading publisher of linguistic data.  Key features of the toolkit
    are that students augment and replace existing components, they
    learn structured programming by example, and they manipulate
    sophisticated models from the outset.
    </para>

    </section> <!-- teaching -->

    <section><title>The Choice of Python</title>

    <para>
    We chose Python because it has a shallow learning curve, its
    syntax and semantics are transparent, and it has good
    string-handling functionality.  As an interpreted language, Python
    facilitates interactive exploration.  As an object-oriented
    language, Python permits data and methods to be encapsulated and
    re-used easily.  Python comes with an extensive standard library,
    including tools for graphical programming and numerical
    processing.
    </para>

    <para>
    [Other languages used for NLP: Perl, Prolog, Java]
    </para>


<figure id="languages">
  <title>Programming Language Samples</title>
  <informaltable frame="all">
    <tgroup cols="2">
      <tbody>
        <row>
          <entry>Perl</entry>
          <entry>Java</entry>
        </row>
        <row>
          <entry>
<programlisting><![CDATA[
while (<>) {
  chomp;
  @words = split();
  for my $word (@words) {
    if ($word =~ /ing$/) {
      printf "%s\n", $word;
    }
  }
}
]]></programlisting>
          </entry>
          <entry>
<programlisting><![CDATA[
import java.util.regex.*;
public class IngWords {
  public static void main(String[] args) throws Exception {
    try {
      BufferedReader in = new BufferedReader(
        new InputStreamReader(System.in));
      String str = "";
      Pattern space_pat = Pattern.compile("[,\\s]+");
      Pattern ing_pat = Pattern.compile("ing$");
      while (str != null) {
        line = in.readLine();
        String[] words = space_pat.split(line);
        for (int i=0; i<words.length; i++) {
          Matcher m = ing_pat.matcher(words[i]);
          if (m.find()) {
              System.out.println(words[i]);
          }
        }
      }
    } catch (IOException e) {
    }
  }
}
]]></programlisting>
          </entry>
        </row>
        <row>
          <entry>Prolog</entry>
          <entry>Python</entry>
        </row>
        <row>
          <entry>
<programlisting><![CDATA[
Prolog
]]></programlisting>
          </entry>
          <entry>
<programlisting><![CDATA[
import re, sys
for line in sys.stdin.readlines():
    for word in line.split():
        if re.search('ing$', word):
            print word
]]></programlisting>
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>


    <para>
    NLTK was designed with six requirements in mind.  First, NLTK is
    <emphasis>easy to use</emphasis>.  The primary purpose of the
    toolkit is to allow students to concentrate on building natural
    language processing systems.  The more time students must spend
    learning to use the toolkit, the less useful it is.  Second, we
    have made a significant effort to ensure that all the data
    structures and interfaces are <emphasis>consistent</emphasis>,
    making it easy to carry out a variety of tasks using a uniform
    framework.  Third, the toolkit is <emphasis>extensible</emphasis>,
    easily accommodating new components, whether those components
    replicate or extend the toolkit's existing functionality.
    Moreover, the toolkit is organized so that it is usually obvious
    where extensions would fit into the toolkit's infrastructure.
    Fourth, the toolkit is designed to be <emphasis>simple</emphasis>,
    providing an intuitive and appealing framework along with
    substantial building blocks, for students to gain a practical
    knowledge of NLP without having to write mountains of code.  Fifth, the toolkit
    is <emphasis>modular</emphasis>, so that the interaction between
    different components of the toolkit is minimized, and uses simple,
    well-defined interfaces.  In particular, it should be possible to
    complete individual projects using small parts of the toolkit,
    without needing to understand how they interact with the rest of
    the toolkit.  This allows students to learn how to use the toolkit
    incrementally throughout a course.  Modularity also makes it
    easier to change and extend the toolkit.  Finally, the toolkit is
    <emphasis>well documented</emphasis>, including nomenclature, data
    structures, and implementations.
    </para>

    <para>
    Contrasting with these requirements are three non-requirements.
    First, while the toolkit provides a wide range of functions, it is
    not intended to be encyclopedic.  There should be a wide variety
    of ways in which students can extend the toolkit.  Second, while
    the toolkit should be efficient enough that students can use their
    NLP systems to perform meaningful tasks, it does not need to be
    highly optimized for runtime performance.  Such optimizations
    often involve more complex algorithms, and sometimes require the
    use of C or C++, making the toolkit harder to install.  Third, we have
    avoided the the use of clever programming tricks, since clear
    implementations are far preferable to ingenious yet indecipherable
    ones.
    </para>


    <para> NLTK defines a basic infrastructure that can be used to
    build NLP programs in Python.  It provides: </para>

    <itemizedlist>
      <listitem><para>Basic classes for representing data relevant to
      natural language processing.</para>
      </listitem>
      <listitem><para>Standard interfaces for performing tasks, such
      as tokenization, tagging, and parsing.</para>
      </listitem>
      <listitem><para>Standard implementations for each task, which
      can be combined to solve complex problems.</para>
      </listitem>
    </itemizedlist>

    <para> This tutorial introduces natural language processing in Python,
      NLTK, and the <literal>nltk.token</literal> module. </para>

  </section> <!-- Goals -->

  <section id="overview">
    <title> Overview of NLTK </title>

    <para>NLTK provides basic classes for representing data relevant to NLP.
    NLTK also provides standard interfaces for performing NLP tasks, along
    with standard implementations of each task.</para>

    <para>NLTK is organized into a collection of task-specific modules
    (Python packages).  Each contains data-oriented classes to
    represent NLP information, and task-oriented classes to
    encapsulate the resources and methods needed to perform a
    particular task.</para>

    <para>NLTK has the following modules:

<itemizedlist>
<listitem><para>token: classes for representing and processing
  individual elements of text, such as words and
  sentences
</para></listitem>

<listitem><para>probability: classess for representing and processing
  probabilistic information.
</para></listitem>

<listitem><para>tree: classes for representing and processing hierarchical
information over text.
</para></listitem>

<listitem><para>cfg: classes for representing and processing context
free grammars.</para></listitem>

<listitem><para>fsa: finite state automata</para></listitem>

<listitem><para>tagger: tagging each word with a part-of-speech, a sense, etc
</para></listitem>

<listitem><para>parser: building trees over text (includes chart, chunk and
  probabilistic parsers)
</para></listitem>

<listitem><para>classifier: classify text into categories
  (includes feature, featureSelection, maxent, naivebayes</para></listitem>

<listitem><para>draw: visualize NLP structures and processes</para></listitem>

<listitem><para>corpus: access (tagged) corpus data</para></listitem>
</itemizedlist>
    </para>

  </section> <!-- overview -->

</section> <!-- NLTK -->

  <section id="corpora">
    <title> Corpora </title>

<figure id="nltk-corpora">
  <title>Corpora and Corpus Samples Distributed with NLTK</title>
  <informaltable frame="all">
    <tgroup cols="4">
      <tbody>
        <row>
          <entry>Corpus</entry>
          <entry>Compiler</entry>
          <entry>Contents</entry>
          <entry>Example Application</entry>
        </row>
        <row>
          <entry>20 Newsgroups (selection)</entry>
          <entry>Ken Lang and Jason Rennie</entry>
          <entry>3 newsgroups, 4000 posts, 780k words</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Brown Corpus</entry>
          <entry>Nelson Francis and Henry Kucera</entry>
          <entry>15 genres, 1.15M words, tagged</entry>
          <entry>training and testing taggers, text classification, language modelling</entry>
        </row>
        <row>
          <entry>CoNLL 2000 Chunking Data</entry>
          <entry>Erik Tjong Kim Sang</entry>
          <entry>270k words, tagged and chunked</entry>
          <entry>training and testing chunk parsers</entry>
        </row>
        <row>
          <entry>Genesis Corpus</entry>
          <entry>Misc web sources</entry>
          <entry>6 texts, 200k words, 6 languages</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Project Gutenberg (selection)</entry>
          <entry>Michael Hart, Gregory Newby, et al</entry>
          <entry>14 texts, 1.7M words</entry>
          <entry>text classification, language modelling</entry>
        </row>
        <row>
          <entry>NIST 1999 Information Extraction (selection)</entry>
          <entry>John Garofolo</entry>
          <entry>63k words, newswire and named-entity SGML markup</entry>
          <entry>training and testing named-entity recognizers</entry>
        </row>
        <row>
          <entry>Levin Verb Index</entry>
          <entry>Beth Levin</entry>
          <entry>3k verbs with Levin classes</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Lexicon Corpus</entry>
          <entry></entry>
          <entry>Words, tags and frequencies from Brown Corpus and WSJ</entry>
          <entry>general purpose</entry>
        </row>
        <row>
          <entry>Names Corpus</entry>
          <entry>Mark Kantrowitz and Bill Ross</entry>
          <entry>8k male and female names</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>PP Attachment Corpus</entry>
          <entry>Adwait Ratnaparkhi</entry>
          <entry>28k prepositional phrases, tagged as noun or verb modifiers</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Roget's Thesaurus</entry>
          <entry>Project Gutenberg</entry>
          <entry>200k words, formatted text</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SEMCOR</entry>
          <entry>Vasile Rus and Rada Mihalcea</entry>
          <entry>880k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SENSEVAL 2 Corpus</entry>
          <entry>Ted Pedersen</entry>
          <entry>600k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>Stopwords Corpus</entry>
          <entry>Martin Porter et al</entry>
          <entry>2,400 stopwords for 11 languages</entry>
          <entry>text retrieval</entry>
        </row>
        <row>
          <entry>Penn Treebank (selection)</entry>
          <entry>LDC</entry>
          <entry>40k words, tagged and parsed</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Wordnet 1.7</entry>
          <entry>George Miller and Christiane Fellbaum</entry>
          <entry>180k words in a semantic network</entry>
          <entry>word-sense disambiguation, natural language understanding</entry>
        </row>
        <row>
          <entry>Wordlist Corpus</entry>
          <entry>OpenOffice.org et al</entry>
          <entry>960k words and 20k affixes for 8 languages</entry>
          <entry></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>
 
</section>

<section>
  <title> Further Reading </title>

  <para>
  Dialogue example is from
  Bob Carpenter and Jennifer Chu-Carroll's ACL-99 Tutorial on Spoken Dialogue Systems
  </para>

</section>

&index;
</article>

<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:2
sgml-indent-data:t
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
End:
-->
