<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Preface</title>
    &versiondate; &copyright;
<!--
    <title>Preface</title>
-->
  </articleinfo>
 
<!--
  <para><emphasis>Sample chapter for Natural Language Processing,
    by Steven Bird, Ewan Klein and Edward Loper, April 2005</emphasis></para>
-->

  <section><title>Learning NLP with the Natural Language Toolkit</title>

    <para>
      The <glossterm>Natural Language Toolkit (NLTK)</glossterm> was
      originally created as part of a computational linguistics course
      in the Department of Computer and Information Science at the
      University of Pennsylvania in 2001.  Since then it has been
      developed and expanded with the help of dozens of contributors.
      It has now been adopted in courses in dozens of universities,
      and serves as the basis of many research projects.  In this
      section we will discuss some of the benefits of learning (and
      teaching) NLP using NLTK.
    </para>

    <para>
      NLP is often taught within the confines of a single-semester
      course, either at advanced undergraduate level, or at
      postgraduate level.  Unfortunately, it turns out to be rather
      difficult to cover both the theoretical and practical sides of
      the subject in such a short span of time.  Some courses focus on
      theory to the exclusion of practical exercises, and deprive
      students of the challenge and excitement of writing programs to
      automatically process natural language.  Other courses are
      simply designed to teach programming for linguists, and do not
      get past the mechanics of programming to cover significant NLP.
      NLTK was developed to address this very problem, making it
      feasible to cover a substantial amount of theory and practice
      within a single-semester course.
    </para>

    <para>
      A significant fraction of any NLP course is made up of
      fundamental data structures and algorithms.  These are usually
      taught with the help of formal notations and complex diagrams.
      Large trees and charts are copied onto the board and edited in
      tedious slow motion, or laboriously prepared for presentation
      slides.  A more effective method is to use live demonstrations
      in which those diagrams are generated and updated automatically.
      NLTK provides interactive graphical user interfaces, making it
      possible to view program state and to study program execution
      step-by-step.  Most NLTK components have a demonstration mode,
      and will perform an interesting task without requiring any
      special input from the user.  It is even possible to make minor
      modifications to programs in response to ``what if'' questions.
      In this way, students learn the mechanics of NLP quickly, gain
      deeper insights into the data structures and algorithms, and
      acquire new problem-solving skills.
    </para>

    <para>
      NLTK supports assignments of varying difficulty and scope.  In
      the simplest assignments, students experiment with existing
      components to perform a wide variety of NLP tasks.  This may
      involve no programming at all, in the case of the existing
      demonstrations, or simply changing a line or two of program
      code.  As students become more familiar with the toolkit they
      can be asked to modify existing components or to create complete
      systems out of existing components.  NLTK also provides students
      with a flexible framework for advanced projects, such as
      developing a multi-component system, by integrating and
      extending NLTK components, and adding on entirely new
      components.  Here NLTK helps by providing standard
      implementations of all the basic data structures and algorithms,
      interfaces to standard corpora, substantial corpus samples, and
      a flexible and extensible architecture.  Thus, as we have seen,
      NLTK offers a fresh approach to NLP pedagogy, in which
      theoretical content is tightly integrated with application.
    </para>

  </section> <!-- Learning NLP with the Natural Language Toolkit -->

  <section><title>The Design of NLTK</title>

    <para>
      NLTK was designed with six requirements in mind.  First, NLTK is
      <emphasis>easy to use</emphasis>.  The primary purpose of the
      toolkit is to allow students to concentrate on building natural
      language processing systems.  The more time students must spend
      learning to use the toolkit, the less useful it is.  Second, we
      have made a significant effort to ensure that all the data
      structures and interfaces are <emphasis>consistent</emphasis>,
      making it easy to carry out a variety of tasks using a uniform
      framework.  Third, the toolkit is
      <emphasis>extensible</emphasis>, easily accommodating new
      components, whether those components replicate or extend the
      toolkit's existing functionality.  Moreover, the toolkit is
      organized so that it is usually obvious where extensions would
      fit into the toolkit's infrastructure.  Fourth, the toolkit is
      designed to be <emphasis>simple</emphasis>, providing an
      intuitive and appealing framework along with substantial
      building blocks, for students to gain a practical knowledge of
      NLP without having to write mountains of code.  Fifth, the
      toolkit is <emphasis>modular</emphasis>, so that the interaction
      between different components of the toolkit is minimized, and
      uses simple, well-defined interfaces.  In particular, it should
      be possible to complete individual projects using small parts of
      the toolkit, without needing to understand how they interact
      with the rest of the toolkit.  This allows students to learn how
      to use the toolkit incrementally throughout a course.
      Modularity also makes it easier to change and extend the
      toolkit.  Finally, the toolkit is <emphasis>well
      documented</emphasis>, including nomenclature, data structures,
      and implementations.
    </para>

    <para>
      Contrasting with these requirements are three non-requirements.
      First, while the toolkit provides a wide range of functions, it
      is not intended to be encyclopedic.  There should be a wide
      variety of ways in which students can extend the toolkit.
      Second, while the toolkit should be efficient enough that
      students can use their NLP systems to perform meaningful tasks,
      it does not need to be highly optimized for runtime performance.
      Such optimizations often involve more complex algorithms, and
      sometimes require the use of C or C++, making the toolkit harder
      to install.  Third, we have avoided clever programming tricks,
      since clear implementations are far preferable to ingenious yet
      indecipherable ones.
    </para>

    <para>
      NLTK is organized into a collection of task-specific components.
      Each module is a combination of data structures for representing
      a particular kind of information such as trees, and
      implementations of standard algorithms involving those
      structures such as parsers.  This approach is a standard feature
      of <glossterm>object-oriented design</glossterm>, in which
      components encapsulate both the resources and methods needed to
      accomplish a particular task.
    </para>

    <para>
    The most fundamental NLTK components are for identifying and
    manipulating individual words of text.  These include:
    <literal>tokenizer</literal>, for breaking up strings of
      characters into word tokens;
    <literal>tokenreader</literal>, for reading tokens from
      different kinds of corpora;
    <literal>stemmer</literal>, for stripping affixes from tokens,
      useful in some text retrieval applications;
    and
    <literal>tagger</literal>, for adding part-of-speech tags,
      including regular-expression taggers, n-gram taggers and
      Brill taggers.
    </para>

    <para>
    The second kind of module is for creating and manipulating
    structured linguistic information.  These components include:
    <literal>tree</literal>, for representing and processing
      parse trees;
    <literal>featurestructure</literal>, for building and
      unifying nested feature structures (or
      attribute-value matrices);
    <literal>cfg</literal>, for specifying free grammars;
    and
    <literal>parser</literal>, for creating parse trees over
      input text, including chart parsers, chunk parsers and
      probabilistic parsers.
    </para>

    <para>
    Several utility components are provided to facilitate processing
    and visualization.  These include:
    <literal>draw</literal>, to visualize NLP structures and
      processes;
    <literal>probability</literal>, to count and collate events,
      and perform statistical estimation;
    and
    <literal>corpus</literal>, to access tagged linguistic corpora.
    </para>

    <para>
      Finally, several advanced components are provided, mostly
      demonstrating NLP applications of machine learning techniques.
      These include: <literal>clusterer</literal>, for discovering
      groups of similar items within a large collection, including
      k-means and expectation maximisation;
      <literal>classifier</literal>, for categorising text into
      different types, including naive Bayes and maximum entropy; and
      <literal>hmm</literal>, for Hidden Markov Models, useful for a
      variety of sequence classification tasks.
    </para>

    <para>
      A further group of components is not part of NLTK proper.  These
      are a wide selection of third-party contributions, often
      developed as student projects at various institutions where NLTK
      is used, and distributed in a separate package called
      <emphasis>NLTK Contrib</emphasis>.  Several of these student
      contributions, such as the Brill tagger and the HMM module, have
      now been incorporated into NLTK.  Although these components are
      not maintained, they may serve as a useful starting point for
      future student projects.
    </para>

    <para>
      In addition to software and documentation, NLTK provides
      substantial <glossterm>corpus</glossterm> samples, listed in
      <xref linkend="nltk-corpora"/>.  Many of these can be accessed
      using the <literal>corpus</literal> module, avoiding the need to
      write specialized file parsing code before you can do NLP tasks.
    </para>


<figure id="nltk-corpora">
  <title>Corpora and Corpus Samples Distributed with NLTK</title>
  <informaltable frame="all">
    <tgroup cols="4">
      <tbody>
        <row>
          <entry>Corpus</entry>
          <entry>Compiler</entry>
          <entry>Contents</entry>
          <entry>Example Application</entry>
        </row>
        <row>
          <entry>20 Newsgroups (selection)</entry>
          <entry>Ken Lang and Jason Rennie</entry>
          <entry>3 newsgroups, 4000 posts, 780k words</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Brown Corpus</entry>
          <entry>Nelson Francis and Henry Kucera</entry>
          <entry>15 genres, 1.15M words, tagged</entry>
          <entry>training and testing taggers, text classification, language modelling</entry>
        </row>
        <row>
          <entry>CoNLL 2000 Chunking Data</entry>
          <entry>Erik Tjong Kim Sang</entry>
          <entry>270k words, tagged and chunked</entry>
          <entry>training and testing chunk parsers</entry>
        </row>
        <row>
          <entry>Genesis Corpus</entry>
          <entry>Misc web sources</entry>
          <entry>6 texts, 200k words, 6 languages</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Project Gutenberg (selection)</entry>
          <entry>Michael Hart, Gregory Newby, et al</entry>
          <entry>14 texts, 1.7M words</entry>
          <entry>text classification, language modelling</entry>
        </row>
        <row>
          <entry>NIST 1999 Information Extraction (selection)</entry>
          <entry>John Garofolo</entry>
          <entry>63k words, newswire and named-entity SGML markup</entry>
          <entry>training and testing named-entity recognizers</entry>
        </row>
        <row>
          <entry>Levin Verb Index</entry>
          <entry>Beth Levin</entry>
          <entry>3k verbs with Levin classes</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Lexicon Corpus</entry>
          <entry></entry>
          <entry>Words, tags and frequencies from Brown Corpus and WSJ</entry>
          <entry>general purpose</entry>
        </row>
        <row>
          <entry>Names Corpus</entry>
          <entry>Mark Kantrowitz and Bill Ross</entry>
          <entry>8k male and female names</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>PP Attachment Corpus</entry>
          <entry>Adwait Ratnaparkhi</entry>
          <entry>28k prepositional phrases, tagged as noun or verb modifiers</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Roget's Thesaurus</entry>
          <entry>Project Gutenberg</entry>
          <entry>200k words, formatted text</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SEMCOR</entry>
          <entry>Vasile Rus and Rada Mihalcea</entry>
          <entry>880k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SENSEVAL 2 Corpus</entry>
          <entry>Ted Pedersen</entry>
          <entry>600k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>Stopwords Corpus</entry>
          <entry>Martin Porter et al</entry>
          <entry>2,400 stopwords for 11 languages</entry>
          <entry>text retrieval</entry>
        </row>
        <row>
          <entry>Penn Treebank (selection)</entry>
          <entry>LDC</entry>
          <entry>40k words, tagged and parsed</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Wordnet 1.7</entry>
          <entry>George Miller and Christiane Fellbaum</entry>
          <entry>180k words in a semantic network</entry>
          <entry>word-sense disambiguation, natural language understanding</entry>
        </row>
        <row>
          <entry>Wordlist Corpus</entry>
          <entry>OpenOffice.org et al</entry>
          <entry>960k words and 20k affixes for 8 languages</entry>
          <entry></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>
 
</section> <!-- The Design of NLTK -->

</article>

<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:2
sgml-indent-data:t
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
End:
-->
