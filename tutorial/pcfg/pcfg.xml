<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!-- Equation symbols -->
<!ENTITY i1 "i<subscript>1</subscript>">
<!ENTITY i2 "i<subscript>2</subscript>">
<!ENTITY inm1 "i<subscript>n-1</subscript>">

<!-- Dot (for edges). -->
<!ENTITY dot "&bull;">
]>

<!-- ==================================
TO DO:
  - Learning grammars
  - Lexicalized grammars
  - InsideOutsidePCFGParser
  - Other minor updates
=================================== -->

<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Probabilistic Parsing</title>
    &versiondate; &copyright;
  </articleinfo>

  <section id="intro"> 
    <title> Introduction </title>

    <!-- ==== Remove this after we write the new tutorials ==== 
    <note> 
      <para> We are currently re-writing the basic parsing tutorials
      to provide more background information.  As a result, this
      tutorial contains several references to "previously discussed
      material" that has not yet been written.  Until we finish
      re-writing the basic tutorials, we refer you to the reference
      documentation for the <ulink
      url="&refdoc;/nltk.parser-module.html"><literal>nltk.parser</literal>
      module</ulink> and the <ulink
      url="&refdoc;/nltk.cfg-module.html"><literal>nltk.cfg</literal>
      module</ulink> for more up-to-date information. </para>

      <para> Before reading this tutorial, you should be familiar with
      symbolic parsing and context free grammars.  This material will
      be covered in the re-written basic parsing tutorials.  But for
      now, we refer you to other published discussions of parsing,
      such as Jurafsky and Martin, Chapter 10.  </para>
    </note> -->

    <para> Parsing allows us to find tree structures representing the
    internal organization of a text.  These trees are useful for a
    wide variety of tasks, including semantic interpretation,
    information retrieval, and machine translation.  Unfortunately,
    most texts have a large number of possible structures.  This
    causes two problems for the symbolic approaches discussed in the
    <ulink url="&tutdoc;/parsing/index.html">parsing tutorial</ulink>:
    </para>

    <itemizedlist>
      <listitem>
        <para>
        <glossterm>Ambiguity</glossterm>: There is no way to decide
        which of the tree structures are likely to correctly represent
        the text's internal organization. </para>
      </listitem>

      <listitem> 
        <para>
        <glossterm>Efficiency</glossterm>: Parsing a text requires
        searching a very large space of possible tree structures.
        With no information about which subtrees are more likely to be
        included in a complete parse, it can take a long time to find
        even a single parse. </para>
      </listitem>
    </itemizedlist>

    <para> Probabilistic techniques provide tools to address both of
    these problems.  We can assign probabilities to parses, and use
    them to decide which structures are more likely to represent a
    text's internal organization; and we can use probabilities to
    guide our search of the space of possible tree structures. </para>

  </section> <!-- Introduction -->

  <section id="pcfg"> 
    <title> Probabilistic Context Free Grammars </title>
 
    <para> A <glossterm>probabilistic context free grammar</glossterm>
    (or <glossterm>PCFG</glossterm>) is a context free grammar that
    associates a probability with each of its productions.  It generates the
    same set of parses for a text that the corresponding context free
    grammar does, and assigns a probability to each parse.  The
    probability of a parse generated by a PCFG is simply the product
    of the probabilities of the productions used to generate it.  </para>

    <para> Probabilistic context free grammars are implemented by the
    <ulink
    url="&refdoc;/nltk.cfg.PCFG-class.html"><literal>PCFG</literal></ulink>
    class, which is defined in the <literal>nltk.cfg</literal> module.
    Like <literal>CFG</literal>s, each <literal>PCFG</literal>
    consists of a start state and a list of productions.  But the
    productions are represented by <ulink
    url="&refdoc;/nltk.cfg.PCFGProduction-class.html"
    ><literal>PCFGProduction</literal></ulink>, a subclass of
    <literal>CFGProduction</literal> that associates a probability
    with a context free grammar production.</para>

    <section id="pcfg.productions"> 
      <title> PCFG Productions </title>
      
      <para>
      Like <literal>CFGProduction</literal>s, each
      <literal>PCFGProduction</literal> specifies that a nonterminal (the
      <glossterm>left-hand side</glossterm>) can be expanded to a
      sequence of terminals and nonterminals (the
      <glossterm>right-hand side</glossterm>).  In addition, each
      <literal>PCFGProduction</literal> has a probability associated with
      it.  <literal>PCFGProduction</literal>s are created using the <ulink
      url="&refdoc;/nltk.cfg.PCFGProduction-class.html#__init__"
      ><literal>PCFGProduction</literal> constructor</ulink>, which takes a
      probability, a nonterminal left-hand side, and zero or more
      terminals and nonterminals for the right-hand side. </para>

<programlisting><![CDATA[
>>> from nltk.cfg import *
        
# Define some nonterminals
>>> S, VP, V, NP = nonterminals('S, VP, V, NP')

# Create some PCFG productions
>>> prod1 = PCFGProduction(VP, [V, NP], prob=0.23)
VP -> V NP (p=0.23)
>>> prod2 = PCFGProduction(V, ['saw'], prob=0.12)
V -> 'saw' (p=0.12)
>>> prod3 = PCFGProduction(NP, ['cookie'], prob=0.04)
NP -> 'cookie' (p=0.04)
]]></programlisting>

      <para> The probability associated with a
      <literal>PCFGProduction</literal> is returned by the
      <ulink url="&refdoc;/nltk.cfg.PCFGProduction-class.html#p"
      ><literal>prob</literal></ulink> method:</para>

<programlisting><![CDATA[
>>> print prod1.prob(), prod2.prob(), prod3.prob() 
0.23 0.12 0.04
]]></programlisting>

      <para> As with <literal>CFGProduction</literal>s, the left-hand side
      of a <literal>PCFGProduction</literal> is returned by the <ulink
      url="&refdoc;/nltk.cfg.PCFGProduction-class.html#lhs"
      ><literal>lhs</literal></ulink> method; and the right-hand side
      is returned by the <ulink
      url="&refdoc;/nltk.cfg.PCFGProduction-class.html#rhs"
      ><literal>rhs</literal></ulink> method: </para>

<programlisting><![CDATA[
>>> prod1.lhs() 
<VP>
>>> prod1.rhs() 
(<V>, <NP>)
]]></programlisting>

    </section> <!-- PCFGProduction -->

    <section id="pcfg.pcfg"> 
      <title> PCFGs </title>

      <para> <literal>PCFG</literal>s are created using the <ulink
      url="&refdoc;/nltk.cfg.PCFG-class.html#__init__"
      ><literal>PCFG</literal> constructor</ulink>, which takes a start
      symbol and a list of productions:</para>

<programlisting><![CDATA[
>>> prods = [PCFGProduction(S, [VP, NP], prob=1.0),
...          PCFGProduction(VP, ['saw', NP], prob=0.4),
...          PCFGProduction(VP, ['ate'], prob=0.3),
...          PCFGProduction(VP, ['gave', NP, NP], prob=0.3),
...          PCFGProduction(NP, ['the', 'cookie'], prob=0.8),
...          PCFGProduction(NP, ['Jack'], prob=0.2)]
>>> grammar = PCFG(S, prods) 
CFG with 6 productions (start state = S)
    S -> VP NP (p=1)
    VP -> 'saw' NP (p=0.4)
    VP -> 'ate' (p=0.3)
    VP -> 'gave' NP NP (p=0.3)
    NP -> 'the' 'cookie' (p=0.8)
    NP -> 'Jack' (p=0.2)
]]></programlisting>

      <para> In order to ensure that the trees generated by the
      grammar form a proper probability distribution,
      <literal>PCFG</literal> imposes the constraint that all productions
      with a given left-hand side must have probabilities that sum to
      one: </para>

        <itemizedlist>
          <listitem>
            <para>
              for all <replaceable>lhs</replaceable>:
              &sum;<subscript><replaceable>rhs</replaceable></subscript>
              P(<replaceable>lhs</replaceable>&rarr;<replaceable
                 >rhs</replaceable>) = 1</para>
          </listitem>
        </itemizedlist>

      <para> The example grammar given
      above obeys this constraint: for <literal>S</literal>, there is
      only one production, with a probability of 1.0; for
      <literal>VP</literal>, 0.4+0.3+0.3=1.0; and for
      <literal>NP</literal>, 0.8+0.2=1.0. </para>

      <para> As with <literal>CFG</literal>s, the start state of a
      <literal>PCFG</literal> is returned by the <ulink
      url="&refdoc;/nltk.cfg.PCFG-class.html#start"
      ><literal>start</literal></ulink> method; and the productions
      are returned by the <ulink
      url="&refdoc;/nltk.cfg.PCFG-class.html#productions"
      ><literal>productions</literal></ulink> method:</para>

<programlisting><![CDATA[
>>> grammar.start()
<S>
>>> grammar.productions()
[[Production: S -> VP NP (p=1)], 
 [Production: VP -> 'saw' NP (p=0.4)],
 [Production: VP -> 'ate' (p=0.4)],
 [Production: VP -> VP PP (p=0.2)],
 [Production: NP -> 'the' 'boy' (p=0.8)],
 [Production: NP -> 'Jack' (p=0.2)],
 [Production: PP -> 'under' NP (p=1.0)]]
]]></programlisting>

    </section> <!-- PCFG -->

  </section> <!-- PCFGs -->

  <section id="probparse"> 
    <title> Probabilistic Parsers </title>

    <section id="probparse.interface"> 
      <title> The Probabilistic Parser Interface </title>

      <para> <ulink
      url="&refdoc;/nltk.parser.probabilistic.ProbabilisticParserI-class.html"
      ><literal>ProbabilisticParserI</literal></ulink> defines a
      standard interface for probabilistic parsers.  It extends the
      <ulink url="&refdoc;/nltk.parser.ParserI-class.html"
      ><literal>ParserI</literal></ulink> interface in two ways.
      First, the parse trees returned by <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken-class.html#parse"
      ><literal>parse</literal></ulink> and <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken-class.html#parse_n"
      ><literal>parse_n</literal></ulink> must define the
      <literal>PROB</literal> property, specifying each tree's
      probability. </para>

<programlisting><![CDATA[
>>> from nltk.parser.probabilistic import *
>>> from nltk.tokenizer import *
>>> pcfg_parser = ViterbiPCFGParser(grammar)
>>> sent_token = Token(TEXT='John baked some cookies')
>>> WhitespaceTokenizer().tokenize(sent_token)
>>> tree = pcfg_parser.get_parse(sent_token)
(S: (NP: <John>)
    (VP: (V: <baked>) (NP: (Det: <some>) (N: <cookies>))
>>> print tree.prob()
0.000282
]]></programlisting>

      <para> Second, <literal>ProbabilisticParser</literal>s are
      required to implement the <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken-class.html#parse_dist"
      ><literal>parse_dist</literal></ulink> method, which returns a
      <literal>ProbDist</literal> with <literal>TreeToken</literal>
      samples. </para>

<programlisting><![CDATA[
>>> pdist = pcfg_parser.parse_dist(words)
<ProbDist with 1 sample>
>>> pdist.prob(pdist.samples()[0])
(S: (NP: <John>)
    (VP: (V: <baked>) (NP: (Det: <some>) (N: <cookies>))
>>> pdist.prob(pdist.samples()[0]['PROB'])
0.000282
]]></programlisting>

      <para> Note that the probabilities in the distribution returned
      by <literal>parse_dist</literal> are <emphasis>not</emphasis>
      required to sum to 1. </para>
      
    </section> <!-- ProbabilisticParserI -->
  </section> <!-- Probabilistic Parsers -->


  <!-- ==== TO DO: write this section ==== -->
  <!--
  <section id="learning"> 
    <title> Learning Grammars </title>

    <note> <para> Work on defining a processing interface for learning
    grammars from treebanks, and implementations for that interface,
    are currently underway.  This section will be written once that
    work has been completed. </para> </note>

  </section> --> <!-- PCFGs -->

  <!-- ==== TO DO: write this section ==== -->
  <!-- <section id="lexical"> 
    <title> Lexicalized PCFGs </title>

    <note> <para> In this section, I will discuss lexicalized PCFGs.
    Producing lexicalized PCFGs requires new classes to derive
    grammars from treebanks, but doesn't require any new parsing
    techniques. </para>
    </note>

  </section> --> <!-- Lexicalized PCFGs -->

  <section id="impl"> 
    <title> Probabilistic Parser Implementations </title>

    <para> The next two sections introduce two probabilistic parsing
    algorithms for PCFGs.  The first is a Viterbi-style algorithm that
    uses dynamic programming to find the single most likely parse for
    a given text.  Whenever it finds multiple possible parses for a
    subtree, it discards all but the most likely parse.  The second is
    a bottom-up chart parser that maintains a queue of edges, and adds
    them to the chart one at a time.  The ordering of this queue is
    based on the probabilities associated with the edges, allowing the
    parser to expand more likely edges before less likely ones.
    Different queue orderings are used to implement a variety of
    different search strategies.  Both of these algorithms are
    implemented in the <ulink url="&refdoc;/nltk.parser.probabilistic-module.html"
    ><literal>nltk.parser.probabilistic</literal></ulink> module. </para>

    <!-- ==== CHANGE NAMES? ==== -->
    <!-- <note> <para> I am considering changing the names for the classes
    that implement these parsers.  Any suggestions are welcome.</para>
    </note> -->

  </section> <!-- Implementations -->

  <section id="viterbi"> 
    <title> A Viterbi-Style PCFG Parser </title>

    <para> <ulink
    url="&refdoc;/nltk.parser.probabilistic.ViterbiPCFGParser-class.html"
    ><literal>ViterbiPCFGParser</literal></ulink> is a bottom-up PCFG
    parser that uses dynamic programming to find the single most
    likely parse for a text.  It parses texts by iteratively filling
    in a <glossterm>most likely constituents table</glossterm>.  This
    table records the most likely tree structure for each span and
    node value.  In particular, it has an entry for every start index,
    end index, and node value, recording the most likely subtree that
    spans from the start index to the end index, and has the given
    node value.  For example, after parsing the sentence "I saw John
    with my cookie" with a simple grammar, the most likely
    constituents table might be: </para>

    <informaltable>
      <tgroup cols="3">
        <!-- Give about 70% of the table to the tree entry -->
        <colspec colnum="1" colwidth="8*" align="center"/>
        <colspec colnum="2" colwidth="7*" align="center"/>
        <colspec colnum="3" colwidth="70*" align="left"/>
        <colspec colnum="3" colwidth="10*" align="left"/>
        <thead>
          <row>
            <entry align="center" valign="top">Span</entry>
            <entry align="center" valign="top">Node Value</entry>
            <entry align="center" valign="top">Tree</entry>
            <entry align="center" valign="top">Prob</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>[0:1]</entry><entry>NP</entry>
            <entry>(NP: <emphasis>I</emphasis>)</entry>
            <entry>0.3</entry>
          </row>
          <row>
            <entry>[2:3]</entry><entry>NP</entry>
            <entry>(NP: <emphasis>John</emphasis>)</entry>
            <entry>0.3</entry>
          </row>
          <row>
            <entry>[4:6]</entry><entry>NP</entry>
            <entry>(NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>)</entry>
            <entry>0.2</entry>
          </row>
          <row>
            <entry>[3:6]</entry><entry>PP</entry>
            <entry>(PP: <emphasis>with</emphasis> 
                     (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))</entry>
            <entry>0.1</entry>
          </row>
          <row>
            <entry>[2:6]</entry><entry>NP</entry>
            <entry>(NP: (NP: <emphasis>John</emphasis>) 
                     (PP: <emphasis>with</emphasis> 
                       (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>)))</entry>
            <entry>0.01</entry>
          </row>
          <row>
            <entry>[1:3]</entry><entry>VP</entry>
            <entry>(VP: <emphasis>saw</emphasis> (NP: <emphasis>John</emphasis>)))</entry>
            <entry>0.03</entry>
          </row>
          <row>
            <entry>[1:6]</entry><entry>VP</entry>
            <entry>(VP: <emphasis>saw</emphasis> 
                     (NP: (NP: <emphasis>John</emphasis>) 
                        (PP: <emphasis>with</emphasis> 
                           (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))))</entry>
            <entry>0.001</entry>
          </row>
          <row>
            <entry>[0:6]</entry><entry>S</entry>
            <entry>(S: (NP: <emphasis>I</emphasis>) 
                      (VP: <emphasis>saw</emphasis> 
                       (NP: (NP: <emphasis>John</emphasis>) 
                          (PP: <emphasis>with</emphasis> 
                             (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>)))))</entry>
            <entry>0.0001</entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>

    <para> Once the table has been completely filled in, the parser
    simply returns the entry for the most likely constituent that
    spans the entire text, and whose node value is the start symbol.
    For this example, it would return the entry with a span of [0:6]
    and a node value of "S". </para>

    <para> Note that we only record the <emphasis>most
    likely</emphasis> constituent for any given span and node value.
    For example, in the table above, there are actually two possible
    constituents that cover the span [1:6] and have "VP" node values:
    </para>

    <itemizedlist>
      <listitem>
        <para>(VP: <emphasis>saw</emphasis> 
                 (NP: (NP: <emphasis>John</emphasis>) 
                    (PP: <emphasis>with</emphasis> 
                       (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))))</para>
        <para>"saw John, who has my cookie"</para>
      </listitem>
      <listitem>
        <para>(VP: (VP: <emphasis>saw</emphasis> 
                      (NP: (NP: <emphasis>John</emphasis>))
                   (PP: <emphasis>with</emphasis> 
                      (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))))</para>
        <para>"used my cookie to see John"</para>
      </listitem>
    </itemizedlist>

    <para> Since the grammar we are using to parse the text indicates
    that the first of these tree structures has a higher probability,
    the parser discards the second one. </para>

    <section id="viterbi.mlc"> 
      <title> Filling in the Most Likely Constituents Table </title>

      <para> Because the grammar used by
      <literal>ViterbiPCFGParser</literal> is a PCFG, the probability
      of each constituent can be calculated from the probabilities of
      its children.  Since a constituent's children can never cover a
      larger span than the constituent itself, each entry of the most
      likely constituents table depends only on entries for
      constituents with <emphasis>shorter</emphasis> spans (or equal
      spans, in the case of unary and epsilon productions). </para>

      <para> <literal>ViterbiPCFGParser</literal> takes advantage of
      this fact, and fills in the most likely constituent table
      incrementally.  It starts by filling in the entries for all
      constituents that span a single element of text.  After it has
      filled in all the table entries for constituents that span one
      element of text, it fills in the entries for constituents that
      span two elements of text.  It continues filling in the entries
      for constituents spanning larger and larger portions of the
      text, until the entire table has been filled.  </para>

      <para> To find the most likely constituent with a given span and
      node value, <literal>ViterbiPCFGParser</literal> considers all
      productions that could produce that node value.  For each production, it
      checks the most likely constituents table for sequences of
      children that collectively cover the span and that have the node
      values specified by the production's right hand side.  If the tree
      formed by applying the production to the children has a higher
      probability than the current table entry, then it updates the
      most likely constituents table with the new tree. </para>

      <section id="viterbi.mlc.unary"> 
        <title> Handling Unary Productions and Epsilon Productions </title>

        <para> A minor difficulty is introduced by unary productions and epsilon
        productions: an entry of the most likely constituents table might
        depend on another entry with the same span.  For example, if
        the grammar contains the production "<literal>V&rarr;VP</literal>,"
        then the table entries for <literal>VP</literal> depend on the
        entries for <literal>V</literal> with the same span.  This can
        be a problem if the constituents are checked in the wrong
        order.  For example, if the parser tries to find the most likely
        constituent for a <literal>VP</literal> spanning [1:3] before
        it finds the most likely constituents for <literal>V</literal>
        spanning [1:3], then it can't apply the
        "<literal>V&rarr;VP</literal>" production. </para>
      
        <para> To solve this problem,
        <literal>ViterbiPCFGParser</literal> repeatedly checks each
        span until it finds no new table entries.  Note that cyclic
        grammar productions (e.g. <literal>V&rarr;V</literal>) will
        <emphasis>not</emphasis> cause this procedure to enter an
        infinite loop.  Since all production probabilities are less
        than or equal to 1, any constituent generated by a cycle in
        the grammar will have a probability that is less than or equal
        to the original constituent; so
        <literal>ViterbiPCFGParser</literal> will discard it. </para>

      </section> <!-- Unary & Epsilon Productions -->

    </section> <!-- MLC table -->

  </section> <!-- ViterbiPCFGParser -->
  <section id="usingviterbi"> 
    <title> Using <literal>ViterbiPCFGParser</literal> </title>

    <para> <literal>ViterbiPCFGParser</literal>s are created from
    PCFGs using the <ulink
    url="&refdoc;/nltk.parser.probabilistic.ViterbiPCFGParser-class.html#__init__"
    ><literal>ViterbiPCFGParser</literal> constructor</ulink>: </para>

<programlisting><![CDATA[
>>> viterbi_parser = ViterbiPCFGParser(grammar)
<ViterbiPCFGParser for <CFG with 16 productions>>
]]></programlisting>

    <para> <literal>ViterbiPCFGParser</literal> implements all of the
    methods defined by the <literal>ProbabilisticParserI</literal>
    interface.  Note, however, that since
    <literal>ViterbiPCFGParser</literal> only finds the single most
    likely parse, that <literal>parse_n</literal> and
    <literal>parse_dist</literal> will never return more than one
    parse. </para>

<!-- ==== ADD A PARSE_DIST EXAMPLE ==== -->
<programlisting><![CDATA[
>>> from nltk.parser.probabilistic import *
      
>>> sent1 = Token(sent1='the dog ate my cookie')
>>> WhitespaceTokenizer().tokenize(sent1)
>>> viterbi_parser.parse(sent1)
>>> print sent1['TREE'], sent1['TREE']['PROB']
(S:
  (NP: (Det: <the>) (N: <dog>))
  (VP: (V: <ate>) (NP: (Det: <my>) 
                             (N: <cookie>)))) 0.00175
      
>>> sent1 = Token(sent1='I saw John with my cookie')
>>> WhitespaceTokenizer().tokenize(sent1)
>>> viterbi_parser.parse_n(sent2)
>>> for parse in sent2['TREES']:
...     print parse['PROB']
...     print parse
5.2040625e-05      
(S:
  (NP: <I>)
  (VP:
    (V: <saw>)
    (NP:
      (NP: <John>)
      (PP:
        (P: <with>)
        (NP: (Det: <my>) (N: <cookie>))))))
3.82192874e-08
(S:
  (NP: <I>)
  (VP:
    (VP:
      (V: <saw>)
      (NP: <John>))
    (PP:
      (P: <with>)
      (NP: (Det: <my>) (N: <cookie>)))))
]]></programlisting>

    <para> The <ulink
    url="&refdoc;/nltk.parser.probabilistic.ViterbiPCFGParser-class.html#trace"
    ><literal>trace</literal></ulink> method can be used to set the
    level of tracing output that is generated when parsing a text.
    Trace output displays the constituents that are considered, and
    indicates which ones are added to the most likely constituent
    table.  It also indicates the likelihood for each
    constituent. </para>

<programlisting><![CDATA[
>>> viterbi_parser.trace(3)
>>> viterbi_parser.parse(text2)

Inserting tokens into the most likely constituents table...
   Insert: |[=] . . . . .| I
   Insert: |. [=] . . . .| saw
   Insert: |. . [=] . . .| John
   Insert: |. . . [=] . .| with
   Insert: |. . . . [=] .| my
   Insert: |. . . . . [=]| cookie
Finding the most likely constituents spanning 1 text elements...
   Insert: |[=] . . . . .| NP -> 'I' (p=0.15)       0.1500000
   Insert: |. [=] . . . .| V -> 'saw' (p=0.65)      0.6500000
   Insert: |. [=] . . . .| VP -> V (p=0.1)          0.0650000
   Insert: |. . [=] . . .| NP -> 'John' (p=0.1)     0.1000000
   Insert: |. . . [=] . .| P -> 'with' (p=0.61)     0.6100000
   Insert: |. . . . [=] .| Det -> 'my' (p=0.2)      0.2000000
   Insert: |. . . . . [=]| N -> 'cookie' (p=0.5)    0.5000000
Finding the most likely constituents spanning 2 text elements...
   Insert: |[=|=] . . . .| S -> NP VP (p=1.0)       0.0097500
   Insert: |. [=|=] . . .| VP -> V NP (p=0.5)       0.0325000
   Insert: |. . . . [=|=]| NP -> Det N (p=0.5)      0.0500000
Finding the most likely constituents spanning 3 text elements...
   Insert: |[=|===] . . .| S -> NP VP (p=1.0)       0.0048750
   Insert: |. . . [=|===]| PP -> P NP (p=1.0)       0.0305000
Finding the most likely constituents spanning 4 text elements...
   Insert: |. . [=|=====]| NP -> NP PP (p=0.25)     0.0007625
Finding the most likely constituents spanning 5 text elements...
   Insert: |. [===|=====]| VP -> VP PP (p=0.4)      0.0003965
  Discard: |. [=|=======]| VP -> V NP (p=0.5)       0.0002478
  Discard: |. [=|=======]| VP -> V NP (p=0.5)       0.0002478
Finding the most likely constituents spanning 6 text elements...
   Insert: |[=|=========]| S -> NP VP (p=1.0)       0.0000594
]]></programlisting>

    <para> The level of tracing output can also be set with an
    optional argument to <ulink
    url="&refdoc;/nltk.parser.probabilistic.ViterbiPCFGParser-class.html#__init__"
    ><literal>ViterbiPCFGParser</literal>'s constructor</ulink>.  By
    default, no tracing output is generated.  Tracing output can be
    turned off by calling <literal>trace</literal> with a value of
    <literal>0</literal>. </para>

  </section> <!-- Using ViterbiPCFGParser -->

  <section id="buchart"> 
    <title> A Bottom-Up PCFG Chart Parser </title>

    <section id="buchart.intro"> 
      <title> Introduction </title>

      <para> The Viterbi-style algorithm described in the previous
      section finds the single most likely parse for a given text.
      But for many applications, it is useful to produce several
      alternative parses.  This is often the case when probabilistic
      parsers are combined with other probabilistic systems.  In
      particular, the most probable parse may be assigned a low
      probability by other systems; and a parse that is given a low
      probability by the parser might have a better overall
      probability. </para>

      <para> For example, a probabilistic parser might decide that the
      most likely parse for "I saw John with the cookie" is is the
      structure with the interpretation "I used my cookie to see
      John"; but that parse would be assigned a low probability by a
      semantic system.  Combining the probability estimates from the
      parser and the semantic system, the parse with the
      interpretation "I saw John, who had my cookie" would be given a
      higher overall probability. </para>

      <para> This section describes
      <literal>BottomUpPCFGChartParser</literal>, a parser for
      <literal>PCFG</literal>s that can find multiple parses for a
      text.  It assumes that you have already read the chart parsing
      tutorial, and are familiar with the data structures and productions
      used for chart parsing. </para>

      <note> <para> The chart parsing tutorial is under construction. 
      Until it is completed, we refer you to the <ulink
      url="&tutdoc;/parsing/index.html">parsing tutorial</ulink> and the reference
      documentation for the
      <ulink url="&refdoc;/nltk.parser.chart-module.html"
      ><literal>nltk.parser.chart</literal></ulink> module for background
      information. </para> </note>

    </section> <!-- Intro -->
    
    <section id="buchart.algorithm"> 
      <title> The Basic Algorithm </title>

      <para> <ulink
      url="&refdoc;/nltk.parser.probabilistic.BottomUpPCFGChartParser-class.html"
      ><literal>BottomUpPCFGChartParser</literal></ulink> is a
      bottom-up parser for <literal>PCFG</literal>s that uses a <ulink
      url="&refdoc;/nltk.parser.chart.Chart-class.html"
      ><literal>Chart</literal></ulink> to record partial results.  It
      maintains a queue of <ulink url="&refdoc;/nltk.parser.chart.EdgeI-class.html"
      ><literal>Edge</literal></ulink>s, and adds them to the chart one
      at a time.  The ordering of this queue is based on the
      probabilities associated with the edges, allowing the parser to
      insert more likely edges before exploring less likely ones.  For
      each edge that the parser adds to the chart, it may become
      possible to insert new edges into the chart; these are added to
      the queue.  <literal>BottomUpPCFGChartParser</literal> continues
      adding the edges in the queue to the chart until enough complete
      parses have been found, or until the edge queue is
      empty. </para>

    </section> <!-- The basic algorithm -->

    <section id="buchart.edges"> 
      <title> Probabilistic Edges </title>

      <para> An <literal>Edge</literal> associates a dotted production and a
      location with a (partial) parse tree.  A <glossterm>probabilistic
      edge</glossterm> can be formed by using a
      <literal>ProbabilisticTreeToken</literal>s to encode an edge's
      parse tree.  The <literal>ProbabilisticTreeToken</literal>'s
      probability is the product of the probability of the production that
      generated it and the probabilities of its children.  For
      example, the probability associated with an edge
      [Edge:&nbsp;S&rarr;NP&dot;VP]@[0:2] is the probability of its
      NP child times the probability of the PCFG production
      [Production:&nbsp;S&rarr;NP&nbsp;VP].  Note that an edge's tree token
      only includes children for elements to the left of the edge's
      dot.  Thus, the edge's probability does <emphasis>not</emphasis>
      include any probabilities for the elements to the right of the
      edge's dot. </para>

    </section> <!-- Probabilistic Edges -->

    <section id="buchart.queue"> 
      <title> The Edge Queue </title>
      <para> The edge queue is a sorted list of
      <literal>Edge</literal>s that can be added to the chart.  It is
      initialized with a single edge for each token in the text.
      These <glossterm>token edges</glossterm> have the form
      [Edge:&nbsp;<replaceable>type</replaceable>&rarr;&dot;]@<replaceable>loc</replaceable>,
      where <replaceable>type</replaceable> is the token's type and
      <replaceable>loc</replaceable> is its location. </para>

      <para> As each edge from the queue is added to the chart, it may
      become possible to insert new edges into the chart; these new
      edges are added to the queue.  There are two ways that it can
      become possible to insert new edges into the chart: </para>

      <itemizedlist>
        <listitem> <para> The <glossterm>bottom-up initialization
        production</glossterm> can be used to add a self-loop edge whenever
        an edge whose dot is in position 0 is added to the chart. </para>
        </listitem>
        <listitem> <para> The <glossterm>fundamental production</glossterm>
        can be used to combine a new edge with edges already
        present in the chart. </para>
        </listitem>
      </itemizedlist>

      <para> The edge queue is implemented using a
      <literal>list</literal>.  For efficiency reasons,
      <literal>BottomUpPCFGChartParser</literal> uses
      <literal>pop</literal> to remove edges from the queue.  Thus,
      the front of the queue is the <emphasis>end</emphasis> of the
      list.  This needs to be kept in mind when implementing sorting
      orders for the queue: edges that should be tried first should be
      placed at the end of the list. </para>

    </section> <!-- The edge queue -->

    <section id="buchart.sorting"> 
      <title> Sorting The Edge Queue </title>

      <para> By changing the sorting order used by the queue, we can
      control the strategy that the parser uses to search for parses
      of a text.  Since there are a wide variety of reasonable search
      strategies, <literal>BottomUpPCFGChartParser</literal> does not
      define the sorting order for the queue.  Instead,
      <literal>BottomUpPCFGChartParser</literal> is defined as an abstract
      class; and subclasses are used to implement a variety of
      different queue orderings.  Each subclass is required to define
      the <ulink
      url="&refdoc;/nltk.parser.probabilistic.BottomUpPCFGChartParser-class.html#sort_queue"
      ><literal>sort_queue</literal></ulink> method, which sorts a
      given queue.  The remainder of this section describes four
      different subclasses of <literal>BottomUpPCFGChartParser</literal>
      that are defined in the <literal>nltk.parser.probabilistic</literal>
      module.</para>

      <section id="buchart.sorting.inside"> 
        <title> InsidePCFGParser </title>

        <!-- <note> <para> We should either explain "inside
        probabilities" or rename this parser (to
        <literal>LowestCostFirstPCFGParser</literal>?). </para>
        </note> -->

        <para> The simplest way to order the queue is to sort the
        edges by the probabilities of their tree tokens.  This
        ordering concentrates the efforts of the parser on edges that
        are more likely to be correct descriptions of the texts that
        they span.  This approach is implemented by the <ulink
        url="&refdoc;/nltk.parser.probabilistic.InsidePCFGParser-class.html"
        ><literal>InsidePCFGParser</literal></ulink> class. </para>

        <para> The probability of an edge's tree token provides an
        upper bound on the probability of any parse produced using
        that edge.  The probabilistic "cost" of using an edge to form a
        parse is one minus its tree token's probability.  Thus,
        inserting the edges with the most likely tree tokens first
        results in a <glossterm>lowest-cost-first</glossterm> search
        strategy.  Lowest-cost-first search is an
        <glossterm>optimal</glossterm> search strategy: the first
        solution it finds is guaranteed to be the best
        solution. </para>

        <para> However, lowest-cost-first search can be rather
        inefficient.  Since a tree's probability is the product of the
        probabilities of all the productions used to generate it, smaller
        trees tend to have higher probabilities than larger ones.
        Thus, lowest-cost-first search tends to insert edges with
        small tree tokens before moving on to edges with larger ones.
        But any complete parse of the text will necessarily have a
        large tree token; so complete parses will tend to be inserted
        after nearly all other edges. </para>

        <para> The basic problem with lowest-cost-first search is that
        it ignores the probability that an edge's tree is part of a
        complete parse.  It will try parses that are locally coherent,
        even if they are unlikely to form part of a complete parse.
        Unfortunately, it can be quite difficult to calculate the
        probability that a tree is part of a complete parse.  However,
        we can use a variety of techniques to approximate that
        probability. </para>

        <para> Since <literal>InsidePCFGParser</literal> is a subclass
        of <literal>BottomUpPCFGChartParser</literal>, it only needs to
        define a <literal>sort_queue</literal> method.  Thus, the
        implementation of <literal>InsidePCFGParser</literal> class is
        quite simple: </para>

<programlisting><![CDATA[
class InsidePCFGParser(BottomUpPCFGChartParser):
    def sort_queue(self, queue, chart):
        # Sort the edges by the probabilities of their tree tokens.
        queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))
]]></programlisting>

        <note> <para> We haven't used <literal>lambda</literal> before
        now; should we have a note here that explains it?
        Alternatively, should we define an external function (e.g.,
        <literal>cmp_tree_prob</literal>), and use that instead?
        Also, it wouldn't be a bad idea to talk about the sort method
        somewhere, e.g., to mention that it does in-place sorting and
        returns None. </para> </note>

      </section> <!-- InsidePCFGParser -->

      <section id="buchart.sorting.longest"> 
        <title> LongestPCFGParser </title>

        <para> <ulink
        url="&refdoc;/nltk.parser.probabilistic.LongestPCFGParser-class.html"
        ><literal>LongestPCFGParser</literal></ulink> sorts its queue
        in descending order of the edges' lengths.  These lengths
        (properly normalized) provide a crude approximations to the
        probabilities that trees are part of complete parses.  Thus,
        the <literal>LongestPCFGParser</literal> employs a
        <glossterm>best-first</glossterm> search strategy, where it
        inserts the edges that are closest to producing complete
        parses before trying any other edges.  Best-first search is
        <emphasis>not</emphasis> an optimal search strategy: the first
        solution it finds is not guaranteed to be the best solution.
        However, it will usually find a complete parse much more
        quickly than lowest-cost-first search. </para>

        <para> Since <literal>LongestPCFGParser</literal> is a
        subclass of <literal>BottomUpPCFGChartParser</literal>, its
        implementation simply defines a <literal>sort_queue</literal>
        method: </para>

<programlisting><![CDATA[
class LongestPCFGParser(BottomUpPCFGChartParser):
    def sort_queue(self, queue, chart):
        # Sort the edges by the lengths of their tree tokens.
        queue.sort(lambda e1,e2: cmp(len(e1.loc()), len(e2.loc())))
]]></programlisting>
        
      </section> <!-- LongestPCFGParser -->
      
      <section id="buchart.sorting.beam"> 
        <title> BeamPCFGParser </title>

        <para> When large grammars are used to parse a text, the edge
        queue can grow quite long.  The edges at the end of a large
        well-sorted queue are unlikely to be used.  Therefore, it is
        reasonable to remove (or <glossterm>prune</glossterm>) these
        edges from the queue. </para>

        <para> <ulink
        url="&refdoc;/nltk.parser.probabilistic.BeamPCFGParser-class.html"
        ><literal>BeamPCFGParser</literal></ulink> provides a simple
        implementation of a pruning PCFG parser.  It uses the same
        sorting order as <literal>InsidePCFGParser</literal>.  But
        whenever the edge queue grows beyond a pre-defined maximum
        length, <literal>BeamPCFGParser</literal> truncates it.  The
        resulting search strategy, lowest-cost-first search with
        pruning, is a type of beam search.  (A <glossterm>beam
        search</glossterm> is a search strategy that only keeps the
        best partial results.)  The queue's predefined maximum length
        is called the <glossterm>beam size</glossterm> (or simply the
        <glossterm>beam</glossterm>).  A
        <literal>BeamPCFGParser</literal>'s beam size is set by the
        first argument to its constructor. </para>

        <para> Beam search reduces the space requirements for
        lowest-cost-first search, by discarding edges that are not
        likely to be used.  But beam search also loses many of
        lowest-cost-first search's more useful properties.  Beam
        search is not optimal: it is not guaranteed to find the best
        parse first.  In fact, since it might prune a necessary edge,
        beam search is not even <glossterm>complete</glossterm>: it is
        not guaranteed to return a parse if one exists. </para>

        <para> The implementation for
        <literal>BeamPCFGParser</literal> defines two methods.  First,
        it overrides the constructor, since it needs to record the
        beam size.  And second, it defines the
        <literal>sort_queue</literal> method, which sorts the queue
        and discards any excess edges. </para>
        
<programlisting><![CDATA[
class BeamPCFGParser(BottomUpPCFGChartParser):
    def __init__(self, beam_size, grammar, trace=0):
        BottomUpPCFGChartParser.__init__(self, grammar, trace)
        self._beam_size = beam_size

    def sort_queue(self, queue, chart):
        # Sort the queue.
        queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))
        # Truncate the queue, if necessary.
        if len(queue) > self._beam_size:
            queue[:] = queue[len(queue)-self._beam_size:]
]]></programlisting>

        <para> Note that when truncating the queue,
        <literal>sort_queue</literal> uses the expression
        "<literal>queue[:]</literal>" to change the
        <emphasis>contents</emphasis> of the <literal>queue</literal>
        variable.  In particular, compare it to the following code,
        which reassigns the local variable <literal>queue</literal>,
        but does not modify the contents of the given list: </para>

<programlisting><![CDATA[
        # WRONG: This does not change the contents of the edge queue. 
        if len(queue) > self._beam_size:
            queue = queue[len(queue)-self._beam_size:]
]]></programlisting>

        <note> <para> This might be too confusing.  Perhaps I should
        change the definition of <literal>sort_queue</literal> to
        return the "new" list, where the new list can simply be the
        old list.  Of course, you still might want to use
        <literal>queue[:]</literal> in this context, for efficiency
        reasons; and if we made that change, then the students would
        need to remember not to write code like the following:</para>

<programlisting><![CDATA[
        # WRONG: The sort method returns None.
        return queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))
]]></programlisting>
        </note>

      </section> <!-- BeamPCFGParser -->
      
      <!-- ==== TO DO: write this? ==== -->
      <!-- <section id="buchart.sorting.insideoutside"> 
        <title> InsideOutsidePCFGParser </title>

        <note> <para> We plan to add an inside/outside parser; when we
        do, we'll add a description of it to this section. </para>
        </note>

      </section> --> <!-- InsideOutsidePCFGParser -->

    </section> <!-- Sorting the edge queue -->

  </section> <!-- BottomUpPCFGChartParser -->

  <section id="buchart.usingbuchart"> 
    <title> Using <literal>BottomUpPCFGChartParser</literal> </title>

    <para> New <literal>BottomUpPCFGChartParser</literal>s are created
    using the <literal>BottomUpPCFGChartParser</literal> subclasses's
    constructors.  These include:</para>

    <itemizedlist>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.parser.probabilistic.InsidePCFGParser-class.html#__init__"
      ><literal>InsidePCFGParser</literal> constructor</ulink></para>
      </listitem>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.parser.probabilistic.LongestPCFGParser-class.html#__init__"
      ><literal>LongestPCFGParser</literal> constructor</ulink></para>
      </listitem>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.parser.probabilistic.BeamPCFGParser-class.html#__init__"
      ><literal>BeamPCFGParser</literal> constructor</ulink></para>
      </listitem>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.parser.probabilistic.RandomPCFGParser-class.html#__init__"
      ><literal>RandomPCFGParser</literal> constructor</ulink></para>
      </listitem>
    </itemizedlist>

    <para> See the reference documentation for the <ulink
    url="&refdoc;/nltk.parser.probabilistic.BottomUpPCFGChartParser-class.html"
    ><literal>BottomUpPCFGChartParser</literal></ulink> module for a
    complete list of subclasses.  Unless a subclass overrides the
    constructor, it takes a single <literal>PCFG</literal>: </para>

<programlisting><![CDATA[
>>> inside_parser = InsidePCFGParser(grammar)
<InsidePCFGParser for <CFG with 16 productions>>
>>> longest_parser = LongestPCFGParser(grammar)
<LongestPCFGParser for <CFG with 16 productions>>
>>> beam_parser = BeamPCFGParser(20, grammar)
<BeamPCFGParser for <CFG with 16 productions>>
]]></programlisting>

    <warning><para> <literal>BottomUpPCFGChartParser</literal> is an
    abstract class; you should not directly instantiate it.  If you
    try to use it to parse a text, it will raise an exception, since
    <literal>sort_queue</literal> will be undefined. </para>
    </warning>

    <para> <literal>BottomUpPCFGChartParser</literal>s implement all
    of the methods defined by the
    <literal>ProbabilisticParserI</literal> interface. </para>

<!-- ==== ADD A PARSE_DIST EXAMPLE ==== -->
<programlisting><![CDATA[
>>> from nltk.parser.probabilistic import *
      
>>> sent1 = Token(sent1='the dog ate my cookie')
>>> WhitespaceTokenizer().tokenize(sent1)
>>> inside_parser.parse(sent1)
>>> print sent1['TREE'], sent1['TREE']['PROB']
(S:
  (NP: (Det: <the>) (N: <dog>))
  (VP: (V: <ate>) (NP: (Det: <my>) 
                             (N: <cookie>)))) 0.00175
      
>>> sent1 = Token(sent1='I saw John with my cookie')
>>> WhitespaceTokenizer().tokenize(sent1)
>>> inside_parser.parse_n(sent2)
>>> for parse in sent2['TREES']:
...     print parse['PROB']
...     print parse
5.2040625e-05      
(S:
  (NP: <I>)
  (VP:
    (V: <saw>)
    (NP:
      (NP: <John>)
      (PP:
        (P: <with>)
        (NP: (Det: <my>) (N: <cookie>))))))
3.82192874e-06
(S:
  (NP: <I>)
  (VP:
    (VP:
      (V: <saw>)
      (NP: <John>))
    (PP:
      (P: <with>)
      (NP: (Det: <my>) (N: <cookie>)))))
]]></programlisting>

    <para> The <ulink
    url="&refdoc;/nltk.parser.probabilistic.BottomUpPCFGChartParser-class.html#trace"
    ><literal>trace</literal></ulink> method can be used to set the
    level of tracing output that is generated when parsing a text.
    Trace output displays edges as they are added to the chart, and
    shows the probability for each edges' tree token. </para>

<programlisting><![CDATA[
>>> inside_parser.trace(3)
>>> inside_parser.parse(text2)

Initializing the queue with token edges...
Processing the edge queue...
  |[=] . . . . .| 'I' -> *          1.0000000
  |. [=] . . . .| 'saw' -> *        1.0000000
  |. . [=] . . .| 'John' -> *       1.0000000
  |. . . [=] . .| 'with' -> *       1.0000000
  |. . . . [=] .| 'my' -> *         1.0000000
  |. . . . . [=]| 'cookie' -> *     1.0000000
  |. > . . . . .| V -> * 'saw'      0.6500000
  |. > . . . . .| VP -> * V NP      0.7000000
  |. [=] . . . .| V -> 'saw' *      0.6500000
  |. . . > . . .| P -> * 'with'     0.6100000
         .               .               .
         .               .               .
         .               .               .
  |. . [=======>| NP -> NP * PP     0.0001906
  |. [=========]| VP -> VP PP *     0.0001387
  |[===========]| S -> NP VP *      0.0000520
  |. [=========>| VP -> VP * PP     0.0000346
  |[===========]| S -> NP VP *      0.0000208
Found 2 parses with 54 edges
]]></programlisting>

  </section> <!-- Using BottomUpPCFGChartParser -->

  &index;
</article>
<!--  LocalWords:  authorinitials edl articleinfo para ulink url refdoc cfg Det
 -->
<!--  LocalWords:  Jurafsky tutdoc glossterm indexterm subtrees TreeToken dist
 -->
<!--  LocalWords:  ProbabilisticParserI ProbabilisticTreeToken WhitespaceTokenizer PCFG
 -->
<!--  LocalWords:  ProbabilisticTreeTokens ParserI ViterbiPCFGParser ProbDist
 -->
<!--  LocalWords:  ProbabilisticParser nonterminal nonterminals lhs rhs PCFGs
 -->
<!--  LocalWords:  treebanks underway Lexicalized lexicalized Viterbi subtree
 -->
<!--  LocalWords:  pcfgparser iteratively Prob constituent's unary viterbi loc
 -->
<!--  LocalWords:  BottomUpPCFGChartParser  subclasses cmp
 -->
<!--  LocalWords:  InsidePCFGParser LowestCostFirstPCFGParser prob len pre
 -->
<!--  LocalWords:  LongestPCFGParser BeamPCFGParser search's subclasses's
 -->
<!--  LocalWords:  InsideOutsidePCFGParser RandomPCFGParser
 -->
