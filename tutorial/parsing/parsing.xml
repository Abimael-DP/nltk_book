<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Parsing</title>
    &versiondate; &copyright;
  </articleinfo>

  <section id="intro"> <title> Introduction </title>

<para>
Native speakers of any language have strong intuitions about the
well-formedness of putative sentences in that language.  These intuitions
are surprisingly detailed.  For example, consider the following six sentences
involving three synonymous verbs <emphasis>loaded</emphasis>,
<emphasis>dumped</emphasis>, and <emphasis>filled</emphasis>.
</para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para>
      The farmer <emphasis>loaded</emphasis> sand into the cart
    </para></listitem>
    <listitem><para>
      The farmer <emphasis>loaded</emphasis> the cart with sand
    </para></listitem>
    <listitem><para>
      The farmer <emphasis>dumped</emphasis> sand into the cart
    </para></listitem>
    <listitem><para>
      *The farmer <emphasis>dumped</emphasis> the cart with sand
    </para></listitem>
    <listitem><para>
      *The farmer <emphasis>filled</emphasis> sand into the cart
    </para></listitem>
    <listitem><para>
      The farmer <emphasis>filled</emphasis> the cart with sand
    </para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

<para>
Two of the sentences (starred) are ill-formed.  As we shall see, patterns of
well-formedness and ill-formedness in a sequence of words can be understood
with respect to the internal <glossterm>phrase structure</glossterm> of the
sentences.  We can develop formal models of these structures using grammars
and parsers.
</para>

<para>
In the context of computational modelling, a language is often viewed
as a set of well-formed sentences.  Sequences of words which are not
grammatical are excluded from this set.  Now, since there is no
upper-bound on the length of a sentence, the number of possible
sentences is unbounded.
For example, it is possible to add an unlimited amount of
material to a sentence by using <literal>and</literal> or by
chaining relative clauses, as illustrated in the following example
from a children's story:
</para>

<blockquote><para>
You can imagine Piglet's joy when at last the ship came in sight of
him. In after-years he liked to think that he had been in Very Great
Danger during the Terrible Flood, but the only danger he had really
been in was the last half-hour of his imprisonment, when Owl, who had
just flown up, sat on a branch of his tree to comfort him, and told
him a very long story about an aunt who had once laid a seagull's egg
by mistake, and the story went on and on, rather like this sentence,
until Piglet who was listening out of his window without much hope,
went to sleep quietly and naturally, slipping slowly out of the window
towards the water until he was only hanging on by his toes, at which
moment, luckily, a sudden loud squawk from Owl, which was really part
of the story, being what his aunt said, woke the Piglet up and just
gave him time to jerk himself back into safety and say, "How
interesting, and did she?"  when -- well, you can imagine his joy when
at last he saw the good ship, Brain of Pooh (Captain, C. Robin; Ist
Mate, P. Bear) coming over the sea to rescue him...
(from A.A. Milne <emphasis>In which Piglet is Entirely
Surrounded by Water</emphasis>)
</para></blockquote>

<para>
Given that the resources of a computer, however large, are still
finite, it is necessary to devise a finite description of this
infinite set.
Such descriptions are called
<glossterm>grammar</glossterm>s.  We have already encountered this
possibility in the context of regular expressions.  For example, the
expression <literal>a+</literal> describes the infinite set
<literal>{a, aa, aaa, aaaa, ...}</literal>.  Apart from their
compactness, grammars usually capture important properties of the
language being studied, and can be used to systematically map between
sequences of words and abstract representations of their meaning.
Thus, even if we were to impose an upper bound on sentence length to
ensure the language was finite, we would still want to come up with a
compact representation in the form of a grammar.
</para>

<para>
A well-formed sentence of a language is more than an arbitrary
sequence of words from the language.  Certain kinds of words usually
go together.  For instance,
determiners like <literal>the</literal> are typically
followed by adjectives or nouns, but not by verbs.  Groups of words form
intermediate structures called phrases or constituents.  These
constituents can be identified using standard syntactic tests, such as
substitution, movement and coordination.  For example, if a sequence
of words can be replaced with a pronoun, then that sequence is likely
to be a constituent.  The following example illustrates this test:
</para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para><emphasis>Ordinary daily multivitamin and mineral
    supplements</emphasis> could help adults with diabetes fight
    off some minor infections</para></listitem>
    <listitem><para><emphasis>They</emphasis> could help adults with
    diabetes fight off some minor infections</para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

<note><para>
Readers are referred to any introductory text on syntax for fuller
treatment of constituency, e.g. McCawley (1998) The Syntactic
Phenomena of English.
</para></note>

<para>
The structure of a sentence may be represented using a
phrase structure tree, in which the terminal symbols are the words of the sentence,
the pre-terminal symbols are parts of speech, and the remaining non-terminals
are syntactic constituents.  An example of such a tree is shown in
<xref linkend="parse_tree"/>.
</para>

<figure id="parse_tree"><title>Phrase Structure Tree</title>
<informaltable frame="all">
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/parse_tree" scale="50"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>
A <emphasis>grammar</emphasis> is a formal system which specifies
which sequences of words are well-formed in the language, and which
provides one or more phrase structures for the sequence.  We will
focus our attention on a particular kind of grammar called a
<emphasis>context-free grammar</emphasis> (CFG),
which is a collection of productions of the form <literal>S &rarr; NP
VP</literal>.  (To be well-formed, each non-terminal node and its children
must correspond to such a production.)
</para>

<para>
A <emphasis>parser</emphasis> is a computational
system which processes input sentences according to the productions of the
grammar, and builds one or more constituent structures which conform
to the grammar.  We take a grammar to be a declarative specification
of well-formedness, and a parser to be a procedural interpretation of
the grammar.
In this chapter we will present context-free
grammars, and describe some simple parsers that work with them.
</para>

<!-- EXPAND THE ABOVE DISCUSSION -->

<para>
Parsing is important in linguistics and natural language processing
for a variety of reasons.  A parser permits a grammar to be evaluated
against a potentially large collection of test sentences, helping the
linguist to identify shortcomings in their analysis.  A parser can be
used as a model of psycholinguistic processing, and used to explain
the processing difficulties that humans have with certain syntactic
constructions (e.g. the so-called "garden path" sentences).  A parser
can serve as the first stage of processing natural language input for
a question-answering system.
</para>

  </section> <!-- Introduction -->

<!--

    <section id="overview"> <title> Linguistic Overview </title>

X.2 linguistic overview (for non-linguist readers)
    - how have linguists addressed the problem?
    - what are the shortcomings of the non-computational approach?

- cover structural ambiguity and PP attachment
- cover subcategorization, Levin classes

<para>
Parsing has a long and interesting history in computational
linguistics.
[NOTES: the perspective of early generative grammar;
early NLP approaches including ATNs and DCGs;
grammar formalisms and development environments.]
</para>

    <para>
      Context-free grammars: CFG productions and their interpretation.
    </para>

    <para>
      Discussion about how CFGs are created by hand.
    </para>

    <para>A simple grammar:
    </para>

<screen>
S &rarr; NP VP
NP &rarr; Det N
NP &rarr; Det N PP
VP &rarr; V NP PP
VP &rarr; V NP
VP &rarr; V
PP &rarr; P NP

NP &rarr; 'I'
Det &rarr; 'the'
Det &rarr; 'a'
N &rarr; 'man'
V &rarr; 'saw'
P &rarr; 'in'
N &rarr; 'park'
P &rarr; 'with'
N &rarr; 'dog'
N &rarr; 'telescope'
</screen>

<para>
Shortcomings of this approach: unreliable, doesn't scale,
complex interactions amongst productions makes manual debugging
almost impossible.  Consequently linguists are unable to
work with large-coverage grammars.
</para>

    </section>
-->

    <section id="approaches"> <title> Computational Approaches to Parsing </title>

<!-- To do: general introduction to TD and BU -->

<!--
X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)
-->

      <section id="approaches.rd"> <title> Recursive Descent Parsing </title>

<para>
The simplest kind of parser interprets the grammar as a
specification of how to break a high-level goal into several
lower-level subgoals.  The top-level goal is to
find an <literal>S</literal>.  The <literal>S &rarr; NP
VP</literal> production permits the parser to replace this goal
with two subgoals: find an <literal>NP</literal>, then find
a <literal>VP</literal>.  Each of these subgoals can be
replaced in turn by sub-sub-goals, using productions that have
<literal>NP</literal> and <literal>VP</literal> on their
left-hand side.  Eventually, this expansion process leads to
subgoals such as: find the word
<literal>telescope</literal>.  Such subgoals can be directly
compared against the input string, and succeed if the next
word is matched.  If there is no match the parser must back up and try a different
alternative.
</para>

<para>
The recursive descent parser builds a parse tree during the
above process.  With the initial goal (find an
<literal>S</literal>), the <literal>S</literal> root node is
created.  As the above process recursively expands its goals
using the productions of the grammar, the parse tree is extended downwards
(hence the name <emphasis>recursive descent</emphasis>).  We can see this
in action using the parser demonstration <literal>nltk.draw.rdparser</literal>.
To run this demonstration, use the following commands:
</para>

<programlisting><![CDATA[
>>> from nltk.draw.rdparser import demo
>>> demo()
]]></programlisting>

<para>
Six stages of the execution of this parser are shown in
<xref linkend="rdparser"/>.
</para>

<figure id="rdparser"><title>Six Stages of a Recursive Descent Parser: initial, after two productions,
after matching "the", failing to match "man", completed parse, backtracking
</title>
<informaltable frame="all">
<tgroup cols="3"><tbody><row><entry>
<graphic fileref="images/rdparser1" scale="24"/>
</entry><entry>
<graphic fileref="images/rdparser2" scale="24"/>
</entry><entry>
<graphic fileref="images/rdparser3" scale="24"/>
</entry></row><row><entry>
<graphic fileref="images/rdparser4" scale="24"/>
</entry><entry>
<graphic fileref="images/rdparser5" scale="24"/>
</entry><entry>
<graphic fileref="images/rdparser6" scale="24"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>Discussion: choosing which of several possible productions to apply;
backtracking; termination.</para>

<para>Problems with recursive descent parsing:
considers structures and words that are not attested;
backtracking may discard parsed constituents that need to be rebuilt;
for example, backtracking over <literal>VP &rarr; V NP</literal>
will discard the structures created for the
<literal>V</literal> and <literal>NP</literal> non-terminals.  If the parser then proceeds
with <literal>VP &rarr; V NP PP</literal>, then the structures for the
<literal>V</literal> and <literal>NP</literal> must be created again.
</para>

<para>
Recursive descent parsing is a kind of <emphasis>top-down
parsing</emphasis>.  These use the grammar to
<emphasis>predict</emphasis> what the input will be, before
inspecting any input.  However, since the input is available
to the parser all along, it would be more sensible to
consider the input sentence from the very beginning.  Such
an approach is called <emphasis>bottom-up
parsing</emphasis>, and is the topic of the next section.
</para>

</section>
<section id="approaches.sr"> <title> Shift-Reduce Parsing </title>

<para>
The simplest kind of bottom-up parsing is known as shift-reduce
parsing.  The parser repeadly pushes the next input word onto a stack;
this is the <emphasis>shift</emphasis> operation.  If the top
<replaceable>n</replaceable> items on the stack match the
<replaceable>n</replaceable> items on the right-hand side of some
production, then they are all popped off the stack, and the item on
the left-hand side of the production is pushed on the stack.  This
replacement of the top <replaceable>n</replaceable> items with a
single item is the <emphasis>reduce</emphasis> operation.  The parser finishes
when all the input is consumed and there is only one item remaining on the stack,
a parse tree with an <literal>S</literal> node as its root.
</para>

<!--
To do: add examples and motivate more - what are we doing with
bottom up - find little pieces and expand...
-->

<note><para>
Note that the reduce operation may only be applied to the top of the stack.
Reducing items lower in the stack must be done before later items are pushed onto
the stack.
</para></note>

<para>
The shift-reduce parser builds a parse tree during the above process.
If the top of stack holds the word <literal>dog</literal> and if the
grammar has a production <literal>N &rarr; dog</literal> then the reduce
operation causes the word to be replaced with the parse tree for this
production.  For convenience we will represent this tree as
<literal>N(dog)</literal>.  At a later stage, if the top of the stack
holds two items <literal>Det(the) N(dog)</literal> and if the grammar
has a production <literal>NP &rarr; Det N</literal> then the reduce operation
causes these two items to be replaced with <literal>NP(Det(the),
N(dog))</literal>.  This process continues until a parse tree for the
entire sentence has been constructed.  We can see this in action using
the parser demonstration <literal>nltk.draw.srparser</literal>.  To
run this demonstration, use the following commands:
</para>

<programlisting><![CDATA[
>>> from nltk.draw.srparser import demo
>>> demo()
]]></programlisting>

<para>
Six stages of the execution of this parser are shown in
<xref linkend="srparser"/>.
</para>

<!--
To do: use letter identifiers for subfigures.
-->

<figure id="srparser"><title>Six Stages of a Shift-Reduce Parser: initial, after one shift,
after shift reduce shift reduce, after recognizing the second NP, complex NP,
final step</title>
<informaltable frame="all">
<tgroup cols="2"><tbody><row><entry>
<graphic fileref="images/srparser1" scale="24"/>
</entry><entry>
<graphic fileref="images/srparser2" scale="24"/>
</entry></row><row><entry>
<graphic fileref="images/srparser3" scale="24"/>
</entry><entry>
<graphic fileref="images/srparser4" scale="24"/>
</entry></row><row><entry>
<graphic fileref="images/srparser5" scale="24"/>
</entry><entry>
<graphic fileref="images/srparser6" scale="24"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>
A shift-reduce parser may fail to parse the sentence, even though the
sentence is well-formed according to the grammar.  In such cases,
there are no remaining input words to shift, and there is no way to
reduce the remaining items on the stack, as exemplified in the left
example in <xref linkend="sr-conflict"/>.  The parser entered this
blind alley at an earlier stage shown in the middle example in
<xref linkend="sr-conflict"/>, when it reduced instead of shifted.
This situation is called a <emphasis>shift-reduce conflict</emphasis>.
At another possible stage of processing shown in the right example
in <xref linkend="sr-conflict"/>, the parser must choose between two
possible reductions, both matching the top items on the stack:
<literal>V &rarr; V NP PP</literal> or <literal>NP &rarr; NP PP</literal>.
This situation is called a <emphasis>reduce-reduce conflict</emphasis>.
</para>

<!--
To do: diagram showing search tree with success and failure.
-->

<figure id="sr-conflict"><title>Conflict in Shift-Reduce Parsing</title>
<informaltable frame="all">
<tgroup cols="3"><tbody><row><entry>
<graphic fileref="images/srparser7" scale="18"/>
</entry><entry>
<graphic fileref="images/srparser8" scale="18"/>
</entry><entry>
<graphic fileref="images/srparser9" scale="18"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>
Shift-reduce parsers may implement policies for resolving such
conflicts.  For example, they may address shift-reduce conflicts by
shifting only when no reduces are possible, and they may address reduce-reduce conflicts
by favouring the reduce operation that removes the most items from the stack.
No such policies are failsafe however.
</para>

<para>
The advantages of shift-reduce parsers over recursive descent parsers is that they
only build structure that corresponds to the words in the input.  Furthermore, they
only build each substructure once, e.g. <literal>NP(Det(the), N(man))</literal> is
only built and pushed onto the stack a single time, regardless of whether it will later
be used by the <literal>V &rarr; V NP PP</literal> reduction or the <literal>NP &rarr; NP PP</literal>
reduction.
</para>

</section>
</section>

<!--
    <section id="advanced"> <title> Advanced Topics in Parsing (optional) </title>

    <warning><para> Some of the following is out of
    date.  See the <ulink
    url="&refdoc;/nltk.parser-module.html">reference
    documentation for <literal>nltk.parser</literal></ulink> for more
    up-to-date information. </para></warning>

    <para>Parsing as a search problem.  Finding the right parse.</para>

<para>late closure etc</para>

X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions

    </section>
-->

    <section id="nltk"> <title> Parsing in NLTK </title>

    <para></para>

<!--
X.5 implementation
    - how does NLTK do it?
    - simple problems and worked solutions
    - suggested projects (e.g. for your MSc students)
-->

    <section id="nltk.grammars">
      <title> Context Free Grammars </title>

      <para> The <ulink url="&refdoc;/nltk.cfg-module.html"
      ><literal>nltk.cfg</literal></ulink> module defines a set of
      classes that are used to define context free grammars: </para>

      <itemizedlist>
        <listitem><para> The <ulink
        url="&refdoc;/nltk.cfg.Nonterminal-class.html"
        ><literal>Nonterminal</literal></ulink> class is used to
        represent nonterminals.</para>
        </listitem>
        <listitem><para> The <ulink
        url="&refdoc;/nltk.cfg.CFGProduction-class.html"
        ><literal>CFGProduction</literal></ulink> class is used to
        represent CFG productions.</para>
        </listitem>
        <listitem><para> The <ulink
        url="&refdoc;/nltk.cfg.CFG-class.html"
        ><literal>CFG</literal></ulink> class is used to
        represent CFGs.</para>
        </listitem>
      </itemizedlist>

      <section id="nltk.grammars.nonterminal">
        <title> Nonterminals </title>

        <para> <ulink url="&refdoc;/nltk.cfg.Nonterminal-class.html"
        ><literal>Nonterminal</literal></ulink> is a simple class
        that's used to let NLTK distinguish terminals from
        nonterminals.  Each <literal>Nonterminal</literal> is defined
        by a <glossterm>symbol</glossterm>, which is represented by a
        case-sensitive string.  Typical symbols for
        <literal>Nonterminals</literal> are <literal>"S"</literal>
        (for sentence) and <literal>"NP"</literal> (for noun phrases).
        To construct a <literal>Nonterminal</literal>, use the <ulink
        url="&refdoc;/nltk.cfg.Nonterminal-class.html#__init__"
        ><literal>Nonterminal</literal> constructor</ulink>: </para>

<programlisting><![CDATA[
>>> from nltk.cfg import *
>>> S = Nonterminal('S')
<S>
>>> NP = Nonterminal('NP')
<NP>
]]></programlisting>

        <!-- Mention the symbol() method? -->
        <!-- Mention division of nonterminal? -->

        <para> If you are defining many nonterminals at once, you can
        use the <ulink
        url="&refdoc;/nltk.cfg-module.html#nonterminals"
        ><literal>nonterminals()</literal></ulink> function.  This
        function takes a string containing a list of symbol names, and
        returns a list of <literal>Nonterminals</literal> constructed
        from those symbol names: </para>

<programlisting><![CDATA[
>>> VP, Adj, V, N = nonterminals('VP, Adj, V, N')
[<VP>, <Adj>, <V>, <N>]
]]></programlisting>

        <para> When using the <literal>nonterminals()</literal>
        function, you should be always be careful to make sure that
        the order of the variables you define matches the order of the
        nonterminals in the string.</para>
      
      </section> <!-- Nonterminals -->

      <section id="nltk.grammars.cfgproductions">
        <title> Productions </title>

        <para> CFG productions are represented with the <ulink
        url="&refdoc;/nltk.cfg.CFGProduction-class.html"
        ><literal>CFGProduction</literal></ulink> class.  Each
        <literal>CFGProduction</literal> specifies that a nonterminal
        (the <glossterm>left-hand side</glossterm>) can be expanded to
        a sequence of terminals and nonterminals (the
        <glossterm>right-hand side</glossterm>).
        <literal>CFGProductions</literal> are created with the <ulink
        url="&refdoc;/nltk.cfg.CFGProduction-class.html#__init__"
        ><literal>CFGProducition</literal> constructor</ulink>, which
        takes a nonterminal left-hand side, and zero or more terminals
        and nonterminals for the right-hand side. </para>
        
<programlisting><![CDATA[
>>> prod1 = CFGProduction(S, [NP, VP])
S -> NP VP
>>> prod2 = CFGProduction(NP, ['the', Adj, N])
NP -> 'the' Adj N
]]></programlisting>

        <para> The right-hand side may contain any number of elements.
        In particular, for so-called <glossterm>epsilon
        productions</glossterm>, the right-hand side is empty.  When parsing
        natural language, epsilon productions are often used for
        <glossterm>traces</glossterm>, which mark the position from
        which a constituent moved:</para>

<programlisting><![CDATA[
>>> trace = Nonterminal('t')
<t>

# A trace does not generate any text. 
>>> prod3 = CFGProduction(trace, [])
t ->
]]></programlisting>

      </section> <!-- CFGProduction -->

      <section id="nltk.grammars.cfg">
        <title> CFGs </title>

        <para> Context free grammars are encoded by the <ulink
        url="&refdoc;/nltk.cfg.CFG-class.html"
        ><literal>CFG</literal></ulink> class.  A
        <literal>CFG</literal> consists of a special
        <glossterm>start</glossterm> nonterminal, and an ordered list
        of productions.  <literal>CFGs</literal> are created with the
        <ulink url="&refdoc;/nltk.cfg.CFG-class.html#__init__"
        ><literal>CFG</literal> constructor</ulink>: </para>

<programlisting><![CDATA[
>>> grammar = CFG(S, [prod1, prod2, prod3])
<CFG with 3 productions>
]]></programlisting>

        <para> A CFG's start state is returned by the <ulink
        url="&refdoc;/nltk.cfg.CFG-class.html#start"
        ><literal>start</literal></ulink> method; and its list of
        productions is returned by the <ulink
        url="&refdoc;/nltk.cfg.CFG-class.html#productions"
        ><literal>productions</literal></ulink> method: </para>
        
<programlisting><![CDATA[
>>> grammar.start()
<S>
>>> grammar.productions()
[ S -> NP V,
  NP -> 'the' Adj N,
  t ->
]
]]></programlisting>

      <para>
        A complete example of a toy grammar is given below:
      </para>


<programlisting><![CDATA[
>>> from nltk.cfg import *

>>> S, VP, NP, PP = nonterminals('S, VP, NP, PP')
>>> V, N, P, Name, Det = nonterminals('V, N, P, Name, Det')

>>> productions = (
...     CFGProduction(S, [NP, VP]),
...     CFGProduction(NP, [Det, N]),
...     CFGProduction(VP, [V, NP]),
...     CFGProduction(VP, [V, NP, PP]),
...     CFGProduction(NP, [Det, N, PP]),
...     CFGProduction(PP, [P, NP]),

...     CFGProduction(NP, ['I']),   CFGProduction(Det, ['the']),
...     CFGProduction(Det, ['a']),  CFGProduction(N, ['man']),
...     CFGProduction(V, ['saw']),  CFGProduction(P, ['in']),
...     CFGProduction(P, ['with']), CFGProduction(N, ['park']),
...     CFGProduction(N, ['dog']),   CFGProduction(N, ['telescope'])
... )
>>> grammar = CFG(S, productions)
]]></programlisting>


<note><para> All of the grammars shown here use atomic non-terminal
symbols, e.g. <literal>VP</literal>.  For extensions that support
feature-based grammars and parsers, please refer to the
<literal>nltk_contrib.mit.rspeer.parser</literal> module.
</para></note>

      </section> <!-- CFG -->
    </section> <!-- Context Free Grammars -->

    <section id="nltk.tree">
      <title> Trees </title>

      <para> The <ulink url="&refdoc;/nltk.tree-module.html"
      ><literal>nltk.tree</literal></ulink> module defines classes and
      functions for working with trees.</para>

      <note>
        <para> The <literal>nltk.tree</literal> module currently has
        no direct support for representing movement, traces, and
        co-indexing.  We plan to extend the class to support these
        features more fully in the future. </para>
      </note>

      <itemizedlist>
        <listitem> <para> The node value is a string containing the
        tree's constituent type (e.g., "NP" or "VP").
        </para></listitem>

        <listitem> <para> The children encode the hierarchical
        contents of the tree.  Each child is either a
        <literal>TreeToken</literal> or a <literal>Token</literal>. </para>
        </listitem>
      </itemizedlist>
    
      <note>
        <para> Although the <literal>Tree</literal> class is usually
        used for encoding syntax trees, it can be used to encode
        <emphasis>any</emphasis> homogeneous hierarchical structure
        that spans a text (such as morphological structure or
        discourse structure).  In the general case, leaves and node
        values do not have to be strings. </para>
      </note>

      <para> A <literal>Tree</literal> with node value
        <replaceable>n</replaceable> and children
        <replaceable>c<subscript>1</subscript></replaceable>,
        <replaceable>c<subscript>2</subscript></replaceable>, ...
        <replaceable>c<subscript>n</subscript></replaceable> is
        written <literal>(<replaceable>n</replaceable>:
        <replaceable>c<subscript>1</subscript></replaceable>
        <replaceable>c<subscript>2</subscript></replaceable> ...
        <replaceable>c<subscript>n</subscript></replaceable>)</literal>.
        <literal>Tree</literal>s are created with the <ulink
        url="&refdoc;/nltk.tree.TreeToken-class.html#__init__"
        ><literal>Tree constructor</literal></ulink>, which takes a
        node value and zero or more children:
      </para>

<programlisting><![CDATA[
>>> from nltk.tree import *
# A tree with one child (a token):
>>> tree1 = Tree('NP', ['John'])
(NP: <John>)

# A tree with two children (both tokens):
>>> tree2 = Tree('NP', ['the', 'man'])
(NP: <the> <man>)

# A tree with two children (a token and a subtree):
>>> tree3 = Tree('VP', ['saw', tree2])
(VP: <saw> (NP: <the> <man>))
]]></programlisting>

        <para> A <literal>Tree</literal>'s node value is accessed with
        the <literal>node</literal> property: </para>

<programlisting><![CDATA[
>>> tree1.node
NP
]]></programlisting>

        <para> A <literal>Tree</literal>'s children are accessed by
        indexing: </para>

<programlisting><![CDATA[
>>> tree3[0]
<saw>
>>> tree3[1]
(NP: <the> <man>)
>>> for child in tree3:
...     print child
<saw>
(NP: <the> <man>)
]]></programlisting>

        <para> The printed representation for complex trees
        can be difficult to read.  In these cases, the <ulink
        url="&refdoc;/nltk.tree.Tree-class.html#draw"
        ><literal>draw</literal></ulink> method can be very useful.
        This method opens a new window, containing a graphical
        representation of the tree.  </para>

<programlisting><![CDATA[
#>>> tree3.draw()
]]></programlisting>

<note><para>This and the following graphical interface are broken in NLTK 1.4, but will
be fixed in NLTK 1.5.</para></note>

        <para> The tree display window allows you to zoom in and out;
        to collapse and expand subtrees; and to print the graphical
        representation to a postscript file. </para>

        <para> To compare multiple trees in a single
        window, use the <ulink
        url="&refdoc;/nltk.draw.tree-module.html#draw_trees"
        ><literal>draw_trees()</literal></ulink> method, from the
        <ulink url="&refdoc;/nltk.draw.tree-module.html"
        ><literal>nltk.draw.tree</literal> module: </ulink> </para>

<programlisting><![CDATA[
#>>> from nltk.draw.tree import draw_trees
#>>> draw_trees(tree1, tree2, tree3)
]]></programlisting>

        <para> The <literal>Tree</literal> class implements a number
        of other useful methods.  See the <ulink
        url="&refdoc;/nltk.tree.Tree-class.html"
        ><literal>Tree</literal> reference documentation</ulink> for
        more information about these methods. </para>

<programlisting><![CDATA[
>>> tree3.leaves()
[<saw>, <the>, <man>]
>>> tree3.height()
3
]]></programlisting>

        <!-- Mention tree positions? -->
        
      <section id="nltk.tree.initializing">
        <title> Initializing Trees from Strings </title>

        <para> A convenient way to initialize a large tree
        is from a labelled bracketing:
        </para>

<programlisting><![CDATA[
>>> sent = '(VP (VBD saw) (NP (DT the) (NN man)))' 
>>> tree = Tree.parse(sent)
(VP: (VBD: <saw>) (NP: (DT: <the>) (NN: <man>)))
]]></programlisting>
        
      </section> <!-- Reading Trees -->

      <section id="nltk.tree.reading">
        <title> Reading Trees from the Treebank Corpus </title>

        <para> The <ulink url="&refdoc;/nltk.corpus-module.html"
        ><literal>nltk.corpus</literal></ulink> module defines the
        <ulink url="&refdoc;/nltk.corpus-module.html#treebank"
        ><literal>treebank</literal></ulink> corpus, which contains a
        collection of hand-annotated parse trees for English
        text. </para> <!-- Add a reference to LDC here? -->

<programlisting><![CDATA[
>>> from nltk.corpus import treebank

# Get a list of the items in the corpus. 
>>> treebank.items()
['wsj_0001.prd', 'wsj_0002.prd', 'wsj_0003.prd', ...]

# Read the first file as a list of Trees. 
#>>> treebank.read('parsed/wsj_0001.prd')
[ (S: (NP-SBJ: (NP: <Pierre> <Vinken>) ...)
        (VP: will ...)),
  (S: (NP-SBJ: <Mr.> <Vinken>)
        (VP: <is> ...)) ]
]]></programlisting>
        
      </section> <!-- Reading Trees -->

    </section> <!-- Trees and TreeTokens -->

  <section id="nltk.interface"> <title> The Parser Interface </title>

    <para>The <ulink url="&refdoc;/nltk.parser-module.html"
    ><literal>parser</literal></ulink> module defines the <ulink
    url="&refdoc;/nltk.parser.ParserI-class.html"
    ><literal>ParserI</literal></ulink> interface, which defines the
    set of methods which all parsers should support.  The
    <literal>ParserI</literal> interface defines two methods: </para>

      <!-- I say "n best" and "in descending order of quality" here,
      but so far we've only talked about grammars with no notion of
      parse quality.  Will that be confusing? -->
      <itemizedlist>
        <listitem><para> The <ulink
        url="&refdoc;/nltk.parser.ParserI-class.html#parse"
        ><literal>parse</literal></ulink> method returns the single
        best parse for a given text.  The text is represented as a
        list of <literal>Tokens</literal>.  If no parses are found for the
        given text, then <literal>parse</literal> returns
        <literal>None</literal>. </para>
        </listitem>
        <listitem> <para> The <literal>get_parse_list</literal>
        method returns a list of the parses for the given text.</para>
        </listitem>
      </itemizedlist>

      <para> For example, here is what the recursive descent parser
      generates for a simple sentence and grammar: </para>
        
<programlisting><![CDATA[
# Tokenize a simple sentence 
>>> from nltk.tokenizer import WhitespaceTokenizer 
>>> sentence = 'I saw a man in the park' 
>>> sent = Token(TEXT=sentence) 
>>> WhitespaceTokenizer().tokenize(sent) 
        
# Build a parser 
>>> from nltk.parser import RecursiveDescentParser 
>>> parser = RecursiveDescentParser(grammar, LEAF='TEXT')
        
# Get all parses. 
>>> for p in parser.get_parse_list(sent):
>>>     print p
(S:
  (NP: <I>)
  <saw>
  (NP:
    (Det: <a>)
    (N: <man>)
    (PP: (P: <in>) (NP: (Det: <the>) (N: <park>)))))
(S:
  (NP: <I>)
  (VP:
    (V: <saw>)
    (NP: (Det: <a>) (N: <man>))
    (PP: (P: <in>) (NP: (Det: <the>) (N: <park>)))))
]]></programlisting>

    </section> <!-- The parser interface -->
    
    <section id="nltk.ShiftReduceParser">
      <title>The Shift Reduce Parser </title>

      <para> The <ulink url="&refdoc;/nltk.parser-module.html"
      ><literal>nltk.parser</literal></ulink> module defines <ulink
      url="&refdoc;/nltk.parser.ShiftReduceParser-class.html"
      ><literal>ShiftReduceParser</literal></ulink>, a simple
      non-backtracking implementation of a bottom-up shift-reduce
      parser.  Since this parser does not implement any backtracking,
      it is not guaranteed to find a parse for a text, even if one
      exists.  Furthermore, it will always find at most one parse,
      even if more parses exist. </para>

      <para> Shift reduce parsers are created from
      <literal>CFGs</literal> by the <ulink
      url="&refdoc;/nltk.parser.ShiftReduceParser-class.html#__init__"
      ><literal>ShiftReduceParser</literal> constructor</ulink>.  The
      constructor takes an optional argument <literal>trace</literal>.
      If <literal>trace</literal> is greater than zero, then the
      parser will describe the steps that it takes as it parses a
      text.  Higher values of <literal>trace</literal> produce more
      verbose descriptions.
      </para>

<programlisting><![CDATA[
>>> from nltk.parser import * 

# Construct a new parser for the grammar 
>>> sr_parser = ShiftReduceParser(grammar, LEAF='TEXT') 
]]></programlisting>

      <para> The following example shows the trace output generated by
      <literal>sr_parser</literal> on a simple sentence: </para>
          
<programlisting><![CDATA[
>>> sent = Token(TEXT='I saw a man') 
>>> WhitespaceTokenizer().tokenize(sent) 
>>> sr_parser.trace()
>>> sr_parser.parse(sent)
    [ * 'I' 'saw' 'a' 'man']
  S [ 'I' * 'saw' 'a' 'man']
  R [ <NP> * 'saw' 'a' 'man']
  S [ <NP> 'saw' * 'a' 'man']
  R [ <NP> <V> * 'a' 'man']
  S [ <NP> <V> 'a' * 'man']
  R [ <NP> <V> <Det> * 'man']
  S [ <NP> <V> <Det> 'man' * ]
  R [ <NP> <V> <Det> <N> * ]
  R [ <NP> <V> <NP> * ]
  R [ <NP> <VP> * ]
  R [ <S> * ]
>>> print sent['TREE']
(S: (NP: <I>) (VP: (V: <saw>) (NP: (Det: <a>) (N: <man>))))
]]></programlisting>

      <para> NLTK also defines a graphical demonstration tool for the
      shift reduce parser: </para>
        
<programlisting><![CDATA[
>>> from nltk.draw.srparser import demo
>>> demo()
]]></programlisting>

    </section> <!-- ShiftReduceParser -->

    <section id="nltk.RecursiveDescentParser">
      <title>The Recursive Descent Parser </title>

      <para> The <ulink url="&refdoc;/nltk.parser-module.html"
      ><literal>nltk.parser</literal></ulink> module defines <ulink
      url="&refdoc;/nltk.parser.RecursiveDescentParser-class.html"
      ><literal>RecursiveDescentParser</literal></ulink>, a simple
      recursive implementation of a top-down parser.  Unlike the
      shift-reduce parser, this parser is guaranteed to find all
      parses for a sentence.  But because it's a simple recursive
      top-down parser, it can enter an infinite loop if the grammar
      contains a left-recursive production. </para>

      <para> Recursive descent parsers are created from
      <literal>CFGs</literal> by the <ulink
      url="&refdoc;/nltk.parser.RecursiveDescentParser-class.html#__init__"
      ><literal>RecursiveDescentParser</literal> constructor</ulink>.
      The constructor takes an optional argument
      <literal>trace</literal>.  As with the shift reduce parser, this
      value specifies how verbosely the parser should describe the
      steps that it takes as it parses a text.
      </para>

<programlisting><![CDATA[
>>> from nltk.parser import * 

# Construct a new parser for the grammar 
>>> rd_parser = RecursiveDescentParser(grammar) 
]]></programlisting>

      <para> The following example shows the trace output generated by
      <literal>rd_parser</literal> on a simple sentence, where
      <literal>E</literal> = expand and <literal>M</literal> = match.
      Material to the left of the asterisk has been matched and
      incorporated into the parse tree.
      </para>
          
<programlisting><![CDATA[
>>> sent = Token(TEXT='I saw a man') 
>>> WhitespaceTokenizer().tokenize(sent) 
>>> rd_parser.trace()
>>> parses = rd_parser.get_parse_list(sent)
Parsing 'I saw a man'
    [ * <S> ]
  E [ * <NP> <VP> ]
  E [ * <Det> <N> <VP> ]
  E [ * <the> <N> <VP> ]
  E [ * <a> <N> <VP> ]
  E [ * <Det> <N> <PP> <VP> ]
  E [ * <the> <N> <PP> <VP> ]
  E [ * <a> <N> <PP> <VP> ]
  E [ * <I> <VP> ]
  M [ <I> * <VP> ]
  E [ <I> * <V> <NP> ]
  E [ <I> * <saw> <NP> ]
  M [ <I> <saw> * <NP> ]
  E [ <I> <saw> * <Det> <N> ]
  E [ <I> <saw> * <the> <N> ]
  E [ <I> <saw> * <a> <N> ]
  ...
  E [ <I> <saw> <a> * <park> <PP> <PP> ]
  E [ <I> <saw> <a> * <dog> <PP> <PP> ]
  E [ <I> <saw> <a> * <telescope> <PP> <PP> ]
  E [ <I> <saw> * <I> <PP> ]
>>> for tree in parses: print tree 
(S: (NP: <I>) (VP: (V: <saw>) (NP: (Det: <a>) (N: <man>))))
]]></programlisting>

      <para> NLTK also defines a graphical demonstration tool for the
      recursive descent parser: </para>
        
<programlisting><![CDATA[
>>> from nltk.draw.rdparser import demo
>>> demo()
]]></programlisting>

    </section> <!-- RecursiveDescentParser -->

  </section> <!-- Parsing in NLTK -->

  &index;
</article>
