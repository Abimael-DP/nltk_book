<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.1//EN" [
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Parsing</title>
  </articleinfo>

  <section> <title> Introduction </title>

    <note><para>This section is to be written.  For now, please see
    other published discussions of parsing, such as Jurafsky and
    Martin, Chapter 10</para></note>

<!--
    <note>
      <para>The introduction should include definitions of:
      <glossterm>constituant</glossterm>, <glossterm>syntax
      tree</glossterm>, etc.  This discussion can be at a pretty high
      level.  Depending on how much background we want to have, this
      section might have subsections... </para>
    </note>

    <para> Sentences have internal structure.  Describe structure
    (hierarchical, etc).  NLTK provides us with classes for
    representing syntax trees.  It also provides us with an interface
    for deriving this structure.  This is called
    <glossterm>parsing</glossterm>. </para>
  </section>
-->



  </section> <!-- Introduction -->

  <section> <title> Grammars and Lexicons </title>

    <para>A <glossterm>grammar</glossterm> is a formal specification
    for the structure of well-formed
    sentences in some language.  At present, only context-free grammars (CFGs) can be
    represented in NLTK.  A CFG consists of a set of context-free rules.  Context-free
    rules have the form <literal>X -&gt; Y</literal> where <literal>X</literal> is
    a non-terminal, and <literal>Y</literal> is a list of terminals and non-terminals.
    <literal>X</literal> and <literal>Y</literal> are known as the left-hand side and
    right-hand side respectively.
    </para>

    <para>
    In the simplest case, non-terminals and terminals are just Python
    strings.  However, it is possible to use any immutable Python object as a
    non-terminal or terminal.
    </para>

    <note><para>The NLTK chart parser makes two additional assumptions about rules.
    First, if a terminal symbol occurs on the right hand side of a rule, it must
    be the only element on the right hand side.  Second, terminal symbols must
    be Python strings, for they are matched against the <literal>type</literal>
    attribute of a token.  Thus, the form of "lexical rules" must be
    <literal>N -&gt; 'cat'</literal>.  Using rules like
    <literal>NP -&gt; 'the' N</literal> will produce unintended results
    (either <literal>the</literal> will be treated as the name of a non-terminal,
    of <literal>N</literal> will be ignored).  In general, this limitation on the
    form of lexical rules does not pose any problems.
    </para></note>

    <para>A grammar can be represented as a tuple of rules, while a lexicon
    can be represented as a tuple of lexical rules.  The only class we need
    to define then is <literal>Rule</literal>.

    </para>

    <section> <title> Rules </title>

      <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.html"
      ><literal>nltk.rule</literal></ulink> module defines the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html"
      ><literal>Rule</literal></ulink> class, which is used to represent
      context free rules.  A <literal>Rule</literal> consists of a
      left-hand side and a right-hand side.</para>

      <itemizedlist>
        <listitem><para>The left-hand side is a single non-terminal, which
        may be any Python object.  In the simplest case it is just a string
        (e.g. "NP" or "VP").
        </para></listitem>

        <listitem><para>The right-hand side is a tuple of non-terminals and
        terminals, which may be any Python object.  In the simplest case
        these are strings (e.g. "Det", "the").
        </para></listitem>
      </itemizedlist>

      <note><para>For the NLTK chart parser, the right-hand side of a rule must
      be either a tuple of non-terminals, or a tuple consisting of exactly one
      terminal (e.g. ("the",))</para></note>

      <para><literal>Rule</literal>s are created with the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html#__init__"
      ><literal>Rule constructor</literal></ulink>, which takes a left-hand side
      and a right-hand side:</para>

<programlisting>
<emphasis># A typical grammar rule S -&gt; NP VP:</emphasis>
&prompt;<command> rule1 = Rule('S', ('NP', 'VP'))</command>
S -> NP VP

<emphasis># A typical lexical rule Det -&gt; 'the':</emphasis>
&prompt;<command> rule2 = Rule('Det', ('the',))</command>
Det -> the
</programlisting>

      <para>A <literal>Rule</literal>'s left-hand side and right-hand
      side are accessed with
      the <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html#lhs"
      ><literal>lhs</literal></ulink>
      and <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html#rhs"
      ><literal>rhs</literal></ulink> methods:
      </para>

<programlisting>
&prompt;<command> rule1.lhs()</command>
'S'

&prompt;<command> rule2.rhs()</command>
('the',)
</programlisting>

      <para> A <literal>Rule</literal>'s right-hand side can also be accessed with
      standard sequence operators: </para>

<programlisting>
&prompt;<command> rule1[0]</command>
'NP'

&prompt;<command> rule2[1]</command>
IndexError: tuple index out of range

&prompt;<command> len(rule1)</command>
2

&prompt;<command> 'the' in rule2</command>
1

&prompt;<command> for cat in rule1: </command>
&prompt2;<command>     print cat</command>
NP
VP
</programlisting>

    </section> <!-- Rules -->

    <section> <title> Building Grammars and Lexicons from Rules </title>

    <para>Grammars and Lexicons can easily be built up from
    <literal>Rule</literal>s as shown in the following examples:</para>

<programlisting>
<emphasis># A simple grammar:</emphasis>
<command>
grammar = (
    Rule('S',('NP','VP')),
    Rule('NP',('Det','N')),
    Rule('NP',('Det','N', 'PP')),
    Rule('VP',('V','NP')),
    Rule('VP',('V','PP')),
    Rule('VP',('V','NP', 'PP')),
    Rule('VP',('V','NP', 'PP', 'PP')),
    Rule('PP',('P','NP'))
)
</command>

<emphasis># A simple lexicon:</emphasis>
<command>
lexicon = (
    Rule('NP',('I',)),
    Rule('Det',('the',)),
    Rule('Det',('a',)),
    Rule('N',('man',)),
    Rule('V',('saw',)),
    Rule('P',('in',)),
    Rule('P',('with',)),
    Rule('N',('park',)),
    Rule('N',('telescope',))
)
</command>
</programlisting>

    </section> <!-- Building Grammars and Lexicons from Rules -->

    <section> <title> Dotted Rules </title>

      <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.html"
      ><literal>nltk.rule</literal></ulink> module defines the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.DottedRule.html"
      ><literal>DottedRule</literal></ulink> class, which is used to represent
      the dotted rules used by a chart parser.  A <literal>DottedRule</literal>
      consists of a left-hand side, a right-hand side, and the dot position.</para>

      <para><literal>DottedRule</literal>s are created with the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.DottedRule.html#__init__"
      ><literal>DottedRule constructor</literal></ulink>, which takes a left-hand side,
      a right-hand side, and a position:</para>

<programlisting>
<emphasis># A dotted rule with position 0 (default value omitted):</emphasis>
&prompt;<command> dr1 = DottedRule('S', ('NP', 'VP'))</command>
S -> * NP VP

<emphasis># A dotted rule with position 0 (default value supplied):</emphasis>
&prompt;<command> dr1 = DottedRule('S', ('NP', 'VP'), 0)</command>
S -> * NP VP

<emphasis># A dotted rule with position 1:</emphasis>
&prompt;<command> dr2 = DottedRule('S', ('NP', 'VP'), 1)</command>
S -> NP * VP

<emphasis># A dotted rule with position 2:</emphasis>
&prompt;<command> dr3 = DottedRule('S', ('NP', 'VP'), 2)</command>
S -> NP VP *
</programlisting>

    <para>Another way to construct a <literal>DottedRule</literal> is from
    a <literal>Rule</literal>.  The <literal>Rule</literal> class has a
    <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html#drule"
    ><literal>drule</literal></ulink> member function which returns the
    dotted version of the rule (with position 0).
    </para>

<programlisting>
&prompt;<command> rule1 = Rule('S', ('NP', 'VP'))</command>
&prompt;<command> rule1.drule()</command>
S ->  * NP VP
</programlisting>

    <para><literal>DottedRule</literal> inherits the member functions of
    <literal>Rule</literal>.  In addition, it defines
    <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.DottedRule.html#pos"
    ><literal>pos</literal></ulink> which returns the dot position,
    <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.DottedRule.html#next"
    ><literal>next</literal></ulink> which returns the next right-hand side
    element following the dot, and
    <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.DottedRule.html#shift"
    ><literal>shift</literal></ulink>
    which returns a new rule with the dot shifted one position to the right.
    </para>

<programlisting>
&prompt;<command> dr1.pos()</command>
0

&prompt;<command> dr1.next()</command>
'NP'

&prompt;<command> dr1.shift()</command>
S -> NP * VP

&prompt;<command> dr3.shift()</command>
IndexError: Attempt to move dot position past end of rule

</programlisting>

    <para><literal>DottedRule</literal> defines a function
    <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.DottedRule.html#complete"
    ><literal>complete</literal></ulink> which tests to see if the dotted rule
    is complete (i.e. the dot is in the rightmost position).</para>

<programlisting>
&prompt;<command> dr1.complete()</command>
0

&prompt;<command> dr3.complete()</command>
0
</programlisting>


    </section>

  </section> <!-- Grammars -->

  <section> <title> Encoding Syntax Trees </title>

    <note>
      <para> The <literal>nltk.tree</literal> module currently has
      only minimal support for representing movement, traces, and
      co-indexing.  We plan to extend the class to support these
      features more fully in the future. </para>
    </note>

    <section> <title> Trees </title>

      <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.tree.html"
      ><literal>nltk.tree</literal></ulink> module defines the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html"
      ><literal>Tree</literal></ulink> class, which is used to
      represent syntax trees.  A <literal>Tree</literal> consists of a
      <glossterm>node value</glossterm>, and one or more
      <glossterm>children</glossterm>. </para>

      <itemizedlist>
        <listitem> <para> The node value is a string containing the
        tree's constituant type (e.g., "NP" or "VP").
        </para></listitem>

        <listitem> <para> The children encode the hierarchical
        contents of the tree.  Each child is either a
        <glossterm>leaf</glossterm> or a
        <glossterm>subtree</glossterm>.</para></listitem>
      </itemizedlist>
    
      <note>
        <para> Although the <literal>Tree</literal> class is usually
        used for encoding syntax trees, it can be used to encode
        <emphasis>any</emphasis> homogenous hierarchical structure
        that spans a text (such as morphological structure or
        discourse structure).  In the general case, leaves and node
        values do not have to be strings. </para>
      </note>

      <para> A <literal>Tree</literal> with node value
      <replaceable>n</replaceable> and children
      <replaceable>c<subscript>1</subscript></replaceable>,
      <replaceable>c<subscript>2</subscript></replaceable>, ...
      <replaceable>c<subscript>n</subscript></replaceable> is written
      <literal>(<replaceable>n</replaceable>:
      <replaceable>c<subscript>1</subscript></replaceable>,
      <replaceable>c<subscript>2</subscript></replaceable>, ...
      <replaceable>c<subscript>n</subscript></replaceable>)</literal>.
      <literal>Tree</literal>s are created with the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html#__init__"
      ><literal>Tree constructor</literal></ulink>, which takes a node
      value and zero or more children: </para>

<programlisting>
    <emphasis># A tree with one child, a leaf:</emphasis>
    &prompt;<command> tree1 = Tree('NP', 'John')</command>
    ('NP': 'John')

    <emphasis># A tree with two children, both of which are leaves:</emphasis>
    &prompt;<command> tree2 = Tree('NP', 'the', 'man')</command>
    ('NP': 'the' 'man')

    <emphasis># A tree with two children, one leaf and one subtree:</emphasis>
    &prompt;<command> tree3 = Tree('VP', 'saw', tree2)</command>
    ('VP': 'saw' ('NP': 'the' 'man'))
</programlisting>

      <para> A <literal>Tree</literal>'s node value is accessed with
      the <ulink url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html#node"
      ><literal>node</literal></ulink> method: </para>

<programlisting>
    &prompt;<command> tree1.node()</command>
    'NP'
</programlisting>

      <para> A <literal>Tree</literal>'s children are accessed with
      standard sequence operators: </para>

<programlisting>
    &prompt;<command> tree3[0]</command>
    'saw'
    &prompt;<command> tree3[1]</command>
    ('NP': 'the' 'man')
    &prompt;<command> len(tree3)</command>
    2
    &prompt;<command> 'saw' in tree3</command>
    1
    &prompt;<command> for child in tree3: </command>
    &prompt2;<command>     print child</command>
    saw 
    ('NP': 'the' 'man')
    &prompt;<command> [child.upper() for child in tree2] </command>
    ['THE', 'MAN']
    &prompt;<command> tree3[:] </command>
    ('saw', ('NP': 'the' 'man'))
</programlisting>

      <para> The printed representation for complex
      <literal>Tree</literal>s can be difficult to read.  In these
      cases, the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html#draw"
      ><literal>draw</literal></ulink> method can be very useful.  This
      method opens a new window, containing a graphical representation
      of the tree.  </para>

<programlisting>
    &prompt;<command> tree3.draw()</command>
</programlisting>

      <para> The tree display window allows you to zoom in and out; to
      collapse and expand subtrees; and to print the graphical
      representation to a postscript file. </para>

      <para> The <literal>Tree</literal> class implements a number of
      other useful methods.  See the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html"
      ><literal>Tree</literal> reference documentation</ulink> for more
      information about these methods. </para>

<programlisting>
    &prompt;<command> tree3.leaves()</command>
    ('saw', 'the', 'man')
    &prompt;<command> tree3.height()</command>
    3
    &prompt;<command> tree3.nodes()</command>
    ('VP': ('NP':))
</programlisting>

    </section> <!-- Trees -->

    <section> <title> Tree Tokens </title>

      <para> NLTK makes a distinction between <glossterm>tree
      type</glossterm>s and <glossterm>tree token</glossterm>s, that
      is analogous to the distinction between word types and word
      tokens.  In particular, a tree token is an individual occurance
      of a tree type in a text; and a tree type is an abstract syntax
      tree, without context. </para>

      <para> Tree tokens are represented with the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.TreeToken.html"
      ><literal>TreeToken</literal></ulink> class.
      <literal>TreeToken</literal>s behave very much like
      <literal>Tree</literal>s, except that the leaves of a
      <literal>TreeToken</literal> are word tokens, not word types.  
      </para>

    </section> <!-- TreeTokens -->

  </section> <!-- Encoding Syntax Trees -->

  <section> <title> The Parser Interface </title>

    <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.html"
    ><literal>parser</literal></ulink> module defines the
    <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.ParserI.html"
    ><literal>ParserI</literal></ulink> class, which is the standard interface
    which all parsers should support.  This class should only be used in the
    definition of other parser classes, which should inherit from the
    <literal>ParserI</literal> class.</para>

    <para>The <literal>ParserI</literal> class defines
    <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.ParserI.html#parse"
    ><literal>parse</literal></ulink>, which takes a list of tokens as its argument
    and returns a list of <literal>TreeToken</literal>s.
    The <literal>parse</literal> function has an optional second argument
    which specifies the maximum number of parses to return.  No program should
    call the <literal>ParserI.parse</literal> method.  Derived classes must
    implement their own <literal>parse</literal> method.</para>

    <para>A parser returns an empty list of trees in the situation where it was
    unable to assign a tree to the list of tokens.  This happens in situations
    where the list of tokens forms an ungrammatical sentence, or when the grammar
    is deficient, or when the search strategy of the parser does not explore the
    section of the search space where the parse tree(s) are found.</para>

    <para>A parser returns
    more than one tree in the case of syntactic ambiguity.  The sentence being
    parsed may be genuinly ambiguous, or the grammar may be deficient.  Statistical
    parsers usually return the most likely parse (based on training data), or the
    set of <replaceable>n</replaceable> most likely parses.  The same interface
    applies to these cases.  (Where the probability of the parse tree needs to
    be returned, we assume this is stored in the root node of the tree.)  When
    asked to return <replaceable>n</replaceable> parses, we assume such a parser
    will rank parses in order of decreasing likelihood, and return the
    <replaceable>n</replaceable>-best parses.</para>

    <para>Parsers may be used anywhere where there is need for processing
    sequences with a grammar.  In the most common case, the sequence consists
    of a string of words forming a sentence.  However, other cases are possible,
    e.g. the string of characters forming a syllable, the string of morphemes
    forming a word, the string of sentences forming a text.  We could also have
    grammars over subsequences.  For example, the collection of XML elements in
    a document forms a subsequence of the document, and we could specify a grammar
    over those elements alone, ignoring document content.</para>

    <para>The <literal>ParserI</literal> class also defines a
    <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.ParserI.html#parse"
    ><literal>parse_types</literal></ulink> method, which takes a list of token types
    (e.g. strings) and returns a list of <literal>Tree</literal>s.  This is
    useful in the case where the sentence to be parsed did not come from a
    tokenized text.  This method works by converting the list of types into
    a list of tokens, then calling <literal>parse</literal> as before.</para>

  </section> <!-- The Parser Interface -->

  <section> <title> Simple Parsers </title>

    <para>Simple top-down and bottom-up parsers can easily be defined
    using classes which inherit from <literal>ParserI</literal>.
    Here is some pseudocode to use as the basis of two simple modules.
    Writing these modules is left as an exercise for the reader.
    More work is required in order to return trees.</para>

<programlisting>
# elementary top-down parser
def tdparse(goal, sent): 
    if goal == sent == empty:
        pass # we're finished
    else:
        if goal[0] == sent[0]:
	    tdparse(goal[1:], sent[1:])
        else:
            for rule in grammar:
                if rule.lhs() == goal[0]:
                    make a local copy of the goal list
                    goal[:1] = rule.rhs()
                    tdparse(goal, sent)

# left-corner parser
def lcparse(goal, sent):
    # this is like tdparse, except the step which iterates over the rules
    # of the grammar only considers rules whose "left corner" matches the next
    # word of the input stream.  The left corner of a lexical rule is the
    # content of the rule.  The left corner of a non-lexical rule R is the left
    # corner of R[0], the first element on the RHS of the rule.

# elementary bottom-up parser
def buparse(sent):
    if sent == [S]:
        pass # we're finished
    else:
        for rule in grammar:
	    does rule.rhs() match any sublist of sent?
            if so, replace the substring with the LHS of the rule
            buparse(sent)
</programlisting>

    <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.srparser_template.html"
    ><literal>srparser_template</literal></ulink> module defines the
    <ulink url="http://nltk.sourceforge.net/ref/nltk.srparser_template.SRParser.html"
    ><literal>SRParser</literal></ulink> class, which is a template for a
    shift-reduce parser.</para>

  </section>


  <section> <title> Chart Parsing </title>

    <para>The elementary parsers discussed above suffer from a major
    efficiency problem.  During processing, they typically build and
    discard the same structure many times over.  The standard solution
    to this general kind of problem involves dynamic programming in
    which intermediate results are kept on a blackboard (the chart) for
    later reuse.
    </para>

    <para>In general, a parser hypothesizes constituents based on the
    the grammar and on the current state of its knowledge about the
    constituents which are already attested.  Any given constituent may
    be proposed during the search of a blind alley; there is no way of
    knowing at the time whether the constituent will be used in any of
    the final parses which are reported.  Locally, what we know about a
    constituent is the C{Rule} which licenses its node and immediate
    daughter nodes (i.e. the "local tree").  Also, in the case of chart
    parsing (which is related to bottom-up parsing), we know the whole
    subtree of this node, how it connects to the tokens of the sentence
    being parsed, and its span within the sentence (i.e. location).
    In a chart parser, these these three things: rule, subtree and location,
    are stored in a special structure called an edge.</para>

    <section> <title> Edges </title>

      <para>In order to understand how edges work, it is first necessary to
      recall NLTK's notion of <literal>Location</literal>.  A location in
      a list of tokens is analgous to a Python slice.  The third token has
      the location <literal>@[2:3]</literal>.  It is helpful to think of the numbers as
      referring not to tokens but to imaginary points between the tokens
      (the "interstices").  Each location is then like an edge connecting
      two adjacent points.  This is where the name "edge" comes from.
      </para>

      <para>It is easy to generalize the idea of edges.  The location which
      spans the third and fourth tokens <literal>@[2:4]</literal>, is essentially
      a longer edge which connects non-adjacent points.</para>

      <para>The notion of an edge is best communicated graphically.  The following
      code sample creates some edges for the phrase <literal>the park</literal>,
      displaying them in text on the screen, and also using tkinter.</para>

<programlisting>
from nltk.chartparser import *

tok_sent = [Token('the', 0), Token('park', 1)]
loc = Location(0,2)
chart = Chart(loc)

dr0 = DottedRule('Det', ('the',), 1)
tt0 = TreeToken(dr0.lhs(), tok_sent[0])
edge0 = Edge(dr0, tt0, Location(0))
chart.insert(edge0)
    
dr1 = DottedRule('N', ('park',), 1)
tt1 = TreeToken(dr1.lhs(), tok_sent[1])
edge1 = Edge(dr1, tt1, Location(1))
chart.insert(edge1)
    
dr2 = DottedRule('NP', ('Det','N'), 2)
tt2 = TreeToken(dr2.lhs(), tt0, tt1)
edge2 = Edge(dr2, tt2, Location(0,2))
chart.insert(edge2)
    
chart.draw()

from nltk.draw.chart import *
ChartView(Tkinter.Tk(), chart, tok_sent)
Tkinter.mainloop()
</programlisting>

      <para>The purpose of an edge is to store a <emphasis>hypothesis</emphasis>
      about the syntactic structure of the sentence.  Such hypotheses may be
      either complete or partial.  A complete hypothesis is represented as an
      edge decorated with a <literal>DottedRule</literal> where the dot position is
      on the right edge of the rule.  This means that the entire right-hand side
      has been processed and that the associated <literal>TreeToken</literal> is
      complete.  A partial hypothesis is represented as an edge decorated with
      a rule where the dot position is in an internal position, e.g.
      <literal>NP -&gt; Det * N</literal>.  This means that only part of the
      right-hand side has been processed and that the associated tree is
      incomplete.  The symbol immediately to the right of the
      dot position is the next symbol (terminal or non-terminal) to be processed for
      this edge.</para>

      <para>There is a special case where edges have zero width.  Such an edge is
      actually a self-loop, starting and ending at the same index position.
      Zero-width edges represent the hypothesis that a syntactic constituent
      <emphasis>begins</emphasis> at this location.</para>

      <para>In the process of chart parsing, edges are combined with other edges to
      form new edges which are then added to the chart.  Note that nothing is ever
      removed from the chart, and nothing is ever modified once it is in the chart.
      </para>

    </section> <!-- Edges -->

    <section> <title> Charts </title>

      <para>A chart is a set of edges and a location.  The edge set represents
      the state of a chart parser during processing, and new edges can be inserted
      into the set at any time.  Once entered, an edge cannot be modified or
      removed.  The chart also stores a location.  This represents the combined
      span of the list of input tokens.  The chart uses this information
      in order to return edges which span the entire sentence.</para>

      <para>In NLTK, a chart is implemented by the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.chartparser.Chart.html">Chart</ulink>
      class.  Full details of the methods are available online.  The key
      methods to note are <literal>insert</literal> and <literal>parses</literal>.
      </para>

    </section> <!-- Charts -->

    <section> <title> Chart Rules and Parsing Strategies </title>

      <para>The
      <ulink url="http://nltk.sourceforge.net/ref/nltk.chartparser.ChartParserStrategy.html">ChartParserStrategy</ulink>
      class encodes the list of rules used by a chart parser to specify the
      conditions under which new edges should be added to the chart.
      There are two kinds of chart rules:</para>

      <itemizedlist>
        <listitem><para>Static Rules: these search the chart for specific contexts
          where edges should be added.  The parse method will
          repeatedly invoke each static rule, until it produces no
          new edges.</para></listitem>
        <listitem><para>Edge Triggered Rules: these add new edges
          to the chart whenever certain kinds of edges are added by any chart rule.
        </para></listitem>
      </itemizedlist>

      <para>A <literal>ChartParserStrategy</literal> consists of a list of
      static rules and a list of edge triggered rules.</para>
          
      <para>Chart rules are defined using functions, which return a list of
      new edges to add to a chart.  These functions should <emphasis>not</emphasis>
      directly modify the chart (e.g., do not add the edges to the chart
      yourself; C{ChartParser} will add them for you).  Static rules
      are defined with functions of the form:</para>

<programlisting>
def static_rule(chart, grammar, basecat):
    # Decide which edges to add (if any)
    return edges
</programlisting>

      <para>where <literal>chart</literal> is the chart that the static rule should act
      upon; <literal>grammar</literal> is the grammar used by the chart parser; and
      <literal>basecat</literal> is the top-level category of the grammar (e.g. 'S').
      The function should return a list of edges.  These edges
      will be added to the chart by the chart parser.</para>
            
      <para>Edge triggered rules are defined with functions of the form:</para>

<programlisting>
def edge_triggered_rule(chart, grammar, basecat, edge):
    # Decide which edges to add (if any)
    return edges
</programlisting>

      <para>
      Where the arguments are as before, and where <literal>edge</literal> is
      the edge that triggered this rule.  As before, the function
      should return a list of edges, and these will be added to
      the chart by the chart parser.</para>

      <para>The main kinds of chart rules are enumerated and discussed below:</para>

      <section> <title> Chart Initialization </title>

        <para>The first step in parsing a tokenized sentence is to initialize
        the chart with word edges.  For each lexical rule which expands to a given
        word, a corresponding edge is created.  This edge hosts a complete
        dotted rule (e.g. <literal>Det -&gt; 'the' *</literal>), a simple treetoken
        containing the syntactic structure, and a location equal to the location
        of the original token.</para>

      </section> <!-- Chart Initialization -->

      <section> <title> Fundamental Rule </title>

        <para>
        The most important way that edges are combined is known as the
        <emphasis>Fundamental Rule</emphasis>.  Suppose that an edge
        <replaceable>e<subscript>1</subscript></replaceable> has a dotted
        rule whose next symbol is <replaceable>X</replaceable>.  Suppose that
        a second edge <replaceable>e<subscript>2</subscript></replaceable>,
        immediately to the right of
        <replaceable>e<subscript>1</subscript></replaceable>,
        represents a complete <replaceable>X</replaceable> constituent.
        Then the Fundamental Rule states that we must add a new edge
        <replaceable>e<subscript>3</subscript></replaceable> spanning both
        <replaceable>e<subscript>1</subscript></replaceable> and
        <replaceable>e<subscript>2</subscript></replaceable>, in which
        the dot is moved one position to the right.  In other words,
        <replaceable>e<subscript>1</subscript></replaceable> was looking for
        an <replaceable>X</replaceable> to its right, which it found on
        <replaceable>e<subscript>2</subscript></replaceable>, and we record
        this fact on <replaceable>e<subscript>3</subscript></replaceable>.
        </para>

        <para>The <literal>Edge</literal> class has a
        <ulink url="http://nltk.sourceforge.net/ref/nltk.chartparser.Edge.html#FR"
        ><literal>FR</literal></ulink> method which applies the fundamental
        rule to a pair of edges.  The operation of the rule is
        accomplished in four lines:</para>

<programlisting>
def FR(self, edge):
    loc = self._loc.union(edge.loc())
    dr = self._drule.shift()
    tree = TreeToken(self._tree.node(), *(self._tree.children() + (edge.tree(),)))
    return Edge(dr, tree, loc)                                                       
</programlisting>

        <para>First we create a new location which is the union of the locations
        of the two existing edges.  Next we create a new dotted rule, just the same
        as the existing one but with the dot position shifted one to the right.
        Next we take the existing tree <literal>self._tree</literal>, break it
        apart, and incorporate the tree from the other edge.  Finally, we construct
        a new edge from the new dotted rule, new tree, and new location.</para>

        <para>This rule is called many times by
        <literal>chartparser.FR()</literal>, as it considers all pairs of
        adjacent edges and all grammar rules.</para>

<programlisting>
def FR(chart, grammar, basecat):
    added = []
    for edge in chart.edges(): # try every edge
        if not edge.complete(): # only do incomplete ones
            for edge2 in chart.complete_edges(): # next edge complete?
                if (edge.drule().next() == edge2.drule().lhs() and
                    edge.end() == edge2.start()):
                    new_edge = edge.FR(edge2)
                    added += [new_edge]
    return added
</programlisting>

    </section> <!-- Fundamental Rule -->

    <section> <title> Top Down Initialization </title>

      <para>Top down parsing is initialized by creating a zero-width
      (or self-loop) edges at the leftmost position in the chart.
      For every grammar rule whose left-hand side is the base category
      of the grammar, we create the corresponding dotted rule with the
      dot position at the start of the right-hand side.</para>

<programlisting>
def TD_init(chart, grammar, basecat):
    added = []
    loc = chart.loc().start_loc()
    for rule in grammar:
        if rule.lhs() == basecat:
            drule = rule.drule()
            new_edge = Edge(drule, TreeToken(drule.lhs()), loc)
            added += [new_edge]
    return added
</programlisting>
      

    </section> <!-- Top Down Initialization -->

    <section> <title> The Top Down Edge-Triggered Rule </title>

      <para>Whenever a chart contains an incomplete edge, with an incomplete
      rule having <replaceable>X</replaceable> as the next symbol (to the right
      of the dot), we know that
      the parser is expecting to find an <replaceable>X</replaceable>
      constituent immediately to the right.  The top-down edge-triggered
      rule looks for all incompleted edges expecting an
      <replaceable>X</replaceable> and, for each grammar rule having
      <replaceable>X</replaceable> on its left-hand side, creates a zero-width
      edge containing this rule.
      </para>

<programlisting>
def TD_edge(chart, grammar, basecat, edge):
    "Top-down init (edge triggered rule)"
    added = []
    for rule in grammar:
        if not edge.complete() and rule.lhs() == edge.drule().next():
            new_edge = edge.self_loop_end(rule)
            added += [new_edge]
    return added
</programlisting>

    </section> <!-- The Top Down Edge-Triggered Rule -->

    <section> <title> Bottom Up Initialization </title>

      <para>A bottom up parser builds syntactic structure starting from the
      words in the input.  For each word, it must consider which grammar rules
      apply.  The bottom up initialization step involves inserting zero-width
      edges for all words which could be at the left corner of a phrase.</para>

<programlisting>
def BU_init(chart, grammar, basecat):
    added = []
    for edge in chart.edges():
        for rule in grammar:
            if edge.drule().lhs() == rule[0]:
                new_edge = edge.self_loop_start(rule)
                added += [new_edge]
    return added
</programlisting>

    </section> <!-- Bottom Up Initialization -->

    <section> <title> Chart Parser Strategies </title>

      <para>Chart parsers use the various rules described above in order
      to emulate top down, bottom up or hybrid parsers.  A policy about
      what chart rule to apply when is called a
      <emphasis>rule-invocation stratagy</emphasis>.  (Note that the rules
      being invoked here are chart rules, not grammar rules.)</para>

      <para>NLTK defines basic stratagies using instances of the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.chartparser.ChartParserStrategy.html">ChartParserStrategy</ulink>
      class.  Chart rules, whether static or edge-triggered, are defined as
      functions (some are listed above), and then these are passed to the
      <literal>ChartParserStrategy</literal> class.  The initializer takes
      two arguments: a list of static rules and a list of edge-triggered rules.
      Here are the definitions of the top down and bottom up strategies:</para>

<programlisting>
TD_STRATEGY = ChartParserStrategy([TD_init, FR], [TD_edge])
BU_STRATEGY = ChartParserStrategy([BU_init, FR], [])
</programlisting>

    </section>

    </section> <!-- Parsing Strategies -->

    <section> <title> Creating and Using Chart Parsers </title>

      <para>Here is a simple chart parser, which makes use of
      <literal>grammar</literal> and <literal>lexicon</literal>
      as defined earlier in this tutorial.</para>

<programlisting>
cp = ChartParser(grammar, lexicon, 'S', strategy = BU_STRATEGY)
cp.parse(WSTokenizer().tokenize(sent))
for parse in cp.parses():
    print parse.pp()
</programlisting>

      <para>The result of running this parser on the sentence
      <emphasis>I saw a man in the park with a telescope</emphasis>
      is the following set of trees:</para>

<programlisting>
('S':
  ('NP': 'I')
  ('VP':
    ('V': 'saw')
    ('NP':
      ('Det': 'a')
      ('N': 'man')
      ('PP':
        ('P': 'in')
        ('NP': ('Det': 'the') ('N': 'park'))))
    ('PP':
      ('P': 'with')
      ('NP': ('Det': 'a') ('N': 'telescope')))))@[0w:10w]

('S':
  ('NP': 'I')
  ('VP':
    ('V': 'saw')
    ('NP': ('Det': 'a') ('N': 'man'))
    ('PP':
      ('P': 'in')
      ('NP':
        ('Det': 'the')
        ('N': 'park')
        ('PP':
          ('P': 'with')
          ('NP': ('Det': 'a') ('N': 'telescope')))))))@[0w:10w]

('S':
  ('NP': 'I')
  ('VP':
    ('V': 'saw')
    ('NP': ('Det': 'a') ('N': 'man'))
    ('PP': ('P': 'in') ('NP': ('Det': 'the') ('N': 'park')))
    ('PP':
      ('P': 'with')
      ('NP': ('Det': 'a') ('N': 'telescope')))))@[0w:10w]

('S':
  ('NP': 'I')
  ('VP':
    ('V': 'saw')
    ('NP':
      ('Det': 'a')
      ('N': 'man')
      ('PP':
        ('P': 'in')
        ('NP':
          ('Det': 'the')
          ('N': 'park')
          ('PP':
            ('P': 'with')
            ('NP': ('Det': 'a') ('N': 'telescope'))))))))@[0w:10w]
</programlisting>

      <para>To see the corresponding chart, use the function
      <literal>Chart.chart().draw()</literal>.</para>

      <note><para>Note that some edges
      are duplicated because they contain different trees.  In order
      for a chart parser to have polynomial complexity the trees
      corresponding to an edge must be stored in a distributed way...
      </para></note>

    </section>

    <section> <title> The Tk Interface </title>

      <note><para>To be written.</para></note>

      <para>Please see the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.draw.chart.html">nltk.draw.chart</ulink>
      module.</para>

    </section>

  </section> <!-- Chart Parsing -->

</article>
