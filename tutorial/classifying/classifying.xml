<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.1//EN" [

<!-- Base URL for the reference documentation -->
<!ENTITY refdoc "http://nltk.sourceforge.net/ref">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!-- Variables for Naive Bayes algorithms -->
<!ENTITY l "<replaceable>l</replaceable>">
<!ENTITY lprime "<replaceable>l</replaceable>'">
<!ENTITY t "<replaceable>t</replaceable>">
<!ENTITY f1 "<replaceable>f<subscript>1</subscript></replaceable>">
<!ENTITY v1 "<replaceable>v<subscript>1</subscript></replaceable>">
<!ENTITY fd1lt "<replaceable>fd<subscript>1</subscript></replaceable>(<replaceable>lt</replaceable>)">
<!ENTITY f2 "<replaceable>f<subscript>2</subscript></replaceable>">
<!ENTITY v2 "<replaceable>v<subscript>2</subscript></replaceable>">
<!ENTITY fd2lt "<replaceable>fd<subscript>2</subscript></replaceable>(<replaceable>lt</replaceable>)">
<!ENTITY fn "<replaceable>f<subscript>n</subscript></replaceable>">
<!ENTITY vn "<replaceable>v<subscript>n</subscript></replaceable>">
<!ENTITY fdnlt "<replaceable>fd<subscript>n</subscript></replaceable>(<replaceable>lt</replaceable>)">
<!ENTITY ltext "<replaceable>lt</replaceable>">
<!ENTITY ltextlt "<literal>LabeledText(<replaceable>l</replaceable>,<replaceable>t</replaceable>)</literal>">
<!ENTITY ltextlprimet "<literal>LabeledText(<replaceable>l</replaceable>',<replaceable>t</replaceable>)</literal>">
<!ENTITY Phat "P<superscript>*</superscript>">
<!ENTITY fi "<replaceable>f<subscript>i</subscript></replaceable>">
<!ENTITY fdilt "<replaceable>fd<subscript>i</subscript></replaceable>(<replaceable>lt</replaceable>)">
]>

<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Text Classification</title>
  </articleinfo>

  <section id="intro" xreflabel="Introduction"> 
    <title> Introduction </title>

    <para> Often, we wish to divide texts into categories.  For
    example, we might want to categorize news stories by topic; or to
    divide sentences into questions and statements.  In this tutorial,
    we will consider <glossterm>single-category text
    classification</glossterm> problems, in which: </para>

    <itemizedlist>
      <listitem> <para> There are a predefined set of
      categories.</para>
      </listitem>
      <listitem> <para> Each text belongs to exactly one
      category.</para>
      </listitem>
    </itemizedlist>

    <para> <glossterm>Text classification</glossterm> is the task of
    chosing the most likely category for a given text. </para>

    <section section="intro.othertasks" xreflabel="Relationship to Other Tasks"> 
      <title> Relationship to Other Tasks </title>

      <para> Single-category text classification is related to a
      number of other categorization problems.  The following list
      describes how single-category text classification differs from
      three related problems: </para>

      <itemizedlist>
        <listitem>
          <para>In <emphasis>multi-category text
          classification</emphasis>, each text can have zero or more
          categories.</para>
        </listitem>
        <listitem>
          <para>In <emphasis>clustering</emphasis>, the set of
          categories is not predefined.</para>
        </listitem>
        <listitem>
          <para>In <emphasis>tagging</emphasis>, we attempt to
          categorize each element in a sequence of texts (as opposed
          to text classification, where we just tag individual
          texts).</para>
        </listitem>
      </itemizedlist>

    </section> <!-- Relationship to other tasks -->

  </section> <!-- Introduction -->

  <section id="labeledtexts" xreflabel="Labeled Texts"> 
    <title> Labeled Texts </title>

    <para> In text categorization, each category is uniquely
    identified by a <glossterm>label</glossterm>.  Labels are
    typically <literal>string</literal>s or
    <literal>integer</literal>s.  For example, if we are classifying
    news stories, our category labels might include
    <literal>'sports'</literal>, <literal>'technology'</literal>, and
    <literal>'foreign affairs'</literal>. </para>

    <para> Categorized text types are represented using the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html"
    ><literal>LabeledText</literal></ulink> class, which is defined by
    the <literal>nltk.classifier</literal> module.  New
    <literal>LabeledText</literal>s are constructed using the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html#__init__"
    ><literal>LabeledText</literal> constructor</ulink>:</para>

<programlisting>
    &prompt; <command>text = "What's your name?"</command>
    &prompt; <command>label = "question"</command>
    &prompt; <command>labeled_text = LabeledText(text, label)</command>
    "What's your name?"/"question"
</programlisting>

    <note>
      <para> We may decide to change the printed representation for
      labeled texts; currently, it is identical to the printed
      representation for <literal>TaggedType</literal>s.</para>
    </note>

    <para> A <literal>LabeledText</literal>'s underlying text is
    accessed via the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html#text">
    <literal>text</literal></ulink> member function; and its label is
    accessed via the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html#label">
    <literal>label</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> labeled_text.text() </command>
    "What's your name?"
    &prompt;<command> labeled_text.label() </command>
    "question"
</programlisting>

    <para> A <glossterm>labeled text token</glossterm> is a
    <literal>Token</literal>s whose type is a
    <literal>LabeledText</literal>: </para>

<programlisting>
    &prompt;<command> loc = Location(3, unit='s')</command>
    @[3s]
    &prompt;<command> labeled_token = Token(labeled_text, loc)</command>
    "What's your name?"/"question"@[3s]
</programlisting>

  </section> <!-- Labeled Texts -->

  <section id="classifieri" xreflabel="The Classifier Interface"> 
    <title> The Classifier Interface </title>

    <para> The <literal>nltk.classifier</literal> module defines
    <ulink url="&refdoc;/nltk.classifier.ClassifierI.html">
    <literal>ClassifierI</literal></ulink>, a general interface for
    classifying texts.  This interface is used by all single-category
    text classifiers.  It requires that classifiers define two
    methods: </para>

    <itemizedlist>
      <listitem>
        <para><ulink
        url="&refdoc;/nltk.classifier.ClassifierI.html#classify">
        <literal>classify</literal></ulink> determines which label is
        most appropriate for a given text token, and returns a labeled
        text token with that label. </para>
      </listitem>
      <listitem>
        <para><ulink
        url="&refdoc;/nltk.classifier.ClassifierI.html#labels">
        <literal>labels</literal></ulink> returns the list of category
        labels that are used by the classifier. </para>
      </listitem>
    </itemizedlist>

    <para> These two methods are illustrated in the following example:
    </para>

<programlisting>
    &prompt;<command> token = Token("What's your name?", loc)</command>
    "What's your name?"@[3s]
    &prompt;<command> my_classifier.classify(token)</command>
    "What's your name?"/"question"@[3s]

    &prompt;<command> my_classifier.labels()</command>
    ("statement", "imperative", "question")
</programlisting>

    <section id="classifieri.optional" xreflabel="Optional Methods"> 
      <title> Optional Methods </title>

      <para> Classifiers are also encouraged (but not required) to
      define four additional methods:</para>

      <itemizedlist>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#prob">
          <literal>prob</literal></ulink> takes a labeled text token,
          and returns the probability that the token's text belongs to
          the category indicated by the token's label. </para>
        </listitem>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#distribution">
          <literal>distribution</literal></ulink> takes a text token,
          and returns a probability distribution, whose samples are
          labeled text tokens.  This probability distribution
          indicates the likelihood that the text belongs to each
          label's category. </para>
        </listitem>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#distribution_dictionary">
          <literal>distribution_dictionary</literal></ulink> takes a
          text token, and returns a dictionary, mapping from each
          label to the probability that the text is a member of that
          label's category.</para>
        </listitem>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#distribution_list">
          <literal>distribution_list</literal></ulink> takes a text
          token, and returns a list of probabilities, where the
          <replaceable>i</replaceable>th element of the list is the
          probability that the text belongs to the category indicated
          by
          <literal>classifier.labels()[<replaceable>i</replaceable>]</literal>.
          </para>
        </listitem>
      </itemizedlist>

      <para> Each of these methods is illustrated in the following
      example: </para>

<programlisting>
    &prompt;<command> print labeled_token</command>
    "What's your name?"/"question"@[3s]

    <emphasis># What's the probability that "What's your name?" is a question?</emphasis>
    &prompt;<command> my_classifier.prob(labeled_token)</command>
    0.67

    <emphasis># What's the probability that "What's your name?" is a question,</emphasis>
    <emphasis># given that it's not a statement? </emphasis>
    &prompt;<command> prob_dist = my_classifier.distribution(token)</command>
    &prompt;<command> def not_statement(ltoken): ltoken.type().label()!='statement'</command>
    &prompt;<command> prob_dist.cond_prob(labeled_token, PredEvent(not_statement))</command>
    0.85

    <emphasis># What is the probability for each category?</emphasis>
    &prompt;<command> prob_dict = my_classifier.distribution_dictionary(token) </command>
    &prompt;<command> for label in prob_dict.keys():</command>
    &prompt2;<command>     print "P(%s) = %.2f" % (label, prob_dict[label])</command>
    P(statement) = 0.21
    P(imperative) = 0.12
    P(question) = 0.67

    <emphasis># What is the probability for each category?</emphasis>
    &prompt;<command> zip(my_classifier.labels(), my_classifier.distribution_list(token))</command>
    [('statement', 0.21), ('imperative', 0.12), ('question', 0.67)]
</programlisting>

    </section> <!-- Optional Methods -->

  </section> <!-- Classifiers -->

  <section id="training" xreflabel="Training Classifiers"> 
    <title> Training Classifiers </title>
    
    <para> Often, we have access to a corpus of classified texts.
    This <glossterm>training corpus</glossterm> is usually
    hand-classified, and we assume that most of the classifications
    are correct. </para>

    <!-- !!!! Add more precise forward reference? !!!! --> <para> We
    can use statistical methods to build classifiers from a training
    corpus.  Some of these statistical methods are discussed in
    following sections.  </para>

    <para> The <literal>nltk.classifier</literal> module defines
    <ulink url="&refdoc;/nltk.classifier.ClassifierTrainerI.html">
    <literal>ClassifierTrainerI</literal></ulink>, a general interface
    for building classifiers from training corpora.  It requires that
    classifiers define a single method, <ulink
    url="&refdoc;/nltk.classifier.ClassifierTrainerI.html#train">
    <literal>train</literal></ulink>, which takes a list of labeled
    tokens, and returns a new classifier.  For example, the following
    code builds a new Naive Bayes classifier from a training corpus:
    </para>

<programlisting>
    &prompt;<command> from nltk.classifier.naivebayes import NBClassifierTrainer</command>
    &prompt;<command> classifier = NBClassifierTrainer().train(labeled_tokens)</command>
    &lt;NBClassifier: 3 labels, 187 features&gt;
</programlisting>

  </section> <!-- Training Classifiers -->

  <section id="features" xreflabel="Feature-Based Classification"> 
    <title> Feature-Based Classification </title>

    <para> Most text classification algorithms do not depend on the
    specific details of the task being performed.  The same algorithms
    that we use to categorize documents by topic can also be used to
    categorize words by their part of speech, or to categorize
    acoustic signals according to which phoneme they represent.
    <glossterm>Features</glossterm> provide a standard way of encoding
    the information used to make classification decisions.  This
    standard encoding abstracts away from the details of specifc
    tasks, allowing the same classifier algorithm to be used to solve
    many different problems. </para>

    <para> Each feature specifies some aspect of a
    <literal>LabeledText</literal> that is relevant to deciding how
    likely that <literal>LabeledText</literal> is to occur.  These
    features can be used by classification algorithms to examine how
    likely different labels are for a given text.  A typical example
    of a feature is:</para>

    <itemizedlist>
      <listitem>
        <para>Whether a document contains the word
        <literal>"ball"</literal> and has the label
        <literal>"sports"</literal>.</para>
      </listitem>
    </itemizedlist>

    <para> Notice that features depend on both the text and the label.
    This allows us to specify which aspects of a text are relevant for
    making decisions about which labels. </para>

    <para> Features can be defined using <glossterm>feature detector
    functions</glossterm>, which map <literal>LabeledText</literal>s
    to values.  For example, the feature given above could be defined
    using the following function:</para>

<screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (("ball" in ltext.text) and (ltext.label == "sports"))</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

    <para> The <glossterm>feature values</glossterm> produced by
    feature detector functions are usually booleans or integers; but
    occasionally, real-valued features are used. </para>
    
    <caution> 

      <para>Many people use the term "feature" as we do, to refer to
      an <emphasis>aspect</emphasis> of a labeled text; but some
      people use the term to refer to the aspect's
      <emphasis>value</emphasis>.  To avoid confusion, the following
      table gives explicit definitions of the terminology used by
      NLTK:</para>

      <informaltable>
        <tgroup cols="2">
          <thead>
            <row>
              <entry>Term</entry>
              <entry>Definition</entry>
            </row>
          </thead>
          <tbody> <!-- colsep="0" rowsep="0"> -->
            <row>
              <entry><emphasis>feature</emphasis></entry> <entry>An
              aspect of a <literal>LabeledText</literal> that is
              relevant to classification. </entry>
            </row>
            <row>
              <entry><emphasis>feature detector
              function</emphasis></entry> <entry>A function that
              defines a feature.  Feature detector functions map
              <literal>LabeledText</literal>s to feature
              values.</entry>
            </row>
            <row>
              <entry><emphasis>feature value</emphasis></entry>
              <entry>The value returned by a feature detector function
              for a particular
              <literal>LabeledText</literal>. </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
    </caution>
    
    <section id="features.types" xreflabel="Feature Types"> 
      <title> Feature Types </title>

      <section id="features.types.bool" xreflabel="Boolean Features"> 
        <title> Boolean Features </title>

        <para> <glossterm>Boolean features</glossterm> are features
        that generate boolean values.  Boolean features are the
        simplest and the most common type of feature.  Boolean feature
        detector functions usually have the form: </para>

<screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if <replaceable>pred</replaceable>(ltext.text) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

        <para> for some boolean function
        <replaceable>pred</replaceable> and some label
        <replaceable>L</replaceable>.  Boolean features are also
        called <glossterm>binary features</glossterm>.</para>

        <para> Boolean features have a simple probablistic
        interpretation: we can think of them as events defined in the
        probability space of <literal>LabeledText</literal>s.</para>

      </section> <!-- Boolean Features -->

      <section id="features.types.int" xreflabel="Integer Features"> 
        <title> Integer Features </title>

        <para> <glossterm>Integer features</glossterm> are features
        that generate integer values.  Integer feature detector
        functions usually have the form:</para>

<screen>
    <replaceable>f</replaceable>(ltext)  =  <replaceable>g</replaceable>(ltext.text)  <emphasis>if ltext.label == <replaceable>L</replaceable></emphasis>
                       0        <emphasis>otherwise</emphasis>
</screen>
        
        <para> for some integer valued function
        <replaceable>g</replaceable> and some label
        <replaceable>L</replaceable>. </para>

        <para> Integer features can be used to give classifiers access
        to more precise information about the text.  A typical example
        of an integer feature detector function is: </para>

<screen>
    <replaceable>f</replaceable>(ltext)  =  num_occurances("ball", ltext.text)  <emphasis>if ltext.label == "sports"</emphasis>
                               0                     <emphasis>otherwise</emphasis>
</screen>

      </section> <!-- Integer Features -->
      
      <section id="features.types.other" xreflabel="Other Feature Types"> 
        <title> Other Feature Types </title>

        <para> Some classifier algorithms use real-valued features, or
        even more exotic kinds of features.  NLTK allows features
        values to be any immutable type; but currently, NLTK only
        implements classifier trainers for boolean and integer
        features. </para>
        
      </section> <!-- Other Features -->

    </section> <!-- Feature Types -->

  </section> <!-- Feature-Based Classification -->    

  <section id="fdetectors" xreflabel="Feature Detectors"> 
    <title> Feature Detectors </title>

    <para> The <literal>nltk.classifier.feature</literal> module
    defines <ulink
    url="&refdoc;/nltk.classifier.feature.FeatureDetectorI.html">
    <literal>FeatureDetectorI</literal></ulink>, a general interface
    for implementing feature detector functions.  Feature detectors
    are required to define a single method, <ulink
    url="&refdoc;/nltk.classifier.feature.FeatureDetectorI.html#detect">
    <literal>detect</literal></ulink>, which takes a labeled text, and
    returns a feature value. </para>

    <para> <ulink
    url="&refdoc;/nltk.classifier.feature.FunctionFeatureDetector.html">
    <literal>FunctionFeatureDetector</literal></ulink> provides a
    simple flexible implementation of the
    <literal>FeatureDetectorI</literal> interface.
    <literal>FunctionFeatureDetector</literal>s are constructed from
    Python functions with the <ulink
    url="&refdoc;/nltk.classifier.feature.FunctionFeatureDetector.html#__init__">
    <literal>FunctionFeatureDetector</literal>
    constructor</ulink>:</para>

<programlisting>
    &prompt; <command>def ball_sports(ltext):</command>
    &prompt2; <command>    return ("ball" in ltext.text()) and (ltext.label() == "sports")</command>
    &prompt; <command>fdetector = FunctionFeatureDetector(ball_sports)</command>
    &lt;FeatureDetector: ball_sports&gt;
</programlisting>

    <para> Once we have constructed a function detector, we can use
    the <literal>detect</literal> method to find the feature values
    for various labeled texts: </para>

<programlisting>
    &prompt; <command>document1 = "John threw the ball over the fence".split()</command>
    &prompt; <command>document1 = "Mary solved the equation".split()</command>
    &prompt; <command>fdetector.detect(LabeledText(document1, "sports"))</command>
    1
    &prompt; <command>fdetector.detect(LabeledText(document2, "sports"))</command>
    0
    &prompt; <command>fdetector.detect(LabeledText(document1, "foreign affairs"))</command>
    0
</programlisting>

  </section> <!-- Feature Detectors -->

  <section id="fdlists" xreflabel="Feature Detector Lists"> 
    <title> Feature Detector Lists </title>

    <para> <glossterm>Feature detector lists</glossterm> are data
    structures that represent the feature detector functions for a set
    of features.  Feature detector lists serve three important
    functions: </para>

    <itemizedlist>
      <listitem>
        <para> They provide a mechanism for grouping feature detectors
        together.</para>
      </listitem>
      <listitem>
        <para> They associate a unique identifier with each feature
        detector.</para>
      </listitem>
      <listitem>
        <para> They allow for efficient implementations for sets of
        related feature detectors. </para>
      </listitem>
    </itemizedlist>

    <para> Abstractly, a feature detector list can be thought of as a
    <literal>list</literal> of <literal>FeatureDetector</literal>s.
    The index of each feature detector in the list serves as a unique
    identifier for that detector's feature.  This identifier is known
    as a <glossterm>feature id</glossterm>.  The feature ids for a
    feature detector list with <emphasis>N</emphasis> features are
    <emphasis>0, 1, ..., N-1</emphasis>. </para>

    <warning>
      <para> Feature ids uniquely identify a feature, given a feature
      detector list.  However, they are not globally unique
      identifiers.  Thus, a single feature might have different
      identifiers in different feature detector lists; and two
      different features might have the same feature id if they are
      from different feature detector lists.  Care must be taken not
      to mix feature ids from different feature detector
      lists. </para>
    </warning>

    <section id="fdlists.fdlisti" xreflabel="FeatureDetectorListI"> 
      <title> FeatureDetectorListI </title>

      <para> The <literal>nltk.classifier.feature</literal> module
      defines <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html">
      <literal>FeatureDetectorListI</literal></ulink>, a general
      interface for implementing feature detector lists.  Feature
      detector lists are required to implement four methods: an
      indexing operator; a length operator; a
      <literal>detect</literal> method; and an addition
      operator.</para>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#__getitem__">indexing
      operator</ulink> allows feature detector lists to be treated as
      lists of <literal>FeatureDetector</literal>s:</para>

<programlisting>
    <emphasis># fdlist is a feature detector list.</emphasis>
    &prompt; <command>print fdlist</command>
    &lt;FeatureDetectorList with 6 features&gt;

    <emphasis># Print the third feature detector. </emphasis>
    &prompt; <command>fdlist[2]</command>
    &lt;FeatureDetector: ball_sports&gt;

    <emphasis># Print the feature values generated for labeled_text.</emphasis>
    &prompt; <command>labeled_text = LabeledText(document2, "foreign affairs")</command>
    &prompt; <command>for fdetector in fdlist:</command>
    &prompt2; <command>    print fdetector, fdetector.detect(labeled_text)</command>
    &lt;FeatureDetector: ball_foreign_affairs&gt; 0
    &lt;FeatureDetector: ball_weather&gt; 0
    &lt;FeatureDetector: ball_sports&gt; 0
    &lt;FeatureDetector: solved_foreign_affairs&gt; 1
    &lt;FeatureDetector: solved_weather&gt; 0
    &lt;FeatureDetector: solved_sports&gt; 0
</programlisting>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#__len__">length
      operator</ulink> returns the number of features represented by a
      feature detector list: </para>

<programlisting>
    &prompt; <command>len(fdlist)</command>
    6
</programlisting>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#detect"><literal>detect</literal></ulink>
      method takes a labeled text, and finds the feature value for
      each feature detector.  These values are returned in a structure
      called a "feature value list," which is discussed in the next
      section. Conceptually, the <literal>detect</literal> method is
      equivalant to applying each individual feature detector's
      <literal>detect</literal> method in parallel.  The following
      diagram compares the use of a single feature detector to the use
      of a feature detector list: </para>
      <screen>
                        FeatureDetectorI
                      +------------------+
   labeled text  -->  | feature detector |  -->    feature value
                      +------------------+


                      FeatureDetectorListI       FeatureValueListI
                      +------------------+       +---------------+
                      | feature detector |  -->  | feature value |
                      |------------------|       |---------------|
                      | feature detector |  -->  | feature value |
                      |------------------|       |---------------|
   labeled text  -->  | feature detector |  -->  | feature value |
                      |------------------|       |---------------|
                      |       . . .      |  -->  |     . . .     | 
                      |------------------|       |---------------|
                      | feature detector |  -->  | feature value |
                      +------------------+       +---------------+
</screen>

      <para> The following example finds the feature values for
      <literal>document2</literal> with label
      <literal>"foreign affairs"</literal>: </para>

<programlisting>
    &prompt; <command>fdlist.detect(LabeledText(document2, "foreign affairs"))</command>
    &lt;FeatureValueList with 6 features&gt;
</programlisting>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#__add__">addition
      operator</ulink> can be used to combine feature detector
      lists. </para>

<programlisting>
    <emphasis># fdlist1 and fdlist2 are feature detector lists.</emphasis>
    &prompt; <command>print fdlist1</command>
    &lt;FeatureDetectorList with 6 features&gt;
    &prompt; <command>print fdlist2</command>
    &lt;FeatureDetectorList with 72 features&gt;

    &prompt; <command>new_fdlist = fdlist1 + fdlist2</command>
    &lt;FeatureDetectorList with 78 features&gt;
</programlisting>

      <para> Note that the feature ids for the feature list produced
      by addition may be different from the feature ids for the two
      constituant feature lists: </para>

<programlisting>
    &prompt; <command>print fdlist2[3]</command>
    &lt;FeatureDetector: running_weathers&gt;

    &prompt; <command>print new_fdlist[3]</command>
    &lt;FeatureDetector: asleep_sports&gt;
</programlisting>

    </section> <!-- FeatureDetectorListI -->

    <section id="fdlists.efficiency" xreflabel="Efficiency"> 
      <title> Efficiency </title>

      <para> Although feature detector lists can be abstractly thought
      of as <literal>list</literal>s of
      <literal>FeatureDetector</literal>s, they are not usually
      implemented that way, for efficiency reasons. </para>

      <para> Many of the features used for classification are closely
      related to each other.  For example, a document classifier might
      use features that examine whether a given word is in a document.
      If the feature detector list were implemented as a list of
      independant feature detectors, then we would need to apply each
      feature detector separately.  This would require scanning
      through the document once for each word we are interested
      in.</para>

      <para> Instead, we can build a feature detector list that will
      check for all relevant words at the same time.  This feature
      detector list makes a single pass through the document.  Each
      time it encounters a word that we are interested in, it updates
      the corresponding feature value.  This approach allows for
      significantly more efficient feature detection. </para>

    </section> <!-- Efficiency -->

    <section id="fdlists.textfunctionfdlist" xreflabel="TextFunctionFDList"> 
      <title> TextFunctionFDList </title>

      <para> The <literal>nltk.classifier.feature</literal> module
      defines <ulink
      url="&refdoc;/nltk.classifier.feature.TextFunctionFDList.html">
      <literal>TextFunctionFDList</literal></ulink>, a flexible
      implementation of the <literal>FeatureDetectorListI</literal>
      interface.  <literal>TextFunctionFDList</literal> implements
      feature detector lists consisting of boolean features whose
      detector functions have the form: </para>

<screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (<replaceable>g</replaceable>(ltext.text) == <replaceable>val</replaceable>) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

      <para> For a given function <replaceable>g</replaceable>, and
      for each function value <replaceable>val</replaceable> and label
      <replaceable>L</replaceable>.
      <literal>TextFunctionFDList</literal> gets its name from the
      function <replaceable>g</replaceable>, since it is a function
      defined over texts. </para>
      
      <para> <literal>TextFunctionFDList</literal>s are constructed
      using the <ulink
      url="&refdoc;/nltk.classifier.feature.TextFunctionFDList.html#__init__">
      <literal>TextFunctionFDList</literal> constructor</ulink>, which
      takes a function, a list of function values, and a list of
      labels.  The new feature detector list contains one feature
      detector for each (<replaceable>val</replaceable>,
      <replaceable>L</replaceable>) pair.  The following example
      constructs a feature detector list that checks the length of an
      sentence: </para>

<programlisting>
    &prompt; <command>def length(text): return len(text) </command>
    &prompt; <command>length_range = range(0, 25) </command>
    &prompt; <command>labels = ('statement', 'imperative', 'question')</command>
    &prompt; <command>fdlist = TextFunctionFDList(length, length_range, labels)</command>
    &lt;FeatureDetectorList with 75 features&lt;
</programlisting>

    </section> <!-- TestFunctionFDList -->

    <section id="fdlists.bagofwordsfdlist" xreflabel="BagOfWordsFDList"> 
      <title> BagOfWordsFDList </title>

      <para> The <literal>nltk.classifier.feature</literal> module
      also defines <ulink
      url="&refdoc;/nltk.classifier.feature.TextFunctionFDList.html">
      <literal>BagOfWordsFDList</literal></ulink>, a feature detector
      list implementation which checks which words are present in a
      text.  In particular, it implements a feature detector list
      containing boolean features whose detector functions have the
      form: </para>

<screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (<replaceable>w</replaceable> in ltext.text) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

      <para> For each word <replaceable>w</replaceable> and label
      <replaceable>L</replaceable>. </para>

      <para> <literal>BagOfWordsFDList</literal>s are constructed
      using the <ulink
      url="&refdoc;/nltk.classifier.feature.BagOfWordsFDList.html#__init__">
      <literal>BagOfWordsFDList</literal> constructor</ulink>, which
      takes a list of relevant words and a list of labels.  The new
      feature detector list contains one feature detector for each
      (<replaceable>w</replaceable>, <replaceable>L</replaceable>)
      pair.  The following example constructs a feature detector list
      that checks sentences for the presence of several informative
      words: </para>

<programlisting>
    &prompt; <command>words = 'ball company chip score attack invade'</command>
    'ball company chip score attack invade'
    &prompt; <command>wordlist = words.split()</command>
    ['ball', 'company', 'chip', 'score', 'attack', 'invade']
    &prompt; <command>labels = ('sports', 'foreign affairs', 'technology')</command>
    ('sports', 'foreign affairs', 'technology')

    &prompt; <command>fdlist = BagOfWordsFDList(wordlist, labels)</command>
    &lt;FeatureDetectorList with 18 features&lt;
</programlisting>

    </section> <!-- BagOfWordsFDList -->

    <section id="fdlists.other" xreflabel="Other Feature Detector List Implementations"> 
      <title> Other Feature Detector List Implementations </title>

      <para> The <literal>nltk.classifier.feature</literal> module
      provides several more feature detector list implementations; see
      their reference documentation for more details. </para>

      <itemizedlist>
        <listitem> <para> <ulink
      url="&refdoc;/nltk.classifier.feature.AlwaysOnFDList.html">
      <literal>AlwaysOnFDList</literal></ulink></para>
        </listitem>
        <listitem> <para> <ulink
      url="&refdoc;/nltk.classifier.feature.SimpleFDList.html">
      <literal>SimpleFDList</literal></ulink></para>
        </listitem>
        <listitem> <para> <ulink
      url="&refdoc;/nltk.classifier.feature.LabeledTextFunctionFDList.html">
      <literal>LabeledTextFunctionFDList</literal></ulink></para>
        </listitem>
        <listitem> <para> <ulink
      url="&refdoc;/nltk.classifier.feature.AbstractFDList.html">
      <literal>AbstractFDList</literal></ulink></para>
        </listitem>
        <listitem> <para> <ulink
      url="&refdoc;/nltk.classifier.feature.MergedFDList.html">
      <literal>MergedFDList</literal></ulink></para>
        </listitem>
      </itemizedlist>

    </section> <!-- Other fdlist implementations -->

  </section> <!-- Feature Detector Lists -->

  <section id="fvlists" xreflabel="Feature Value Lists"> 
    <title> Feature Value Lists </title>

    <para> <glossterm>Feature value lists</glossterm> are data
    structures that represent the feature values for a set of
    features.  Feature value lists serve three important functions:
      </para>

    <itemizedlist>
      <listitem>
        <para> They provide a mechanism for grouping feature values
        together.</para>
      </listitem>
      <listitem>
        <para> They associate a unique identifier with each feature
        value.</para>
      </listitem>
      <listitem>
        <para> They allow for efficient encoding of sparse sets of
        feature values. </para>
      </listitem>
    </itemizedlist>

    <para> Abstractly, a feature value list can be thought of as a
    <literal>list</literal> of the feature values for a set of
    features.  The value for the feature whose id is
    <replaceable>i</replaceable> is the <replaceable>i</replaceable>th
    element of the list. </para>

    <para> Feature value lists are usually created with the
    <literal>detect</literal> method of a feature detector list.  The
    feature ids in the feature value list correspond with the feature
    ids in the feature detector list that was used to create it.  In
    other words, the <replaceable>i</replaceable>th feature detector
    in the feature detector list is responsible for generating the
    <replaceable>i</replaceable>th feature value in the feature value
    list.  This correspondance was illustrated in the diagram used to
    illustrate the <literal>detect</literal> method, above. </para>

    <para> Usually, most of the features in a
    <literal>FeatureValueList</literal> have the same value.  For
    example, a sentence-type classifier might use features of the
    form: </para>

<screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (<replaceable>w</replaceable> in ltext.text) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

    <para> for each word <replaceable>w</replaceable> and each label
    <replaceable>L</replaceable>.  If there are 3 labels, and we are
    interested in 1,000 words, then there are 3,000 features; but for
    a typical sentence, only a handful of those features will have a
    nonzero value. </para>

    <para> If most of the features in a
    <literal>FeatureValueList</literal>s, have the same value, the
    <literal>FeatureValueList</literal> is said to be
    <glossterm>sparse</glossterm>.  The common value is called the
    <glossterm>default</glossterm>, and is typically zero. </para>

    <section id="fvlists.fvlisti" xreflabel="FeatureValueListI"> 
      <title> FeatureValueListI </title>

      <para> The <literal>nltk.classifier.feature</literal> module
      defines <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html">
      <literal>FeatureValueListI</literal></ulink>, a general
      interface for implementing feature value lists.  Feature value
      lists are required to implement four methods: an indexing
      operator; a length operator; an <literal>assignments</literal>
      method; and a <literal>default</literal> method. </para>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#__getitem__">indexing
      operator</ulink> allows feature value lists to be treated as
      lists of feature values:</para>

<programlisting>
    <emphasis># fvlist is a feature value list.</emphasis>
    &prompt; <command>print fvlist</command>
    &lt;FeatureValueList with 6 features&gt;

    <emphasis># Print the third feature value. </emphasis>
    &prompt; <command>fvlist[2]</command>
    0

    <emphasis># Print each feature value.</emphasis>
    &prompt; <command>print [fvalue for fvalue in fvlist]</command>
    [0, 0, 0, 1, 0, 0]
</programlisting>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#__len__">length
      operator</ulink> returns the number of features represented by a
      feature value list: </para>

<programlisting>
    &prompt; <command>len(fvlist)</command>
    6
</programlisting>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#assignments"><literal>assignments</literal></ulink>
      method and the <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#default"><literal>default</literal></ulink>
      method provide an efficient way of processing sparse feature
      value lists.  <literal>default</literal> returns the feature
      value list's default value.  This is typically the value for
      most features in the list.  <literal>assignments</literal>
      returns a list of
      <emphasis>(feature-id,&nbsp;feature-value)</emphasis> pairs,
      which specifies the feature value for all of the remaining
      features. </para>

      <para> <literal>default</literal> and
      <literal>assignments</literal> can be used together to
      efficiently process large sparse feature value lists.  For
      example, the following code calculates the sum of all feature
      values in <literal>fvlist</literal>: </para>

<programlisting>
    &prompt; <command>assignments = fvlist.assignments()</command>
    &prompt; <command>sum = fvlist.default() * (len(fvlist)-len(assignments))</command>
    &prompt; <command>for (fid, fval) in assignments:</command>
    &prompt2; <command>    sum += fval</command>
    &prompt; <command>print sum</command>
    7
</programlisting>

    </section> <!-- FeatureValueListI -->
    
  </section> <!-- Feature Value Lists -->
  
  <section id="using" xreflabel="Using Classifiers"> 
    <title> Using Classifiers </title>

    <para> Before we start explaining how various classifiers work, we
    will look at a complete example of how classifiers can be used.
    This should help tie together the various data structures and
    interfaces that have been discussed so far.  The task we will look
    at is sentence-type classification.  In particular, we wish to
    categorize sentences as <emphasis>statements</emphasis>,
    <emphasis>imperatives</emphasis>, or
    <emphasis>questions</emphasis>.  We will use a
    <literal>ClassifierTrainer</literal> to build a feature-based
    classifier from a training corpus. </para>

    <section id="using.features" xreflabel="Features"> 
      <title> Features </title>

      <para> The first thing we need to do is decide which aspects of
      a sentence are relevant to this classification task.  In other
      words, we must decide which features to use.  After examining
      the training corpus, we might decide that the three following
      kinds of information are relevant: </para>

      <itemizedlist>
        <listitem> <para> Which words are present in the sentence </para>
        </listitem>
        <listitem> <para> The first word of the sentence </para>
        </listitem>
        <listitem> <para> The length of the sentence </para>
        </listitem>
      </itemizedlist>

      <section id="using.features.1" xreflabel="Which words are present"> 
        <title> Which words are present </title>

        <para> To check which words are present in a sentence, we can
        define a <literal>BagOfWordsFDList</literal>.  </para>

<programlisting>
    &prompt; <command>word_list = open('wordlist.txt').read().split()</command>
    &prompt; <command>labels = ('statement', 'question', 'imperative')</command>
    &prompt; <command>bag_of_words_fdlist = BagOfWordsFDList(word_list, labels)</command>
    &lt;FeatureDetectorList with 1800 features&lt;
</programlisting>

        <para> The list of words in the file
        <literal>wordlist.txt</literal> indicates which words we
        believe are relevant to this classification task. </para>

      </section> <!-- Which words are present -->

      <section id="using.features.2" xreflabel="The first word"> 
        <title> The first word </title>

        <para> To check what the first word of a sentence is, we can
        use a <literal>TextFunctionFDList</literal>. </para>

<programlisting>
    &prompt; <command>first_word_list = open('firstwordlist.txt').read().split()</command>
    &prompt; <command>def first_word(text): return text[0] </command>
    &prompt; <command>first_word_fdlist = TextFunctionFDList(first_word, first_word_list, labels)</command>
    &lt;FeatureDetectorList with 852 features&lt;
</programlisting>

        <para> The list of words in the file
        <literal>firstwordlist.txt</literal> indicates which first
        words we believe are relevant to this classification
        task. </para>

      </section> <!-- The first word -->

      <section id="using.features.3" xreflabel="Sentence length"> 
        <title> Sentence length </title>

        <para> To check the length of the sentence, we can use another
        <literal>TextFunctionFDList</literal>. </para>

<programlisting>
    &prompt; <command>def length(text): return len(text) </command>
    &prompt; <command>length_range = range(0, 25) </command>
    &prompt; <command>length_fdlist = TextFunctionFDList(length, length_range, labels)</command>
    &lt;FeatureDetectorList with 75 features&lt;
</programlisting>

      </section> <!-- Sentence length -->

      <section id="using.features.combine" xreflabel="Combining the Feature Lists"> 
        <title> Combining the Feature Lists </title>

        <para> Finally, we can combine these three sets of feature
        detectors into a single feature detector list, using the
        addition operator: </para>

<programlisting>
    &prompt; <command>fdlist = bag_of_words_fdlist + first_word_fdlist + length_fdlist </command>
    &lt;FeatureDetectorList with 2727 features&lt;
</programlisting>

      </section> <!-- Combining -->

    </section> <!-- Features -->

    <section id="using.corpus" xreflabel="Training Corpus"> 
      <title> Training Corpus </title>

      <para> Next, we need to load the training corpus.  Training
      corpera are often stored in different formats for different
      tasks.  For this task, we have three files:
      <literal>statements.txt</literal>,
      <literal>imperatives.txt</literal>, and
      <literal>questions.txt</literal>.  These files contain training
      samples for the types of sentences indicated by their filenames.
      Each sentence has one sentence per line.  The following code
      reads these sentences, and labels them appropriately: </para>

<programlisting>
    &prompt; <command>tokenizer = LineTokenizer()</command>

    &prompt; <command>statements = open('statements.txt').read() </command>
    &prompt; <command>statement_toks = tokenizer.tokenize(statements) </command>

    &prompt; <command>imperatives = open('imperatives.txt').read() </command>
    &prompt; <command>imperative_toks = tokenizer.tokenize(imperatives) </command>

    &prompt; <command>questions = open('questions.txt').read() </command>
    &prompt; <command>question_toks = tokenizer.tokenize(questions) </command>

    &prompt; <command>train_toks = (label_tokens(statement_toks, 'statement') +</command>
    &prompt2; <command>              label_tokens(imperative_toks, 'imperative') +</command>
    &prompt2; <command>              label_tokens(question_toks, 'question'))</command>
</programlisting>

      <para> <ulink
      url="&refdoc;/nltk.classifier.label_tokens.html"><literal>label_tokens</literal></ulink>
      is a simple helper function that takes a list of unlabeled
      tokens, and returns a corresponding list of labeled tokens. </para>

    </section> <!-- Training Corpus -->

    <section id="using.training" xreflabel="Training the Classifier"> 
      <title> Training the Classifier </title>

      <para> Once we've constructed a feature list and a training
      corpus, we can train a new classifier.  First, we build a
      <literal>ClassifierTrainer</literal>, using the feature detector
      list:
      </para>

<programlisting>
    <emphasis># Naive Bayes classifier trainer</emphasis>
    &prompt; <command>trainer = NBClassifierTrainer(fdlist) </command>
    &lt;NBClassifierTrainer: 2727 features&gt;
</programlisting>

      <para> Then, we use the <literal>ClassifierTrainer</literal> to
      train a new classifier, using the training corpus: </para>

<programlisting>
    &prompt; <command>classifier = trainer.train(train_toks) </command>
    &lt;NBClassifier: 3 labels, 2727 features&gt;
</programlisting>

      <para> If we wanted to build another kind of classifier, we
      could simply use a different kind of
      <literal>ClassifierTrainer</literal>.  For example, the
      following code uses our feature detector list and training
      corpus to build a new maximum entropy classifier, using a
      <literal>ClassifierTrainer</literal> that implements improved
      iterative scaling:</para>

<programlisting>
    <emphasis># Improved Iterative Scaling maxent classifier trainer</emphasis>
    &prompt; <command>trainer = IISMaxentClassifierTrainer(fdlist) </command>
    &lt;IISMaxentClassifierTrainer: 2727 features&gt;

    &prompt; <command>classifier = trainer.train(train_toks) </command>
    &lt;MaxentClassifier: 3 labels, 2727 features&gt;
</programlisting>

    </section> <!-- Training the Classifier -->

    <section id="using.classifying" xreflabel="Classifying New Texts"> 
      <title> Classifying New Texts </title>

      <para> Finally, we can use the classifier we built to classify
      new texts: </para>

<programlisting>
    &prompt; <command>loc = Location(3, unit='s')</command>
    &prompt; <command>test_tok = Token("Do you enjoy classification?", loc)</command>

    &prompt; <command>classifier.classify(test_tok)</command>
    "Do you enjoy classification?"/"question"@[3s]

    &prompt;<command> prob_dict = classifier.distribution_dictionary(test_tok) </command>
    &prompt;<command> for label in prob_dict.keys():</command>
    &prompt2;<command>     print "P(%s) = %.2f" % (label, prob_dict[label])</command>
    P(statement) = 0.24
    P(imperative) = 0.02
    P(question) = 0.74
</programlisting>

    </section> <!-- Classifying New Texts -->

  </section> <!-- Using -->

  <section id="nb" xreflabel="The Naive Bayes Classifier"> 
    <title> The Naive Bayes Classifier </title>

    <para> The Naive Bayes classification algorithm is a simple
    algorithm that can achieve relatively good performance on
    classification tasks.  The algorithm can be used with any type of
    feature; but it is usually used with binary features. </para>

    <section id="nb.using" xreflabel="Using NBClassifier"> 
      <title> Using NBClassifier </title>

      <para> The <literal>nltk.classifier.naivebayes</literal> module
      defines <ulink
      url="&refdoc;/nltk.classifier.naivebayes.NBClassifier.html"><literal>NBClassifier</literal></ulink>,
      a text classifier based on the Naive Bayes classificaiton
      algorithm.  <literal>NBClassifier</literal> uses probability
      estimates for feature value assignments to classify texts.
      These probability estimates are specified using a
      <literal>ProbDist</literal>, whose samples are
      <literal>FeatureValueList</literal>s.  </para>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.naivebayes.NBClassifier.html#__init__"><literal>NBClassifier</literal>
      constructor</ulink> takes three arguments: a
      <literal>FeatureDetectorList</literal>; a list of labels; and a
      probability distribution that provides probability estimates for
      feature value assignments. </para>

<programlisting>
    <emphasis># Build a probability distribution over feature value lists.</emphasis>
    <emphasis># This probability distribution will be used to estimate the</emphasis>
    <emphasis># probabilities of feature value assignments.</emphasis>
    &prompt; <command>prob_dist = SimpleProbDist() </command>
    &prompt; <command>for tok in train_toks: </command>
    &prompt2; <command>    prob_dist.inc(fdlist.detect(tok))</command>

    <emphasis># Build a new Naive Bayes classifier.</emphasis>
    &prompt; <command>classifier = NBClassifier(fdlist, labels, prob_dist)</command>
    &lt;NBClassifier: 3 labels, 2727 features&gt;
</programlisting>

      <para> <literal>NBClassifier</literal> supports all of the
      methods specified by <literal>ClassifierI</literal>: </para>

<programlisting>
    &prompt;<command> classifier.prob(labeled_token)</command>
    0.67
    &prompt;<command> classifier.distribution(token)</command>
    &lt;ProbDist&gt;
    &prompt;<command> classifier.distribution_dictionary(token) </command>
    {'statement': 0.21, 'imperative': 0.12, 'question': 0.67}
    &prompt;<command> classifier.distribution_list(token) </command>
    [0.21, 0.12, 0.67]
</programlisting>

      <note> <para> <literal>NBClassifier</literal> uses
      <literal>AssignmentEvent</literal>s to find the probability of
      each feature value assignment.  If the
      <literal>NBClassifier</literal> is to be efficient, then its
      probability distribution's <literal>prob()</literal> method
      should process <literal>AssignmentEvent</literal>s efficiently.
      This will be discussed further in <xref
      linkend="nb.impl"></xref>. </para> </note>

    </section> <!-- Using NBClassifier -->

    <section id="nb.algorithms" xreflabel="Algorithms"> 
      <title> Algorithms </title>
      
      <para> This section discusses the algorithms used by
      <literal>NBClassifier</literal>; the next section will describe
      how these algorithms are implemented. </para>

      <para> Given a labeled text, the Naive Bayes classifier finds
      the feature value assignments for that labeled text; and uses
      the joint probability of those assignments to estimate the
      probability of the labeled text.  In other words, let: </para>

      <itemizedlist>
        <listitem> <para> &ltext; = &ltextlt; be a labeled
        text.</para>
        </listitem>
        <listitem> <para>
        <replaceable>f<subscript>i</subscript></replaceable> be the
        feature whose feature id is
        <replaceable>i</replaceable>.</para>
        </listitem>
        <listitem> <para>
        <replaceable>fd<subscript>i</subscript></replaceable> be the
        feature detector for
        <replaceable>f<subscript>i</subscript></replaceable>. </para>
        </listitem>
      </itemizedlist>
      
      <para> Then the Naive Bayes classifier uses the following
      estimate for the probability of &ltext;:
      </para>

      <itemizedlist>
        <listitem>
          <para>
          P(&ltext;) &ap; P(&f1; = &fd1lt;, &f2; = &fd2lt;, ..., &fn;
          = &fdnlt;)
          </para>
        </listitem>
      </itemizedlist>
      
      <para> However, we usually don't have direct access to reliable
      estimates of P(&f1; = &v1;, &f2; = &v2;, ..., &fn; = &vn;).  The
      Naive Bayes classifier therefore combines separate estimates for
      the probability of each assignment, using the Naive Bayes
      assumption. </para>

      <section id="nb.algorithms.nbassumption" xreflabel="The Naive Bayes Assumption"> 
        <title> The Naive Bayes Assumption </title>

        <para> The Naive Bayes assumption states that each feature
        value assignment is probabilistically independant of all other
        feature value assignments.  In other words: </para>

        <itemizedlist>
          <listitem>
            <para>
          P(<replaceable>f<subscript>i</subscript></replaceable> =
          <replaceable>x</replaceable>,
          <replaceable>f<subscript>j</subscript></replaceable> =
          <replaceable>y</replaceable>) =
          P(<replaceable>f<subscript>i</subscript></replaceable> =
          <replaceable>x</replaceable>) &times;
          P(<replaceable>f<subscript>j</subscript></replaceable> =
          <replaceable>y</replaceable>)
          </para>
            <para> For all <replaceable>i</replaceable> &ne;
            <replaceable>j</replaceable>. </para>
          </listitem>
        </itemizedlist>

        <para> Although the Naive Bayes assumption is usually not
        strictly true, it is often approximately correct.  Thus, we
        can use it to find a reasonable estimate for P(&f1; = &v1;,
        &f2; = &v2;, ..., &fn; = &vn;): </para>

        <itemizedlist>
          <listitem>
            <para>
          P(&f1; = &v1;, &f2; = &v2;, ..., &fn; = &vn;) &ap; P(&f1; =
          &v1;) &times; P(&f2; = &v2;) &times; ... &times; P(&fn; =
          &vn;)
          </para>
          </listitem>
        </itemizedlist>

        <para> Thus, we have the following estimate for the
        probability of a labeled text: </para>

        <itemizedlist>
          <listitem>
            <para>
          &Phat;(&ltext;) &ap; P(&f1; = &fd1lt;) &times; P(&f2; =
          &fd2lt;) &times; ... &times; P(&fn; = &fdnlt;)
          </para>
          </listitem>
        </itemizedlist>

        <para> This estimate forms the basis for all of the
        classification decisions made by the Naive Bayes
        classifier. </para>

      </section> <!-- Naive Bayes Assumption -->

      <section id="nb.algorithms.classifying" xreflabel="Classifying Texts"> 
        <title> Classifying Texts </title>

        <para> To classify an unlabeled text
        <replaceable>t</replaceable>, we find the label &l; that
        maximizes P(&l;|&t;):
      </para>

        <itemizedlist>
          <listitem>
            <para>classify(&t;) = ARGMAX<subscript>&l;</subscript>
            P(&l;|&t;)
          </para>
          </listitem>
        </itemizedlist>

        <para> Using Bayes Rule, we can rewrite this as: </para>
        
        <itemizedlist>
          <listitem>
            <para>classify(&t;) = ARGMAX<subscript>&l;</subscript>
            P(&ltextlt;) / P(&t;)</para>
          </listitem>
        </itemizedlist>
        
        <para> Since P(&t;) does not depend on &l;, we can simplify
        this equation to:</para>

        <itemizedlist>
          <listitem>
            <para>classify(&t;) = ARGMAX<subscript>&l;</subscript>
            P(&ltextlt;)</para>
          </listitem>
        </itemizedlist>

        <para> Using the Naive Bayes assumption, we can approximate
        this as: </para>

        <itemizedlist>
          <listitem>
            <para>classify(&t;) &ap; ARGMAX<subscript>&l;</subscript>
            &Phat;(&ltextlt;) </para>
          </listitem>
        </itemizedlist>

      </section> <!-- Classifying -->

      <section id="nb.algorithms.prob" xreflabel="Estimating Label Probabilities"> 
        <title> Estimating Label Probabilities </title>

        <para> We use a similar approach for estimating the
        probability of a label, given a text.  We wish to find
        P(&l;|&t;).  Using Bayes Rule, we can rewrite this as: </para>
        
        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) = P(&ltextlt;) / P(&t;)</para>
          </listitem>
        </itemizedlist>
        
        <para> P(&t;) is equal to the sum of P(&ltextlt;) over all
        values of &l;: </para>

        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) = P(&ltextlt;) /
            &sum;<subscript>&lprime;</subscript>
            P(&ltextlprimet;)</para>
          </listitem>
        </itemizedlist>
        
        <para> Using the Naive Bayes approximation, we can approximate
        P(&l;|&t;) as:</para>
        
        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) &ap; &Phat;(&ltextlt;) /
            &sum;<subscript>&lprime;</subscript>
            &Phat;(&ltextlprimet;)</para>
          </listitem>
        </itemizedlist>
        
      </section> <!-- Estimating P(label|text) -->

      <section id="nb.algorithms.optimization" xreflabel="Factoring Out Default Assignments">
        <title> Factoring Out Default Assignments </title>

        <para> In <xref linkend="nb.algorithms.nbassumption"></xref>,
        we proposed the following estimation for the probability of a
        labeled text: </para>

        <itemizedlist>
          <listitem>
            <para> &Phat;(&ltext;) &ap; &prod; <subscript>0&le;i
            &lt;n</subscript> P(&fi; = &fdilt;) </para>
          </listitem>
        </itemizedlist>

        <para> Direct implementation of this formula is inefficient
        for sparse feature value lists.  Since most feature value
        lists are sparse, <literal>NBClassifier</literal> uses the
        following equivalant formulation, which factors out the
        product of probabilities for feature assignments with default
        values: </para>

        <itemizedlist>
          <listitem>
            <para> &Phat;(&ltext;) &ap; C &times; Q(&ltext;) </para>

            <para> C = &prod; <subscript>0&le;i&lt;n</subscript>
            P(&fi; = D) </para>

            <para> Q(&ltext;) = &prod;<subscript>{i: &fdilt; &ne;
            D}</subscript> (P(&fi; = &fdilt;) / P(&fi; = D))</para>

          </listitem>
        </itemizedlist>

        <para> Where D is the <literal>FeatureValueList</literal>'s
        default value.  Note that C does not depend on &ltext; in any
        way.  Q only depends on the non-default features of the
        labeled text; thus, we can efficiently calculate its value
        using the <literal>assignments</literal> method. </para>

        <para> Plugging this reformulation into our equation for
        classifying texts yields: </para>

        <itemizedlist>
          <listitem>
            <para>classify(&t;) &ap; ARGMAX<subscript>&l;</subscript>
            &Phat;(&ltextlt;) </para>
            <para>classify(&t;) &ap; ARGMAX<subscript>&l;</subscript>
            C &times; Q(&ltextlt;) </para>
          </listitem>
        </itemizedlist>

        <para> Since C does not depend on &l;, we can simplify this
        equation to: </para>

        <itemizedlist>
          <listitem>
            <para>classify(&t;) &ap; ARGMAX<subscript>&l;</subscript>
            Q(&ltextlt;) </para>
          </listitem>
        </itemizedlist>

        <para> Plugging the refolmulation into our equation for
        finding P(&l;|&t;) yields:</para>

        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) &ap; &Phat;(&ltextlt;) /
            &sum;<subscript>&lprime;</subscript>
            &Phat;(&ltextlprimet;)</para>

            <para>P(&l;|&t;) &ap; (C &times; Q(&ltextlt;)) /
            &sum;<subscript>&lprime;</subscript>
            (C &times; Q(&ltextlprimet;))</para>
          </listitem>
        </itemizedlist>

        <para> Which simplifies to: </para>

        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) &ap; Q(&ltextlt;) /
            &sum;<subscript>&lprime;</subscript>
            Q(&ltextlprimet;)</para>
          </listitem>
        </itemizedlist>

        <para> Since the simplified equations for classifying texts
        and finding P(&l;|&t;) do not depend on C, we don't actually
        need to calculate it. </para>
        
      </section> <!-- optimization -->

    </section> <!-- Algorithms -->

    <section id="nb.impl" xreflabel="NBClassifier Implementation"> 
      <title> NBClassifier Implementation </title>

      <para> describe the NBClassifier implementation/interface
      </para>

    </section> <!-- Classifying -->

    <section id="nb.training" xreflabel="Training the Naive Bayes Classifier"> 
      <title> Training the Naive Bayes Classifier </title>

      <para> describe manual training; then NBClassifierTrainer
      </para>

    </section> <!-- Classifying -->

  </section> <!-- NB Classifier -->

  <section id="maxent" xreflabel="The Maximum Entropy Classifier"> 
    <title> The Maximum Entropy Classifier </title>

    <para> ... </para>

  </section> <!-- NB Classifier -->

  <section id="fselection" xreflabel="Feature Selection"> 
    <title> Feature Selection </title>

    <para> ... </para>

  </section> <!-- Feature Selection -->

</article>