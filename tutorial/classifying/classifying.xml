<?xml version="1.0"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.1//EN" [

<!-- Base URL for the reference documentation -->
<!ENTITY refdoc "http://nltk.sourceforge.net/ref">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!-- Variables for algorithms -->
<!ENTITY l "<replaceable>l</replaceable>">
<!ENTITY lprime "<replaceable>l</replaceable>'">
<!ENTITY t "<replaceable>t</replaceable>">
<!ENTITY v "<replaceable>v</replaceable>">
<!ENTITY f1 "<replaceable>f</replaceable><subscript>1</subscript>">
<!ENTITY v1 "<replaceable>v</replaceable><subscript>1</subscript>">
<!ENTITY fd1lt "<replaceable>fd</replaceable><subscript>1</subscript>(<replaceable>lt</replaceable>)">
<!ENTITY f2 "<replaceable>f</replaceable><subscript>2</subscript>">
<!ENTITY v2 "<replaceable>v</replaceable><subscript>2</subscript>">
<!ENTITY fd2lt "<replaceable>fd</replaceable><subscript>2</subscript>(<replaceable>lt</replaceable>)">
<!ENTITY fn "<replaceable>f</replaceable><subscript>n</subscript>">
<!ENTITY vn "<replaceable>v</replaceable><subscript>n</subscript>">
<!ENTITY fdnlt "<replaceable>fd</replaceable><subscript>n</subscript>(<replaceable>lt</replaceable>)">
<!ENTITY fdilt "<replaceable>fd</replaceable><subscript>i</subscript>(<replaceable>lt</replaceable>)">
<!ENTITY fdiltextlt "<replaceable>fd</replaceable><subscript>i</subscript>(<literal>LabeledText(<replaceable>l</replaceable>, <replaceable>t</replaceable>)</literal>)">
<!ENTITY ltext "<replaceable>lt</replaceable>">
<!ENTITY fvlist "<replaceable>fvlist</replaceable>">
<!ENTITY ltextlt "<literal>LabeledText(<replaceable>l</replaceable>,<replaceable>t</replaceable>)</literal>">
<!ENTITY ltextlprimet "<literal>LabeledText(<replaceable>l</replaceable>',<replaceable>t</replaceable>)</literal>">
<!ENTITY Phat "P<superscript>*</superscript>">
<!ENTITY Lhat "L<superscript>*</superscript>">
<!ENTITY vi "<replaceable>v</replaceable><subscript>i</subscript>">
<!ENTITY fi "<replaceable>f</replaceable><subscript>i</subscript>">
<!ENTITY fj "<replaceable>f</replaceable><subscript>j</subscript>">
<!ENTITY wi "<replaceable>w</replaceable><subscript>i</subscript>">
<!ENTITY fdilt "<replaceable>fd</replaceable><subscript>i</subscript>(<replaceable>lt</replaceable>)">
<!ENTITY lambdai "<replaceable>&lambda;</replaceable><subscript>i</subscript>">
]>

<book>
  <bookinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Text Classification</title>
  </bookinfo>

  <chapter id="intro" xreflabel="Text Classification"> 
    <title> Introduction </title>

    <para> Often, we wish to divide texts into categories.  For
    example, we might want to categorize news stories by topic; or to
    divide sentences into questions and statements.  In this tutorial,
    we will consider <glossterm>single-category text
    classification</glossterm> problems, in which: </para>

    <itemizedlist>
      <listitem> <para> There are a predefined set of
      categories.</para>
      </listitem>
      <listitem> <para> Each text belongs to exactly one
      category.</para>
      </listitem>
    </itemizedlist>

    <para> <glossterm>Text classification</glossterm> is the task of
    chosing the most likely category for a given text. </para>

    <section id="intro.othertasks" xreflabel="Relationship to Other Tasks"> 
      <title> Relationship to Other Tasks </title>

      <para> Single-category text classification is related to a
      number of other categorization problems.  The following list
      describes how single-category text classification differs from
      three related problems: </para>

      <itemizedlist>
        <listitem>
          <para>In <emphasis>multi-category text
          classification</emphasis>, each text can have zero or more
          categories.</para>
        </listitem>
        <listitem>
          <para>In <emphasis>clustering</emphasis>, the set of
          categories is not predefined.</para>
        </listitem>
        <listitem>
          <para>In <emphasis>tagging</emphasis>, we attempt to
          categorize each element in a sequence of texts (as opposed
          to text classification, where we categorize individual
          texts).</para>
        </listitem>
      </itemizedlist>

    </section> <!-- Relationship to other tasks -->

  </chapter> <!-- Introduction -->

  <chapter id="labeledtexts" xreflabel="Labeled Texts"> 
    <title> Labeled Texts </title>

    <para> Each category is uniquely identified by a
    <glossterm>label</glossterm>.  Labels are typically
    <literal>string</literal>s or <literal>integer</literal>s.  For
    example, if we are classifying news stories, our category labels
    might include <literal>'sports'</literal>,
    <literal>'technology'</literal>, and <literal>'foreign
    affairs'</literal>. </para>

    <para> Categorized text types are represented using the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html"
    ><literal>LabeledText</literal></ulink> class, which is defined by
    the <literal>nltk.classifier</literal> module.  New
    <literal>LabeledText</literal>s are created using the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html#__init__"
    ><literal>LabeledText</literal> constructor</ulink>:</para>

<programlisting>
    &prompt; <command>text = "What's your name?"</command>
    &prompt; <command>label = "question"</command>
    &prompt; <command>labeled_text = LabeledText(text, label)</command>
    "What's your name?"/"question"
</programlisting>

    <important>
      <para> We may decide to change the printed representation for
      labeled texts; currently, it is identical to the printed
      representation for <literal>TaggedType</literal>s.</para>
    </important>

    <para> A <literal>LabeledText</literal>'s underlying text is
    accessed via the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html#text">
    <literal>text</literal></ulink> member function; and its label is
    accessed via the <ulink
    url="&refdoc;/nltk.classifier.LabeledText.html#label">
    <literal>label</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> labeled_text.text() </command>
    "What's your name?"
    &prompt;<command> labeled_text.label() </command>
    "question"
</programlisting>

    <para> A <glossterm>labeled text token</glossterm> is a
    <literal>Token</literal>s whose type is a
    <literal>LabeledText</literal>: </para>

<programlisting>
    &prompt;<command> loc = Location(3, unit='s')</command>
    @[3s]
    &prompt;<command> labeled_token = Token(labeled_text, loc)</command>
    "What's your name?"/"question"@[3s]
</programlisting>

  </chapter> <!-- Labeled Texts -->

  <chapter id="classifieri" xreflabel="The Classifier Interface"> 
    <title> The Classifier Interface </title>

    <para> The <literal>nltk.classifier</literal> module defines
    <ulink url="&refdoc;/nltk.classifier.ClassifierI.html">
    <literal>ClassifierI</literal></ulink>, a general interface for
    classifying texts.  This interface is used by all single-category
    text classifiers.  It requires that classifiers define two
    methods: </para>

    <itemizedlist>
      <listitem>
        <para><ulink
        url="&refdoc;/nltk.classifier.ClassifierI.html#classify">
        <literal>classify</literal></ulink> determines which label is
        most appropriate for a given text token, and returns a labeled
        text token with that label. </para>
      </listitem>
      <listitem>
        <para><ulink
        url="&refdoc;/nltk.classifier.ClassifierI.html#labels">
        <literal>labels</literal></ulink> returns the list of category
        labels that are used by the classifier. </para>
      </listitem>
    </itemizedlist>

    <para> These two methods are illustrated in the following example:
    </para>

<programlisting>
    &prompt;<command> token = Token("What's your name?", loc)</command>
    "What's your name?"@[3s]
    &prompt;<command> my_classifier.classify(token)</command>
    "What's your name?"/"question"@[3s]

    &prompt;<command> my_classifier.labels()</command>
    ("statement", "imperative", "question")
</programlisting>

    <section id="classifieri.optional" xreflabel="Optional Methods"> 
      <title> Optional Methods </title>

      <para> Classifiers are also encouraged (but not required) to
      define four additional methods:</para>

      <itemizedlist>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#prob">
          <literal>prob</literal></ulink> takes a labeled text token,
          and returns the probability that the token's text belongs to
          the category indicated by the token's label. </para>
        </listitem>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#distribution">
          <literal>distribution</literal></ulink> takes a text token,
          and returns a probability distribution, whose samples are
          labeled text tokens.  This probability distribution
          indicates the likelihood that the text belongs to each
          label's category. </para>
        </listitem>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#distribution_dictionary">
          <literal>distribution_dictionary</literal></ulink> takes a
          text token, and returns a dictionary, mapping from each
          label to the probability that the text is a member of that
          label's category.</para>
        </listitem>
        <listitem>
          <para><ulink
          url="&refdoc;/nltk.classifier.ClassifierI.html#distribution_list">
          <literal>distribution_list</literal></ulink> takes a text
          token, and returns a list of probabilities, where the
          <replaceable>i</replaceable>th element of the list is the
          probability that the text belongs to the category indicated
          by
          <literal>classifier.labels()[<replaceable>i</replaceable>]</literal>.
          </para>
        </listitem>
      </itemizedlist>

      <para> Each of these methods is illustrated in the following
      example: </para>

<programlisting>
    &prompt;<command> print labeled_token</command>
    "What's your name?"/"question"@[3s]

    <emphasis># What's the probability that "What's your name?" is a question?</emphasis>
    &prompt;<command> my_classifier.prob(labeled_token)</command>
    0.67

    <emphasis># What's the probability that "What's your name?" is a question,</emphasis>
    <emphasis># given that it's not a statement? </emphasis>
    &prompt;<command> prob_dist = my_classifier.distribution(token)</command>
    &prompt;<command> def not_statement(ltoken): ltoken.type().label()!='statement'</command>
    &prompt;<command> prob_dist.cond_prob(labeled_token, PredEvent(not_statement))</command>
    0.85

    <emphasis># What is the probability for each category?</emphasis>
    &prompt;<command> prob_dict = my_classifier.distribution_dictionary(token) </command>
    &prompt;<command> for label in prob_dict.keys():</command>
    &prompt2;<command>     print "P(%s) = %.2f" % (label, prob_dict[label])</command>
    P(statement) = 0.21
    P(imperative) = 0.12
    P(question) = 0.67

    <emphasis># What is the probability for each category?</emphasis>
    &prompt;<command> zip(my_classifier.labels(), my_classifier.distribution_list(token))</command>
    [('statement', 0.21), ('imperative', 0.12), ('question', 0.67)]
</programlisting>

    </section> <!-- Optional Methods -->

  </chapter> <!-- Classifier Interface -->

  <chapter id="training" xreflabel="Training Classifiers"> 
    <title> Training Classifiers </title>
    
    <para> Often, we have access to a corpus of classified texts.
    This <glossterm>training corpus</glossterm> is usually
    hand-classified, and we assume that most of the classifications
    are correct. </para>

    <!-- !!!! Add more precise forward reference? !!!! --> <para> We
    can use statistical methods to build classifiers from a training
    corpus.  Some of these statistical methods are discussed in
    following sections.  </para>

    <para> The <literal>nltk.classifier</literal> module defines
    <ulink url="&refdoc;/nltk.classifier.ClassifierTrainerI.html">
    <literal>ClassifierTrainerI</literal></ulink>, a general interface
    for building classifiers from training corpora.  It requires that
    classifiers define a single method, <ulink
    url="&refdoc;/nltk.classifier.ClassifierTrainerI.html#train">
    <literal>train</literal></ulink>, which takes a list of labeled
    tokens, and returns a new classifier.  For example, the following
    code builds a new Naive Bayes classifier from a training corpus:
    </para>

<programlisting>
    &prompt;<command> from nltk.classifier.naivebayes import NBClassifierTrainer</command>
    &prompt;<command> classifier = NBClassifierTrainer().train(labeled_tokens)</command>
    &lt;NBClassifier: 3 labels, 187 features&gt;
</programlisting>

  </chapter> <!-- Training Classifiers -->

  <chapter id="features" xreflabel="Feature-Based Classification"> 
    <title> Feature-Based Classification </title>

    <para> Most text classification algorithms do not depend on the
    specific details of the task being performed.  The same algorithms
    that we use to categorize documents by topic can also be used to
    categorize words by their part of speech, or to categorize
    acoustic signals according to which phoneme they represent.
    <glossterm>Features</glossterm> provide a standard way of encoding
    the information used to make classification decisions.  This
    standard encoding abstracts away from the details of specifc
    tasks, allowing the same classifier algorithm to be used to solve
    many different problems. </para>

    <para> Each feature specifies some aspect of a
    <literal>LabeledText</literal> that is relevant to deciding how
    likely that <literal>LabeledText</literal> is to occur.  These
    features can be used by classification algorithms to examine how
    likely different labels are for a given text.  A typical example
    of a feature is:</para>

    <itemizedlist>
      <listitem>
        <para>Whether a document contains the word
        <literal>"ball"</literal> and has the label
        <literal>"sports"</literal>.</para>
      </listitem>
    </itemizedlist>

    <para> Notice that features depend on both the text and the label.
    This allows us to specify which aspects of a text are relevant for
    making decisions about which labels. </para>

    <para> Features can be defined using <glossterm>feature detector
    functions</glossterm>, which map <literal>LabeledText</literal>s
    to values.  For example, the feature given above could be defined
    using the following function:</para>

    <screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (("ball" in ltext.text) and (ltext.label == "sports"))</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

    <para> The <glossterm>feature values</glossterm> produced by
    feature detector functions are usually booleans or integers; but
    occasionally, real-valued features are used. </para>
    
    <caution> 

      <para>Many people use the term "feature" as we do, to refer to
      an <emphasis>aspect</emphasis> of a labeled text; but some
      people use the term to refer to the aspect's
      <emphasis>value</emphasis>.  To avoid confusion, the following
      list gives explicit definitions of the terminology used by
      NLTK:</para>

      <itemizedlist>
        <listitem> <para> <emphasis>feature:</emphasis> An aspect of a
        <literal>LabeledText</literal> that is relevant to
        classification. </para>
        </listitem>
        <listitem> <para> <emphasis>feature detector
        function:</emphasis> A function that defines a feature.
        Feature detector functions map <literal>LabeledText</literal>s
        to feature values.</para>
        </listitem>
        <listitem> <para> <emphasis>feature value:</emphasis> The
        value returned by a feature detector function for a particular
        <literal>LabeledText</literal>. </para>
        </listitem>
      </itemizedlist>

    </caution>

    <section id="features.types" xreflabel="Feature Types"> 
      <title> Feature Types </title>

      <section id="features.types.bool" xreflabel="Boolean Features"> 
        <title> Boolean Features </title>

        <para> <glossterm>Boolean features</glossterm> are features
        that generate boolean values.  Boolean features are the
        simplest and the most common type of feature.  Boolean feature
        detector functions usually have the form: </para>

        <screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if <replaceable>pred</replaceable>(ltext.text) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

        <para> for some boolean function
        <replaceable>pred</replaceable> and some label
        <replaceable>L</replaceable>.  Boolean features are also
        called <glossterm>binary features</glossterm>.</para>

        <para> Boolean features have a simple probablistic
        interpretation: we can think of them as events defined in the
        probability space of <literal>LabeledText</literal>s.</para>

      </section> <!-- Boolean Features -->

      <section id="features.types.int" xreflabel="Integer Features"> 
        <title> Integer Features </title>

        <para> <glossterm>Integer features</glossterm> are features
        that generate integer values.  Integer feature detector
        functions usually have the form:</para>

        <screen>
    <replaceable>f</replaceable>(ltext)  =  <replaceable>g</replaceable>(ltext.text)  <emphasis>if ltext.label == <replaceable>L</replaceable></emphasis>
                       0        <emphasis>otherwise</emphasis>
</screen>
        
        <para> for some integer valued function
        <replaceable>g</replaceable> and some label
        <replaceable>L</replaceable>. </para>

        <para> Integer features can be used to give classifiers access
        to more precise information about the text.  A typical example
        of an integer feature detector function is: </para>

        <screen>
    <replaceable>f</replaceable>(ltext)  =  num_occurances("ball", ltext.text)  <emphasis>if ltext.label == "sports"</emphasis>
                               0                     <emphasis>otherwise</emphasis>
</screen>

      </section> <!-- Integer Features -->
      
      <section id="features.types.other" xreflabel="Other Feature Types"> 
        <title> Other Feature Types </title>

        <para> Some classifier algorithms use real-valued features, or
        even more exotic kinds of features.  NLTK allows features
        values to be any immutable type; but currently, NLTK only
        implements classifier trainers for boolean and integer
        features. </para>
        
      </section> <!-- Other Features -->

    </section> <!-- Feature Types -->

    <section id="fdetectors" xreflabel="Feature Detectors"> 
      <title> Feature Detectors </title>

      <para> The <literal>nltk.classifier.feature</literal> module
      defines <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureDetectorI.html">
      <literal>FeatureDetectorI</literal></ulink>, a general interface
      for implementing feature detector functions.  Feature detectors
      are required to define a single method, <ulink
      url="&refdoc;/nltk.classifier.feature.FeatureDetectorI.html#detect">
      <literal>detect</literal></ulink>, which takes a labeled text,
      and returns a feature value. </para>

      <para> <ulink
      url="&refdoc;/nltk.classifier.feature.FunctionFeatureDetector.html">
      <literal>FunctionFeatureDetector</literal></ulink> provides a
      simple flexible implementation of the
      <literal>FeatureDetectorI</literal> interface.
      <literal>FunctionFeatureDetector</literal>s are created from
      Python functions with the <ulink
      url="&refdoc;/nltk.classifier.feature.FunctionFeatureDetector.html#__init__">
      <literal>FunctionFeatureDetector</literal>
      constructor</ulink>:</para>

<programlisting>
    &prompt; <command>def ball_sports(ltext):</command>
    &prompt2; <command>    return ("ball" in ltext.text()) and (ltext.label() == "sports")</command>
    &prompt; <command>fdetector = FunctionFeatureDetector(ball_sports)</command>
    &lt;FeatureDetector: ball_sports&gt;
</programlisting>

      <para> Once we have constructed a function detector, we can use
      the <literal>detect</literal> method to find the feature values
      for various labeled texts: </para>

<programlisting>
    &prompt; <command>document1 = "John threw the ball over the fence".split()</command>
    &prompt; <command>document1 = "Mary solved the equation".split()</command>
    &prompt; <command>fdetector.detect(LabeledText(document1, "sports"))</command>
    1
    &prompt; <command>fdetector.detect(LabeledText(document2, "sports"))</command>
    0
    &prompt; <command>fdetector.detect(LabeledText(document1, "foreign affairs"))</command>
    0
</programlisting>

    </section> <!-- Feature Detectors -->

    <section id="fdlists" xreflabel="Feature Detector Lists"> 
      <title> Feature Detector Lists </title>

      <para> <glossterm>Feature detector lists</glossterm> are data
      structures that represent the feature detector functions for a
      set of features.  Feature detector lists serve three important
      functions: </para>

      <itemizedlist>
        <listitem>
          <para> They provide a mechanism for grouping feature
          detectors together.</para>
        </listitem>
        <listitem>
          <para> They associate a unique identifier with each feature
          detector.</para>
        </listitem>
        <listitem>
          <para> They allow for efficient implementations for sets of
          related feature detectors. </para>
        </listitem>
      </itemizedlist>

      <para> Abstractly, a feature detector list can be thought of as
      a <literal>list</literal> of
      <literal>FeatureDetector</literal>s.  The index of each feature
      detector in the list serves as a unique identifier for that
      detector's feature.  This identifier is known as a
      <glossterm>feature id</glossterm>.  The feature ids for a
      feature detector list with <emphasis>N</emphasis> features are
      <emphasis>0, 1, ..., N-1</emphasis>. </para>

      <warning>
        <para> Feature ids uniquely identify a feature, given a
        feature detector list.  However, they are not globally unique
        identifiers.  Thus, a single feature might have different
        identifiers in different feature detector lists; and two
        different features might have the same feature id if they are
        from different feature detector lists.  Care must be taken not
        to mix feature ids from different feature detector
        lists. </para>
      </warning>

      <section id="fdlists.fdlisti" xreflabel="FeatureDetectorListI"> 
        <title> FeatureDetectorListI </title>

        <para> The <literal>nltk.classifier.feature</literal> module
        defines <ulink
        url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html">
        <literal>FeatureDetectorListI</literal></ulink>, a general
        interface for implementing feature detector lists.  Feature
        detector lists are required to implement four methods: an
        indexing operator; a length operator; a
        <literal>detect</literal> method; and an addition
        operator.</para>

        <section id="fdlists.fdlisti.indexing" xreflabel="The indexing operator">
          <title> The indexing operator </title>

          <para> The <ulink
          url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#__getitem__">indexing
          operator</ulink> allows feature detector lists to be treated
          as lists of <literal>FeatureDetector</literal>s:</para>

<programlisting>
    <emphasis># fdlist is a feature detector list.</emphasis>
    &prompt; <command>print fdlist</command>
    &lt;FeatureDetectorList with 6 features&gt;

    <emphasis># Print the third feature detector. </emphasis>
    &prompt; <command>fdlist[2]</command>
    &lt;FeatureDetector: ball_sports&gt;

    <emphasis># Print the feature values generated for labeled_text.</emphasis>
    &prompt; <command>labeled_text = LabeledText(document2, "foreign affairs")</command>
    &prompt; <command>for fdetector in fdlist:</command>
    &prompt2; <command>    print fdetector, fdetector.detect(labeled_text)</command>
    &lt;FeatureDetector: ball_foreign_affairs&gt; 0
    &lt;FeatureDetector: ball_weather&gt; 0
    &lt;FeatureDetector: ball_sports&gt; 0
    &lt;FeatureDetector: solved_foreign_affairs&gt; 1
    &lt;FeatureDetector: solved_weather&gt; 0
    &lt;FeatureDetector: solved_sports&gt; 0
</programlisting>

        </section> <!-- The indexing operator -->

        <section id="fdlists.fdlisti.length" xreflabel="The length operator">
          <title> The length operator </title>

          <para> The <ulink
          url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#__len__">length
          operator</ulink> returns the number of features represented
          by a feature detector list: </para>

<programlisting>
    &prompt; <command>len(fdlist)</command>
    6
</programlisting>

        </section> <!-- The length operator -->

        <section id="fdlists.fdlisti.detect" xreflabel="The detect method">
          <title> The detect method </title>

          <para> The <ulink
          url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#detect"><literal>detect</literal></ulink>
          method takes a labeled text, and finds the feature value for
          each feature detector.  These values are returned in a
          structure called a "feature value list," which is discussed
          in the next section. Conceptually, the
          <literal>detect</literal> method is equivalant to applying
          each individual feature detector's <literal>detect</literal>
          method in parallel.  The following diagram compares the use
          of a single feature detector to the use of a feature
          detector list: </para>
          <screen>
                        FeatureDetectorI
                      +------------------+
   labeled text  -->  | feature detector |  -->    feature value
                      +------------------+


                      FeatureDetectorListI       FeatureValueListI
                      +------------------+       +---------------+
                      | feature detector |  -->  | feature value |
                      |------------------|       |---------------|
                      | feature detector |  -->  | feature value |
                      |------------------|       |---------------|
   labeled text  -->  | feature detector |  -->  | feature value |
                      |------------------|       |---------------|
                      |       . . .      |  -->  |     . . .     | 
                      |------------------|       |---------------|
                      | feature detector |  -->  | feature value |
                      +------------------+       +---------------+
</screen>

          <para> The following example finds the feature values for
          <literal>document2</literal> with label <literal>"foreign
          affairs"</literal>: </para>

<programlisting>
    &prompt; <command>fdlist.detect(LabeledText(document2, "foreign affairs"))</command>
    &lt;FeatureValueList with 6 features&gt;
</programlisting>

        </section> <!-- The detect method -->

        <section id="fdlists.fdlisti.add" xreflabel="The addition operator">
          <title> The addition operator </title>

          <para> The <ulink
          url="&refdoc;/nltk.classifier.feature.FeatureDetectorListI.html#__add__">addition
          operator</ulink> can be used to combine feature detector
          lists. </para>

<programlisting>
    <emphasis># fdlist1 and fdlist2 are feature detector lists.</emphasis>
    &prompt; <command>print fdlist1</command>
    &lt;FeatureDetectorList with 6 features&gt;
    &prompt; <command>print fdlist2</command>
    &lt;FeatureDetectorList with 72 features&gt;

    &prompt; <command>new_fdlist = fdlist1 + fdlist2</command>
    &lt;FeatureDetectorList with 78 features&gt;
</programlisting>

          <para> Note that the feature ids for the feature list
          produced by addition may be different from the feature ids
          for the two constituant feature lists: </para>

<programlisting>
    &prompt; <command>print fdlist2[3]</command>
    &lt;FeatureDetector: running_weathers&gt;

    &prompt; <command>print new_fdlist[3]</command>
    &lt;FeatureDetector: asleep_sports&gt;
</programlisting>

        </section> <!-- The addition operator -->

      </section> <!-- FeatureDetectorListI -->

      <section id="fdlists.efficiency" xreflabel="Efficiency"> 
        <title> Efficiency </title>

        <para> Although feature detector lists can be abstractly
        thought of as <literal>list</literal>s of
        <literal>FeatureDetector</literal>s, they are not usually
        implemented that way, for efficiency reasons. </para>

        <para> Many of the features used for classification are
        closely related to each other.  For example, a document
        classifier might use features that examine whether a given
        word is in a document.  If the feature detector list were
        implemented as a list of independant feature detectors, then
        we would need to apply each feature detector separately.  This
        would require scanning through the document once for each word
        we are interested in.</para>

        <para> Instead, we can build a feature detector list that will
        check for all relevant words at the same time.  This feature
        detector list makes a single pass through the document.  Each
        time it encounters a word that we are interested in, it
        updates the corresponding feature value.  This approach allows
        for significantly more efficient feature detection. </para>

      </section> <!-- Efficiency -->

      <section id="fdlists.textfunctionfdlist" xreflabel="TextFunctionFDList"> 
        <title> TextFunctionFDList </title>

        <para> The <literal>nltk.classifier.feature</literal> module
        defines <ulink
        url="&refdoc;/nltk.classifier.feature.TextFunctionFDList.html">
        <literal>TextFunctionFDList</literal></ulink>, a flexible
        implementation of the <literal>FeatureDetectorListI</literal>
        interface.  <literal>TextFunctionFDList</literal> implements
        feature detector lists consisting of boolean features whose
        detector functions have the form: </para>

        <screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (<replaceable>g</replaceable>(ltext.text) == <replaceable>val</replaceable>) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

        <para> For a given function <replaceable>g</replaceable>, and
        for each function value <replaceable>val</replaceable> and
        label <replaceable>L</replaceable>.
        <literal>TextFunctionFDList</literal> gets its name from the
        function <replaceable>g</replaceable>, since it is a function
        defined over texts. </para>
        
        <para> <literal>TextFunctionFDList</literal>s are created
        using the <ulink
        url="&refdoc;/nltk.classifier.feature.TextFunctionFDList.html#__init__">
        <literal>TextFunctionFDList</literal> constructor</ulink>,
        which takes a function, a list of function values, and a list
        of labels.  The new feature detector list contains one feature
        detector for each (<replaceable>val</replaceable>,
        <replaceable>L</replaceable>) pair.  The following example
        constructs a feature detector list that checks the length of
        an sentence: </para>

<programlisting>
    &prompt; <command>def length(text): return len(text) </command>
    &prompt; <command>length_range = range(0, 25) </command>
    &prompt; <command>labels = ('statement', 'imperative', 'question')</command>
    &prompt; <command>fdlist = TextFunctionFDList(length, length_range, labels)</command>
    &lt;FeatureDetectorList with 75 features&gt;
</programlisting>

      </section> <!-- TestFunctionFDList -->

      <section id="fdlists.bagofwordsfdlist" xreflabel="BagOfWordsFDList"> 
        <title> BagOfWordsFDList </title>

        <para> The <literal>nltk.classifier.feature</literal> module
        also defines <ulink
        url="&refdoc;/nltk.classifier.feature.TextFunctionFDList.html">
        <literal>BagOfWordsFDList</literal></ulink>, a feature
        detector list implementation which checks which words are
        present in a text.  In particular, it implements a feature
        detector list containing boolean features whose detector
        functions have the form: </para>

        <screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (<replaceable>w</replaceable> in ltext.text) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

        <para> For each word <replaceable>w</replaceable> and label
        <replaceable>L</replaceable>. </para>

        <para> <literal>BagOfWordsFDList</literal>s are created using
        the <ulink
        url="&refdoc;/nltk.classifier.feature.BagOfWordsFDList.html#__init__">
        <literal>BagOfWordsFDList</literal> constructor</ulink>, which
        takes a list of relevant words and a list of labels.  The new
        feature detector list contains one feature detector for each
        (<replaceable>w</replaceable>, <replaceable>L</replaceable>)
        pair.  The following example constructs a feature detector
        list that checks sentences for the presence of several
        informative words: </para>

<programlisting>
    &prompt; <command>words = 'ball company chip score attack invade'</command>
                            'ball company chip score attack invade'
    &prompt; <command>wordlist = words.split()</command>
    ['ball', 'company', 'chip', 'score', 'attack', 'invade']
    &prompt; <command>labels = ('sports', 'foreign affairs', 'technology')</command>
    ('sports', 'foreign affairs', 'technology')

    &prompt; <command>fdlist = BagOfWordsFDList(wordlist, labels)</command>
    &lt;FeatureDetectorList with 18 features&gt;
</programlisting>

      </section> <!-- BagOfWordsFDList -->

      <section id="fdlists.other" xreflabel="Other Feature Detector List Implementations"> 
        <title> Other Feature Detector List Implementations </title>

        <para> The <literal>nltk.classifier.feature</literal> module
        provides several more feature detector list implementations;
        see their reference documentation for more details. </para>

        <itemizedlist>
          <listitem> <para> <ulink
          url="&refdoc;/nltk.classifier.feature.AlwaysOnFDList.html">
          <literal>AlwaysOnFDList</literal></ulink></para>
          </listitem>
          <listitem> <para> <ulink
          url="&refdoc;/nltk.classifier.feature.SimpleFDList.html">
          <literal>SimpleFDList</literal></ulink></para>
          </listitem>
          <listitem> <para> <ulink
          url="&refdoc;/nltk.classifier.feature.LabeledTextFunctionFDList.html">
          <literal>LabeledTextFunctionFDList</literal></ulink></para>
          </listitem>
          <listitem> <para> <ulink
          url="&refdoc;/nltk.classifier.feature.MergedFDList.html">
          <literal>MergedFDList</literal></ulink></para>
          </listitem>
          <listitem> <para> <ulink
          url="&refdoc;/nltk.classifier.feature.MemoizedFDList.html">
          <literal>MemoizedFDList</literal></ulink></para>
          </listitem>
        </itemizedlist>

        <para> <literal>nltk.classifier.feature</literal> also
        implements the <ulink
        url="&refdoc;/nltk.classifier.feature.AbstractFDList.html">
        <literal>AbstractFDList</literal></ulink> class, which
        provides a convenient basis for building custom feature
        detector lists.  It also serves as a base class for most of
        the feature value lists provided by nltk. </para>

      </section> <!-- Other fdlist implementations -->

    </section> <!-- Feature Detector Lists -->

    <section id="fvlists" xreflabel="Feature Value Lists"> 
      <title> Feature Value Lists </title>

      <para> <glossterm>Feature value lists</glossterm> are data
      structures that represent the feature values for a set of
      features.  Feature value lists serve three important functions:
      </para>

      <itemizedlist>
        <listitem>
          <para> They provide a mechanism for grouping feature values
          together.</para>
        </listitem>
        <listitem>
          <para> They associate a unique identifier with each feature
          value.</para>
        </listitem>
        <listitem>
          <para> They allow for efficient encoding of sparse sets of
          feature values. </para>
        </listitem>
      </itemizedlist>

      <para> Abstractly, a feature value list can be thought of as a
      <literal>list</literal> of the feature values for a set of
      features.  The value for the feature whose id is
      <replaceable>i</replaceable> is the
      <replaceable>i</replaceable>th element of the list. </para>

      <para> Feature value lists are usually created with the
      <literal>detect</literal> method of a feature detector list.
      The feature ids in the feature value list correspond with the
      feature ids in the feature detector list that was used to create
      it.  In other words, the <replaceable>i</replaceable>th feature
      detector in the feature detector list is responsible for
      generating the <replaceable>i</replaceable>th feature value in
      the feature value list.  This correspondance was illustrated in
      the diagram used to illustrate the <literal>detect</literal>
      method, above. </para>

      <para> Usually, most of the features in a
      <literal>FeatureValueList</literal> have the same value.  For
      example, a sentence-type classifier might use features of the
      form: </para>

      <screen>
    <replaceable>f</replaceable>(ltext)  =  1  <emphasis>if (<replaceable>w</replaceable> in ltext.text) and (ltext.label == <replaceable>L</replaceable>)</emphasis>
                 0  <emphasis>otherwise</emphasis>
</screen>

      <para> for each word <replaceable>w</replaceable> and each label
      <replaceable>L</replaceable>.  If there are 3 labels, and we are
      interested in 1,000 words, then there are 3,000 features; but
      for a typical sentence, only a handful of those features will
      have a nonzero value. </para>

      <para> If most of the features in a
      <literal>FeatureValueList</literal>s, have the same value, the
      <literal>FeatureValueList</literal> is said to be
      <glossterm>sparse</glossterm>.  The common value is called the
      <glossterm>default</glossterm>, and is typically zero. </para>

      <section id="fvlists.fvlisti" xreflabel="FeatureValueListI"> 
        <title> FeatureValueListI </title>

        <para> The <literal>nltk.classifier.feature</literal> module
        defines <ulink
        url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html">
        <literal>FeatureValueListI</literal></ulink>, a general
        interface for implementing feature value lists.  Feature value
        lists are required to implement four methods: an indexing
        operator; a length operator; an <literal>assignments</literal>
        method; and a <literal>default</literal> method. </para>

        <section id="fvlists.fvlisti.indexing" xreflabel="The indexing operator">
          <title> The indexing operator </title>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#__getitem__">indexing
        operator</ulink> allows feature value lists to be treated as
        lists of feature values:</para>

<programlisting>
    <emphasis># fvlist is a feature value list.</emphasis>
    &prompt; <command>print fvlist</command>
    &lt;FeatureValueList with 6 features&gt;

    <emphasis># Print the third feature value. </emphasis>
    &prompt; <command>fvlist[2]</command>
    0

    <emphasis># Print each feature value.</emphasis>
    &prompt; <command>print [fvalue for fvalue in fvlist]</command>
    [0, 0, 0, 1, 0, 0]
</programlisting>

        </section> <!-- The indexing operator -->

        <section id="fvlists.fvlisti.length" xreflabel="The length operator">
          <title> The length operator </title>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#__len__">length
        operator</ulink> returns the number of features represented by
        a feature value list: </para>

<programlisting>
    &prompt; <command>len(fvlist)</command>
    6
</programlisting>

        </section> <!-- The length operator -->

        <section id="fvlists.fvlisti.assign" xreflabel="The assignment method and the default method">
          <title> The assignment method and the default method </title>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#assignments"><literal>assignments</literal></ulink>
        method and the <ulink
        url="&refdoc;/nltk.classifier.feature.FeatureValueListI.html#default"><literal>default</literal></ulink>
        method provide an efficient way of processing sparse feature
        value lists.  <literal>default</literal> returns the feature
        value list's default value.  This is typically the value for
        most features in the list.  <literal>assignments</literal>
        returns a list of
        <emphasis>(feature-id,&nbsp;feature-value)</emphasis> pairs,
        which specifies the feature value for all of the remaining
        features. </para>

        <para> <literal>default</literal> and
        <literal>assignments</literal> can be used together to
        efficiently process large sparse feature value lists.  For
        example, the following code calculates the sum of all feature
        values in <literal>fvlist</literal>: </para>

<programlisting>
    &prompt; <command>assignments = fvlist.assignments()</command>
    &prompt; <command>sum = fvlist.default() * (len(fvlist)-len(assignments))</command>
    &prompt; <command>for (fid, fval) in assignments:</command>
    &prompt2; <command>    sum += fval</command>
    &prompt; <command>print sum</command>
                                                7
</programlisting>

        </section> <!-- assignment & default methods -->

      </section> <!-- FeatureValueListI -->
      
    </section> <!-- Feature Value Lists -->

  </chapter> <!-- Feature-Based Classification -->

  <chapter id="using" xreflabel="Using Classifiers"> 
    <title> Using Classifiers </title>

    <para> Before we start explaining how various classifiers work, we
    will look at a complete example of how classifiers can be used.
    This should help tie together the various data structures and
    interfaces that have been discussed so far.  The task we will look
    at is sentence-type classification.  In particular, we wish to
    categorize sentences as <emphasis>statements</emphasis>,
    <emphasis>imperatives</emphasis>, or
    <emphasis>questions</emphasis>.  We will use a
    <literal>ClassifierTrainer</literal> to build a feature-based
    classifier from a training corpus. </para>

    <section id="using.features" xreflabel="Features"> 
      <title> Features </title>

      <para> The first thing we need to do is decide which aspects of
      a sentence are relevant to this classification task.  In other
      words, we must decide which features to use.  After examining
      the training corpus, we might decide that the three following
      kinds of information are relevant: </para>

      <itemizedlist>
        <listitem> <para> Which words are present in the sentence
        </para>
        </listitem>
        <listitem> <para> The first word of the sentence </para>
        </listitem>
        <listitem> <para> The length of the sentence </para>
        </listitem>
      </itemizedlist>

      <section id="using.features.1" xreflabel="Which words are present"> 
        <title> Which words are present </title>

        <para> To check which words are present in a sentence, we can
        define a <literal>BagOfWordsFDList</literal>.  </para>

<programlisting>
    &prompt; <command>word_list = open('wordlist.txt').read().split()</command>
    &prompt; <command>labels = ('statement', 'question', 'imperative')</command>
    &prompt; <command>bag_of_words_fdlist = BagOfWordsFDList(word_list, labels)</command>
    &lt;FeatureDetectorList with 1800 features&gt;
</programlisting>

        <para> The list of words in the file
        <literal>wordlist.txt</literal> indicates which words we
        believe are relevant to this classification task. </para>

      </section> <!-- Which words are present -->

      <section id="using.features.2" xreflabel="The first word"> 
        <title> The first word </title>

        <para> To check what the first word of a sentence is, we can
        use a <literal>TextFunctionFDList</literal>. </para>

<programlisting>
    &prompt; <command>first_word_list = open('firstwordlist.txt').read().split()</command>
    &prompt; <command>def first_word(text): return text[0] </command>
    &prompt; <command>first_word_fdlist = TextFunctionFDList(first_word, first_word_list, labels)</command>
    &lt;FeatureDetectorList with 852 features&gt;
</programlisting>

        <para> The list of words in the file
        <literal>firstwordlist.txt</literal> indicates which first
        words we believe are relevant to this classification
        task. </para>

      </section> <!-- The first word -->

      <section id="using.features.3" xreflabel="Sentence length"> 
        <title> Sentence length </title>

        <para> To check the length of the sentence, we can use another
        <literal>TextFunctionFDList</literal>. </para>

<programlisting>
    &prompt; <command>def length(text): return len(text) </command>
    &prompt; <command>length_range = range(0, 25) </command>
    &prompt; <command>length_fdlist = TextFunctionFDList(length, length_range, labels)</command>
    &lt;FeatureDetectorList with 75 features&gt;
</programlisting>

      </section> <!-- Sentence length -->

      <section id="using.features.combine" xreflabel="Combining the Feature Lists"> 
        <title> Combining the Feature Lists </title>

        <para> Finally, we can combine these three sets of feature
        detectors into a single feature detector list, using the
        addition operator: </para>

<programlisting>
    &prompt; <command>fdlist = bag_of_words_fdlist + first_word_fdlist + length_fdlist </command>
    &lt;FeatureDetectorList with 2727 features&gt;
</programlisting>

      </section> <!-- Combining -->

    </section> <!-- Features -->

    <section id="using.corpus" xreflabel="Training Corpus"> 
      <title> Training Corpus </title>

      <para> Next, we need to load the training corpus.  Training
      corpera are often stored in different formats for different
      tasks.  For this task, we have three files:
      <literal>statements.txt</literal>,
      <literal>imperatives.txt</literal>, and
      <literal>questions.txt</literal>.  These files contain training
      samples for the types of sentences indicated by their filenames.
      Each file has one sentence per line.  The following code
      reads these sentences, and labels them appropriately: </para>

<programlisting>
    &prompt; <command>tokenizer = LineTokenizer()</command>

    &prompt; <command>statements = open('statements.txt').read() </command>
    &prompt; <command>statement_toks = tokenizer.tokenize(statements) </command>

    &prompt; <command>imperatives = open('imperatives.txt').read() </command>
    &prompt; <command>imperative_toks = tokenizer.tokenize(imperatives) </command>

    &prompt; <command>questions = open('questions.txt').read() </command>
    &prompt; <command>question_toks = tokenizer.tokenize(questions) </command>

    &prompt; <command>train_toks = (label_tokens(statement_toks, 'statement') +</command>
    &prompt2; <command>              label_tokens(imperative_toks, 'imperative') +</command>
    &prompt2; <command>              label_tokens(question_toks, 'question'))</command>
</programlisting>

      <para> <ulink
      url="&refdoc;/nltk.classifier.html#label_tokens"><literal>label_tokens</literal></ulink>
      is a simple helper function that takes a list of unlabeled
      tokens, and returns a corresponding list of labeled
      tokens. </para>

    </section> <!-- Training Corpus -->

    <section id="using.training" xreflabel="Training the Classifier"> 
      <title> Training the Classifier </title>

      <para> Once we've constructed a feature list and a training
      corpus, we can train a new classifier.  First, we build a
      <literal>ClassifierTrainer</literal>, using the feature detector
      list:
      </para>

<programlisting>
    <emphasis># Naive Bayes classifier trainer</emphasis>
    &prompt; <command>trainer = NBClassifierTrainer(fdlist) </command>
    &lt;NBClassifierTrainer: 2727 features&gt;
</programlisting>

      <para> Then, we use the <literal>ClassifierTrainer</literal> to
      train a new classifier, using the training corpus: </para>

<programlisting>
    &prompt; <command>classifier = trainer.train(train_toks) </command>
    &lt;NBClassifier: 3 labels, 2727 features&gt;
</programlisting>

      <para> If we wanted to build another kind of classifier, we
      could simply use a different kind of
      <literal>ClassifierTrainer</literal>.  For example, the
      following code uses our feature detector list and training
      corpus to build a new maximum entropy classifier, using a
      <literal>ClassifierTrainer</literal> that implements improved
      iterative scaling:</para>

<programlisting>
    <emphasis># Improved Iterative Scaling maxent classifier trainer</emphasis>
    &prompt; <command>trainer = IISMaxentClassifierTrainer(fdlist) </command>
    &lt;IISMaxentClassifierTrainer: 2727 features&gt;

    &prompt; <command>classifier = trainer.train(train_toks) </command>
    &lt;MaxentClassifier: 3 labels, 2727 features&gt;
</programlisting>

    </section> <!-- Training the Classifier -->

    <section id="using.classifying" xreflabel="Classifying New Texts"> 
      <title> Classifying New Texts </title>

      <para> Finally, we can use the classifier we built to classify
      new texts: </para>

<programlisting>
    &prompt; <command>loc = Location(3, unit='s')</command>
    &prompt; <command>test_tok = Token("Do you enjoy classification?", loc)</command>

    &prompt; <command>classifier.classify(test_tok)</command>
    "Do you enjoy classification?"/"question"@[3s]

    &prompt;<command> prob_dict = classifier.distribution_dictionary(test_tok) </command>
    &prompt;<command> for label in prob_dict.keys():</command>
    &prompt2;<command>     print "P(%s) = %.2f" % (label, prob_dict[label])</command>
    P(statement) = 0.24
    P(imperative) = 0.02
    P(question) = 0.74
</programlisting>

    </section> <!-- Classifying New Texts -->

  </chapter> <!-- Using -->

  <chapter id="abstract.models" xreflabel="Classifier Models">
    <title> Classifier Models </title>
    
    <para> The next three sections discuss two classifier models that
    are implemented by nltk: the Naive Bayes classifier and the
    maximum entropy classifier.  This discussion serves to explain the
    algorithms used by these classifiers; and to explore the issues
    that go into implementing a classifier. </para>

  </chapter> <!-- Classifier Models -->

  <chapter id="abstract" xreflabel="AbstractFeatureClassifier">
    <title> AbstractFeatureClassifier </title>

    <para> Many feature-based classification models are based on a
    function that estimates the likelihood of a given
    <literal>FeatureValueList</literal>.  This likelihood indicates
    how probable it is that a randomly selected labeled text will
    produce the given <literal>FeatureValueList</literal>.  These
    models can then classify a given text by selecting the label which
    maximizes the likelihood function for the text. </para>

    <para> This general approach to classification is captured by
    <ulink
    url="&refdoc;/nltk.classifier.feature.AbstractFeatureClassifier.html"><literal>AbstractFeatureClassifier</literal></ulink>,
    which is defined by the <literal>nltk.classifier.feature</literal>
    module.  This abstract base class provides default definitions for
    all of the methods defined by <literal>ClassifierI</literal>. </para>

    <note> <para> An <glossterm>abstract class</glossterm> is a class
    that is only intended for use as a base class. </para>
    </note>

    <para> These default definitions rely on the method <ulink
    url="&refdoc;/nltk.classifier.feature.AbstractFeatureClassifier.html#fvlist_likelihood"><literal>fvlist_likelihood</literal></ulink>,
    which should be defined by a subclass.  This method returns a
    likelihood estimate for a given
    <literal>FeatureValueList</literal>.  This likelihood should be
    proportional to the probability of the labeled text that generated
    the feature value list.  In other words: </para>

    <itemizedlist>
      <listitem>
        <para>
          <literal>fvlist_likelihood</literal>(<literal>fdlist.detect</literal>(&ltext;))
          &ap; Z &times; P(&ltext;) </para>
      </listitem>
    </itemizedlist>

    <para> Where: </para>

    <itemizedlist>
      <listitem> <para> &ltext; is a labeled text. </para>
      </listitem>
      <listitem> <para> <literal>fdlist</literal> is the
      <literal>FeatureDetectorList</literal> used by the classifier. </para>
      </listitem>
      <listitem>
        <para> <emphasis>Z</emphasis> is a positive normalization
        constant that does not depend &ltext;<literal>.label()</literal>.
        </para>
      </listitem>
    </itemizedlist>
      
    <para> The reason for the requirement that <emphasis>Z</emphasis>
    not depend on the label used to generate <literal>fvlist</literal>
    will become clear when we discuss how
    <literal>AbstractFeatureClassifier</literal> uses
    <literal>fvlist_likelihood</literal>.  Note that this requirement
    is trivially satisfied if <literal>fvlist_likelihood</literal>
    returns the probability of the labeled text that generated the
    feature value list. </para>

    <note> <para> We use the term "likelihood" to emphasize that this
    value is not actually a probability.  However, we do require that
    the likelihood be proportional to a probability.</para>
    </note>

    <section id="abstract.algorithms" xreflabel="Algorithms"> 
      <title> Algorithms </title>
      
      <para> This section discusses the algorithms used by
      <literal>AbstractFeatureClassifier</literal> to classify texts;
      the next section will describe how these algorithms are
      implemented.  Let: </para>
      
      <itemizedlist>
        <listitem> <para> &t; be a text. </para>
        </listitem>
        <listitem> <para> &l; be a label. </para>
        </listitem>
        <listitem> 
          <para> &Lhat;(&ltextlt;) =
          <literal>fvlist_likelihood</literal>(<literal>fdlist.detect</literal>(&ltextlt;))</para>
        </listitem>
      </itemizedlist>

      <section id="abstract.algorithms.classifying" xreflabel="Classifying Texts"> 
        <title> Classifying Texts </title>

        <para> To classify an unlabeled text
        <replaceable>t</replaceable>, we find the label &l; that
        maximizes P(&l;|&t;): </para>
      
        <itemizedlist>
          <listitem>
            <para>classify(&t;) = ARGMAX<subscript>&l;</subscript>
            P(&l;|&t;) </para>
          </listitem>
        </itemizedlist>

        <para> Using Bayes Rule, we can rewrite this as: </para>
      
        <itemizedlist>
          <listitem>
            <para>classify(&t;) = ARGMAX<subscript>&l;</subscript>
            P(&ltextlt;) / P(&t;)</para>
          </listitem>
        </itemizedlist>

        <para> We can then use our likelihood estimate to replace
        P(&ltextlt;): </para>

        <itemizedlist>
          <listitem>
            <para>classify(&t;) = ARGMAX<subscript>&l;</subscript>
            (&Lhat;(&ltextlt;) / Z) / P(&t;)</para>
          </listitem>
        </itemizedlist>

        <para> Since P(&t;) and Z do not depend on &l;, we can
        simplify this equation to:</para>

        <itemizedlist>
          <listitem>
            <para>classify(&t;) = ARGMAX<subscript>&l;</subscript>
            &Lhat;(&ltextlt;)</para>
          </listitem>
        </itemizedlist>

        <para> We can then use <literal>fvlist_likelihood</literal> to
        solve for &t;. </para>

      </section> <!-- Classifying -->

      <section id="default.algorithms.prob" xreflabel="Estimating Label Probabilities"> 
        <title> Estimating Label Probabilities </title>

        <para> We use a similar approach for estimating the
        probability of a label, given a text.  We wish to find
        P(&l;|&t;).  Using Bayes Rule, we can rewrite this as: </para>
        
        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) = P(&ltextlt;) / P(&t;)</para>
          </listitem>
        </itemizedlist>
                    
        <para> P(&t;) is equal to the sum of P(&ltextlt;) over all
        values of &l;: </para>

        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) = P(&ltextlt;) /
            &sum;<subscript>&lprime;</subscript>
            P(&ltextlprimet;)</para>
          </listitem>
        </itemizedlist>
      
        <para> We can then use our likelihood estimate to replace
        P(&ltextlt;) and P(&ltextlprimet;): </para>

        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) = (&Lhat;(&ltextlt;)/Z) /
            &sum;<subscript>&lprime;</subscript>
            (&Lhat;(&ltextlprimet;)/Z)</para>
          </listitem>
        </itemizedlist>

        <para> Since Z does not depend on &l;, we can cancel all of
        the Zs: </para>
      
        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) = &Lhat;(&ltextlt;) /
            &sum;<subscript>&lprime;</subscript>
            &Lhat;(&ltextlprimet;)</para>
          </listitem>
        </itemizedlist>

        <para> We can then use <literal>fvlist_likelihood</literal> to
        solve for P(&l;|&t;). </para>
        
      </section> <!-- Estimating P(label|text) -->

    </section> <!-- Algorithms -->


    <section id="default.impl" xreflabel="Implementation"> 
      <title> Implementation </title>

      <para> In this section, we will examine the implementation of
      several of the methods implemnted by
      <literal>AbstractFeatureClassifier</literal>.  For information
      about the methods not discussed here, see
      <literal>nltk/classifier/feature.py</literal>. </para>

      <section id="default.impl.constructor" xreflabel="Constructing a New AbstractFeatureClassifier">
        <title> Constructing a New AbstractFeatureClassifier </title>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.feature.AbstractFeatureClassifier.html#__init__"><literal>AbstractFeatureClassifier</literal>
        constructor</ulink> takes two arguments: a
        <literal>FeatureDetectorList</literal>; and a list of labels.
        The constructor simply records its arguments for later use. </para>

<programlisting>
<command>    def __init__(self, fdlist, labels):</command>
<command>        self._fdlist = fdlist</command>
<command>        self._labels = labels</command>
</programlisting>

      </section> <!-- Constructor -->

      <section id="default.impl.classify" xreflabel="AbstractFeatureClassfier.classify()">
        <title> AbstractFeatureClassifier.classify() </title>

        <para> The <literal>classify</literal> method loops through
        each label, looking for the label that maximizes
        &Lhat;(&ltext;).  First, it initializes a variable to hold the
        current maximum: </para>

<programlisting>
<emphasis>    # (label, likelihood) pair that maximizes likelihood</emphasis>
<command>    max = (None, 0)</command>
</programlisting>

        <para> Then it loops through each label, using
        <literal>fvlist_likelihood</literal> to assess that label's
        likelihood: </para>

<programlisting>
<command>    for label in self._labels:</command>
<emphasis>        # Find &Lhat;(LabeledText(text, label))</emphasis>
<command>        fvlist = self._fdlist.detect(LabeledText(text, label))</command>
<command>        l = self.fvlist_likelihood(fvlist)</command>

<emphasis>        # Update the maxiumum, if appropriate. </emphasis>
<command>        if l > max[1]: max = (label, l) </command>
</programlisting>

        <para> Finally, it uses the maximum label to construct and
        return a new labeled token: </para>

<programlisting>
<command>    return Token(LabeledText(text, label), unlabeled_token.loc())</command>
</programlisting>

        <para> The complete listing for
        <literal>AbstractFeatureClassifier.classify</literal> is:</para>

<programlisting>
<command>    def classify(self, unlabeled_token):</command>
<emphasis>        # (label, likelihood) pair that maximizes likelihood</emphasis>
<command>        max = (None, 0)</command>

<emphasis>        # Find the label that maximizes &Lhat;(LabeledText(text, label))</emphasis>
<command>        for label in self._labels:</command>
<command>            fvlist = self._fdlist.detect(LabeledText(text, label))</command>
<command>            l = self.fvlist_likelihood(fvlist)</command>
<command>            if l > max[1]: max = (label, l) </command>

<command>        return Token(LabeledText(text, max[0]), unlabeled_token.loc())</command>
</programlisting>

      </section> <!-- Classifying Texts -->

      <section id="default.impl.distribution_list" xreflabel="AbstractFeatureClassfier.distribution_list()">
        <title> AbstractFeatureClassifier.distribution_list() </title>

        <para> <literal>AbstractFeatureClassifier.distribution_list</literal>
        returns a list that specifies the probability for each label.
        Its implementation is based on the equation for P(&l;|&t;)
        from <xref linkend="default.algorithms.prob"></xref>:
        </para>

        <itemizedlist>
          <listitem>
            <para>P(&l;|&t;) = &Lhat;(&ltextlt;) /
            &sum;<subscript>&lprime;</subscript>
            &Lhat;(&ltextlprimet;)</para>
          </listitem>
        </itemizedlist>

        <para> It begins by initializing variables to hold a list of
        &Lhat;(&ltext;) values, and the the sum of these values: </para>

<programlisting>
<command>    l_list = [] </command>
<command>    l_sum = 0.0</command>
</programlisting>

        <para> It then loops through each label, updating
        <literal>l_list</literal> and <literal>l_sum</literal> with
        the likelihood value for each label: </para>

<programlisting>
<command>    for label in self._labels:</command>
<emphasis>        # Find &Lhat;(LabeledText(text, label))</emphasis>
<command>        fvlist = self._fdlist.detect(LabeledText(text, label))</command>
<command>        l = self.fvlist_likelihood(fvlist)</command>

<emphasis>        # Update l_list and l_sum</emphasis>
<command>        l_list.append(l)</command>
<command>        l_sum += l</command>
</programlisting>

        <para> If &Lhat;(&ltext;) was zero for all labels, it calls a
        <literal>zero_distribution_list</literal>.  By default, this
        method returns a uniform distribution; but it can be
        overridden by subclasses. </para>

<programlisting>
<command>    if l_sum == 0:</command>
<command>        return self.zero_distribution_list(unlabeled_token)</command>
</programlisting>

        <para> Otherwise, it estimates P(&l;|&t;) by dividing each
        &Lhat;(&ltext;) in <literal>l_list</literal> by
        <literal>l_sum</literal>: </para>

<programlisting>
<command>    return [l/l_sum for l in l_list]</command>
</programlisting>

        <para> The complete listing for
        <literal>AbstractFeatureClassifier.distribution_list</literal> is:</para>

<programlisting>
<command>    def distribution_list(self, unlabeled_token):</command>
<command>        text = unlabeled_token.type()</command>
<command>        q_list = [] </command>
<command>        q_sum = 0.0</command>

<emphasis>      # Find &Lhat;(LabeledText(text, label)) for each label</emphasis>
<command>        for label in self._labels:</command>
<command>            fvlist = self._fdlist.detect(LabeledText(text, label))</command>
<command>            l = self.fvlist_likelihood(fvlist)</command>
<command>            q_list.append(q)</command>
<command>            q_sum += q</command>

<emphasis>        # If q=0 for all labels, use zero_distribution_list</emphasis>
<command>        if q_sum == 0:</command>
<command>            return self.zero_distribution_list(unlabeled_token)</command>

<emphasis>        # Normalize the probability estimates. </emphasis>
<command>        return [q/q_sum for q in q_list]</command>
</programlisting>

      </section> <!-- AbstractFeatureClassifier.distribution_list -->

    </section> <!-- AbstractFeatureClassifier Implementation -->

  </chapter> <!-- AbstractFeatureClassifier -->

  <chapter id="nb" xreflabel="The Naive Bayes Classifier"> 
    <title> The Naive Bayes Classifier </title>

    <para> The Naive Bayes classifier implements a simple algorithm
    that can achieve relatively good performance on classification
    tasks.  This algorithm can be used with any type of feature; but
    it is usually used with binary features. </para>

    <section id="nb.using" xreflabel="Using NBClassifier"> 
      <title> Using NBClassifier </title>

      <para> The <literal>nltk.classifier.naivebayes</literal> module
      defines <ulink
      url="&refdoc;/nltk.classifier.naivebayes.NBClassifier.html"><literal>NBClassifier</literal></ulink>,
      a text classifier based on the Naive Bayes classificaiton
      algorithm.  <literal>NBClassifier</literal> uses probability
      estimates for feature value assignments to classify texts.
      These probability estimates are specified using a
      <literal>ProbDist</literal>, whose samples are
      <literal>FeatureValueList</literal>s.  </para>

      <para> The <ulink
      url="&refdoc;/nltk.classifier.naivebayes.NBClassifier.html#__init__"><literal>NBClassifier</literal>
      constructor</ulink> takes three arguments: a
      <literal>FeatureDetectorList</literal>; a list of labels; and a
      probability distribution that provides probability estimates for
      feature value assignments. </para>

<programlisting>
    <emphasis># Build a probability distribution over feature value lists.</emphasis>
    <emphasis># This probability distribution will be used to estimate the</emphasis>
    <emphasis># probabilities of feature value assignments.</emphasis>
    &prompt; <command>freq_dist = SimpleFreqDist() </command>
    &prompt; <command>for tok in train_toks: </command>
    &prompt2; <command>    freq_dist.inc(fdlist.detect(tok.type()))</command>
    &prompt; <command>prob_dist = MLEProbDist(freq_dist)</command>

    <emphasis># Build a new Naive Bayes classifier.</emphasis>
    &prompt; <command>classifier = NBClassifier(fdlist, labels, prob_dist)</command>
    &lt;NBClassifier: 3 labels, 2727 features&gt;
</programlisting>

      <para> <literal>NBClassifier</literal> supports all of the
      methods specified by <literal>ClassifierI</literal>: </para>

<programlisting>
    &prompt;<command> classifier.classify(token)</command>
    "What's your name?"/'question'
    &prompt;<command> classifier.prob(labeled_token)</command>
    0.67
    &prompt;<command> classifier.distribution(token)</command>
    &lt;ProbDist&gt;
    &prompt;<command> classifier.distribution_dictionary(token) </command>
    {'statement': 0.21, 'imperative': 0.12, 'question': 0.67}
    &prompt;<command> classifier.distribution_list(token) </command>
    [0.21, 0.12, 0.67]
</programlisting>

      <note> <para> <literal>NBClassifier</literal> uses
      <literal>AssignmentEvent</literal>s to find the probability of
      each feature value assignment.  If the
      <literal>NBClassifier</literal> is to be efficient, then its
      probability distribution's <literal>prob()</literal> method
      should process <literal>AssignmentEvent</literal>s efficiently.
      This will be discussed further in <xref
      linkend="nb.impl"></xref>. </para> </note>

    </section> <!-- Using NBClassifier -->

    <section id="nb.algorithms" xreflabel="Algorithms"> 
      <title> Algorithms </title>
      
      <para> This section discusses the algorithms used by
      <literal>NBClassifier</literal>; the next section will describe
      how these algorithms are implemented.  Let: </para>

      <itemizedlist>
        <listitem> <para> &fi; be the feature whose feature id is
        <replaceable>i</replaceable>.</para>
        </listitem>
        <listitem> <para> &v; be a feature value lists, where &vi; is
        the feature value for &fi; </para>
        </listitem>
      </itemizedlist>

      <para> Using these variables, we can write the probability for
      a <literal>FeatureValueList</literal> &v; as: </para>

      <itemizedlist> 
        <listitem>
          <para> P(&v;) = 
          P(&f1; = &v1;, &f2; = &v2;, ..., &fn; = &vn;) </para>
        </listitem>
      </itemizedlist>

      <section id="nb.algorithms.nbassumption" xreflabel="The Naive Bayes Assumption"> 
        <title> The Naive Bayes Assumption </title>

        <para> The Naive Bayes assumption states that each feature
        value assignment is probabilistically independant of all other
        feature value assignments.  In other words: </para>

        <itemizedlist>
          <listitem>
            <para>
          P(&fi; = <replaceable>x</replaceable>, &fj; =
          <replaceable>y</replaceable>) = P(&fi; =
          <replaceable>x</replaceable>) &times; P(&fj; =
          <replaceable>y</replaceable>)
          </para>
            <para> For all <replaceable>i</replaceable> &ne;
            <replaceable>j</replaceable>. </para>
          </listitem>
        </itemizedlist>

        <para> Although the Naive Bayes assumption is usually not
        strictly true, it is often approximately correct.  Thus, we
        can use it to find a reasonable estimate for P(&v;):</para>

        <itemizedlist>
          <listitem>
            <para> P(&v;) = &prod; <subscript>0&le;i &lt;n</subscript>
            P(&fi; = &vi;) </para>
          </listitem>
        </itemizedlist>

        <para> <literal>NBClassifier</literal> can then solve for
        P(&v;) using the probability distribution it was constructed
        from to find probability estimates for individual feature
        value assignments. </para>

        <para> Recall that if we use
        <literal>AbstractFeatureClassifier</literal> as a base class,
        then the only method we need to implement is
        <literal>fvlist_likelihood</literal>, which returns a
        likelihood estimate for a given feature value list.  Clearly,
        we could use this estimate of P(&v;) to implement
        <literal>fvlist_likelihood</literal>.  However, direct
        implementation of this estimation formula is inefficient for
        sparse feature value lists. </para>

      </section> <!-- Naive Bayes Assumption -->

      <section id="nb.algorithms.optimization" xreflabel="Factoring Out Default Assignments">
        <title> Factoring Out Default Assignments </title>

        <para> Since most feature value lists are sparse,
        <literal>NBClassifier</literal> uses the following equivalant
        formulation, which factors out the product of probabilities
        for feature assignments with default values: </para>

        <itemizedlist>
          <listitem>
            <para> P(&v;) &ap; Z &times; &Lhat;(&v;) </para>

            <para> Z = &prod; <subscript>0&le;i&lt;n</subscript>
            P(&fi; = D) </para>

            <para> &Lhat;(&v;) = &prod;<subscript>{i: &vi; &ne;
            D}</subscript> (P(&fi; = &vi;) / P(&fi; = D))</para>

          </listitem>
        </itemizedlist>

        <para> Where D is the <literal>FeatureValueList</literal>'s
        default value (usually 0).  Note that Q only depends on the
        non-default features of the labeled text; thus, we can
        efficiently calculate its value using the
        <literal>assignments</literal> method.  </para>

        <para> Z is a constant, so it does depend on the label used
        to generate &v;.  We can therefore use &Lhat; as our
        likelihood estimate.  </para>

        <important>
          <para> There is a minor problem with the formulation of
          &Lhat; that is presented here: if P(&fi; = D) is 0 for any
          feature, then the given equation will produce divison by
          zero.  We can avoid this problem by excluding all features
          where P(&fi;=D) is 0 from Z; and not dividing by P(&fi;=D)
          in &Lhat;.  This gives the modified formula: </para>

          <itemizedlist>
            <listitem>
              <para> &Lhat;(&v;) = &prod;<subscript>{i: &vi; &ne; D
              and P(&fi; = D) &ne; 0}</subscript> P(&fi; = &vi;) /
              P(&fi; = D)</para>
            </listitem>
          </itemizedlist>
        </important>

      </section> <!-- optimization -->

    </section> <!-- Algorithms -->

    <section id="nb.impl" xreflabel="NBClassifier Implementation"> 
      <title> NBClassifier Implementation </title>

      <para> <literal>NBClassifier</literal> is implemented as a
      subclass of <ulink
      url="&refdoc;/nltk.classifier.feature.AbstractFeatureClassifier.html">
      <literal>AbstractFeatureClassifier</literal></ulink>.  This base
      class handles most of the classifier's work;
      <literal>NBClassifier</literal> itself only needs to implement a
      few simple methods.  The <literal>NBClassifier</literal>
      constructor records the probability distribution it is given,
      and calls its base constructor: </para>

<programlisting>
<command>    def __init__(self, fdlist, labels, prob_dist):</command>
<command>        self._prob_dist = prob_dist</command>
<command>        AbstractFeatureClassifier.__init__(self, fdlist, labels)</command>
</programlisting>

      <para> The <literal>fvlist_likelihood</literal> method is a
      streight-forward implementation of the algorithm discussed in
      <xref linkend="nb.algorithms.optimization"></xref>. </para>

      <itemizedlist>
        <listitem>
          <para> &Lhat;(&v;) = &prod;<subscript>{i: &vi; &ne; D and
          P(&fi; = D) &ne; 0}</subscript> P(&fi; = &vi;) / P(&fi; =
          D)</para>
        </listitem>
      </itemizedlist>

<programlisting>
<command>    def fvlist_likelihood(self, fvlist):</command>
<command>        l = 1.0</command>
<command>        default = fvlist.default()</command>

<emphasis>        # Loop through each non-default value. </emphasis>
<command>        for (fid, val) in fvlist.assignments():</command>

<emphasis>            # Find the probability of fvlist's value and the </emphasis>
<emphasis>            # probability of the default value for this feature. </emphasis>
<command>            p1 = self._prob_dist.prob(AssignmentEvent((fid, val)))</command>
<command>            p2 = self._prob_dist.prob(AssignmentEvent((fid, default)))</command>

<emphasis>            # Update the likelihood. </emphasis>
<command>            if p2 != 0:</command>
<command>                l *= p1 / p2</command>

<command>        return l</command>
</programlisting>

      <note> <para> The note at the end of <xref
      linkend="nb.algorithms.optimization"></xref> explains why we
      must check that <literal>p2</literal> &ne; 0. </para>
      </note>

      <para> <literal>NBClassifier</literal> also implements <ulink
      url="&refdoc;/nltk.classifier.naivebayes.NBClassifier.html#prob_dist">
      <literal>prob_dist</literal></ulink>, which returns the
      probability distribution that the classifier uses to estimate
      probabilities of feature value assignments; and a <ulink
      url="&refdoc;/nltk.classifier.naivebayes.NBClassifier.html#__repr__">string
      representation operator</ulink>.  The complete listing for
      <literal>NBClassifier</literal> is: </para>

<programlisting>
<command>class NBClassifier(AbstractFeatureClassifier):</command>
<command>    def __init__(self, fdlist, labels, prob_dist):</command>
<command>        self._prob_dist = prob_dist</command>
<command>        AbstractFeatureClassifier.__init__(self, fdlist, labels)</command>

<command>    def fvlist_likelihood(self, fvlist):</command>
<command>        p = 1.0</command>
<command>        default = fvlist.default()</command>
<command>        for (fid, val) in fvlist.assignments():</command>
<command>            p1 = self._prob_dist.prob(AssignmentEvent((fid, val)))</command>
<command>            p2 = self._prob_dist.prob(AssignmentEvent((fid, default)))</command>
<command>            if p2 != 0:</command>
<command>                p *= p1 / p2</command>
<command>        return p</command>

<command>    def prob_dist(self):</command>
<command>        return self._prob_dist</command>

<command>    def __repr__(self):</command>
<command>        return ('&lt;NBClassifier: %d labels, %d features&gt;' %</command>
<command>                (len(self.labels()), len(self.fdlist())))</command>
</programlisting>

    </section> <!-- NBClassifier Implementation -->

    <section id="nb.training" xreflabel="Training the Naive Bayes Classifier"> 
      <title> Training the Naive Bayes Classifier </title>

      <para> The <literal>nltk.classifier.naivebayes</literal> module
      defines
      <ulink url="&refdoc;/nltk.classifier.naivebayes.NBClassifierTrainer.html">
      <literal>NBClassifierTrainer</literal></ulink>, a factory class
      for Naive Bayes classifiers.
      <literal>NBClassifierTrainer</literal>s are created with the
      <ulink
             url="&refdoc;/nltk.classifier.naivebayes.NBClassifierTrainer.html#__init__"><literal>NBClassifierTrainer</literal>
      constructor</ulink>, which takes a single
      <literal>FeatureDetectorList</literal>: </para>

<programlisting>
    &prompt; <command>trainer = NBClassifierTrainer(fdlist) </command>
    &lt;NBClassifierTrainer: 2727 features&gt;
</programlisting>

      <note> <para> Currently, <literal>NBClassifierTrainer</literal>
      only supports binary features.</para>
      </note>

      <para><literal>NBClassifierTrainer</literal>s can be used to
      build new <literal>NBClassifier</literal>s from lists of
      training tokens, using the
      <ulink
             url="&refdoc;/nltk.classifier.naivebayes.NBClassifierTrainer.html#train">
      <literal>train</literal></ulink> method:
      </para>

<programlisting>
    &prompt; <command>classifier = trainer.train(train_toks) </command>
    &lt;NBClassifier: 3 labels, 2727 features&gt;
</programlisting>

      <section id="nb.training.impl" xreflabel="Implementation">
        <title> Implementation </title>

        <para> <literal>NBClassifierTrainer</literal>'s
        <literal>train</literal> method consists of four simple steps:
        </para>

        <orderedlist>
          <listitem> <para> Create a new frequency distribution.  This
          frequency distribution will form the basis for the
          probability distribution that the
          <literal>NBClassifier</literal> uses to estimate
          probabilities of feature value assignments.</para>
          </listitem>

          <listitem> <para> Populate the frequency distribution with
          the <literal>FeatureValueList</literal>s for each of the
          training tokens. </para>
          </listitem>

          <listitem> <para> Use the frequency distribution to build a
          probability distribution. </para>
          </listitem>

          <listitem> <para> Use the probability distribution to build
          a <literal>NBClassifier</literal>. </para>
          </listitem>
        </orderedlist>

        <para> For now, we will skip the first step, and assume that
        we already have an empty frequency distribution
        <literal>freq_dist</literal>.  To implement the second step,
        we simply loop over the training tokens, incrementing the
        count for each token's feature value list: </para>

<programlisting>
    &prompt; <command>for tok in train_toks: </command>
    &prompt2; <command>    freq_dist.inc(fdlist.detect(tok.type()))</command>
</programlisting>

        <para> For the third step, we can use the
        <literal>MLEProbDist</literal> class to convert the frequency
        distribution into a probability distribution: </para>

<programlisting>
    &prompt; <command>prob_dist = MLEProbDist(freq_dist)</command>
</programlisting>

        <note> <para> To smooth the probability distribution
        distribution, we might use some statistical estimator other
        than MLE.  To do so, we would simply use a different
        <literal>ProbDist</literal>, such as
        <literal>LaplaceProbDist</literal>.  The implementation of
        <literal>NBClassifierTrainer</literal> in the toolkit allows
        you to specify which statistical estimation you would like to
        use, via the keyword argument <literal>estimator</literal>.
        See the reference documentation for <ulink
        url="&refdoc;/nltk.classifier.naivebayes.NBClassifierTrainer.html#train">
        <literal>NBClassifier.train</literal></ulink> for more
        details. </para>
        </note>

        <para> To implement the fourth step, we simply call the
        <literal>NBClassifier</literal> constructor: </para>

<programlisting>
    &prompt; <command>return NBClassifier(self._fdlist, labels, probdist)</command>
</programlisting>

        <para> The only task remaining is to construct the frequency
        distribution.  Although a <literal>SimpleFreqDist</literal>
        could be used, it would result in very poor performance;
        instead, <literal>NBClassifierTrainer</literal> uses a
        frequency distribution that is optimized for finding the
        probabilities of <literal>AssignmentEvent</literal>s:
        <literal>NBClassifierFreqDist</literal>. </para>

        <section id="nb.training.impl.nbcfd" xreflabel="NBClassifierFreqDist">
          <title> NBClassifierFreqDist </title>

          <para> <ulink
          url="&refdoc;/nltk.classifier.naivebayes.NBClassifierFreqDist.html">
          <literal>NBClassifierFreqDist</literal></ulink> is a
          relatively simple frequency distribution that's specialized
          for use with <literal>NBClassifier</literal>.  By placing
          restrictions on the types of samples and events that the
          probability distribution supports,
          <literal>NBClassifierFreqDist</literal> can be made quite
          efficient.  The restrictions that
          <literal>NBClassifierFreqDist</literal> employs are: </para>

          <itemizedlist>
            <listitem> <para> Samples be boolean C{FeatureValueList}s of
            fixed length.</para>
            </listitem>
            <listitem> <para> The <literal>freq</literal> method only
            supports <literal>AssignmentEvent</literal>s.</para>
            </listitem>
            <listitem> <para> The <literal>count</literal> method only
            supports <literal>AssignmentEvent</literal>s.</para>
            </listitem>
            <listitem> <para> The methods <literal>samples</literal>,
            <literal>cond_samples</literal>, <literal>B</literal>,
            <literal>bins</literal>, <literal>Nr</literal>,
            <literal>max</literal>, <literal>cond_max</literal>, and
            <literal>cond_freq</literal> are not supported.</para>
            </listitem>
          </itemizedlist>

          <para> Internally, <literal>NBClassifierFreqDist</literal>
          keeps track of the samples for which each feature has been
          active (<replaceable>fcount[id]</replaceable>); and the
          total number of samples (<replaceable>N</replaceable>).  From
          these values, it can calculate the number of samples that
          are contained in a given <literal>AssignmentEvent</literal>:
          </para>

          <itemizedlist>
            <listitem> <para> count(AssignmentEvent(id, 1)) = fcount[id]</para>
              <para> count(AssignmentEvent(id, 0)) = N - fcount[id]</para>
            </listitem>
          </itemizedlist>

          <para> Similarly, it can calculate the frequency for a given
          <literal>AssignmentEvent</literal>: </para>

          <itemizedlist>
            <listitem> <para> freq(AssignmentEvent(id, 1)) = fcount[id]/N</para>
              <para> freq(AssignmentEvent(id, 0)) = 1 - fcount[id]/N</para>
            </listitem>
          </itemizedlist>

          <para> <literal>NBClassifierFreqDist</literal> uses an
          <literal>array</literal> to store <literal>fcount</literal>.
          <literal>Array</literal>s are efficient data structures for
          storing homogenous data; they are defined by the
          <literal>Numeric</literal> module.  In most cases, they can
          be acessed like <literal>list</literal>s.  The function
          <literal>zeros</literal> can be used to create a new
          <literal>array</literal> of a given length, all of whose
          elements are <literal>zero</literal>.  </para>

          <para> The implementation of
          <literal>NBClassifierFreqDist</literal> is streight-forward:
          </para>

<programlisting>
class NBClassifierFreqDist(FreqDistI):

    def __init__(self, fvlist_size):
        self._fcount = zeros(fvlist_size)
        self._N = 0.0

    def N(self):
        return self._N

    def inc(self, sample):
        self._N += 1

        # Increment the feature count array
        for (fid,val) in sample.assignments():
            if val != 0:
                self._fcount[fid] += 1

    def freq(self, event):
        (fid, value) = event.assignment()
        if value:
            return (self._fcount[fid] / self._N)
        else:
            return 1 - (self._fcount[fid] / self._N)

    def count(self, event):
        (fid, value) = event.assignment()
        if value:
            return self._fcount[fid]
        else:
            return self._N - self._fcount[fid]
</programlisting>

          <note> <para> Although the implementation described here is
          sufficient for use with simple probability distribution,
          such as <literal>MLEProbDist</literal>, we need to implement
          more methods to allow more complex probability distributions
          to be used.  For this reason, the version of
          <literal>NBClassifierFreqDist</literal> implemented in
          <literal>nltk.classifier.naivebayes</literal> supports
          several additonal methods; and will likely grow to support
          even more methods. </para>
          </note>

        </section> <!-- NBClassifierFreqDist -->

      </section> <!-- Implementation -->

    </section> <!-- Training NBClassifier -->

  </chapter> <!-- NB Classifier -->

  <chapter id="maxent" xreflabel="The Maximum Entropy Classifier"> 
    <title> The Maximum Entropy Classifier </title>

    <para> One problem with the Naive Bayes classifier is that its
    performance depends on the degree to which the features are
    independant.  But the feature sets used for classification are
    rarely independant; and often, we wish to use features which are
    highly dependant on each other.  Under these circumstances,
    <literal>NaiveBayesClassifier</literal> tends to give poor
    performance.  One alternative is to use a different classifier
    model. </para>

    <!-- INSERT REFERENCES!! (Berger, Pietra) -->

    <para> The <glossterm>maximum entropy</glossterm> classifier can
    use mutually dependant features to reliably classify texts.  This
    classifier is based on the idea that we should "model all that is
    known and assume nothing about that which is unknown."  To
    accomplish this goal, we considers all classifiers that are
    "emperically consistant" with a set of training data; and chooses
    the classifier that maximizes entropy.  </para>

    <section id="maxent.emperical" xref="Emperical Consistancy">
      <title> Emperical Consistancy</title>

      <para> A classifier is <glossterm>emperically
      consistant</glossterm> with a set of training data if its
      estimate for the frequency of each feature is equal to the
      actual frequency of that feature in the training data.  In other
      words:
      </para>
      

      <itemizedlist>
        <listitem><para> &sum;<subscript>&ltext;</subscript>
        P<subscript>classifier</subscript>(&ltext;) &times; &fdilt; =
        &sum;<subscript>&ltext;</subscript>
        freq<subscript>training</subscript>(&ltext;) &times; &fdilt;
        </para>
        </listitem>
      </itemizedlist>

      <para> for all <replaceable>i</replaceable>, where: </para>

      <itemizedlist>
        <listitem><para> P<subscript>classifier</subscript>(&ltext;)
        is the classifier's estimated probability for &ltext;.</para>
        </listitem>
        <listitem><para> freq<subscript>training</subscript>(&ltext;)
        is the frequency of &ltext; in the training data.</para>
        </listitem>
      </itemizedlist>
        
      <note> <para> In general, the more features we use, the stronger
      the constraints that emperical consistancy puts on the
      classifier.  At one extreme, if we use no features, then any
      classifier will be emperically consistant with any training set.
      At the other extreme, if we use one feature for each possible
      labeled text, then only one classifier will be emperically
      consistant with a given training set. </para>
      </note>
      
      <para> It seems reasonable to require our classifier to be
      emperically consistant with the training data.  It is this
      requirement allows our classifier to capture the structure of
      the training data.  However, it is not immediately obvious why
      we should want to choose the classifier that maximizes
      entropy. </para>

    </section> <!-- Emperical Consistancy -->

    <section id="maxent.entropy" xref="The Principle of Maximum Entropy">
      <title> The Principle of Maximum Entropy</title>

      <para> A classifier's <glossterm>entropy</glossterm> provides a
      measure of how predictable that classifier's decisions are: the
      higher a classifier's entropy, the more randomly it behaves.
      For example: </para>

      <itemizedlist>
        <listitem> <para> A classifier that always categorizes texts
        using the same label has an entropy of zero. </para>
        </listitem>
        <listitem> <para> A classifier that randomly classsifies texts
        into categories (each with equal probability) has a very high
        entropy. </para>
        </listitem>
      </itemizedlist>

      <para> The intuition behind maximizing entropy is that
      classifiers with lower entropy introduce biases that are not
      justified.  In order to avoid these biases, we must pick the
      classifier with the highest entropy.  This is most easily seen
      with an example.  </para>

      <para> Consider the task of classifying numerical expressions as
      distances, volumes, dates, counts, or monetary amounts.
      Examining our training corpus, we notice that 3/10 of the
      numerical expressions are distances or volumes.  There are an
      infinite number of classifiers that are consistant with this
      statistic.  For example, we could hypothesize the following
      category probabilities: </para>

      <informaltable>
        <tgroup cols="2">
          <thead>
            <row>
              <entry>Category</entry>
              <entry>Probability</entry>
            </row>
          </thead>
          <tbody> 
            <row>
              <entry>distance</entry>
              <entry>0.1</entry>
            </row>
            <row>
              <entry>volume</entry>
              <entry>0.2</entry>
            </row>
            <row>
              <entry>date</entry>
              <entry>0.6</entry>
            </row>
            <row>
              <entry>count</entry>
              <entry>0.1</entry>
            </row>
            <row>
              <entry>monetary amount</entry>
              <entry>0.0</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para> But this classifier introduces biases that are not justified
      by what we know about the training data.  For example, we have
      no reason to believe that dates are signifigantly more common
      than counts, or that monetary amounts never occur.  In contrast,
      the emperically consistant distribution that maximizes entropy
      does not introduce any such biases: </para>

      <informaltable>
        <tgroup cols="2">
          <thead>
            <row>
              <entry>Category</entry>
              <entry>Probability</entry>
            </row>
          </thead>
          <tbody> 
            <row>
              <entry>distance</entry>
              <entry>0.15</entry>
            </row>
            <row>
              <entry>volume</entry>
              <entry>0.15</entry>
            </row>
            <row>
              <entry>date</entry>
              <entry>0.23333</entry>
            </row>
            <row>
              <entry>count</entry>
              <entry>0.23333</entry>
            </row>
            <row>
              <entry>monetary amount</entry>
              <entry>0.23333</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para> For simple examples like the one presented here, we can
      find the maximum entropy classifier by hand.  However, for
      classification tasks involving many mutually dependant features,
      we must employ more advanced methods. </para>

    </section> <!-- Principle of maxent -->

    <section id="maxent.condexp" xref="The Conditional Exponential Model">
      <title> The Conditional Exponential Model </title>

      <para> In <xref linkend="maxent"></xref>, we said that we can
      find the maximum entropy classifier by considering all
      classifiers that are emperically consistant with the training
      data, and choosing the classifier that maximizes entropy.
      However, there are an infinite number of classifiers that are
      emperically consistant with the training data.  Fortunately, it
      can be proven that the maximum entropy classifier will always
      have the same form.  In particular, the emperically consistant
      classifier that maximizes entropy can always be expressed as:
      </para>

      <itemizedlist>
        <listitem> <para> P(&ltext;) = Z(&t;) &times; &Lhat; </para>
        </listitem>
        <listitem> <para> &Lhat; = 
        &prod;<subscript>i</subscript> &wi;
        <superscript><replaceable>fd</replaceable><subscript>i</subscript>(&ltext;)</superscript> </para>
        </listitem>
      </itemizedlist>

      <para> Where &wi; are a set of model parameters, called
      <glossterm>weights</glossterm>; and Z(&t;) is a normalizing
      factor that does not depend on &l;.  This form is known as the
      <glossterm>conditional exponential model</glossterm>.
      </para>

      <note>
        <para> The conditional exponential model is sometimes
        expressed with the following equivalant formula: </para>

        <itemizedlist>
          <listitem> <para> P(&l;|&t;) = Z &times; exp (
          &sum;<subscript>i</subscript> &lambdai; &times;
          <replaceable>fd</replaceable><subscript>i</subscript>(&ltext;) ) </para>
          </listitem>
        </itemizedlist>

        <para> Where &lambdai; = log(&wi;) </para>
      </note>

      <para> Thus, the problem of finding the maximum entropy model
      for a classification task reduces to the problem of finding the
      best conditional exponential model.  Usually, this best model is
      found by iteratively refining a conditional exponential model's
      weights, until we arrive at the maximum entropy model. </para>

      <section id="maxent.condexp.weights" xref="Weights">
        <title> Weights </title>

        <para> Informally, the weights that paramaterize the
        conditional exponential model can be thought of as indications
        of how each feature affects classification decisions: </para>

        <itemizedlist>
          <listitem>
            <para> If a weight is one, then the corresponding feature
            has no effect on classificaiton decisions. </para>
          </listitem>
          <listitem>
            <para> If a weight is greater than one, then the
            corresponding feature will increase probability estimates
            for labels that cause the feature to fire.  Greater values
            cause a greater increase. </para>
          </listitem>
          <listitem>
            <para> If a weight is less than one, then the
            corresponding feature will decrease probability estimates
            for labels that cause the feature to fire.  Lesser values
            cause a greater increase. </para>
          </listitem>
        </itemizedlist>

      </section> <!-- Weights -->

      <section id="maxent.condexp.ceclassifier" xref="ConditionalExponentialClassifier">
        <title> ConditionalExponentialClassifier </title>

        <para> The conditional exponential classifier model is implemented by 
        <ulink
        url="&refdoc;/nltk.classifier.maxent.ConditionalExponentialClassifier.html">
        <literal>ConditionalExponentialClassifier</literal></ulink>,
        which is defined by the
        <literal>nltk.classifier.maxent</literal> module.  The <ulink
        url="&refdoc;/nltk.classifier.maxent.ConditionalExponentialClassifier.html#__init__"><literal>ConditionalExponentialClassifier</literal>
        constructor</ulink> takes three arguments: a
        <literal>FeatureDetectorList</literal>; a list of labels; and
        a list of weights: </para>

<programlisting>
    <emphasis># Define the features and labels used by the classifier. </emphasis>
    &prompt; <command>fdlist = BagOfWordsFDList(word_list, labels)</command>
    &lt;FeatureDetectorList with 8 features&gt;
    &prompt; <command>labels = ('statement', 'imperative', 'question')</command>
    ('statement', 'imperative', 'question')

    <emphasis># Define the weights by hand.  Each weight corresponds to a feature</emphasis>
    <emphasis># in fdlist. </emphasis>
    &prompt; <command>weights = [0.4, 1.0, 2.2, 0.1, 10.0, 3.8, 1.1, 0.3]</command>
    [0.4, 1.0, 2.2, 0.1, 10.0, 3.8, 1.1, 0.3]

    <emphasis># Build a new conditional exponential classifier.</emphasis>
    &prompt; <command>classifier = ConditionalExponentialClassifier(fdlist, labels, weights)</command>
    &lt;ConditionalExponentialClassifier: 3 labels, 8 features&gt;
</programlisting>

        <para> <literal>ConditionalExponentialClassifier</literal>
        supports all of the methods specified by
        <literal>ClassifierI</literal>: </para>

<programlisting>
    &prompt;<command> classifier.classify(token)</command>
    "What's your name?"/'question'
    &prompt;<command> classifier.prob(labeled_token)</command>
    0.5
    &prompt;<command> classifier.distribution(token)</command>
    &lt;ProbDist&gt;
    &prompt;<command> classifier.distribution_dictionary(token) </command>
    {'statement': 0.4, 'imperative': 0.1, 'question': 0.5}
    &prompt;<command> classifier.distribution_list(token) </command>
    [0.1, 0.4, 0.5]
</programlisting>

        <section id="maxent.condexp.ceclassifier.impl" xref="Implementation">
          <title> Implementation </title>

          <para> <literal>ConditionalExponentialClassifier</literal>
          is implemented as a subclass of <ulink
          url="&refdoc;/nltk.classifier.feature.AbstractFeatureClassifier.html">
          <literal>AbstractFeatureClassifier</literal></ulink>.  This
          base class handles most of the classifier's work;
          <literal>ConditionalExponentialClassifier</literal> itself
          only needs to implement a few simple methods. </para>

          <para> The
          <literal>ConditionalExponentialClassifier</literal>
          constructor records the weights it is given, and calls its
          base constructor: </para>

<programlisting>
<command>    def __init__(self, fdlist, labels, weights):</command>
<command>        self._weights = weights</command>
<command>        AbstractFeatureClassifier.__init__(self, fdlist, labels)</command>
</programlisting>

          <para> The <literal>fvlist_likelihood</literal> method
          calculaltes the value of &Lhat; for a given
          <literal>LabeledText</literal>: </para>

          <itemizedlist>
            <listitem> <para> &Lhat; = &prod;<subscript>i</subscript>
            &wi;
            <superscript><replaceable>fd</replaceable><subscript>i</subscript>(&ltext;)</superscript>
            </para>
            </listitem>
          </itemizedlist>

          <para> Notice that for any feature whose value &fdilt; is
          zero, the corresponding term
          &wi;<superscript>&fdilt;</superscript> in the product is one;
          so we can reduce this equation to: </para>

          <itemizedlist>
            <listitem> <para> &Lhat; = &prod;<subscript>{i: &fdilt; &ne; 0}</subscript>
            &wi;
            <superscript><replaceable>fd</replaceable><subscript>i</subscript>(&ltext;)</superscript>
            </para>
            </listitem>
          </itemizedlist>

          <para> In other words, we only need to consider the features
          that do not have the default value of zero.  These are
          exactly the features that are returned by the
          <literal>FeatureValueList</literal>'s
          <literal>assignments</literal> method.
          <literal>fvlist_likelihood</literal> is a streight-forward
          implementation of this formula:</para>

<programlisting>
<command>    def fvlist_likelihood(self, fvlist):</command>
<command>        prod = 1.0</command>
<command>        for (fid, val) in fvlist.assignments():</command>
<command>            prod *= (self._weights[id] ** val)</command>
<command>        return prod</command>
</programlisting>

          <para> <literal>ConditionalExponentialClassifier</literal>
          also provides the <ulink
          url="&refdoc;/nltk.classifier.maxent.ConditionalExponentialClassifier.html#weights">
          <literal>weights</literal></ulink> and <ulink
          url="&refdoc;/nltk.classifier.maxent.ConditionalExponentialClassifier.html#set_weights">
          <literal>set_weights</literal></ulink> methods for returning
          and updating its weights weights; and a <ulink
          url="&refdoc;/nltk.classifier.maxent.ConditionalExponentialClassifier.html#__repr__">
          <literal>string representation
          operator</literal></ulink>.The complete listing for
          <literal>ConditionalExponentialClassifier</literal>
          is:</para>

<programlisting>
class ConditionalExponentialClassifier(AbstractFeatureClassifier):

    def __init__(self, fdlist, labels, weights, **kwargs):
        self._weights = weights
        AbstractFeatureClassifier.__init__(self, fdlist, labels)

    def fvlist_likelihood(self, fvlist):
        prod = 1.0
        for (id, val) in fvlist.assignments():
            prod *= (self._weights[id] ** val)
        return prod

    def weights(self):
        return self._weights

    def set_weights(self, weights):
        self._weights = weights

    def __repr__(self):
        return ('&lt;ConditionalExponentialClassifier: %d labels, %d features&gt;' %
                (len(self._labels), len(self._fdlist)))
</programlisting>

        </section> <!-- Implementaiton -->

      </section> <!-- ConditionalExponentialClassifier -->

    </section> <!-- Conditional exponential model -->

    <section id="maxent.training" xref="Training Maximum Entropy Classifiers">
      <title> Training Maximum Entropy Classifiers</title>

      <para> The <literal>nltk.classifier.maxent</literal> module
      defines two <literal>ClassifierTrainer</literal>s that can be
      used to build maximum entropy classifiers: <ulink
      url="&refdoc;/nltk.classifier.maxent.GISMaxentClassifierTrainer.html">
      <literal>GISMaxentClassifierTrainer</literal></ulink> and <ulink
      url="&refdoc;/nltk.classifier.maxent.IISMaxentClassifierTrainer.html">
      <literal>IISMaxentClassifierTrainer</literal></ulink>.  Both of
      these <literal>ClassifierTrainer</literal>s use an iterative
      algorithm to successively refine a
      <literal>ConditionalExponentialClassifier</literal> until they
      converge on the maximum entropy classifier.  The algorithm
      employed by <literal>GISMaxentClasifierTrainer</literal> is
      known as <glossterm>generalized iterative scaling</glossterm>;
      and the algorithm used by
      <literal>IISMaxentClasifierTrainer</literal> is called
      <glossterm>improved iterative scaling</glossterm>. </para>

      <para> The next two sections give a brief introduction to each
      algorithm, and describe how to use their
      <literal>ClassifierTrainer</literal>s.  A complete explanation
      of these algorithms is beyond the scope of this tutorial; see
      <xref linkend="bibliography"></xref> for references to more
      complete descriptions.
      </para>

    </section> <!-- Training -->

    <section id="maxent.training.gis" xref="Generalized Iterative Scaling">
      <title> Generalized Iterative Scaling </title>

      <para> Generalized iterative scaling is a procedure to find the
      <literal>ConditionalExponentialModel</literal> weights that
      define the maximum entropy classifier for a given feature set
      and training corpus.  This procedure is guaranteed to converge
      on the correct weights. </para>

      <para> Generalized iterative scaling places three constraints on
      the feature set: </para>

      <itemizedlist>
        <listitem> <para> Feature values must be boolean (i.e., 0 or
        1). </para>
        </listitem>
        <listitem> <para> The feature value list for every
        <literal>LabeledText</literal> must have at least one feature
        with a value of 1. </para>
        </listitem>
        <listitem>
          <para> The sum of the feature values for each
          <literal>LabeledText</literal> must have the same value.  In
          other words, there must exist a single value C such that:
          </para> <para> &sum;<subscript>i</subscript> &fdilt; = C
          </para> <para> for every <literal>LabeledText</literal>
          &ltext;.  C is known as the <glossterm>correction
          constant</glossterm>correction>. </para>
        </listitem>
      </itemizedlist>

      <para> The second constraint can always be satisfied by adding a
      single feature detector that always returns a value of one.  The
      third constraint can always be satisfied by adding a
      "correction" feature that returns
      C-&sum;<subscript>i</subscript>&fdilt;.</para>

      <note> <para>In general, adding new features can change which
      classifier is the maximum entropy classifier for a given task.
      However, these two new features are completely dependant on the
      other features.  They add no new information, and therefore
      place no new constraints on the classifier.  As a result, they
      will not affect which classifier is the maximum entropy
      classifier. </para> </note>
      

      <para> Generalized iterative scaling begins by initializing each
      weight to 1.  It then iteratively updates the weights using the
      formula: </para>

      <itemizedlist>
        <listitem> <para> &wi; := &wi; &times;
        ((&sum;<subscript>&ltext;</subscript>
        freq<subscript>training</subscript>(&ltext;) &times; &fdilt;)
        / (&sum;<subscript>&ltext;</subscript>
        P<subscript>classifier</subscript>(&ltext;) &times; &fdilt;))
        ** (1/C) </para>
        </listitem>
      </itemizedlist>

      <para> Where: </para>
      
      <itemizedlist>
        <listitem><para> freq<subscript>training</subscript>(&ltext;)
        is the frequency of &ltext; in the training data.
        </para></listitem> <listitem><para>
        P<subscript>classifier</subscript>(&ltext;) is the
        classifier's estimated probability for &ltext;.</para>
        </listitem>
        <listitem> <para> C is the correction constant. </para>
        </listitem>
      </itemizedlist>

      <para> To avoid summing over all values of &ltext; (which would
      be very inefficient), we can make the following approximation:
      </para>

      <itemizedlist>
        <listitem><para> &sum;<subscript>&ltext;</subscript> 
        P<subscript>classifier</subscript>(&ltext;) &times; &fdilt; &ap;
        &sum;<subscript>&t;</subscript>
        &sum;<subscript>&l;</subscript> 
        freq<subscript>training</subscript>(&t;) &times; 
        P<subscript>classifier</subscript>(&l;|&t;) &times; &fdiltextlt; 
        </para></listitem>
      </itemizedlist>

      <remark> I make the claim that feature values must be boolean --
      is that true?  Or do they have to be integer?  nonnegative
      integer?  real?  nonnegative real?  The current implementation
      assumes binary features; but if the algorithm doesn't make that
      requirement, I could change the implementation to work with
      other feature types. </remark>

      <section id="maxent.training.gis.trainer" xref="GISMaxentClassifierTrainer">
        <title> GISMaxentClassifierTrainer </title>
          
        <para> The <literal>nltk.classifier.maxent</literal> module
        defines <ulink
        url="&refdoc;/nltk.classifier.maxent.GISMaxentClassifierTrainer.html">
        <literal>GISMaxentClassifierTrainer</literal></ulink>, a
        classifier trainer based on the generalized iterative scaling
        algorithm.  For a given training sets,
        <literal>GISMaxentClassifierTrainer</literal> creates a
        <literal>ConditionalExponentialModel</literal> that represents
        the maximum entropy classifier. </para>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.maxent.GISMaxentClassifierTrainer.html#__init__">
        <literal>GISMaxentClassifierTrainer</literal>
        constructor</ulink> takes a single
        <literal>FeatureDetectorList</literal>, which encodes the
        features that will be used by the classifiers it creates. </para>

<programlisting>
    &prompt; <command>classifier = GISMaxentClassifierTrainer(fdlist)</command>
    &lt;GISMaxentClassifierTrainer: 13318 features&gt;
 </programlisting>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.maxent.GISMaxentClassifierTrainer.html#train">
        <literal>train</literal></ulink> method is used to train a
        new classifier from a given set of training tokens: </para>

<programlisting>
    &prompt; <command>classifier.train(train_toks)</command>
    &lt;ConditionalExponentialClassifier: 72 labels, 13318 features&gt;
</programlisting>

        <para> The <literal>train</literal> method takes a number of
        optional keyword arguments: </para>

        <itemizedlist>
          <listitem> 
            <para> <literal>C</literal>: specifies the correction
            constant.  This constant must satisfy the constraint:
            </para>
            <itemizedlist>
              <listitem> <para> C &ge;
              &sum;<subscript>i</subscript>&fdilt;</para>
              </listitem>
            </itemizedlist>
            <para> for every <literal>LabeledText</literal> &ltext;.
            Lower values of <literal>C</literal> cause GIS to converge
            more quickly.  Thus, you should usually pick the lowest
            value that you are sure satisfies the above constraint.
            The default value of <literal>C</literal> is the length of
            the <literal>FeatureDetectorList</literal>, which is
            guaranteed to be safe for binary-valued features.
            However, this default value may result in very slow
            convergence. </para>
          </listitem>
          <listitem>
            <para> <literal>debug</literal>: specifies whether to
            output messages describing the trainer's progress.  Higher
            values result in more verbose output.  The default value
            is zero, which indicates that no messages should be
            produced. </para>
          </listitem>
          <listitem>
            <para> <literal>labels</literal>: specifies the set of
            possible labels.  If none is given, then the set of all
            labels attested in the training data will be used. </para>
          </listitem>
          <listitem>
            <para> <literal>iterations</literal>: specifies the
            maximum number of iterations for the GIS algorithm. </para>
          </listitem>
          <listitem>
            <para> <literal>accuracy_cutoff</literal>: specifies
            what accuracy value should be taken to indicate
            convergence.  If the accuracy becomes closer to one
            than the specified value, then GIS will terminate.  The
            default value is <literal>None</literal>, which indicates
            that no accuracy cutoff should be used. </para>
          </listitem>
          <listitem>
            <para> <literal>delta_accuracy_cutoff</literal>:
            specifies what change in accuracy should be taken to
            indicate convergence.  If the accuracy changes by
            less than this value in a single iteration, then GIS will
            terminate.  The default value is <literal>None</literal>,
            which indicates that no accuracy-change cutoff should be used. </para>
          </listitem>
          <listitem>
            <para> <literal>log_likelihood_cutoff</literal>: specifies
            what log-likelihood value should be taken to indicate
            convergence.  If the log-likelihod becomes closer to zero
            than the specified value, then GIS will terminate.  The
            default value is <literal>None</literal>, which indicates
            that no log-likelihood cutoff should be used. </para>
          </listitem>
          <listitem>
            <para> <literal>delta_log_likelihood_cutoff</literal>:
            specifies what change in log-likelihood should be taken to
            indicate convergence.  If the log-likelihood changes by
            less than this value in a single iteration, then GIS will
            terminate.  The default value is <literal>None</literal>,
            which indicates that no log-likelihood-change cutoff
            should be used. </para>
          </listitem>
        </itemizedlist>
        
      </section> <!-- GISMaxentClassifierTrainer -->

    </section> <!-- GIS -->

    <section id="maxent.training.iis" xref="Iterative Iterative Scaling">
      <title> Iterative Iterative Scaling </title>
      
      <para> Improved iterative scaling is a procedure to find the
      <literal>ConditionalExponentialModel</literal> weights that
      define the maximum entropy classifier for a given feature set
      and training corpus.  This procedure is guaranteed to converge
      on the correct weights.  It usually converges more quickly than
      generalized iterative scaling. </para>
      
      <para> Improved iterative scaling begins by initializing each
      weight to 1.  It then iteratively updates the weights using the
      formula: </para>

      <itemizedlist>
        <listitem>
          <para> &wi; := &wi; &times; e
          <superscript>&delta;<subscript>i</subscript></superscript></para>
        </listitem>
      </itemizedlist>

      <para> Where &delta;<subscript>i</subscript> is the solution to
      the equation: </para>

      <itemizedlist>
        <listitem>
          <para> &sum;<subscript>&ltext;</subscript>
          freq<subscript>training</subscript>(&ltext;) &times; &fdilt;
          = &sum; <subscript>&ltext;</subscript>
          P<subscript>classifier</subscript>(&ltext;) &times; &fdilt;
          &times; e <superscript> &delta; &times;
          nf(&ltext;)</superscript></para>
        </listitem>
      </itemizedlist>

      <para> To avoid summing over all values of &ltext; (which would
      be very inefficient), we can make the following approximation:
      </para>

      <itemizedlist>
        <listitem><para> &sum;<subscript>&ltext;</subscript>
        P<subscript>classifier</subscript>(&l;|&t;) &times; &fdilt;
        &times; e <superscript> &delta; &times;
        nf(&ltext;)</superscript> &ap; </para> <para>
        &sum;<subscript>&t;</subscript>
        &sum;<subscript>&l;</subscript>
        freq<subscript>training</subscript>(&t;) &times;
        P<subscript>classifier</subscript>(&l;|&t;) &times;
        &fdiltextlt; &times; e <superscript> &delta; &times;
        nf(&ltext;)</superscript> </para>
        </listitem>
      </itemizedlist>

      <para> For each iteration, &delta;<subscript>i</subscript> can
      be found using Newton's method. </para>

      <section id="maxent.training.iis.trainer" xref="IISMaxentClassifierTrainer">
        <title> IISMaxentClassifierTrainer </title>
        
        <para> The <literal>nltk.classifier.maxent</literal> module
        defines <ulink
        url="&refdoc;/nltk.classifier.maxent.IISMaxentClassifierTrainer.html">
        <literal>IISMaxentClassifierTrainer</literal></ulink>, a
        classifier trainer based on the improved iterative scaling
        algorithm.  For a given training sets,
        <literal>IISMaxentClassifierTrainer</literal> creates a
        <literal>ConditionalExponentialModel</literal> that represents
        the maximum entropy classifier. </para>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.maxent.IISMaxentClassifierTrainer.html#__init__">
        <literal>IISMaxentClassifierTrainer</literal>
        constructor</ulink> takes a single
        <literal>FeatureDetectorList</literal>, which encodes the
        features that will be used by the classifiers it creates. </para>

<programlisting>
    &prompt; <command>classifier = IISMaxentClassifierTrainer(fdlist)</command>
    &lt;IISMaxentClassifierTrainer: 13318 features&gt;
 </programlisting>

        <para> The <ulink
        url="&refdoc;/nltk.classifier.maxent.IISMaxentClassifierTrainer.html#train">
        <literal>train</literal></ulink> method is used to train a
        new classifier from a given set of training tokens: </para>

<programlisting>
    &prompt; <command>classifier.train(train_toks)</command>
    &lt;ConditionalExponentialClassifier: 72 labels, 13318 features&gt;
</programlisting>

        <para> The <literal>train</literal> method supports all of the
        optional keyword arguments that were described for
        <literal>GISMaxentClassifierTrainer</literal> (except
        <literal>C</literal>, since IIS does not use a correction
        constant). </para>

      </section> <!-- IISMaxentClassifierTrainer -->
      
    </section> <!-- IIS -->
    
  </chapter> <!-- Maxent Classifier -->

  <chapter id="fselection" xreflabel="Feature Selection"> 
    <title> Feature Selection </title>

    <para> Features are used to specify which information is relevant
    to a classification task.  Unfortunately, it's often difficult to
    decide which information is relevant.  One possible solution would
    be to include a large number of features, and let the
    classification algorithm decide which information is relevant. </para>

    <para> However, this approach has some serious disadvantages.  The
    number of features affects the classifier's speed.  Including a
    large number of features can result in long training and
    classification times.  An even more serious problem arises because
    the number of model parameters increases with the number of
    features.  If we use too many features, we may not be able to
    reliably estimate the correct values for all of these parameters.
    This can result in <glossterm>overfitting</glossterm>, where the
    classifier models the idiosyncrasies of the training set, rather
    than the class of texts that the training set represents.  In an
    extreme example of overfitting, the classifier might assign a
    probability of zero to any labeled text not included in the
    training data. </para>

    <para> A more promising approach is to begin with a large number
    of features, and use statistical techniques to decide which
    features are relevant.  A classifier can then be constructed from
    this reduced set of features.  This task of deciding which
    features are relevant for a classification task is known as
    <glossterm>feature selection</glossterm>. </para>

    <para> The <literal>nltk.classifier.featureselection</literal>
    module defines interfaces and classes for performing feature
    selection.  Currently, this module only provides the basic
    framework for performing feature selection, and a few simple
    feature selectors; it does not implement any advanced feature
    selection techniques.  However, we plan to add new feature
    detectors to this module in the near future. </para>

    <section id="fselection.featureselector" xreflabel="FeatureSelectorI"> 
      <title> FeatureSelectorI </title>

      <para> The <literal>nltk.classifier.featureselection</literal>
      module defines <ulink
      url="&refdoc;/nltk.classifier.featureselection.FeatureSelectorI.html">
      <literal>FeatureSelectorI</literal></ulink>, a general interface
      for performing feature selection.  It requires that classifiers
      define a single method, <ulink
      url="&refdoc;/nltk.classifier.featureselection.FeatureSelectorI.html#select">
      <literal>select</literal></ulink>, which takes a
      <literal>FeatureDetectorList</literal>, and decides which
      features are releveant for classification.  It returns a new
      <literal>FeatureDetectorList</literal> that includes the feature
      detectors for the relevant features. </para>

<programlisting>
    &prompt; <command>print fdlist</command>
    &lt;FeatureDetectorList with 26193 features&gt;
    &prompt; <command>selected_fdlist = feature_selector.select(fdlist)</command>
    &lt;FeatureDetectorList with 1827 features&gt;
</programlisting>

    </section> <!-- FeatureSelectorI -->

    <section id="fselection.selectedfdlist" xreflabel="SelectedFDList"> 
      <title> SelectedFDList </title>

      <para> Feature selectors decide which features are relevant for
      a particular classification task; and return a new
      <literal>FeatureDetectorList</literal> containing those
      features' detectors.  <ulink
      url="&refdoc;/nltk.classifier.featureselection.SelectedFDList.html">
      <literal>SelectedFDList</literal></ulink> provides feature
      selectors with a convenient way of building this new
      <literal>FeatureDetectorList</literal>.  </para>

      <para> A <literal>SelectedFDList</literal> consists of a subset
      of the feature detectors defined by a <glossterm>base
      <literal>FeatureDetectorList</literal></glossterm>.  The <ulink
      url="&refdoc;/nltk.classifier.featureselection.SelectedFDList.html#__init__">
      <literal>SelectedFDList</literal> constructor</ulink> takes a
      base <literal>FeatureDetectorList</literal> and a list of
      feature ids, specifying which features should be included:
      </para>

<programlisting>
    &prompt; <command>print fdlist</command>
    &lt;FeatureDetectorList with 26193 features&gt;

    <emphasis># Select every 50th feature. </emphasis>
    &prompt; <command>ids = range(0, 26193, 50)</command>
    [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, ...]
    &prompt; <command>selected_fdlist = SelectedFDList(fdlist, ids) </command>
    &lt;FeatureDetectorList with 524 features&gt;
</programlisting>

      <para> Internally, <literal>SelectedFDList</literal> maintains a
      dictionary mapping from old feature ids to new feature ids.
      When the <literal>SelectedFDList</literal> is applied to a text,
      it first applies its base <literal>FeatureDetectorList</literal>
      to the text.  Then, it uses its dictionary to transform the
      resulting <literal>FeatureValueList</literal>. </para>
      
    </section> <!-- SelectedFDList -->

    <section id="fselection.selectors" xreflabel="Feature Selectors"> 
      <title> Feature Selectors </title>

      <para> For information about the feature selectors currently
      available, see the reference documentation for the <ulink
      url="&refdoc;/nltk.classifier.featureselection.html">
      <literal>nltk.classifier.featureselection</literal>
      module</ulink>. </para>

    </section> <!-- Feature Selectors -->

  </chapter> <!-- Feature Selection -->

  <!-- I'm really not sure how you're supposed to use the bibliography -->
  <!-- docbook elements.  I tried looking up info about it on the web, -->
  <!-- but it didn't make a whole lot of sense to me.  So this is my best -->
  <!-- guess, partially driven by making sure that it produces reasonable -->
  <!-- output. -->
  <bibliography id="bibliography" xreflabel="The Bibliography">
    <title> Bibliography </title>

    <biblioentry>
      <authorgroup>
        <author>
          <firstname>Adam</firstname>
          <surname>Berger</surname>
        </author>
      </authorgroup>
      <title>The Improved Iterative Scaling Algorithm: A Gentle Introduction</title>
      <date> December, 1997</date>
      <citetitle><ulink url="http://citeseer.nj.nec.com/31826.html">http://citeseer.nj.nec.com/31826.html</ulink></citetitle>
    </biblioentry>

    <biblioentry>
      <authorgroup>
        <author>
          <firstname>Adam</firstname>
          <surname>Berger</surname>
        </author>
        <author>
          <firstname>Stephen</firstname>
          <surname>Della Pietra</surname>
        </author>
        <author>
          <firstname>Vincent</firstname>
          <surname>Della Pietra</surname>
        </author>
      </authorgroup>
      <title>A Maximum Entropy Approach to Natural Language Processing</title>
      <citetitle>Computational Linguistics</citetitle>
      <volumenum>22</volumenum>
      <issuenum>1</issuenum>
      <artpagenums>pp. 39-71</artpagenums>
      <date> 1996 </date>
      <citetitle><ulink url="http://citeseer.nj.nec.com/berger96maximum.html">http://citeseer.nj.nec.com/berger96maximum.html</ulink></citetitle>
    </biblioentry>

    <biblioentry>
      <authorgroup>
        <author>
          <firstname>Kamal</firstname>
          <surname>Nigam</surname>
        </author>
        <author>
          <firstname>John</firstname>
          <surname>Lafferty</surname>
        </author>
        <author>
          <firstname>Andrew</firstname>
          <surname>McCallum</surname>
        </author>
      </authorgroup>
      <title>Using Maximum Entropy for Text Classification</title>
      <citetitle>IJCAI-99 Workshop on Machine Learning for
        Information Filtering</citetitle>
      <artpagenums>pp. 61-67</artpagenums>
      <date> 1999 </date> 
      <citetitle><ulink url="http://citeseer.nj.nec.com/nigam99using.html">http://citeseer.nj.nec.com/nigam99using.html</ulink></citetitle>
    </biblioentry>

    <biblioentry>
      <authorgroup>
        <author>
          <firstname>Adwait</firstname>
          <surname>Ratnaparkhi</surname>
        </author>
      </authorgroup>
      <title>A Simple Introduction to Maximum Entropy Models for Natural Langauge Processing</title>
      <citetitle>Technical Report 97-08, Institute for Research in
      Cognitive Science, University of Pennsylvania</citetitle>
      <date> 1997 </date> 
      <citetitle><ulink url="http://citeseer.nj.nec.com/128751.html">http://citeseer.nj.nec.com/128751.html</ulink></citetitle>
    </biblioentry>

  </bibliography>
</book>

